{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Bodo \u00b6 Bodo is a new just-in-time (JIT) inferential compiler that brings supercomputing-style performance and scalability to native Python analytics code automatically. Bodo has several advantages over other big data analytics systems (which are usually distributed scheduler libraries): Simple programming with native Python APIs such as Pandas and Numpy (no \"Pandas-like\" API layers) Extreme performance and scalability using true parallelism and advanced compiler technology Very high reliability due to binary code generation, which avoids distributed library failures Simple deployment using standard Python workflows Flexible integration with other systems such as cloud storage, data warehouses, and visualization tools This documentation covers the basics of using Bodo and provides a reference of supported Python features and APIs. In a nutshell, Bodo provides a JIT compilation workflow using the @bodo.jit decorator . It replaces the decorated Python functions with an optimized and parallelized binary version automatically. For example, the program below can perform data transformation on large datasets: @bodo . jit def data_transform ( file_name ): df = pd . read_parquet ( file_name ) df = df [ df . C . dt . month == 1 ] df2 = df . groupby ( \"A\" )[ \"B\" , \"D\" ] . agg ( lambda S : ( S == \"ABC\" ) . sum () ) df2 . to_parquet ( \"output.pq\" ) To run Bodo programs such as this example, programmers can simply use the command line such as mpiexec -n 1024 python data_transform.py (to run on 1024 cores), or use Jupyter Notebook .","title":"About Bodo"},{"location":"#about-bodo","text":"Bodo is a new just-in-time (JIT) inferential compiler that brings supercomputing-style performance and scalability to native Python analytics code automatically. Bodo has several advantages over other big data analytics systems (which are usually distributed scheduler libraries): Simple programming with native Python APIs such as Pandas and Numpy (no \"Pandas-like\" API layers) Extreme performance and scalability using true parallelism and advanced compiler technology Very high reliability due to binary code generation, which avoids distributed library failures Simple deployment using standard Python workflows Flexible integration with other systems such as cloud storage, data warehouses, and visualization tools This documentation covers the basics of using Bodo and provides a reference of supported Python features and APIs. In a nutshell, Bodo provides a JIT compilation workflow using the @bodo.jit decorator . It replaces the decorated Python functions with an optimized and parallelized binary version automatically. For example, the program below can perform data transformation on large datasets: @bodo . jit def data_transform ( file_name ): df = pd . read_parquet ( file_name ) df = df [ df . C . dt . month == 1 ] df2 = df . groupby ( \"A\" )[ \"B\" , \"D\" ] . agg ( lambda S : ( S == \"ABC\" ) . sum () ) df2 . to_parquet ( \"output.pq\" ) To run Bodo programs such as this example, programmers can simply use the command line such as mpiexec -n 1024 python data_transform.py (to run on 1024 cores), or use Jupyter Notebook .","title":"About Bodo"},{"location":"file_io/","text":"Scalable Data I/O \u00b6 Efficient parallel data processing requires data I/O to be parallelized effectively as well. Bodo provides parallel file I/O for many different formats such as Parquet , CSV, JSON, Numpy binaries, HDF5 and SQL databases. This diagram demonstrates how chunks of data are partitioned among parallel execution engines by Bodo. Bodo automatically parallelizes I/O for any number of cores and cluster size without any additional API layers. Supported formats \u00b6 Currently, Bodo supports I/O for Parquet , CSV, SQL, JSON, HDF5 , and Numpy binaries formats. It can read these formats from multiple filesystems, including S3, HDFS and Azure Data Lake (ADLS) (see File Systems below for more information). Also see supported pandas Operations for supported arguments of I/O functions. Parquet \u00b6 Parquet is a commonly used file format in analytics due to its efficient columnar storage. Bodo supports the standard pandas API for reading Parquet: pd.read_parquet(path) , where path can be a parquet file or a directory with multiple parquet files (all are part of the same dataframe): import pandas as pd import bodo @bodo . jit def write_pq ( df ): df . to_parquet ( 'example.pq' ) @bodo . jit def read_pq (): df = pd . read_parquet ( 'example.pq' ) return df to_parquet(name) with distributed data writes to a folder called name . Each process writes one file into the folder, but if the data is not distributed, to_parquet(name) writes to a single file called name : df = pd . DataFrame ({ 'A' : range ( 10 )}) @bodo . jit def example1_pq ( df ): df . to_parquet ( 'example1.pq' ) @bodo . jit ( distributed = { 'df' }) def example2_pq ( df ): df . to_parquet ( 'example2.pq' ) if bodo . get_rank () == 0 : example1_pq ( df ) example2_pq ( df ) Run the code above with 4 processors: mpiexec -n 4 python example_pq.py example1_pq(df) writes 1 single file, and example2_pq(df) writes a folder containing 4 parquet files: . \u251c\u2500\u2500 example1.pq \u251c\u2500\u2500 example2.pq \u2502 \u251c\u2500\u2500 part-00.parquet \u2502 \u251c\u2500\u2500 part-01.parquet \u2502 \u251c\u2500\u2500 part-02.parquet \u2502 \u2514\u2500\u2500 part-03.parquet See read_parquet() , to_parquet() for supported arguments. Filter pushdown \u00b6 Filter Pushdown Bodo can detect filters used by the code and optimize the read_parquet call by pushing the filters down to the storage layer, so that only the rows required by the program are read. This can significantly speed up I/O in many cases and will reduce the program's memory footprint, sometimes substantially. For example, suppose we have a large dataset that spans many years and we only need to read data for a particular year. With pandas, we might perform a query on the year 2021 like this: @bodo . jit def query (): df = pd . read_parquet ( \"s3://my-bucket/data.pq\" ) df = df [( df [ \"year\" ] == 2021 )] return df . groupby ( \"customer_key\" )[ \"revenue\" ] . max () When compiling the above, Bodo detects the df[(df[\"year\"] == 2021)] filter and optimizes the read_parquet call so that it only reads data for year 2021 from S3. Because the data will have already been filtered after reading, Bodo removes the filter operation during compilation. Note that this requires code transformation and optimization and is something that pandas cannot do. Bodo automatically infers which filters can be pushed down. If your dataset is hive-partitioned and partition columns appear in filter expressions, only the files that contain relevant data are read, and the rest are discarded based on their path. For example, if year is a partition column above and we have a dataset: . \u2514\u2500\u2500 data.pq/ \u2502 ... \u251c\u2500\u2500\u2500year=2020/ \u2502 \u251c\u2500\u2500 part-00.parquet \u2502 \u2514\u2500\u2500 part-01.parquet \u2514\u2500\u2500\u2500year=2021/ \u251c\u2500\u2500 part-02.parquet \u2514\u2500\u2500 part-03.parquet Bodo will only read the files in the year=2021 directory. For non-partition columns, Bodo may discard files entirely just by looking at their parquet metadata (depending on the filters and statistics contained in the metadata) or filter the rows during read. Note Filter pushdown can be a very significant optimization. Please refer to the inlining section to make sure these optimizations are applied in your program. Exploring Large Data Without Full Read \u00b6 Exploring Large Data Without Full Read Exploring large datasets often requires seeing its shape and a sample of the data. Bodo is able to provide this information quickly without loading the full Parquet dataset, which means there is no need for a large cluster with a lot of memory. For example: @bodo . jit def head_only_read (): df = pd . read_parquet ( \"example.pq\" ) print ( df . shape ) print ( df . head ()) In this example, Bodo provides the shape information for the full dataset in df.shape , but only loads the first few rows that are necessary for df.head() . CSV \u00b6 CSV is a common text format for data exchange. Bodo supports most of the standard pandas API to read CSV files: import pandas as pd import bodo @bodo . jit def write_csv ( df ): df . to_csv ( 'example.csv' ) @bodo . jit def read_csv (): df = pd . read_csv ( 'example.csv' ) return df Unlike read_csv in regular pandas, Bodo can read a directory that contains multiple partitioned CSV files as well. All files in the folder must have the same number and datatype of columns. They can have different number of rows. Usage: @bodo . jit def read_csv_folder (): df = pd . read_csv ( \"/path/to/folder/foldername\" ) doSomething ( df ) Use sep=\"n\" to read text files line by line into a single-column dataframe (without creating separate columns, useful when text data is unstructured or there are too many columns to read efficiently): @bodo . jit def read_test (): df = pd . read_csv ( \"example.csv\" , sep = \"n\" , names = [ \"value\" ], dtype = { \"value\" : \"str\" }) return df Note Bodo uses nullable integer types of pandas to ensure type stability (see integer NA issue in pandas for more details). Therefore, data types must be specified explicitly for accurate performance comparisons of Bodo and pandas for read_csv . to_csv(name) has different behaviors for different file systems: POSIX file systems: always writes to a single file, regardless of the number of processes and whether the data is distributed, but writing is still done in parallel when more than 1 processor is used: df = pd . DataFrame ({ 'A' : np . arange ( n )}) @bodo . jit def example1_csv ( df ): df . to_csv ( 'example1.csv' ) @bodo . jit ( distributed = { 'df' }) def example2_csv ( df ): df . to_csv ( 'example2.csv' ) if bodo . get_rank () == 0 : example1_csv ( df ) example2_csv ( df ) Run the code above with 4 processors: mpiexec -n 4 python example_csv.py each example1_csv(df) and example2_csv(df) writes to a single file: . \u251c\u2500\u2500 example1.csv \u251c\u2500\u2500 example2.csv S3 and HDFS : distributed data is written to a folder called name . Each process writes one file into the folder, but if the data is not distributed, to_csv(name) writes to a single file called name : df = pd . DataFrame ({ 'A' : np . arange ( n )}) @bodo . jit def example1_csv ( df ): df . to_csv ( 's3://bucket-name/example1.csv' ) @bodo . jit ( distributed = { 'df' }) def example2_csv ( df ): df . to_csv ( 's3://bucket-name/example2.csv' ) if bodo . get_rank () == 0 : example1_csv ( df ) example2_csv ( df ) Run the code above with 4 processors: mpiexec -n 4 python example_csv.py example1_csv(df) writes 1 single file, and example2_csv(df) writes a folder containing 4 csv files: . \u251c\u2500\u2500 example1.csv \u251c\u2500\u2500 example2.csv \u2502 \u251c\u2500\u2500 part-00.csv \u2502 \u251c\u2500\u2500 part-01.csv \u2502 \u251c\u2500\u2500 part-02.csv \u2502 \u2514\u2500\u2500 part-03.csv See read_csv() , to_csv() for supported arguments. JSON \u00b6 For JSON, the syntax is also the same as pandas. Usage: @bodo . jit def example_write_json ( df , fname ): df . to_json ( fname ) @bodo . jit def example_read_json_lines_format (): df = pd . read_json ( 'example.json' , orient = 'records' , lines = True ) @bodo . jit def example_read_json_multi_lines (): df = pd . read_json ( 'example_file.json' , orient = 'records' , lines = False , dtype = { \"A\" : float , \"B\" : \"bool\" , \"C\" : int }) Note The dtype argument is required when reading a regular multi-line JSON file. Bodo cannot read a directory containing multiple multi-line JSON files to_json(name) has different behaviors for different file systems: POSIX file systems: to_json(name) behavior depends on orient and lines arguments. DataFrame.to_json(name, orient='records', lines=True) (i.e. writing JSON Lines text file format ) always writes to a single file, regardless of the number of processes and whether the data is distributed, but writing is still done in parallel when more than 1 processor is used: df = pd . DataFrame ({ 'A' : np . arange ( n )}) @bodo . jit def example1_json ( df ): df . to_json ( 'example1.json' , orient = 'records' , lines = True ) @bodo . jit ( distributed = { 'df' }) def example2_json ( df ): df . to_json ( 'example2.json' , orient = 'records' , lines = True ) if bodo . get_rank () == 0 : example1_json ( df ) example2_jsons ( df ) Run the code above with 4 processors: mpiexec -n 4 python example_json.py each example1_json(df) and example2_json(df) writes to a single file: . \u251c\u2500\u2500 example1.json \u251c\u2500\u2500 example2.json All other combinations of values for orient and lines have the same behavior as S3 and HDFS explained below. S3 and HDFS : distributed data is written to a folder called name . Each process writes one file into the folder, but if the data is not distributed, to_json(name) writes to a file called name : df = pd . DataFrame ({ 'A' : np . arange ( n )}) @bodo . jit def example1_json ( df ): df . to_json ( 's3://bucket-name/example1.json' ) @bodo . jit ( distributed = { 'df' }) def example2_json ( df ): df . to_json ( 's3://bucket-name/example2.json' ) if bodo . get_rank () == 0 : example1_json ( df ) example2_json ( df ) Run the code above with 4 processors: mpiexec -n 4 python example_json.py example1_json(df) writes 1 single file, and example2_json(df) writes a folder containing 4 json files: . \u251c\u2500\u2500 example1.json \u251c\u2500\u2500 example2.json \u2502 \u251c\u2500\u2500 part-00.json \u2502 \u251c\u2500\u2500 part-01.json \u2502 \u251c\u2500\u2500 part-02.json \u2502 \u2514\u2500\u2500 part-03.json See read_json()][pandas-f-in], [ to_json()` for supported arguments. SQL \u00b6 For SQL, the syntax is also the same as pandas. For reading: @bodo . jit def example_read_sql (): df = pd . read_sql ( 'select * from employees' , 'mysql+pymysql://admin:server' ) See read_sql() for supported arguments. For writing: @bodo . jit def example_write_sql ( df ): df . to_sql ( 'table_name' , 'mysql+pymysql://admin:server' ) See to_sql() for supported arguments. Note sqlalchemy must be installed in order to use pandas.read_sql . Delta Lake \u00b6 Reading parquet files from Delta Lake is supported locally, from S3, and from Azure ADLS. The Delta Lake binding python packaged needs to be installed using pip: pip install deltalake . For S3, the AWS_DEFAULT_REGION environment variable should be set to the region of the bucket hosting the Delta Lake table. For ADLS, the AZURE_STORAGE_ACCOUNT and AZURE_STORAGE_KEY environment variables need to be set. Example code for reading: @bodo . jit def example_read_deltalake (): df = pd . read_parquet ( 'path/to/deltalake' ) Note Writing is currently not supported. Numpy binaries \u00b6 Numpy's fromfile and tofile are supported as below: @bodo . jit def example_np_io (): A = np . fromfile ( \"myfile.dat\" , np . float64 ) ... A . tofile ( \"newfile.dat\" ) Bodo has the same behavior as Numpy for numpy.ndarray.tofile() , where we always write to a single file. However, writing distributed data to POSIX is done in parallel, but writing to S3 & HDFS is done sequentially (due to file system limitations). HDF5 \u00b6 HDF5 is a common format in scientific computing, especially for multi-dimensional numerical data. HDF5 can be very efficient at scale, since it has native parallel I/O support. For HDF5, the syntax is the same as the h5py package. For example: @bodo . jit def example_h5 (): f = h5py . File ( \"data.hdf5\" , \"r\" ) X = f [ 'points' ][:] Y = f [ 'responses' ][:] Filepaths determined at runtime \u00b6 When reading from a file, Bodo needs to know the types of the resulting dataframe. If the file name is a constant string or function argument, Bodo can look at the file at compile time and infer the types. If the the filepath is not constant, this information must be supplied by the user. For pd.read_csv and pd.read_excel , this information can be supplied through the names and dtypes keyword arguments: @bodo . jit def example_csv ( fname1 , fname2 , flag )): if flag : file_name = fname1 else : file_name = fname2 return pd . read_csv ( file_name , names = [ \"A\" , \"B\" , \"C\" ], dtype = { \"A\" : int , \"B\" : float , \"C\" : str }) @bodo . jit def example_excel ( fname1 , fname2 , flag )): if flag : file_name = fname1 else : file_name = fname2 return pd . read_excel ( file_name , names = [ \"A\" , \"B\" , \"C\" , \"D\" , \"E\" ], dtype = { \"A\" : int , \"B\" : float , \"C\" : str , \"D\" : str , \"E\" : np . bool_ }, ) For the remaining pandas read functions, the existing APIs do not currently allow this information to be supplied. Users can still provide this information by adding type information in the bodo.jit decorator, similar to Numba's typing syntax . For example: @bodo . jit ( locals = { 'df' :{ 'one' : bodo . float64 [:], 'two' : bodo . string_array_type , 'three' : bodo . bool_ [:], 'four' : bodo . float64 [:], 'five' : bodo . string_array_type , }}) def example_df_schema ( fname1 , fname2 , flag ): if flag : file_name = fname1 else : file_name = fname2 df = pd . read_parquet ( file_name ) return df @bodo . jit ( locals = { 'X' : bodo . float64 [:,:], 'Y' : bodo . float64 [:]}) def example_h5 ( fname1 , fname2 , flag ): if flag : file_name = fname1 else : file_name = fname2 f = h5py . File ( file_name , \"r\" ) X = f [ 'points' ][:] Y = f [ 'responses' ][:] For the complete list of supported types, please see the pandas dtype section . In the event that the dtypes are improperly specified, Bodo will throw a runtime error. File Systems \u00b6 Amazon S3 \u00b6 Reading and writing CSV , Parquet , JSON , and Numpy binary files from and to Amazon S3 is supported. The fsspec package must be available, and the file path should start with s3:// : @bodo . jit def example_s3_parquet (): df = pd . read_parquet ( 's3://bucket-name/file_name.parquet' ) These environment variables are used for File I/O with S3 credentials: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_DEFAULT_REGION : default as us-east-1 AWS_S3_ENDPOINT : specify custom host name, default as AWS endpoint( s3.amazonaws.com ) Connecting to S3 endpoints through a proxy is supported. The proxy URI can be provided by setting one of the following environment variables (listed in order of precedence): http_proxy https_proxy HTTP_PROXY HTTPS_PROXY Bodo uses Apache Arrow internally for read and write of data on S3. Google Cloud Storage \u00b6 Reading and writing Parquet files from and to Google Cloud is supported. The file path should start with gs:// or gcs:// : @bodo . jit def example_gcs_parquet (): df = pd . read_parquet ( 'gcs://bucket-name/file_name.parquet' ) These environment variables are used for File I/O with GCS credentials: GOOGLE_APPLICATION_CREDENTIALS Details for GOOGLE_APPLICATION_CREDENTIALS can be seen in the Google docs here . Bodo uses the fsspec-based gcsfs library internally for read and write of data on GCS. Hadoop Distributed File System (HDFS) and Azure Data Lake Storage (ADLS) Gen2 \u00b6 Reading and writing CSV , Parquet , JSON , and Numpy binary files from and to Hadoop Distributed File System (HDFS) is supported. Note that Azure Data Lake Storage Gen2 can be accessed through HDFS. The openjdk version 8 package must be available, and the file path should start with hdfs:// or abfs[s]:// : @bodo . jit def example_hdfs_parquet (): df = pd . read_parquet ( 'hdfs://host:port/dir/file_name.pq' ) These environment variables are used for File I/O with HDFS: HADOOP_HOME : the root of your installed Hadoop distribution. Often is lib/native/libhdfs.so . ARROW_LIBHDFS_DIR : location of libhdfs. Often is $HADOOP_HOME/lib/native . CLASSPATH : must contain the Hadoop jars. You can set these using: export CLASSPATH = ` $HADOOP_HOME /bin/hdfs classpath --glob ` Bodo uses Apache Arrow internally for read and write of data on HDFS. $HADOOP_HOME/etc/hadoop/hdfs-site.xml provides default behaviors for the HDFS client used by Bodo. Inconsistent configurations (e.g. dfs.replication ) could potentially cause errors in Bodo programs. Databases \u00b6 Currently, Bodo supports most RDBMS that work with SQLAlchemy, with a corresponding driver. Snowflake \u00b6 To read a dataframe from a Snowflake database, users can use pd.read_sql with their Snowflake username and password: pd.read_sql(query,snowflake://<username>:<password>@url) . Prerequisites \u00b6 In order to be able to query Snowflake from Bodo, you will have to install the Snowflake connector. If you're using Bodo in a conda environment: conda install -c conda-forge snowflake-connector-python If you've installed Bodo using pip, then you can install the Snowflake connector using pip as well: pip install snowflake-connector-python Usage \u00b6 Bodo requires the Snowflake connection string to be passed as an argument to the pd.read_sql function. The complete code looks as follows: import bodo import pandas as pd @bodo . jit ( distributed = [ \"df\" ]) def read_snowflake ( db_name , table_name ): df = pd . read_sql ( f \"SELECT * FROM { table_name } \" , f \"snowflake://user:password@url/ { db_name } /schema?warehouse=warehouse_name\" , ) return df df = read_snowflake ( db_name , temp_table_name ) We can use the pd.to_sql method to persist a dataframe to a Snowflake table: df . to_sql ( '<table_name>' , f \"snowflake://<username>:<password>@url/<db_name>/public?warehouse=XL_WH\" , schema = \"<schema>\" , if_exists = \"append\" , index = False ) Note index=False is required as Snowflake does not support indexes. if_exists=append is needed if the table already exists in snowflake.","title":"Scalable File I/O"},{"location":"file_io/#file_io","text":"Efficient parallel data processing requires data I/O to be parallelized effectively as well. Bodo provides parallel file I/O for many different formats such as Parquet , CSV, JSON, Numpy binaries, HDF5 and SQL databases. This diagram demonstrates how chunks of data are partitioned among parallel execution engines by Bodo. Bodo automatically parallelizes I/O for any number of cores and cluster size without any additional API layers.","title":"Scalable Data I/O"},{"location":"file_io/#supported-formats","text":"Currently, Bodo supports I/O for Parquet , CSV, SQL, JSON, HDF5 , and Numpy binaries formats. It can read these formats from multiple filesystems, including S3, HDFS and Azure Data Lake (ADLS) (see File Systems below for more information). Also see supported pandas Operations for supported arguments of I/O functions.","title":"Supported formats"},{"location":"file_io/#parquet-section","text":"Parquet is a commonly used file format in analytics due to its efficient columnar storage. Bodo supports the standard pandas API for reading Parquet: pd.read_parquet(path) , where path can be a parquet file or a directory with multiple parquet files (all are part of the same dataframe): import pandas as pd import bodo @bodo . jit def write_pq ( df ): df . to_parquet ( 'example.pq' ) @bodo . jit def read_pq (): df = pd . read_parquet ( 'example.pq' ) return df to_parquet(name) with distributed data writes to a folder called name . Each process writes one file into the folder, but if the data is not distributed, to_parquet(name) writes to a single file called name : df = pd . DataFrame ({ 'A' : range ( 10 )}) @bodo . jit def example1_pq ( df ): df . to_parquet ( 'example1.pq' ) @bodo . jit ( distributed = { 'df' }) def example2_pq ( df ): df . to_parquet ( 'example2.pq' ) if bodo . get_rank () == 0 : example1_pq ( df ) example2_pq ( df ) Run the code above with 4 processors: mpiexec -n 4 python example_pq.py example1_pq(df) writes 1 single file, and example2_pq(df) writes a folder containing 4 parquet files: . \u251c\u2500\u2500 example1.pq \u251c\u2500\u2500 example2.pq \u2502 \u251c\u2500\u2500 part-00.parquet \u2502 \u251c\u2500\u2500 part-01.parquet \u2502 \u251c\u2500\u2500 part-02.parquet \u2502 \u2514\u2500\u2500 part-03.parquet See read_parquet() , to_parquet() for supported arguments.","title":"Parquet"},{"location":"file_io/#filter-pushdown","text":"Filter Pushdown Bodo can detect filters used by the code and optimize the read_parquet call by pushing the filters down to the storage layer, so that only the rows required by the program are read. This can significantly speed up I/O in many cases and will reduce the program's memory footprint, sometimes substantially. For example, suppose we have a large dataset that spans many years and we only need to read data for a particular year. With pandas, we might perform a query on the year 2021 like this: @bodo . jit def query (): df = pd . read_parquet ( \"s3://my-bucket/data.pq\" ) df = df [( df [ \"year\" ] == 2021 )] return df . groupby ( \"customer_key\" )[ \"revenue\" ] . max () When compiling the above, Bodo detects the df[(df[\"year\"] == 2021)] filter and optimizes the read_parquet call so that it only reads data for year 2021 from S3. Because the data will have already been filtered after reading, Bodo removes the filter operation during compilation. Note that this requires code transformation and optimization and is something that pandas cannot do. Bodo automatically infers which filters can be pushed down. If your dataset is hive-partitioned and partition columns appear in filter expressions, only the files that contain relevant data are read, and the rest are discarded based on their path. For example, if year is a partition column above and we have a dataset: . \u2514\u2500\u2500 data.pq/ \u2502 ... \u251c\u2500\u2500\u2500year=2020/ \u2502 \u251c\u2500\u2500 part-00.parquet \u2502 \u2514\u2500\u2500 part-01.parquet \u2514\u2500\u2500\u2500year=2021/ \u251c\u2500\u2500 part-02.parquet \u2514\u2500\u2500 part-03.parquet Bodo will only read the files in the year=2021 directory. For non-partition columns, Bodo may discard files entirely just by looking at their parquet metadata (depending on the filters and statistics contained in the metadata) or filter the rows during read. Note Filter pushdown can be a very significant optimization. Please refer to the inlining section to make sure these optimizations are applied in your program.","title":"Filter pushdown"},{"location":"file_io/#exploring-large-data-without-full-read","text":"Exploring Large Data Without Full Read Exploring large datasets often requires seeing its shape and a sample of the data. Bodo is able to provide this information quickly without loading the full Parquet dataset, which means there is no need for a large cluster with a lot of memory. For example: @bodo . jit def head_only_read (): df = pd . read_parquet ( \"example.pq\" ) print ( df . shape ) print ( df . head ()) In this example, Bodo provides the shape information for the full dataset in df.shape , but only loads the first few rows that are necessary for df.head() .","title":"Exploring Large Data Without Full Read"},{"location":"file_io/#csv-section","text":"CSV is a common text format for data exchange. Bodo supports most of the standard pandas API to read CSV files: import pandas as pd import bodo @bodo . jit def write_csv ( df ): df . to_csv ( 'example.csv' ) @bodo . jit def read_csv (): df = pd . read_csv ( 'example.csv' ) return df Unlike read_csv in regular pandas, Bodo can read a directory that contains multiple partitioned CSV files as well. All files in the folder must have the same number and datatype of columns. They can have different number of rows. Usage: @bodo . jit def read_csv_folder (): df = pd . read_csv ( \"/path/to/folder/foldername\" ) doSomething ( df ) Use sep=\"n\" to read text files line by line into a single-column dataframe (without creating separate columns, useful when text data is unstructured or there are too many columns to read efficiently): @bodo . jit def read_test (): df = pd . read_csv ( \"example.csv\" , sep = \"n\" , names = [ \"value\" ], dtype = { \"value\" : \"str\" }) return df Note Bodo uses nullable integer types of pandas to ensure type stability (see integer NA issue in pandas for more details). Therefore, data types must be specified explicitly for accurate performance comparisons of Bodo and pandas for read_csv . to_csv(name) has different behaviors for different file systems: POSIX file systems: always writes to a single file, regardless of the number of processes and whether the data is distributed, but writing is still done in parallel when more than 1 processor is used: df = pd . DataFrame ({ 'A' : np . arange ( n )}) @bodo . jit def example1_csv ( df ): df . to_csv ( 'example1.csv' ) @bodo . jit ( distributed = { 'df' }) def example2_csv ( df ): df . to_csv ( 'example2.csv' ) if bodo . get_rank () == 0 : example1_csv ( df ) example2_csv ( df ) Run the code above with 4 processors: mpiexec -n 4 python example_csv.py each example1_csv(df) and example2_csv(df) writes to a single file: . \u251c\u2500\u2500 example1.csv \u251c\u2500\u2500 example2.csv S3 and HDFS : distributed data is written to a folder called name . Each process writes one file into the folder, but if the data is not distributed, to_csv(name) writes to a single file called name : df = pd . DataFrame ({ 'A' : np . arange ( n )}) @bodo . jit def example1_csv ( df ): df . to_csv ( 's3://bucket-name/example1.csv' ) @bodo . jit ( distributed = { 'df' }) def example2_csv ( df ): df . to_csv ( 's3://bucket-name/example2.csv' ) if bodo . get_rank () == 0 : example1_csv ( df ) example2_csv ( df ) Run the code above with 4 processors: mpiexec -n 4 python example_csv.py example1_csv(df) writes 1 single file, and example2_csv(df) writes a folder containing 4 csv files: . \u251c\u2500\u2500 example1.csv \u251c\u2500\u2500 example2.csv \u2502 \u251c\u2500\u2500 part-00.csv \u2502 \u251c\u2500\u2500 part-01.csv \u2502 \u251c\u2500\u2500 part-02.csv \u2502 \u2514\u2500\u2500 part-03.csv See read_csv() , to_csv() for supported arguments.","title":"CSV"},{"location":"file_io/#json-section","text":"For JSON, the syntax is also the same as pandas. Usage: @bodo . jit def example_write_json ( df , fname ): df . to_json ( fname ) @bodo . jit def example_read_json_lines_format (): df = pd . read_json ( 'example.json' , orient = 'records' , lines = True ) @bodo . jit def example_read_json_multi_lines (): df = pd . read_json ( 'example_file.json' , orient = 'records' , lines = False , dtype = { \"A\" : float , \"B\" : \"bool\" , \"C\" : int }) Note The dtype argument is required when reading a regular multi-line JSON file. Bodo cannot read a directory containing multiple multi-line JSON files to_json(name) has different behaviors for different file systems: POSIX file systems: to_json(name) behavior depends on orient and lines arguments. DataFrame.to_json(name, orient='records', lines=True) (i.e. writing JSON Lines text file format ) always writes to a single file, regardless of the number of processes and whether the data is distributed, but writing is still done in parallel when more than 1 processor is used: df = pd . DataFrame ({ 'A' : np . arange ( n )}) @bodo . jit def example1_json ( df ): df . to_json ( 'example1.json' , orient = 'records' , lines = True ) @bodo . jit ( distributed = { 'df' }) def example2_json ( df ): df . to_json ( 'example2.json' , orient = 'records' , lines = True ) if bodo . get_rank () == 0 : example1_json ( df ) example2_jsons ( df ) Run the code above with 4 processors: mpiexec -n 4 python example_json.py each example1_json(df) and example2_json(df) writes to a single file: . \u251c\u2500\u2500 example1.json \u251c\u2500\u2500 example2.json All other combinations of values for orient and lines have the same behavior as S3 and HDFS explained below. S3 and HDFS : distributed data is written to a folder called name . Each process writes one file into the folder, but if the data is not distributed, to_json(name) writes to a file called name : df = pd . DataFrame ({ 'A' : np . arange ( n )}) @bodo . jit def example1_json ( df ): df . to_json ( 's3://bucket-name/example1.json' ) @bodo . jit ( distributed = { 'df' }) def example2_json ( df ): df . to_json ( 's3://bucket-name/example2.json' ) if bodo . get_rank () == 0 : example1_json ( df ) example2_json ( df ) Run the code above with 4 processors: mpiexec -n 4 python example_json.py example1_json(df) writes 1 single file, and example2_json(df) writes a folder containing 4 json files: . \u251c\u2500\u2500 example1.json \u251c\u2500\u2500 example2.json \u2502 \u251c\u2500\u2500 part-00.json \u2502 \u251c\u2500\u2500 part-01.json \u2502 \u251c\u2500\u2500 part-02.json \u2502 \u2514\u2500\u2500 part-03.json See read_json()][pandas-f-in], [ to_json()` for supported arguments.","title":"JSON"},{"location":"file_io/#sql-section","text":"For SQL, the syntax is also the same as pandas. For reading: @bodo . jit def example_read_sql (): df = pd . read_sql ( 'select * from employees' , 'mysql+pymysql://admin:server' ) See read_sql() for supported arguments. For writing: @bodo . jit def example_write_sql ( df ): df . to_sql ( 'table_name' , 'mysql+pymysql://admin:server' ) See to_sql() for supported arguments. Note sqlalchemy must be installed in order to use pandas.read_sql .","title":"SQL"},{"location":"file_io/#deltalake-section","text":"Reading parquet files from Delta Lake is supported locally, from S3, and from Azure ADLS. The Delta Lake binding python packaged needs to be installed using pip: pip install deltalake . For S3, the AWS_DEFAULT_REGION environment variable should be set to the region of the bucket hosting the Delta Lake table. For ADLS, the AZURE_STORAGE_ACCOUNT and AZURE_STORAGE_KEY environment variables need to be set. Example code for reading: @bodo . jit def example_read_deltalake (): df = pd . read_parquet ( 'path/to/deltalake' ) Note Writing is currently not supported.","title":"Delta Lake"},{"location":"file_io/#numpy-binary-section","text":"Numpy's fromfile and tofile are supported as below: @bodo . jit def example_np_io (): A = np . fromfile ( \"myfile.dat\" , np . float64 ) ... A . tofile ( \"newfile.dat\" ) Bodo has the same behavior as Numpy for numpy.ndarray.tofile() , where we always write to a single file. However, writing distributed data to POSIX is done in parallel, but writing to S3 & HDFS is done sequentially (due to file system limitations).","title":"Numpy binaries"},{"location":"file_io/#hdf5","text":"HDF5 is a common format in scientific computing, especially for multi-dimensional numerical data. HDF5 can be very efficient at scale, since it has native parallel I/O support. For HDF5, the syntax is the same as the h5py package. For example: @bodo . jit def example_h5 (): f = h5py . File ( \"data.hdf5\" , \"r\" ) X = f [ 'points' ][:] Y = f [ 'responses' ][:]","title":"HDF5"},{"location":"file_io/#non-constant-filepaths","text":"When reading from a file, Bodo needs to know the types of the resulting dataframe. If the file name is a constant string or function argument, Bodo can look at the file at compile time and infer the types. If the the filepath is not constant, this information must be supplied by the user. For pd.read_csv and pd.read_excel , this information can be supplied through the names and dtypes keyword arguments: @bodo . jit def example_csv ( fname1 , fname2 , flag )): if flag : file_name = fname1 else : file_name = fname2 return pd . read_csv ( file_name , names = [ \"A\" , \"B\" , \"C\" ], dtype = { \"A\" : int , \"B\" : float , \"C\" : str }) @bodo . jit def example_excel ( fname1 , fname2 , flag )): if flag : file_name = fname1 else : file_name = fname2 return pd . read_excel ( file_name , names = [ \"A\" , \"B\" , \"C\" , \"D\" , \"E\" ], dtype = { \"A\" : int , \"B\" : float , \"C\" : str , \"D\" : str , \"E\" : np . bool_ }, ) For the remaining pandas read functions, the existing APIs do not currently allow this information to be supplied. Users can still provide this information by adding type information in the bodo.jit decorator, similar to Numba's typing syntax . For example: @bodo . jit ( locals = { 'df' :{ 'one' : bodo . float64 [:], 'two' : bodo . string_array_type , 'three' : bodo . bool_ [:], 'four' : bodo . float64 [:], 'five' : bodo . string_array_type , }}) def example_df_schema ( fname1 , fname2 , flag ): if flag : file_name = fname1 else : file_name = fname2 df = pd . read_parquet ( file_name ) return df @bodo . jit ( locals = { 'X' : bodo . float64 [:,:], 'Y' : bodo . float64 [:]}) def example_h5 ( fname1 , fname2 , flag ): if flag : file_name = fname1 else : file_name = fname2 f = h5py . File ( file_name , \"r\" ) X = f [ 'points' ][:] Y = f [ 'responses' ][:] For the complete list of supported types, please see the pandas dtype section . In the event that the dtypes are improperly specified, Bodo will throw a runtime error.","title":"Filepaths determined at runtime"},{"location":"file_io/#File","text":"","title":"File Systems"},{"location":"file_io/#S3","text":"Reading and writing CSV , Parquet , JSON , and Numpy binary files from and to Amazon S3 is supported. The fsspec package must be available, and the file path should start with s3:// : @bodo . jit def example_s3_parquet (): df = pd . read_parquet ( 's3://bucket-name/file_name.parquet' ) These environment variables are used for File I/O with S3 credentials: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_DEFAULT_REGION : default as us-east-1 AWS_S3_ENDPOINT : specify custom host name, default as AWS endpoint( s3.amazonaws.com ) Connecting to S3 endpoints through a proxy is supported. The proxy URI can be provided by setting one of the following environment variables (listed in order of precedence): http_proxy https_proxy HTTP_PROXY HTTPS_PROXY Bodo uses Apache Arrow internally for read and write of data on S3.","title":"Amazon S3"},{"location":"file_io/#GCS","text":"Reading and writing Parquet files from and to Google Cloud is supported. The file path should start with gs:// or gcs:// : @bodo . jit def example_gcs_parquet (): df = pd . read_parquet ( 'gcs://bucket-name/file_name.parquet' ) These environment variables are used for File I/O with GCS credentials: GOOGLE_APPLICATION_CREDENTIALS Details for GOOGLE_APPLICATION_CREDENTIALS can be seen in the Google docs here . Bodo uses the fsspec-based gcsfs library internally for read and write of data on GCS.","title":"Google Cloud Storage"},{"location":"file_io/#HDFS","text":"Reading and writing CSV , Parquet , JSON , and Numpy binary files from and to Hadoop Distributed File System (HDFS) is supported. Note that Azure Data Lake Storage Gen2 can be accessed through HDFS. The openjdk version 8 package must be available, and the file path should start with hdfs:// or abfs[s]:// : @bodo . jit def example_hdfs_parquet (): df = pd . read_parquet ( 'hdfs://host:port/dir/file_name.pq' ) These environment variables are used for File I/O with HDFS: HADOOP_HOME : the root of your installed Hadoop distribution. Often is lib/native/libhdfs.so . ARROW_LIBHDFS_DIR : location of libhdfs. Often is $HADOOP_HOME/lib/native . CLASSPATH : must contain the Hadoop jars. You can set these using: export CLASSPATH = ` $HADOOP_HOME /bin/hdfs classpath --glob ` Bodo uses Apache Arrow internally for read and write of data on HDFS. $HADOOP_HOME/etc/hadoop/hdfs-site.xml provides default behaviors for the HDFS client used by Bodo. Inconsistent configurations (e.g. dfs.replication ) could potentially cause errors in Bodo programs.","title":"Hadoop Distributed File System (HDFS) and Azure Data Lake Storage (ADLS) Gen2"},{"location":"file_io/#db","text":"Currently, Bodo supports most RDBMS that work with SQLAlchemy, with a corresponding driver.","title":"Databases"},{"location":"file_io/#snowflake-section","text":"To read a dataframe from a Snowflake database, users can use pd.read_sql with their Snowflake username and password: pd.read_sql(query,snowflake://<username>:<password>@url) .","title":"Snowflake"},{"location":"file_io/#prerequisites","text":"In order to be able to query Snowflake from Bodo, you will have to install the Snowflake connector. If you're using Bodo in a conda environment: conda install -c conda-forge snowflake-connector-python If you've installed Bodo using pip, then you can install the Snowflake connector using pip as well: pip install snowflake-connector-python","title":"Prerequisites"},{"location":"file_io/#usage","text":"Bodo requires the Snowflake connection string to be passed as an argument to the pd.read_sql function. The complete code looks as follows: import bodo import pandas as pd @bodo . jit ( distributed = [ \"df\" ]) def read_snowflake ( db_name , table_name ): df = pd . read_sql ( f \"SELECT * FROM { table_name } \" , f \"snowflake://user:password@url/ { db_name } /schema?warehouse=warehouse_name\" , ) return df df = read_snowflake ( db_name , temp_table_name ) We can use the pd.to_sql method to persist a dataframe to a Snowflake table: df . to_sql ( '<table_name>' , f \"snowflake://<username>:<password>@url/<db_name>/public?warehouse=XL_WH\" , schema = \"<schema>\" , if_exists = \"append\" , index = False ) Note index=False is required as Snowflake does not support indexes. if_exists=append is needed if the table already exists in snowflake.","title":"Usage"},{"location":"objmode/","text":"Using Regular Python inside JIT (Object Mode) \u00b6 Regular Python functions and Bodo JIT functions can be used together in applications arbitrarily, but there are cases where regular Python code needs to be used inside JIT code. For example, you may want to use Bodo's parallel constructs with some code that does not have JIT support yet. Object Mode allows switching to a Python interpreted context to be able to run non-jittable code. The main requirement is that the user has to specify the type of variables used in later JIT code. For example, the following code calls a non-JIT function on rows of a distributed dataframe. The code inside with bodo.objmode runs as regular Python, but variable y is returned to JIT code (since it is used after the with block). Therefore, the y=\"float64\" type annotation is required. import pandas as pd import numpy as np import bodo import scipy.special as sc def my_non_jit_function ( a , b ): return np . log ( a ) + sc . entr ( b ) @bodo . jit def f ( row ): with bodo . objmode ( y = \"float64\" ): y = my_non_jit_function ( row . A , row . B ) return y @bodo . jit def objmode_example ( n ): df = pd . DataFrame ({ \"A\" : np . random . ranf ( n ), \"B\" : np . arange ( n )}) df [ \"C\" ] = df . apply ( f , axis = 1 ) print ( df [ \"C\" ] . sum ()) objmode_example ( 10 ) We recommend keeping the code inside the with bodo.objmode block minimal and call outside Python functions instead (as in this example). This reduces compilation time and sidesteps potential compiler limitations. Object Mode Type Annotations \u00b6 There are various ways to specify the data types in objmode . Basic data types such as float64 and int64 can be specified as string values (as in the previous example). For more complex data types like dataframes, bodo.typeof() can be used on sample data that has the same type as expected outputs. For example: df_sample = pd . DataFrame ({ \"A\" : [ 0 ], \"B\" : [ \"AB\" ]}, index = [ 0 ]) df_type = bodo . typeof ( df_sample ) @bodo . jit def f (): with bodo . objmode ( df = df_type ): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"ab\" , \"bc\" , \"cd\" ]}, index = [ 3 , 2 , 1 ]) return df This is equivalent to creating the DataFrameType directly: @bodo . jit def f (): with bodo . objmode ( df = bodo . DataFrameType ( ( bodo . int64 [:: 1 ], bodo . string_array_type ), bodo . NumericIndexType ( bodo . int64 ), ( \"A\" , \"B\" ), ) ): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"ab\" , \"bc\" , \"cd\" ]}, index = [ 3 , 2 , 1 ]) return df The data type can be registered in Bodo so it can be referenced using a string name later: df_sample = pd . DataFrame ({ \"A\" : [ 0 ], \"B\" : [ \"AB\" ]}, index = [ 0 ]) bodo . register_type ( \"my_df_type\" , bodo . typeof ( df_sample )) @bodo . jit def f (): with bodo . objmode ( df = \"my_df_type\" ): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"ab\" , \"bc\" , \"cd\" ]}, index = [ 3 , 2 , 1 ]) return df See pandas datatypes for more details on Bodo data types in general. Bodo's Object Mode is built on top of Numba's Object Mode (see Numba objmode for more details). What Can Be Done Inside Object Mode \u00b6 The code inside Object Mode runs in regular Python on all parallel processes, which means Object Mode does not include Bodo compiler's automatic parallel communication management. Therefore, the computation inside Object Mode should be independent on different processors and not require communication. In general: Operations on scalars are safe Operations that compute on rows independently are safe Operations that compute across rows may not be safe The example below demonstrates a valid use of Object Mode, since it uses df.apply(axis=1) which runs on different rows independently. df_type = bodo . typeof ( pd . DataFrame ({ \"A\" : [ 1 ], \"B\" : [ 1 ], \"C\" : [ 1 ]})) def f ( df ): return df . assign ( C = df . apply ( lambda r : r . A + r . B , axis = 1 )) @bodo . jit def valid_objmode (): df = pd . read_parquet ( \"in_file.pq\" ) with bodo . objmode ( df2 = df_type ): df2 = f ( df ) df2 . to_parquet ( \"out_file.pq\" ) valid_objmode () In contrast, the example below demonstrates an invalid use of Object Mode. The reason is that groupby computation requires grouping together all rows with the same key across all chunks. However, on each processor, Bodo passes a chunk of df to Object Mode which returns results from local groupby computation. Therefore, df2 does not include valid global groupby output. df_type = bodo . typeof ( pd . DataFrame ({ \"A\" : [ 1 ], \"B\" : [ 1 ]})) def f ( df ): return df . groupby ( \"A\" , as_index = False ) . sum () @bodo . jit def invalid_objmode (): df = pd . read_parquet ( \"in_file.pq\" ) # Invalid use of objmode with bodo . objmode ( df2 = df_type ): df2 = f ( df ) df2 . to_parquet ( \"out_file.pq\" ) invalid_objmode () Groupby/Apply Object Mode Pattern \u00b6 ML algorithms and other complex data science computations are often called on groups of dataframe rows. Bodo supports parallelizing these computations (which may not have JIT support yet) using Object Mode inside groupby/apply . For example, the code below runs Prophet on groups of rows. This is a valid use of Object Mode since Bodo handles shuffle communication for groupby/apply and brings all rows of each group in the same local chunk. Therefore, the apply function running in Object Mode has all the data it needs. import bodo import pandas as pd import numpy as np from fbprophet import Prophet prophet_output_type = bodo . typeof ( pd . DataFrame ({ \"ds\" : pd . date_range ( \"2017-01-03\" , periods = 1 ), \"yhat\" : [ 0.0 ]})) def run_prophet ( df ): m = Prophet () m . fit ( df ) return m . predict ( df )[[ \"ds\" , \"yhat\" ]] @bodo . jit def apply_func ( df ): with bodo . objmode ( df2 = prophet_output_type ): df2 = run_prophet ( df ) return df2 @bodo . jit def f ( df ): df2 = df . groupby ( \"A\" ) . apply ( apply_func ) return df2 n = 10 df = pd . DataFrame ({ \"A\" : np . arange ( n ) % 3 , \"ds\" : pd . date_range ( \"2017-01-03\" , periods = n ), \"y\" : np . arange ( n )}) print ( f ( df ))","title":"Using Regular Python inside Bodo (Object Mode)"},{"location":"objmode/#objmode","text":"Regular Python functions and Bodo JIT functions can be used together in applications arbitrarily, but there are cases where regular Python code needs to be used inside JIT code. For example, you may want to use Bodo's parallel constructs with some code that does not have JIT support yet. Object Mode allows switching to a Python interpreted context to be able to run non-jittable code. The main requirement is that the user has to specify the type of variables used in later JIT code. For example, the following code calls a non-JIT function on rows of a distributed dataframe. The code inside with bodo.objmode runs as regular Python, but variable y is returned to JIT code (since it is used after the with block). Therefore, the y=\"float64\" type annotation is required. import pandas as pd import numpy as np import bodo import scipy.special as sc def my_non_jit_function ( a , b ): return np . log ( a ) + sc . entr ( b ) @bodo . jit def f ( row ): with bodo . objmode ( y = \"float64\" ): y = my_non_jit_function ( row . A , row . B ) return y @bodo . jit def objmode_example ( n ): df = pd . DataFrame ({ \"A\" : np . random . ranf ( n ), \"B\" : np . arange ( n )}) df [ \"C\" ] = df . apply ( f , axis = 1 ) print ( df [ \"C\" ] . sum ()) objmode_example ( 10 ) We recommend keeping the code inside the with bodo.objmode block minimal and call outside Python functions instead (as in this example). This reduces compilation time and sidesteps potential compiler limitations.","title":"Using Regular Python inside JIT (Object Mode)"},{"location":"objmode/#object-mode-type-annotations","text":"There are various ways to specify the data types in objmode . Basic data types such as float64 and int64 can be specified as string values (as in the previous example). For more complex data types like dataframes, bodo.typeof() can be used on sample data that has the same type as expected outputs. For example: df_sample = pd . DataFrame ({ \"A\" : [ 0 ], \"B\" : [ \"AB\" ]}, index = [ 0 ]) df_type = bodo . typeof ( df_sample ) @bodo . jit def f (): with bodo . objmode ( df = df_type ): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"ab\" , \"bc\" , \"cd\" ]}, index = [ 3 , 2 , 1 ]) return df This is equivalent to creating the DataFrameType directly: @bodo . jit def f (): with bodo . objmode ( df = bodo . DataFrameType ( ( bodo . int64 [:: 1 ], bodo . string_array_type ), bodo . NumericIndexType ( bodo . int64 ), ( \"A\" , \"B\" ), ) ): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"ab\" , \"bc\" , \"cd\" ]}, index = [ 3 , 2 , 1 ]) return df The data type can be registered in Bodo so it can be referenced using a string name later: df_sample = pd . DataFrame ({ \"A\" : [ 0 ], \"B\" : [ \"AB\" ]}, index = [ 0 ]) bodo . register_type ( \"my_df_type\" , bodo . typeof ( df_sample )) @bodo . jit def f (): with bodo . objmode ( df = \"my_df_type\" ): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"ab\" , \"bc\" , \"cd\" ]}, index = [ 3 , 2 , 1 ]) return df See pandas datatypes for more details on Bodo data types in general. Bodo's Object Mode is built on top of Numba's Object Mode (see Numba objmode for more details).","title":"Object Mode Type Annotations"},{"location":"objmode/#what-can-be-done-inside-object-mode","text":"The code inside Object Mode runs in regular Python on all parallel processes, which means Object Mode does not include Bodo compiler's automatic parallel communication management. Therefore, the computation inside Object Mode should be independent on different processors and not require communication. In general: Operations on scalars are safe Operations that compute on rows independently are safe Operations that compute across rows may not be safe The example below demonstrates a valid use of Object Mode, since it uses df.apply(axis=1) which runs on different rows independently. df_type = bodo . typeof ( pd . DataFrame ({ \"A\" : [ 1 ], \"B\" : [ 1 ], \"C\" : [ 1 ]})) def f ( df ): return df . assign ( C = df . apply ( lambda r : r . A + r . B , axis = 1 )) @bodo . jit def valid_objmode (): df = pd . read_parquet ( \"in_file.pq\" ) with bodo . objmode ( df2 = df_type ): df2 = f ( df ) df2 . to_parquet ( \"out_file.pq\" ) valid_objmode () In contrast, the example below demonstrates an invalid use of Object Mode. The reason is that groupby computation requires grouping together all rows with the same key across all chunks. However, on each processor, Bodo passes a chunk of df to Object Mode which returns results from local groupby computation. Therefore, df2 does not include valid global groupby output. df_type = bodo . typeof ( pd . DataFrame ({ \"A\" : [ 1 ], \"B\" : [ 1 ]})) def f ( df ): return df . groupby ( \"A\" , as_index = False ) . sum () @bodo . jit def invalid_objmode (): df = pd . read_parquet ( \"in_file.pq\" ) # Invalid use of objmode with bodo . objmode ( df2 = df_type ): df2 = f ( df ) df2 . to_parquet ( \"out_file.pq\" ) invalid_objmode ()","title":"What Can Be Done Inside Object Mode"},{"location":"objmode/#groupbyapply-object-mode-pattern","text":"ML algorithms and other complex data science computations are often called on groups of dataframe rows. Bodo supports parallelizing these computations (which may not have JIT support yet) using Object Mode inside groupby/apply . For example, the code below runs Prophet on groups of rows. This is a valid use of Object Mode since Bodo handles shuffle communication for groupby/apply and brings all rows of each group in the same local chunk. Therefore, the apply function running in Object Mode has all the data it needs. import bodo import pandas as pd import numpy as np from fbprophet import Prophet prophet_output_type = bodo . typeof ( pd . DataFrame ({ \"ds\" : pd . date_range ( \"2017-01-03\" , periods = 1 ), \"yhat\" : [ 0.0 ]})) def run_prophet ( df ): m = Prophet () m . fit ( df ) return m . predict ( df )[[ \"ds\" , \"yhat\" ]] @bodo . jit def apply_func ( df ): with bodo . objmode ( df2 = prophet_output_type ): df2 = run_prophet ( df ) return df2 @bodo . jit def f ( df ): df2 = df . groupby ( \"A\" ) . apply ( apply_func ) return df2 n = 10 df = pd . DataFrame ({ \"A\" : np . arange ( n ) % 3 , \"ds\" : pd . date_range ( \"2017-01-03\" , periods = n ), \"y\" : np . arange ( n )}) print ( f ( df ))","title":"Groupby/Apply Object Mode Pattern"},{"location":"quick_start/","text":"Quick Start \u00b6 This section provides a quick start guide to Bodo and explains its important concepts briefly. We strongly recommend reading this page before using Bodo. Installation \u00b6 Bodo can be installed using Conda : conda create -n Bodo python=3.9 -c conda-forge conda activate Bodo conda install bodo -c bodo.ai -c conda-forge This command installs Bodo Community Edition by default, which is free and works on up to 8 cores. You can also request a 30 day free trial on up to 128 cores. If you need a trial license for even more cores, please contact us . See the installation section for more details of setting up Bodo. Data Transform Example with Bodo \u00b6 We use a simple data transformation example to discuss some of the key Bodo concepts. Generate data \u00b6 Let's generate some example data and write to a Parquet file: import pandas as pd import numpy as np # 10m data points df = pd . DataFrame ( { \"A\" : np . repeat ( pd . date_range ( \"2013-01-03\" , periods = 1000 ), 10_000 ), \"B\" : np . arange ( 10_000_000 ), } ) # set some values to NA df . iloc [ np . arange ( 1000 ) * 3 , 0 ] = pd . NA # using row_group_size helps with efficient parallel read of data later df . to_parquet ( \"pd_example.pq\" , row_group_size = 100_000 ) Save this code in gen_data.py and run in command line: python gen_data.py Example Pandas Code \u00b6 Here is a simple data transformation code in Pandas that processes a column of datetime values and creates two new columns: import pandas as pd import time def data_transform (): t0 = time . time () df = pd . read_parquet ( \"pd_example.pq\" ) df [ \"B\" ] = df . apply ( lambda r : \"NA\" if pd . isna ( r . A ) else \"P1\" if r . A . month < 5 else \"P2\" , axis = 1 ) df [ \"C\" ] = df . A . dt . month df . to_parquet ( \"pandas_output.pq\" ) print ( \"Total time: {:.2f} \" . format ( time . time () - t0 )) if __name__ == \"__main__\" : data_transform () Save this code in data_transform.py and run in command line: $ python data_transform.py Total time: 166.18 Standard Python is quite slow for these data transforms since: The use of custom code inside apply() does not let Pandas run an optimized prebuilt C library in its backend. Therefore, the Python interpreter overheads dominate. Python uses a single CPU core and does not parallelize computation. Bodo solves both of these problems as we demonstrate below. Using the Bodo JIT Decorator \u00b6 Bodo optimizes and parallelizes data workloads by providing just-in-time (JIT) compilation. This code is identical to the original Pandas code, except that it annotates the data_transform function with the bodo.jit decorator: import pandas as pd import time import bodo @bodo . jit def data_transform (): t0 = time . time () df = pd . read_parquet ( \"pd_example.pq\" ) df [ \"B\" ] = df . apply ( lambda r : \"NA\" if pd . isna ( r . A ) else \"P1\" if r . A . month < 5 else \"P2\" , axis = 1 ) df [ \"C\" ] = df . A . dt . month df . to_parquet ( \"bodo_output.pq\" ) print ( \"Total time: {:.2f} \" . format ( time . time () - t0 )) if __name__ == \"__main__\" : data_transform () Save this code in bodo_data_transform.py and run on a single core from command line: $ python bodo_data_transform.py Total time: 1.78 This code is 94x faster with Bodo than Pandas even on a single core, because Bodo compiles the function into a native binary, eliminating the interpreter overheads in apply . Now let's run the code on 8 CPU cores using mpiexec in command line: $ mpiexec -n 8 python bodo_data_transform.py Total time: 0.38 Using 8 cores gets an additional ~5x speedup. The same program can be scaled to larger datasets and as many cores as necessary in compute clusters and cloud environments (e.g. mpiexec -n 10000 python bodo_data_transform.py ). See the section on bodo parallelism basics for more details about Bodo's JIT compilation workflow and parallel computation model. Compilation Time and Caching \u00b6 Bodo's JIT workflow compiles the function the first time it is called, but reuses the compiled version for subsequent calls. In the previous code, we added timers inside the function to avoid measuring compilation time. Let's move the timers outside and call the function twice: import pandas as pd import time import bodo @bodo . jit def data_transform (): df = pd . read_parquet ( \"pd_example.pq\" ) df [ \"B\" ] = df . apply ( lambda r : \"NA\" if pd . isna ( r . A ) else \"P1\" if r . A . month < 5 else \"P2\" , axis = 1 ) df [ \"C\" ] = df . A . dt . month df . to_parquet ( \"bodo_output.pq\" ) if __name__ == \"__main__\" : t0 = time . time () data_transform () print ( \"Total time first call: {:.2f} \" . format ( time . time () - t0 )) t0 = time . time () data_transform () print ( \"Total time second call: {:.2f} \" . format ( time . time () - t0 )) Save this code in data_transform2.py and run in command line: $ python data_transform2.py Total time first call: 4.72 Total time second call: 1.92 The first call is slower due to compilation of the function, but the second call reuses the compiled version and runs faster. Compilation time can be avoided across program runs by using the cache=True flag: import pandas as pd import time import bodo @bodo . jit ( cache = True ) def data_transform (): df = pd . read_parquet ( \"pd_example.pq\" ) df [ \"B\" ] = df . apply ( lambda r : \"NA\" if pd . isna ( r . A ) else \"P1\" if r . A . month < 5 else \"P2\" , axis = 1 ) df [ \"C\" ] = df . A . dt . month df . to_parquet ( \"bodo_output.pq\" ) if __name__ == \"__main__\" : t0 = time . time () data_transform () print ( \"Total time: {:.2f} \" . format ( time . time () - t0 )) Save this code in data_transform_cache.py and run in command line twice: $ python data_transform_cache.py Total time: 4.70 $ python data_transform_cache.py Total time: 1.96 In this case, Bodo saves the compiled version of the function to a file and reuses it in the second run since the code has not changed. We plan to make caching default in the future. See caching for more information. Parallel Python Processes \u00b6 Bodo uses the MPI parallelism model, which runs the full program on all cores from the beginning. Essentially, mpiexec launches identical Python processes but Bodo divides the data and computation in JIT functions to exploit parallelism. Let's try a simple example that demonstrates how chunks of data are loaded in parallel: import pandas as pd import bodo def load_data_pandas (): df = pd . read_parquet ( \"pd_example.pq\" ) print ( \"pandas dataframe: \" , df ) @bodo . jit def load_data_bodo (): df = pd . read_parquet ( \"pd_example.pq\" ) print ( \"Bodo dataframe: \" , df ) if __name__ == \"__main__\" : load_data_pandas () load_data_bodo () Save this code in load_data.py and run on two cores (output prints of the cores are mixed): Click to expand output $ mpiexec -n 2 python load_data.py pandas dataframe: A B 0 NaT 0 1 2013-01-03 1 2 2013-01-03 2 3 NaT 3 4 2013-01-03 4 ... ... ... 9999995 2015-09-29 9999995 9999996 2015-09-29 9999996 9999997 2015-09-29 9999997 9999998 2015-09-29 9999998 9999999 2015-09-29 9999999 [10000000 rows x 2 columns] pandas dataframe: A B 0 NaT 0 1 2013-01-03 1 2 2013-01-03 2 3 NaT 3 4 2013-01-03 4 ... ... ... 9999995 2015-09-29 9999995 9999996 2015-09-29 9999996 9999997 2015-09-29 9999997 9999998 2015-09-29 9999998 9999999 2015-09-29 9999999 [10000000 rows x 2 columns] Bodo dataframe: A B 0 1970-01-01 0 1 2013-01-03 1 2 2013-01-03 2 3 2013-01-03 3 4 2013-01-03 4 ... ... ... 4999995 2014-05-17 4999995 4999996 2014-05-17 4999996 4999997 2014-05-17 4999997 4999998 2014-05-17 4999998 4999999 2014-05-17 4999999 [5000000 rows x 2 columns] pandas dataframe: A B 5000000 2014-05-18 5000000 5000001 2014-05-18 5000001 5000002 2014-05-18 5000002 5000003 2014-05-18 5000003 5000004 2014-05-18 5000004 ... ... ... 9999995 2015-09-29 9999995 9999996 2015-09-29 9999996 9999997 2015-09-29 9999997 9999998 2015-09-29 9999998 9999999 2015-09-29 9999999 [5000000 rows x 2 columns] The first two dataframes printed are regular Pandas dataframes which are replicated on both processes and have all 10 million rows. However, the last two dataframes printed are Bodo parallelized Pandas dataframes, with 5 million rows each. In this case, Bodo parallelizes read_parquet automatically and loads different chunks of data in different cores. Therefore, the non-JIT parts of the Python program are replicated across cores whereas Bodo JIT functions are parallelized. Parallel Computation \u00b6 Bodo automatically divides computation and manages communication across cores as this example demonstrates: import pandas as pd import bodo @bodo . jit def data_groupby (): df = pd . read_parquet ( \"pd_example.pq\" ) df2 = df . groupby ( \"A\" , as_index = False ) . sum () df2 . to_parquet ( \"bodo_output.pq\" ) if __name__ == \"__main__\" : data_groupby () Save this code as data_groupby.py and run from command line: $ mpiexec -n 8 python data_groupby.py This program uses groupby which requires rows with the same key to be aggregated together. Therefore, Bodo shuffles the data automatically under the hoods using MPI, and the user doesn't need to worry about parallelism challenges like communication. Bodo JIT Requirements \u00b6 Bodo JIT supports specific APIs in Pandas currently, and other APIs cannot be used inside JIT functions. For example: import pandas as pd import bodo @bodo . jit def df_unsupported (): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]) df2 = df . transpose () return df2 if __name__ == \"__main__\" : df_unsupported () Save this code as df_unsupported.py and run from command line: $ python df_unsupported.py # bodo.utils.typing.BodoError: Dataframe.transpose not supported yet As the error indicates, Bodo doesn't currently support the transpose call in JIT functions. In these cases, an alternative API should be used or this portion of the code should be in regular Python with Bodo's object mode . See supported Pandas API for the complete list of supported Pandas operations. Type Stability \u00b6 The main requirement of JIT compilation is being able to infer data types for all variables and values. In Bodo, column names are part of dataframe data types, so Bodo tries to infer column name related inputs in all operations. For example, key names in groupby are used to determine the output data type and need to be known to Bodo: import pandas as pd import bodo @bodo . jit def groupby_keys ( extra_keys ): df = pd . read_parquet ( \"pd_example.pq\" ) keys = [ c for c in df . columns if c not in [ \"B\" , \"C\" ]] if extra_keys : keys . append ( \"B\" ) df2 = df . groupby ( keys ) . sum () print ( df2 ) if __name__ == \"__main__\" : groupby_keys ( False ) Save this code as groupby_keys.py and run from command line: $ python groupby_keys.py # bodo.utils.typing.BodoError: groupby(): argument 'by' requires a constant value but variable 'keys' is updated inplace using 'append' In this case, the list of groupby keys is determined using the runtime value of extra_keys in a way that Bodo is not able to infer it from the program during compilation time. The alternative is to compute the keys in a separate JIT function to make it easier for Bodo to infer: import pandas as pd import bodo @bodo . jit def get_keys ( df_columns , extra_keys ): keys = [ c for c in df_columns if c not in [ \"B\" , \"C\" ]] if extra_keys : keys . append ( \"B\" ) return keys @bodo . jit def groupby_keys ( extra_keys ): df = pd . read_parquet ( \"pd_example.pq\" ) keys = get_keys ( df . columns , extra_keys ) df2 = df . groupby ( keys ) . sum () print ( df2 ) if __name__ == \"__main__\" : keys = get_keys () groupby_keys ( False ) This program works since get_keys can be evaluated in compile time. It only uses df.columns and extra_keys values that can be constant at compile time, and does not use non-deterministic features like I/O. Python Features \u00b6 Bodo uses Numba for compiling regular Python features and some of Numba's requirements apply to Bodo as well. For example, values in data structures like lists should have the same data type. This example fails since list values are either integers or strings: import bodo @bodo . jit def create_list (): out = [] out . append ( 0 ) out . append ( \"A\" ) out . append ( 1 ) out . append ( \"B\" ) return out if __name__ == \"__main__\" : create_list () Using tuples can often solve these problems since tuples can hold values of different types: import bodo @bodo . jit def create_list (): out = [] out . append (( 0 , \"A\" )) out . append (( 1 , \"B\" )) return out if __name__ == \"__main__\" : create_list () See our Unsupported Python Programs section for more details. Using Bodo in Jupyter Notebooks \u00b6 See Interactive Bodo Cluster Setup using IPyParallel for more information.","title":"Quick Start"},{"location":"quick_start/#quick-start","text":"This section provides a quick start guide to Bodo and explains its important concepts briefly. We strongly recommend reading this page before using Bodo.","title":"Quick Start"},{"location":"quick_start/#installation","text":"Bodo can be installed using Conda : conda create -n Bodo python=3.9 -c conda-forge conda activate Bodo conda install bodo -c bodo.ai -c conda-forge This command installs Bodo Community Edition by default, which is free and works on up to 8 cores. You can also request a 30 day free trial on up to 128 cores. If you need a trial license for even more cores, please contact us . See the installation section for more details of setting up Bodo.","title":"Installation"},{"location":"quick_start/#data-transform-example-with-bodo","text":"We use a simple data transformation example to discuss some of the key Bodo concepts.","title":"Data Transform Example with Bodo"},{"location":"quick_start/#generate-data","text":"Let's generate some example data and write to a Parquet file: import pandas as pd import numpy as np # 10m data points df = pd . DataFrame ( { \"A\" : np . repeat ( pd . date_range ( \"2013-01-03\" , periods = 1000 ), 10_000 ), \"B\" : np . arange ( 10_000_000 ), } ) # set some values to NA df . iloc [ np . arange ( 1000 ) * 3 , 0 ] = pd . NA # using row_group_size helps with efficient parallel read of data later df . to_parquet ( \"pd_example.pq\" , row_group_size = 100_000 ) Save this code in gen_data.py and run in command line: python gen_data.py","title":"Generate data"},{"location":"quick_start/#example_code_in_pandas","text":"Here is a simple data transformation code in Pandas that processes a column of datetime values and creates two new columns: import pandas as pd import time def data_transform (): t0 = time . time () df = pd . read_parquet ( \"pd_example.pq\" ) df [ \"B\" ] = df . apply ( lambda r : \"NA\" if pd . isna ( r . A ) else \"P1\" if r . A . month < 5 else \"P2\" , axis = 1 ) df [ \"C\" ] = df . A . dt . month df . to_parquet ( \"pandas_output.pq\" ) print ( \"Total time: {:.2f} \" . format ( time . time () - t0 )) if __name__ == \"__main__\" : data_transform () Save this code in data_transform.py and run in command line: $ python data_transform.py Total time: 166.18 Standard Python is quite slow for these data transforms since: The use of custom code inside apply() does not let Pandas run an optimized prebuilt C library in its backend. Therefore, the Python interpreter overheads dominate. Python uses a single CPU core and does not parallelize computation. Bodo solves both of these problems as we demonstrate below.","title":"Example Pandas Code"},{"location":"quick_start/#using-the-bodo-jit-decorator","text":"Bodo optimizes and parallelizes data workloads by providing just-in-time (JIT) compilation. This code is identical to the original Pandas code, except that it annotates the data_transform function with the bodo.jit decorator: import pandas as pd import time import bodo @bodo . jit def data_transform (): t0 = time . time () df = pd . read_parquet ( \"pd_example.pq\" ) df [ \"B\" ] = df . apply ( lambda r : \"NA\" if pd . isna ( r . A ) else \"P1\" if r . A . month < 5 else \"P2\" , axis = 1 ) df [ \"C\" ] = df . A . dt . month df . to_parquet ( \"bodo_output.pq\" ) print ( \"Total time: {:.2f} \" . format ( time . time () - t0 )) if __name__ == \"__main__\" : data_transform () Save this code in bodo_data_transform.py and run on a single core from command line: $ python bodo_data_transform.py Total time: 1.78 This code is 94x faster with Bodo than Pandas even on a single core, because Bodo compiles the function into a native binary, eliminating the interpreter overheads in apply . Now let's run the code on 8 CPU cores using mpiexec in command line: $ mpiexec -n 8 python bodo_data_transform.py Total time: 0.38 Using 8 cores gets an additional ~5x speedup. The same program can be scaled to larger datasets and as many cores as necessary in compute clusters and cloud environments (e.g. mpiexec -n 10000 python bodo_data_transform.py ). See the section on bodo parallelism basics for more details about Bodo's JIT compilation workflow and parallel computation model.","title":"Using the Bodo JIT Decorator"},{"location":"quick_start/#compilation-time-and-caching","text":"Bodo's JIT workflow compiles the function the first time it is called, but reuses the compiled version for subsequent calls. In the previous code, we added timers inside the function to avoid measuring compilation time. Let's move the timers outside and call the function twice: import pandas as pd import time import bodo @bodo . jit def data_transform (): df = pd . read_parquet ( \"pd_example.pq\" ) df [ \"B\" ] = df . apply ( lambda r : \"NA\" if pd . isna ( r . A ) else \"P1\" if r . A . month < 5 else \"P2\" , axis = 1 ) df [ \"C\" ] = df . A . dt . month df . to_parquet ( \"bodo_output.pq\" ) if __name__ == \"__main__\" : t0 = time . time () data_transform () print ( \"Total time first call: {:.2f} \" . format ( time . time () - t0 )) t0 = time . time () data_transform () print ( \"Total time second call: {:.2f} \" . format ( time . time () - t0 )) Save this code in data_transform2.py and run in command line: $ python data_transform2.py Total time first call: 4.72 Total time second call: 1.92 The first call is slower due to compilation of the function, but the second call reuses the compiled version and runs faster. Compilation time can be avoided across program runs by using the cache=True flag: import pandas as pd import time import bodo @bodo . jit ( cache = True ) def data_transform (): df = pd . read_parquet ( \"pd_example.pq\" ) df [ \"B\" ] = df . apply ( lambda r : \"NA\" if pd . isna ( r . A ) else \"P1\" if r . A . month < 5 else \"P2\" , axis = 1 ) df [ \"C\" ] = df . A . dt . month df . to_parquet ( \"bodo_output.pq\" ) if __name__ == \"__main__\" : t0 = time . time () data_transform () print ( \"Total time: {:.2f} \" . format ( time . time () - t0 )) Save this code in data_transform_cache.py and run in command line twice: $ python data_transform_cache.py Total time: 4.70 $ python data_transform_cache.py Total time: 1.96 In this case, Bodo saves the compiled version of the function to a file and reuses it in the second run since the code has not changed. We plan to make caching default in the future. See caching for more information.","title":"Compilation Time and Caching"},{"location":"quick_start/#parallel-python-processes","text":"Bodo uses the MPI parallelism model, which runs the full program on all cores from the beginning. Essentially, mpiexec launches identical Python processes but Bodo divides the data and computation in JIT functions to exploit parallelism. Let's try a simple example that demonstrates how chunks of data are loaded in parallel: import pandas as pd import bodo def load_data_pandas (): df = pd . read_parquet ( \"pd_example.pq\" ) print ( \"pandas dataframe: \" , df ) @bodo . jit def load_data_bodo (): df = pd . read_parquet ( \"pd_example.pq\" ) print ( \"Bodo dataframe: \" , df ) if __name__ == \"__main__\" : load_data_pandas () load_data_bodo () Save this code in load_data.py and run on two cores (output prints of the cores are mixed): Click to expand output $ mpiexec -n 2 python load_data.py pandas dataframe: A B 0 NaT 0 1 2013-01-03 1 2 2013-01-03 2 3 NaT 3 4 2013-01-03 4 ... ... ... 9999995 2015-09-29 9999995 9999996 2015-09-29 9999996 9999997 2015-09-29 9999997 9999998 2015-09-29 9999998 9999999 2015-09-29 9999999 [10000000 rows x 2 columns] pandas dataframe: A B 0 NaT 0 1 2013-01-03 1 2 2013-01-03 2 3 NaT 3 4 2013-01-03 4 ... ... ... 9999995 2015-09-29 9999995 9999996 2015-09-29 9999996 9999997 2015-09-29 9999997 9999998 2015-09-29 9999998 9999999 2015-09-29 9999999 [10000000 rows x 2 columns] Bodo dataframe: A B 0 1970-01-01 0 1 2013-01-03 1 2 2013-01-03 2 3 2013-01-03 3 4 2013-01-03 4 ... ... ... 4999995 2014-05-17 4999995 4999996 2014-05-17 4999996 4999997 2014-05-17 4999997 4999998 2014-05-17 4999998 4999999 2014-05-17 4999999 [5000000 rows x 2 columns] pandas dataframe: A B 5000000 2014-05-18 5000000 5000001 2014-05-18 5000001 5000002 2014-05-18 5000002 5000003 2014-05-18 5000003 5000004 2014-05-18 5000004 ... ... ... 9999995 2015-09-29 9999995 9999996 2015-09-29 9999996 9999997 2015-09-29 9999997 9999998 2015-09-29 9999998 9999999 2015-09-29 9999999 [5000000 rows x 2 columns] The first two dataframes printed are regular Pandas dataframes which are replicated on both processes and have all 10 million rows. However, the last two dataframes printed are Bodo parallelized Pandas dataframes, with 5 million rows each. In this case, Bodo parallelizes read_parquet automatically and loads different chunks of data in different cores. Therefore, the non-JIT parts of the Python program are replicated across cores whereas Bodo JIT functions are parallelized.","title":"Parallel Python Processes"},{"location":"quick_start/#parallel-computation","text":"Bodo automatically divides computation and manages communication across cores as this example demonstrates: import pandas as pd import bodo @bodo . jit def data_groupby (): df = pd . read_parquet ( \"pd_example.pq\" ) df2 = df . groupby ( \"A\" , as_index = False ) . sum () df2 . to_parquet ( \"bodo_output.pq\" ) if __name__ == \"__main__\" : data_groupby () Save this code as data_groupby.py and run from command line: $ mpiexec -n 8 python data_groupby.py This program uses groupby which requires rows with the same key to be aggregated together. Therefore, Bodo shuffles the data automatically under the hoods using MPI, and the user doesn't need to worry about parallelism challenges like communication.","title":"Parallel Computation"},{"location":"quick_start/#bodo-jit-requirements","text":"Bodo JIT supports specific APIs in Pandas currently, and other APIs cannot be used inside JIT functions. For example: import pandas as pd import bodo @bodo . jit def df_unsupported (): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]) df2 = df . transpose () return df2 if __name__ == \"__main__\" : df_unsupported () Save this code as df_unsupported.py and run from command line: $ python df_unsupported.py # bodo.utils.typing.BodoError: Dataframe.transpose not supported yet As the error indicates, Bodo doesn't currently support the transpose call in JIT functions. In these cases, an alternative API should be used or this portion of the code should be in regular Python with Bodo's object mode . See supported Pandas API for the complete list of supported Pandas operations.","title":"Bodo JIT Requirements"},{"location":"quick_start/#type-stability","text":"The main requirement of JIT compilation is being able to infer data types for all variables and values. In Bodo, column names are part of dataframe data types, so Bodo tries to infer column name related inputs in all operations. For example, key names in groupby are used to determine the output data type and need to be known to Bodo: import pandas as pd import bodo @bodo . jit def groupby_keys ( extra_keys ): df = pd . read_parquet ( \"pd_example.pq\" ) keys = [ c for c in df . columns if c not in [ \"B\" , \"C\" ]] if extra_keys : keys . append ( \"B\" ) df2 = df . groupby ( keys ) . sum () print ( df2 ) if __name__ == \"__main__\" : groupby_keys ( False ) Save this code as groupby_keys.py and run from command line: $ python groupby_keys.py # bodo.utils.typing.BodoError: groupby(): argument 'by' requires a constant value but variable 'keys' is updated inplace using 'append' In this case, the list of groupby keys is determined using the runtime value of extra_keys in a way that Bodo is not able to infer it from the program during compilation time. The alternative is to compute the keys in a separate JIT function to make it easier for Bodo to infer: import pandas as pd import bodo @bodo . jit def get_keys ( df_columns , extra_keys ): keys = [ c for c in df_columns if c not in [ \"B\" , \"C\" ]] if extra_keys : keys . append ( \"B\" ) return keys @bodo . jit def groupby_keys ( extra_keys ): df = pd . read_parquet ( \"pd_example.pq\" ) keys = get_keys ( df . columns , extra_keys ) df2 = df . groupby ( keys ) . sum () print ( df2 ) if __name__ == \"__main__\" : keys = get_keys () groupby_keys ( False ) This program works since get_keys can be evaluated in compile time. It only uses df.columns and extra_keys values that can be constant at compile time, and does not use non-deterministic features like I/O.","title":"Type Stability"},{"location":"quick_start/#python-features","text":"Bodo uses Numba for compiling regular Python features and some of Numba's requirements apply to Bodo as well. For example, values in data structures like lists should have the same data type. This example fails since list values are either integers or strings: import bodo @bodo . jit def create_list (): out = [] out . append ( 0 ) out . append ( \"A\" ) out . append ( 1 ) out . append ( \"B\" ) return out if __name__ == \"__main__\" : create_list () Using tuples can often solve these problems since tuples can hold values of different types: import bodo @bodo . jit def create_list (): out = [] out . append (( 0 , \"A\" )) out . append (( 1 , \"B\" )) return out if __name__ == \"__main__\" : create_list () See our Unsupported Python Programs section for more details.","title":"Python Features"},{"location":"quick_start/#jupyter","text":"See Interactive Bodo Cluster Setup using IPyParallel for more information.","title":"Using Bodo in Jupyter Notebooks"},{"location":"api_docs/","text":"API Reference \u00b6 Pandas Numpy User Defined Functions (UDFs) Machine Learning Miscellaneous Supported Python API","title":"Index"},{"location":"api_docs/#apireference","text":"Pandas Numpy User Defined Functions (UDFs) Machine Learning Miscellaneous Supported Python API","title":"API Reference"},{"location":"api_docs/bodo_parallel_apis/","text":"Bodo Parallel APIs \u00b6 bodo.allgatherv \u00b6 bodo. allgatherv (data, warn_if_rep=True) gather data from all ranks and send to all, effectively replicating the data: Arguments data : data to gather. warn_if_rep : prints a BodoWarning if data to gather is replicated. Example Usage import bodo import pandas as pd @bodo . jit def mean_power (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) return bodo . allgatherv ( df ) df = mean_power () print ( df ) Save code in test_allgatherv.py file and run with mpiexec . mpiexec -n 4 python test_allgatherv.py Output: [stdout:0] Unnamed: 0 altitude cadence ... power speed time 0 0 185.800003 51 ... 45 3.459 2016-10-20 22:01:26 1 1 185.800003 68 ... 0 3.710 2016-10-20 22:01:27 2 2 186.399994 38 ... 42 3.874 2016-10-20 22:01:28 3 3 186.800003 38 ... 5 4.135 2016-10-20 22:01:29 4 4 186.600006 38 ... 1 4.250 2016-10-20 22:01:30 ... ... ... ... ... ... ... ... 3897 1127 178.199997 0 ... 0 3.497 2016-10-20 23:14:31 3898 1128 178.199997 0 ... 0 3.289 2016-10-20 23:14:32 3899 1129 178.199997 0 ... 0 2.969 2016-10-20 23:14:33 3900 1130 178.399994 0 ... 0 2.969 2016-10-20 23:14:34 3901 1131 178.399994 0 ... 0 2.853 2016-10-20 23:14:35 [3902 rows x 10 columns] [stdout:1] Unnamed: 0 altitude cadence ... power speed time 0 0 185.800003 51 ... 45 3.459 2016-10-20 22:01:26 1 1 185.800003 68 ... 0 3.710 2016-10-20 22:01:27 2 2 186.399994 38 ... 42 3.874 2016-10-20 22:01:28 3 3 186.800003 38 ... 5 4.135 2016-10-20 22:01:29 4 4 186.600006 38 ... 1 4.250 2016-10-20 22:01:30 ... ... ... ... ... ... ... ... 3897 1127 178.199997 0 ... 0 3.497 2016-10-20 23:14:31 3898 1128 178.199997 0 ... 0 3.289 2016-10-20 23:14:32 3899 1129 178.199997 0 ... 0 2.969 2016-10-20 23:14:33 3900 1130 178.399994 0 ... 0 2.969 2016-10-20 23:14:34 3901 1131 178.399994 0 ... 0 2.853 2016-10-20 23:14:35 [3902 rows x 10 columns] [stdout:2] Unnamed: 0 altitude cadence ... power speed time 0 0 185.800003 51 ... 45 3.459 2016-10-20 22:01:26 1 1 185.800003 68 ... 0 3.710 2016-10-20 22:01:27 2 2 186.399994 38 ... 42 3.874 2016-10-20 22:01:28 3 3 186.800003 38 ... 5 4.135 2016-10-20 22:01:29 4 4 186.600006 38 ... 1 4.250 2016-10-20 22:01:30 ... ... ... ... ... ... ... ... 3897 1127 178.199997 0 ... 0 3.497 2016-10-20 23:14:31 3898 1128 178.199997 0 ... 0 3.289 2016-10-20 23:14:32 3899 1129 178.199997 0 ... 0 2.969 2016-10-20 23:14:33 3900 1130 178.399994 0 ... 0 2.969 2016-10-20 23:14:34 3901 1131 178.399994 0 ... 0 2.853 2016-10-20 23:14:35 [3902 rows x 10 columns] [stdout:3] Unnamed: 0 altitude cadence ... power speed time 0 0 185.800003 51 ... 45 3.459 2016-10-20 22:01:26 1 1 185.800003 68 ... 0 3.710 2016-10-20 22:01:27 2 2 186.399994 38 ... 42 3.874 2016-10-20 22:01:28 3 3 186.800003 38 ... 5 4.135 2016-10-20 22:01:29 4 4 186.600006 38 ... 1 4.250 2016-10-20 22:01:30 ... ... ... ... ... ... ... ... 3897 1127 178.199997 0 ... 0 3.497 2016-10-20 23:14:31 3898 1128 178.199997 0 ... 0 3.289 2016-10-20 23:14:32 3899 1129 178.199997 0 ... 0 2.969 2016-10-20 23:14:33 3900 1130 178.399994 0 ... 0 2.969 2016-10-20 23:14:34 3901 1131 178.399994 0 ... 0 2.853 2016-10-20 23:14:35 [3902 rows x 10 columns] bodo.barrier \u00b6 bodo. barrier () Synchronize all processes. Block process from proceeding until all processes reach this point. Example Usage A typical example is to make sure all processes see side effects simultaneously. For example, a process can delete files from storage while others wait before writing to file: import shutil , os import numpy as np # remove file if exists if bodo . get_rank () == 0 : if os . path . exists ( \"data/data.pq\" ): shutil . rmtree ( \"data/data.pq\" ) # make sure all processes are synchronized # (e.g. all processes need to see effect of rank 0's work) bodo . barrier () @bodo . jit def f ( n ): df = pd . DataFrame ({ \"A\" : np . arange ( n )}) df . to_parquet ( \"data/data.pq\" ) f ( 10 ) The following figure illustrates what happens when processes call bodo.barrier() . When barrier is called, a process pauses and waits until all other processes have reached the barrier: Danger The example above shows that it is possible to have each process follow a different control flow, but all processes must always call the same Bodo functions in the same order. bodo.gatherv \u00b6 bodo. gatherv (data, allgather=False, warn_if_rep=True, root=0) Collect distributed data manually by gathering them into a single rank. Arguments data : data to gather. root : specify rank to collect the data. Default: rank 0 . warn_if_rep : prints a BodoWarning if data to gather is replicated. allgather : send gathered data to all ranks. Default: False . Same behavior as bodo.allgatherv . Example Usage import bodo import pandas as pd @bodo . jit def mean_power (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) return bodo . gatherv ( df , root = 1 ) df = mean_power () print ( df ) Save code in test_gatherv.py file and run with mpiexec . mpiexec -n 4 python test_gatherv.py Output: [stdout:1] Unnamed: 0 altitude cadence ... power speed time 0 0 185.800003 51 ... 45 3.459 2016-10-20 22:01:26 1 1 185.800003 68 ... 0 3.710 2016-10-20 22:01:27 2 2 186.399994 38 ... 42 3.874 2016-10-20 22:01:28 3 3 186.800003 38 ... 5 4.135 2016-10-20 22:01:29 4 4 186.600006 38 ... 1 4.250 2016-10-20 22:01:30 ... ... ... ... ... ... ... ... 3897 1127 178.199997 0 ... 0 3.497 2016-10-20 23:14:31 3898 1128 178.199997 0 ... 0 3.289 2016-10-20 23:14:32 3899 1129 178.199997 0 ... 0 2.969 2016-10-20 23:14:33 3900 1130 178.399994 0 ... 0 2.969 2016-10-20 23:14:34 3901 1131 178.399994 0 ... 0 2.853 2016-10-20 23:14:35 [3902 rows x 10 columns] [stdout:0] Empty DataFrame Columns: [Unnamed: 0, altitude, cadence, distance, hr, latitude, longitude, power, speed, time] Index: [] [0 rows x 10 columns] [stdout:2] Empty DataFrame Columns: [Unnamed: 0, altitude, cadence, distance, hr, latitude, longitude, power, speed, time] Index: [] [0 rows x 10 columns] [stdout:3] Empty DataFrame Columns: [Unnamed: 0, altitude, cadence, distance, hr, latitude, longitude, power, speed, time] Index: [] [0 rows x 10 columns] bodo.get_rank \u00b6 bodo. get_rank () Get the process number from Bodo (called rank in MPI terminology). Example Usage Save following code in get_rank.py file and run with mpiexec . import bodo # some work only on rank 0 if bodo . get_rank () == 0 : print ( \"rank 0 done\" ) # some work on every process print ( \"rank\" , bodo . get_rank (), \"here\" ) mpiexec -n 4 python get_rank.py Output rank 0 done rank 0 here rank 1 here rank 2 here rank 3 here bodo.get_size \u00b6 bodo. get_size () Get the total number of processes. Example Usage Save following code in get_rank_size.py file and run with mpiexec . import bodo # some work only on rank 0 if bodo . get_rank () == 0 : print ( \"rank 0 done\" ) # some work on every process print ( \"rank\" , bodo . get_rank (), \"here\" ) print ( \"total ranks:\" , bodo . get_size ()) mpiexec -n 4 python get_rank_size.py Output rank 0 done rank 0 here total ranks: 4 rank 1 here total ranks: 4 rank 2 here total ranks: 4 rank 3 here total ranks: 4 bodo.random_shuffle \u00b6 bodo. random_shuffle (data, seed=None, dests=None, parallel=False) Manually shuffle data evenly across selected ranks. Arguments data : data to shuffle. seed : number to initialze random number generator. dests : selected ranks to distribute shuffled data to. By default, distribution includes all ranks. parallel : flag to indicate whether data is distributed. Default: False . Inside JIT default value depends on Bodo's distribution analysis algorithm for the data passed (For more information, see Data Distribution section below). Example Usage import bodo import pandas as pd @bodo . jit def test_random_shuffle (): df = pd . DataFrame ({ \"A\" : range ( 100 )}) return df df = test_random_shuffle () print ( df . head ()) df = bodo . random_shuffle ( res , parallel = True ) print ( df . head ()) Save code in test_random_shuffle.py file and run with mpiexec . mpiexec -n 4 python test_random_shuffle.py Output: [stdout:1] A 0 25 1 26 2 27 3 28 4 29 A 19 19 10 10 17 42 9 9 17 17 [stdout:3] A 0 75 1 76 2 77 3 78 4 79 A 6 31 0 25 24 49 22 22 5 30 [stdout:2] A 0 50 1 51 2 52 3 53 4 54 A 11 36 24 24 15 65 14 14 10 35 [stdout:0] A 0 0 1 1 2 2 3 3 4 4 A 4 29 18 18 8 58 15 15 3 28 bodo.rebalance \u00b6 bodo. rebalance (data, dests=None, random=False, random_seed=None, parallel=False) Manually redistribute data evenly across [selected] ranks. Arguments data : data to rebalance. dests : selected ranks to distribute data to. By default, distribution includes all ranks. random : flag to randomize order of the rows of the data. Default: False . random_seed : number to initialize random number generator. parallel : flag to indicate whether data is distributed. Default: False . Inside JIT default value depends on Bodo's distribution analysis algorithm for the data passed (For more information, see Data Distribution section below). Example Usage Example with just the parallel flag set to True : import bodo import pandas as pd @bodo . jit def mean_power (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df = df . sort_values ( \"power\" )[ df [ \"power\" ] > 400 ] return df df = mean_power () print ( df . shape ) df = bodo . rebalance ( df , parallel = True ) print ( \"After rebalance: \" , df . shape ) Save code in test_rebalance.py file and run with mpiexec . mpiexec -n 4 python test_rebalance.py [stdout:0] (5, 10) After rebalance: (33, 10) [stdout:1] (18, 10) After rebalance: (33, 10) [stdout:2] (82, 10) After rebalance: (33, 10) [stdout:3] (26, 10) After rebalance: (32, 10) Example to distribute the data from all ranks to subset of ranks using dests argument. import bodo import pandas as pd @bodo . jit def mean_power (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df = df . sort_values ( \"power\" )[ df [ \"power\" ] > 400 ] return df df = mean_power () print ( df . shape ) df = bodo . rebalance ( df , dests = [ 1 , 3 ], parallel = True ) print ( \"After rebalance: \" , df . shape ) Save code in test_rebalance.py file and run with mpiexec . mpiexec -n 4 python test_rebalance.py Output: [stdout:0] (5, 10) After rebalance: (0, 10) [stdout:1] (18, 10) After rebalance: (66, 10) [stdout:2] (82, 10) After rebalance: (0, 10) [stdout:3] (26, 10) After rebalance: (65, 10) bodo.scatterv \u00b6 bodo. scatterv (data, warn_if_dist=True) Distribute data manually by scattering data from one process to all processes. Arguments data : data to distribute. warn_if_dist : flag to print a BodoWarning if data is already distributed. Note Currently, bodo.scatterv only supports scattering from rank 0. Example Usage When used outside of JIT code, we recommend that the argument be set to None for all ranks except rank 0. For example: import bodo import pandas as pd @bodo . jit ( distributed = [ \"df\" ]) def mean_power ( df ): x = df . power . mean () return x df = None # only rank 0 reads the data if bodo . get_rank () == 0 : df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df = bodo . scatterv ( df ) res = mean_power ( df ) print ( res ) Save the code in test_scatterv.py file and run with mpiexec . mpiexec -n 4 python test_scatterv.py Output: [stdout:0] 102.07842132239877 [stdout:1] 102.07842132239877 [stdout:2] 102.07842132239877 [stdout:3] 102.07842132239877 Note data/cycling_dataset.pq is located in the Bodo tutorial repo . This is not a strict requirement. However, since this might be bad practice in certain situations, Bodo will throw a warning if the data is not None on other ranks. import bodo import pandas as pd df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df = bodo . scatterv ( df ) res = mean_power ( df ) print ( res ) Save code in test_scatterv.py file and run with mpiexec . mpiexec -n 4 python test_scatterv.py Output: BodoWarning: bodo.scatterv(): A non-None value for 'data' was found on a rank other than the root. This data won't be sent to any other ranks and will be overwritten with data from rank 0. [stdout:0] 102.07842132239877 [stdout:1] 102.07842132239877 [stdout:2] 102.07842132239877 [stdout:3] 102.07842132239877 When using scatterv inside of JIT code, the argument must have the same type on each rank due to Bodo's typing constraints. All inputs except for rank 0 are ignored. import bodo import pandas as pd @bodo . jit () def impl (): if bodo . get_rank () == 0 : df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]}) else : df = pd . DataFrame ({ \"A\" : [ - 1 ] * 8 }) return bodo . scatterv ( df ) print ( impl ()) Save code in test_scatterv.py file and run with mpiexec . mpiexec -n 8 python test_scatterv.py Output: [stdout:6] A 6 7 [stdout:0] A 0 1 [stdout:1] A 1 2 [stdout:4] A 4 5 [stdout:7] A 7 8 [stdout:3] A 3 4 [stdout:2] A 2 3 [stdout:5] A 5 6 Note scatterv , gatherv , allgatherv , rebalance , and random_shuffle work with all distributable data types. This includes: All supported numpy array types. All supported pandas array types (with the exception of Interval Arrays). All supported pandas Series types. All supported DataFrame types. All supported Index types (with the exception of Interval Index). Tuples of the above types.","title":"Bodo Parallel APIs"},{"location":"api_docs/bodo_parallel_apis/#bodoparallelapis","text":"","title":"Bodo Parallel APIs"},{"location":"api_docs/bodo_parallel_apis/#bodoallgatherv","text":"bodo. allgatherv (data, warn_if_rep=True) gather data from all ranks and send to all, effectively replicating the data: Arguments data : data to gather. warn_if_rep : prints a BodoWarning if data to gather is replicated. Example Usage import bodo import pandas as pd @bodo . jit def mean_power (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) return bodo . allgatherv ( df ) df = mean_power () print ( df ) Save code in test_allgatherv.py file and run with mpiexec . mpiexec -n 4 python test_allgatherv.py Output: [stdout:0] Unnamed: 0 altitude cadence ... power speed time 0 0 185.800003 51 ... 45 3.459 2016-10-20 22:01:26 1 1 185.800003 68 ... 0 3.710 2016-10-20 22:01:27 2 2 186.399994 38 ... 42 3.874 2016-10-20 22:01:28 3 3 186.800003 38 ... 5 4.135 2016-10-20 22:01:29 4 4 186.600006 38 ... 1 4.250 2016-10-20 22:01:30 ... ... ... ... ... ... ... ... 3897 1127 178.199997 0 ... 0 3.497 2016-10-20 23:14:31 3898 1128 178.199997 0 ... 0 3.289 2016-10-20 23:14:32 3899 1129 178.199997 0 ... 0 2.969 2016-10-20 23:14:33 3900 1130 178.399994 0 ... 0 2.969 2016-10-20 23:14:34 3901 1131 178.399994 0 ... 0 2.853 2016-10-20 23:14:35 [3902 rows x 10 columns] [stdout:1] Unnamed: 0 altitude cadence ... power speed time 0 0 185.800003 51 ... 45 3.459 2016-10-20 22:01:26 1 1 185.800003 68 ... 0 3.710 2016-10-20 22:01:27 2 2 186.399994 38 ... 42 3.874 2016-10-20 22:01:28 3 3 186.800003 38 ... 5 4.135 2016-10-20 22:01:29 4 4 186.600006 38 ... 1 4.250 2016-10-20 22:01:30 ... ... ... ... ... ... ... ... 3897 1127 178.199997 0 ... 0 3.497 2016-10-20 23:14:31 3898 1128 178.199997 0 ... 0 3.289 2016-10-20 23:14:32 3899 1129 178.199997 0 ... 0 2.969 2016-10-20 23:14:33 3900 1130 178.399994 0 ... 0 2.969 2016-10-20 23:14:34 3901 1131 178.399994 0 ... 0 2.853 2016-10-20 23:14:35 [3902 rows x 10 columns] [stdout:2] Unnamed: 0 altitude cadence ... power speed time 0 0 185.800003 51 ... 45 3.459 2016-10-20 22:01:26 1 1 185.800003 68 ... 0 3.710 2016-10-20 22:01:27 2 2 186.399994 38 ... 42 3.874 2016-10-20 22:01:28 3 3 186.800003 38 ... 5 4.135 2016-10-20 22:01:29 4 4 186.600006 38 ... 1 4.250 2016-10-20 22:01:30 ... ... ... ... ... ... ... ... 3897 1127 178.199997 0 ... 0 3.497 2016-10-20 23:14:31 3898 1128 178.199997 0 ... 0 3.289 2016-10-20 23:14:32 3899 1129 178.199997 0 ... 0 2.969 2016-10-20 23:14:33 3900 1130 178.399994 0 ... 0 2.969 2016-10-20 23:14:34 3901 1131 178.399994 0 ... 0 2.853 2016-10-20 23:14:35 [3902 rows x 10 columns] [stdout:3] Unnamed: 0 altitude cadence ... power speed time 0 0 185.800003 51 ... 45 3.459 2016-10-20 22:01:26 1 1 185.800003 68 ... 0 3.710 2016-10-20 22:01:27 2 2 186.399994 38 ... 42 3.874 2016-10-20 22:01:28 3 3 186.800003 38 ... 5 4.135 2016-10-20 22:01:29 4 4 186.600006 38 ... 1 4.250 2016-10-20 22:01:30 ... ... ... ... ... ... ... ... 3897 1127 178.199997 0 ... 0 3.497 2016-10-20 23:14:31 3898 1128 178.199997 0 ... 0 3.289 2016-10-20 23:14:32 3899 1129 178.199997 0 ... 0 2.969 2016-10-20 23:14:33 3900 1130 178.399994 0 ... 0 2.969 2016-10-20 23:14:34 3901 1131 178.399994 0 ... 0 2.853 2016-10-20 23:14:35 [3902 rows x 10 columns]","title":"bodo.allgatherv"},{"location":"api_docs/bodo_parallel_apis/#bodobarrier","text":"bodo. barrier () Synchronize all processes. Block process from proceeding until all processes reach this point. Example Usage A typical example is to make sure all processes see side effects simultaneously. For example, a process can delete files from storage while others wait before writing to file: import shutil , os import numpy as np # remove file if exists if bodo . get_rank () == 0 : if os . path . exists ( \"data/data.pq\" ): shutil . rmtree ( \"data/data.pq\" ) # make sure all processes are synchronized # (e.g. all processes need to see effect of rank 0's work) bodo . barrier () @bodo . jit def f ( n ): df = pd . DataFrame ({ \"A\" : np . arange ( n )}) df . to_parquet ( \"data/data.pq\" ) f ( 10 ) The following figure illustrates what happens when processes call bodo.barrier() . When barrier is called, a process pauses and waits until all other processes have reached the barrier: Danger The example above shows that it is possible to have each process follow a different control flow, but all processes must always call the same Bodo functions in the same order.","title":"bodo.barrier"},{"location":"api_docs/bodo_parallel_apis/#bodogatherv","text":"bodo. gatherv (data, allgather=False, warn_if_rep=True, root=0) Collect distributed data manually by gathering them into a single rank. Arguments data : data to gather. root : specify rank to collect the data. Default: rank 0 . warn_if_rep : prints a BodoWarning if data to gather is replicated. allgather : send gathered data to all ranks. Default: False . Same behavior as bodo.allgatherv . Example Usage import bodo import pandas as pd @bodo . jit def mean_power (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) return bodo . gatherv ( df , root = 1 ) df = mean_power () print ( df ) Save code in test_gatherv.py file and run with mpiexec . mpiexec -n 4 python test_gatherv.py Output: [stdout:1] Unnamed: 0 altitude cadence ... power speed time 0 0 185.800003 51 ... 45 3.459 2016-10-20 22:01:26 1 1 185.800003 68 ... 0 3.710 2016-10-20 22:01:27 2 2 186.399994 38 ... 42 3.874 2016-10-20 22:01:28 3 3 186.800003 38 ... 5 4.135 2016-10-20 22:01:29 4 4 186.600006 38 ... 1 4.250 2016-10-20 22:01:30 ... ... ... ... ... ... ... ... 3897 1127 178.199997 0 ... 0 3.497 2016-10-20 23:14:31 3898 1128 178.199997 0 ... 0 3.289 2016-10-20 23:14:32 3899 1129 178.199997 0 ... 0 2.969 2016-10-20 23:14:33 3900 1130 178.399994 0 ... 0 2.969 2016-10-20 23:14:34 3901 1131 178.399994 0 ... 0 2.853 2016-10-20 23:14:35 [3902 rows x 10 columns] [stdout:0] Empty DataFrame Columns: [Unnamed: 0, altitude, cadence, distance, hr, latitude, longitude, power, speed, time] Index: [] [0 rows x 10 columns] [stdout:2] Empty DataFrame Columns: [Unnamed: 0, altitude, cadence, distance, hr, latitude, longitude, power, speed, time] Index: [] [0 rows x 10 columns] [stdout:3] Empty DataFrame Columns: [Unnamed: 0, altitude, cadence, distance, hr, latitude, longitude, power, speed, time] Index: [] [0 rows x 10 columns]","title":"bodo.gatherv"},{"location":"api_docs/bodo_parallel_apis/#bodoget_rank","text":"bodo. get_rank () Get the process number from Bodo (called rank in MPI terminology). Example Usage Save following code in get_rank.py file and run with mpiexec . import bodo # some work only on rank 0 if bodo . get_rank () == 0 : print ( \"rank 0 done\" ) # some work on every process print ( \"rank\" , bodo . get_rank (), \"here\" ) mpiexec -n 4 python get_rank.py Output rank 0 done rank 0 here rank 1 here rank 2 here rank 3 here","title":"bodo.get_rank"},{"location":"api_docs/bodo_parallel_apis/#bodoget_size","text":"bodo. get_size () Get the total number of processes. Example Usage Save following code in get_rank_size.py file and run with mpiexec . import bodo # some work only on rank 0 if bodo . get_rank () == 0 : print ( \"rank 0 done\" ) # some work on every process print ( \"rank\" , bodo . get_rank (), \"here\" ) print ( \"total ranks:\" , bodo . get_size ()) mpiexec -n 4 python get_rank_size.py Output rank 0 done rank 0 here total ranks: 4 rank 1 here total ranks: 4 rank 2 here total ranks: 4 rank 3 here total ranks: 4","title":"bodo.get_size"},{"location":"api_docs/bodo_parallel_apis/#bodorandom_shuffle","text":"bodo. random_shuffle (data, seed=None, dests=None, parallel=False) Manually shuffle data evenly across selected ranks. Arguments data : data to shuffle. seed : number to initialze random number generator. dests : selected ranks to distribute shuffled data to. By default, distribution includes all ranks. parallel : flag to indicate whether data is distributed. Default: False . Inside JIT default value depends on Bodo's distribution analysis algorithm for the data passed (For more information, see Data Distribution section below). Example Usage import bodo import pandas as pd @bodo . jit def test_random_shuffle (): df = pd . DataFrame ({ \"A\" : range ( 100 )}) return df df = test_random_shuffle () print ( df . head ()) df = bodo . random_shuffle ( res , parallel = True ) print ( df . head ()) Save code in test_random_shuffle.py file and run with mpiexec . mpiexec -n 4 python test_random_shuffle.py Output: [stdout:1] A 0 25 1 26 2 27 3 28 4 29 A 19 19 10 10 17 42 9 9 17 17 [stdout:3] A 0 75 1 76 2 77 3 78 4 79 A 6 31 0 25 24 49 22 22 5 30 [stdout:2] A 0 50 1 51 2 52 3 53 4 54 A 11 36 24 24 15 65 14 14 10 35 [stdout:0] A 0 0 1 1 2 2 3 3 4 4 A 4 29 18 18 8 58 15 15 3 28","title":"bodo.random_shuffle"},{"location":"api_docs/bodo_parallel_apis/#bodorebalance","text":"bodo. rebalance (data, dests=None, random=False, random_seed=None, parallel=False) Manually redistribute data evenly across [selected] ranks. Arguments data : data to rebalance. dests : selected ranks to distribute data to. By default, distribution includes all ranks. random : flag to randomize order of the rows of the data. Default: False . random_seed : number to initialize random number generator. parallel : flag to indicate whether data is distributed. Default: False . Inside JIT default value depends on Bodo's distribution analysis algorithm for the data passed (For more information, see Data Distribution section below). Example Usage Example with just the parallel flag set to True : import bodo import pandas as pd @bodo . jit def mean_power (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df = df . sort_values ( \"power\" )[ df [ \"power\" ] > 400 ] return df df = mean_power () print ( df . shape ) df = bodo . rebalance ( df , parallel = True ) print ( \"After rebalance: \" , df . shape ) Save code in test_rebalance.py file and run with mpiexec . mpiexec -n 4 python test_rebalance.py [stdout:0] (5, 10) After rebalance: (33, 10) [stdout:1] (18, 10) After rebalance: (33, 10) [stdout:2] (82, 10) After rebalance: (33, 10) [stdout:3] (26, 10) After rebalance: (32, 10) Example to distribute the data from all ranks to subset of ranks using dests argument. import bodo import pandas as pd @bodo . jit def mean_power (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df = df . sort_values ( \"power\" )[ df [ \"power\" ] > 400 ] return df df = mean_power () print ( df . shape ) df = bodo . rebalance ( df , dests = [ 1 , 3 ], parallel = True ) print ( \"After rebalance: \" , df . shape ) Save code in test_rebalance.py file and run with mpiexec . mpiexec -n 4 python test_rebalance.py Output: [stdout:0] (5, 10) After rebalance: (0, 10) [stdout:1] (18, 10) After rebalance: (66, 10) [stdout:2] (82, 10) After rebalance: (0, 10) [stdout:3] (26, 10) After rebalance: (65, 10)","title":"bodo.rebalance"},{"location":"api_docs/bodo_parallel_apis/#bodoscatterv","text":"bodo. scatterv (data, warn_if_dist=True) Distribute data manually by scattering data from one process to all processes. Arguments data : data to distribute. warn_if_dist : flag to print a BodoWarning if data is already distributed. Note Currently, bodo.scatterv only supports scattering from rank 0. Example Usage When used outside of JIT code, we recommend that the argument be set to None for all ranks except rank 0. For example: import bodo import pandas as pd @bodo . jit ( distributed = [ \"df\" ]) def mean_power ( df ): x = df . power . mean () return x df = None # only rank 0 reads the data if bodo . get_rank () == 0 : df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df = bodo . scatterv ( df ) res = mean_power ( df ) print ( res ) Save the code in test_scatterv.py file and run with mpiexec . mpiexec -n 4 python test_scatterv.py Output: [stdout:0] 102.07842132239877 [stdout:1] 102.07842132239877 [stdout:2] 102.07842132239877 [stdout:3] 102.07842132239877 Note data/cycling_dataset.pq is located in the Bodo tutorial repo . This is not a strict requirement. However, since this might be bad practice in certain situations, Bodo will throw a warning if the data is not None on other ranks. import bodo import pandas as pd df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df = bodo . scatterv ( df ) res = mean_power ( df ) print ( res ) Save code in test_scatterv.py file and run with mpiexec . mpiexec -n 4 python test_scatterv.py Output: BodoWarning: bodo.scatterv(): A non-None value for 'data' was found on a rank other than the root. This data won't be sent to any other ranks and will be overwritten with data from rank 0. [stdout:0] 102.07842132239877 [stdout:1] 102.07842132239877 [stdout:2] 102.07842132239877 [stdout:3] 102.07842132239877 When using scatterv inside of JIT code, the argument must have the same type on each rank due to Bodo's typing constraints. All inputs except for rank 0 are ignored. import bodo import pandas as pd @bodo . jit () def impl (): if bodo . get_rank () == 0 : df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]}) else : df = pd . DataFrame ({ \"A\" : [ - 1 ] * 8 }) return bodo . scatterv ( df ) print ( impl ()) Save code in test_scatterv.py file and run with mpiexec . mpiexec -n 8 python test_scatterv.py Output: [stdout:6] A 6 7 [stdout:0] A 0 1 [stdout:1] A 1 2 [stdout:4] A 4 5 [stdout:7] A 7 8 [stdout:3] A 3 4 [stdout:2] A 2 3 [stdout:5] A 5 6 Note scatterv , gatherv , allgatherv , rebalance , and random_shuffle work with all distributable data types. This includes: All supported numpy array types. All supported pandas array types (with the exception of Interval Arrays). All supported pandas Series types. All supported DataFrame types. All supported Index types (with the exception of Interval Index). Tuples of the above types.","title":"bodo.scatterv"},{"location":"api_docs/miscellaneous/","text":"Miscellaneous Supported Python API \u00b6 In this section, we will discuss some useful Bodo features. Nullable Integers in Pandas \u00b6 DataFrame and Series objects with integer data need special care due to integer NA issues in Pandas . By default, Pandas dynamically converts integer columns to floating point when missing values (NAs) are needed, which can result in loss of precision as well as type instability. Pandas introduced a new nullable integer datatype that can solve this issue, which is also supported by Bodo. For example, this code reads column A into a nullable integer array (the capital \"I\" denotes nullable integer type): data = ( \"11,1.2 \\n \" \"-2, \\n \" \",3.1 \\n \" \"4,-0.1 \\n \" ) with open ( \"data/data.csv\" , \"w\" ) as f : f . write ( data ) @bodo . jit ( distributed = [ \"df\" ]) def f (): dtype = { \"A\" : \"Int64\" , \"B\" : \"float64\" } df = pd . read_csv ( \"data/data.csv\" , dtype = dtype , names = dtype . keys ()) return df f () Checking NA Values \u00b6 When an operation iterates over the values in a Series or Array, type stability requires special handling for NAs using pd.isna() . For example, Series.map() applies an operation to each element in the series and failing to check for NAs can result in garbage values propagating. S = pd . Series ( pd . array ([ 1 , None , None , 3 , 10 ], dtype = \"Int8\" )) @bodo . jit def map_copy ( S ): return S . map ( lambda a : a if not pd . isna ( a ) else None ) print ( map_copy ( S )) 0 1 1 <NA> 2 <NA> 3 3 4 10 dtype: Int8 Boxing/Unboxing Overheads \u00b6 Bodo uses efficient native data structures which can be different than Python. When Python values are passed to Bodo, they are unboxed to native representation. On the other hand, returning Bodo values requires boxing to Python objects. Boxing and unboxing can have significant overhead depending on size and type of data. For example, passing string column between Python/Bodo repeatedly can be expensive: @bodo . jit ( distributed = [ \"df\" ]) def gen_data (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df [ \"hr\" ] = df [ \"hr\" ] . astype ( str ) return df @bodo . jit ( distributed = [ \"df\" , \"x\" ]) def mean_power ( df ): x = df . hr . str [ 1 :] return x df = gen_data () res = mean_power ( df ) print ( res ) Output: 0 1 1 2 2 2 3 3 4 3 .. 3897 00 3898 00 3899 00 3900 00 3901 00 Name: hr, Length: 3902, dtype: object One can try to keep data in Bodo functions as much as possible to avoid boxing/unboxing overheads: @bodo . jit ( distributed = [ \"df\" ]) def gen_data (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df [ \"hr\" ] = df [ \"hr\" ] . astype ( str ) return df @bodo . jit ( distributed = [ \"df\" , \"x\" ]) def mean_power ( df ): x = df . hr . str [ 1 :] return x @bodo . jit def f (): df = gen_data () res = mean_power ( df ) print ( res ) f () 0 1 1 2 2 2 3 3 4 3 .. 3897 00 3898 00 3899 00 3900 00 3901 00 Name: hr, Length: 3902, dtype: object Iterating Over Columns \u00b6 Iterating over columns in a dataframe can cause type stability issues, since column types in each iteration can be different. Bodo supports this usage for many practical cases by automatically unrolling loops over dataframe columns when possible. For example, the example below computes the sum of all data frame columns: @bodo . jit def f (): n = 20 df = pd . DataFrame ({ \"A\" : np . arange ( n ), \"B\" : np . arange ( n ) ** 2 , \"C\" : np . ones ( n )}) s = 0 for c in df . columns : s += df [ c ] . sum () return s f () 2680.0 For automatic unrolling, the loop needs to be a for loop over column names that can be determined by Bodo at compile time. Regular Expressions using re \u00b6 Bodo supports string processing using Pandas and the re standard package, offering significant flexibility for string processing applications. For example, re can be used in user-defined functions (UDFs) applied to Series and DataFrame values: import re @bodo . jit def f ( S ): def g ( a ): res = 0 if re . search ( \".*AB.*\" , a ): res = 3 if re . search ( \".*23.*\" , a ): res = 5 return res return S . map ( g ) S = pd . Series ([ \"AABCDE\" , \"BBABCE\" , \"1234\" ]) f ( S ) 0 3 1 3 2 5 dtype: int64 Below is a reference list of supported functionality. Full functionality is documented in standard re documentation . All functions except finditer are supported. Note that currently, Bodo JIT uses Python's re package as backend and therefore the compute speed of these functions is similar to Python. re.A \u00b6 re. A re.ASCII \u00b6 re. ASCII re.DEBUG \u00b6 re. DEBUG re.I \u00b6 re. I re.IGNORECASE \u00b6 re. IGNORECASE re.L \u00b6 re. L re.LOCALE \u00b6 re. LOCALE re.M \u00b6 re. M re.MULTILINE \u00b6 re. MULTILINE re.S \u00b6 re. S re.DOTALL \u00b6 re. DOTALL re.X \u00b6 re. X re.VERBOSE \u00b6 re. VERBOSE re.search \u00b6 re. search (pattern, string, flags=0) re.match \u00b6 re. match (pattern, string, flags=0) re.fullmatch \u00b6 re. fullmatch (pattern, string, flags=0) re.split \u00b6 re. split (pattern, string, maxsplit=0, flags=0) re.findall \u00b6 re. findall (pattern, string, flags=0) The pattern argument should be a constant string for multi-group patterns (for Bodo to know the output will be a list of string tuples). An error is raised otherwise. Example Usage : >>> @bodo . jit ... def f ( pat , in_str ): ... return re . findall ( pat , in_str ) ... >>> f ( r \"\\w+\" , \"Words, words, words.\" ) [ 'Words' , 'words' , 'words' ] Constant multi-group pattern works: >>> @bodo . jit ... def f2 ( in_str ): ... return re . findall ( r \"(\\w+).*(\\d+)\" , in_str ) ... >>> f2 ( \"Words, 123\" ) [( 'Words' , '3' )] Non-constant multi-group pattern throws an error: >>> @bodo . jit ... def f ( pat , in_str ): ... return re . findall ( pat , in_str ) ... >>> f ( r \"(\\w+).*(\\d+)\" , \"Words, 123\" ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/dev/bodo/bodo/libs/re_ext.py\" , line 338 , in _pat_findall_impl raise ValueError ( ValueError : pattern string should be constant for 'findall' with multiple groups re.sub \u00b6 re. sub (pattern, repl, string, count=0, flags=0) re.subn \u00b6 re. subn (pattern, repl, string, count=0, flags=0) re.escape \u00b6 re. escape (pattern) re.purge \u00b6 re. purge re.Pattern.search \u00b6 re.Pattern. search (string[, pos[, endpos]]) re.Pattern.match \u00b6 re.Pattern. match (string[, pos[, endpos]]) re.Pattern.fullmatch \u00b6 re.Pattern. fullmatch (string[, pos[, endpos]]) re.Pattern.split \u00b6 re.Pattern. split (string, maxsplit=0) re.Pattern.findall \u00b6 re.Pattern. findall (string[, pos[, endpos]]) This has the same limitation as re.findall . re.Pattern.sub \u00b6 re.Pattern. sub (repl, string, count=0) re.Pattern.subn \u00b6 re.Pattern. subn (repl, string, count=0) re.Pattern.flags \u00b6 re.Pattern. flags re.Pattern.groups \u00b6 re.Pattern. groups re.Pattern.groupindex \u00b6 re.Pattern. groupindex re.Pattern.pattern \u00b6 re.Pattern. pattern re.Match.expand \u00b6 re.Match. expand (template) re.Match.group \u00b6 re.Match. group ([group1, ...]) re.Match.__getitem__ \u00b6 re.Match. __getitem__ (g) re.Match.groups \u00b6 re.Match. groups (default=None) re.Match.groupdict \u00b6 re.Match. groupdict (default=None) (does not support default=None for groups that did not participate in the match) re.Match.start \u00b6 re.Match. start ([group]) re.Match.end \u00b6 re.Match. end ([group]) re.Match.span \u00b6 re.Match. span ([group]) re.Match.pos \u00b6 re.Match. pos re.Match.endpos \u00b6 re.Match. endpos re.Match.lastindex \u00b6 re.Match. lastindex re.Match.lastgroup \u00b6 re.Match. lastgroup re.Match.re \u00b6 re.Match. re re.Match.string \u00b6 re.Match. string Class Support using @jitclass \u00b6 Bodo supports Python classes using the @bodo.jitclass decorator. It requires type annotation of the fields, as well as distributed annotation where applicable. For example, the example class below holds a distributed dataframe and a name filed. Types can either be specified directly using the imports in the bodo package or can be inferred from existing types using bodo.typeof . The %%init%% function is required, and has to initialize the attributes. In addition, subclasses are not supported in jitclass yet. Warning Class support is currently experimental and therefore we recommend refactoring computation into regular JIT functions instead if possible. @bodo . jitclass ( { \"df\" : bodo . typeof ( pd . DataFrame ({ \"A\" : [ 1 ], \"B\" : [ 0.1 ]})), \"name\" : bodo . string_type , }, distributed = [ \"df\" ], ) class MyClass : def %% init %% ( self , n , name ): self . df = pd . DataFrame ({ \"A\" : np . arange ( n ), \"B\" : np . ones ( n )}) self . name = name def sum ( self ): return self . df . A . sum () @property def sum_vals ( self ): return self . df . sum () . sum () def get_name ( self ): return self . name @staticmethod def add_one ( a ): return a + 1 This JIT class can be used in regular Python code, as well as other Bodo JIT code. # From a compiled function @bodo . jit def f (): my_instance = MyClass ( 32 , \"my_name_jit\" ) print ( my_instance . sum ()) print ( my_instance . sum_vals ) print ( my_instance . get_name ()) f () 496 528.0 my_name_jit # From regular Python my_instance = MyClass ( 32 , \"my_name_python\" ) print ( my_instance . sum ()) print ( my_instance . sum_vals ) print ( my_instance . get_name ()) print ( MyClass . add_one ( 8 )) 496 528.0 my_name_python 9 Bodo's jitclass is built on top of Numba's jitclass (see Numba jitclass for more details).","title":"Miscellaneous Supported Python API"},{"location":"api_docs/miscellaneous/#miscapi","text":"In this section, we will discuss some useful Bodo features.","title":"Miscellaneous Supported Python API"},{"location":"api_docs/miscellaneous/#nullable-integers-in-pandas","text":"DataFrame and Series objects with integer data need special care due to integer NA issues in Pandas . By default, Pandas dynamically converts integer columns to floating point when missing values (NAs) are needed, which can result in loss of precision as well as type instability. Pandas introduced a new nullable integer datatype that can solve this issue, which is also supported by Bodo. For example, this code reads column A into a nullable integer array (the capital \"I\" denotes nullable integer type): data = ( \"11,1.2 \\n \" \"-2, \\n \" \",3.1 \\n \" \"4,-0.1 \\n \" ) with open ( \"data/data.csv\" , \"w\" ) as f : f . write ( data ) @bodo . jit ( distributed = [ \"df\" ]) def f (): dtype = { \"A\" : \"Int64\" , \"B\" : \"float64\" } df = pd . read_csv ( \"data/data.csv\" , dtype = dtype , names = dtype . keys ()) return df f ()","title":"Nullable Integers in Pandas"},{"location":"api_docs/miscellaneous/#checking-na-values","text":"When an operation iterates over the values in a Series or Array, type stability requires special handling for NAs using pd.isna() . For example, Series.map() applies an operation to each element in the series and failing to check for NAs can result in garbage values propagating. S = pd . Series ( pd . array ([ 1 , None , None , 3 , 10 ], dtype = \"Int8\" )) @bodo . jit def map_copy ( S ): return S . map ( lambda a : a if not pd . isna ( a ) else None ) print ( map_copy ( S )) 0 1 1 <NA> 2 <NA> 3 3 4 10 dtype: Int8","title":"Checking NA Values"},{"location":"api_docs/miscellaneous/#boxingunboxing-overheads","text":"Bodo uses efficient native data structures which can be different than Python. When Python values are passed to Bodo, they are unboxed to native representation. On the other hand, returning Bodo values requires boxing to Python objects. Boxing and unboxing can have significant overhead depending on size and type of data. For example, passing string column between Python/Bodo repeatedly can be expensive: @bodo . jit ( distributed = [ \"df\" ]) def gen_data (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df [ \"hr\" ] = df [ \"hr\" ] . astype ( str ) return df @bodo . jit ( distributed = [ \"df\" , \"x\" ]) def mean_power ( df ): x = df . hr . str [ 1 :] return x df = gen_data () res = mean_power ( df ) print ( res ) Output: 0 1 1 2 2 2 3 3 4 3 .. 3897 00 3898 00 3899 00 3900 00 3901 00 Name: hr, Length: 3902, dtype: object One can try to keep data in Bodo functions as much as possible to avoid boxing/unboxing overheads: @bodo . jit ( distributed = [ \"df\" ]) def gen_data (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) df [ \"hr\" ] = df [ \"hr\" ] . astype ( str ) return df @bodo . jit ( distributed = [ \"df\" , \"x\" ]) def mean_power ( df ): x = df . hr . str [ 1 :] return x @bodo . jit def f (): df = gen_data () res = mean_power ( df ) print ( res ) f () 0 1 1 2 2 2 3 3 4 3 .. 3897 00 3898 00 3899 00 3900 00 3901 00 Name: hr, Length: 3902, dtype: object","title":"Boxing/Unboxing Overheads"},{"location":"api_docs/miscellaneous/#iterating-over-columns","text":"Iterating over columns in a dataframe can cause type stability issues, since column types in each iteration can be different. Bodo supports this usage for many practical cases by automatically unrolling loops over dataframe columns when possible. For example, the example below computes the sum of all data frame columns: @bodo . jit def f (): n = 20 df = pd . DataFrame ({ \"A\" : np . arange ( n ), \"B\" : np . arange ( n ) ** 2 , \"C\" : np . ones ( n )}) s = 0 for c in df . columns : s += df [ c ] . sum () return s f () 2680.0 For automatic unrolling, the loop needs to be a for loop over column names that can be determined by Bodo at compile time.","title":"Iterating Over Columns"},{"location":"api_docs/miscellaneous/#regular-expressions-using-re","text":"Bodo supports string processing using Pandas and the re standard package, offering significant flexibility for string processing applications. For example, re can be used in user-defined functions (UDFs) applied to Series and DataFrame values: import re @bodo . jit def f ( S ): def g ( a ): res = 0 if re . search ( \".*AB.*\" , a ): res = 3 if re . search ( \".*23.*\" , a ): res = 5 return res return S . map ( g ) S = pd . Series ([ \"AABCDE\" , \"BBABCE\" , \"1234\" ]) f ( S ) 0 3 1 3 2 5 dtype: int64 Below is a reference list of supported functionality. Full functionality is documented in standard re documentation . All functions except finditer are supported. Note that currently, Bodo JIT uses Python's re package as backend and therefore the compute speed of these functions is similar to Python.","title":"Regular Expressions using re"},{"location":"api_docs/miscellaneous/#rea","text":"re. A","title":"re.A"},{"location":"api_docs/miscellaneous/#reascii","text":"re. ASCII","title":"re.ASCII"},{"location":"api_docs/miscellaneous/#redebug","text":"re. DEBUG","title":"re.DEBUG"},{"location":"api_docs/miscellaneous/#rei","text":"re. I","title":"re.I"},{"location":"api_docs/miscellaneous/#reignorecase","text":"re. IGNORECASE","title":"re.IGNORECASE"},{"location":"api_docs/miscellaneous/#rel","text":"re. L","title":"re.L"},{"location":"api_docs/miscellaneous/#relocale","text":"re. LOCALE","title":"re.LOCALE"},{"location":"api_docs/miscellaneous/#rem","text":"re. M","title":"re.M"},{"location":"api_docs/miscellaneous/#remultiline","text":"re. MULTILINE","title":"re.MULTILINE"},{"location":"api_docs/miscellaneous/#res","text":"re. S","title":"re.S"},{"location":"api_docs/miscellaneous/#redotall","text":"re. DOTALL","title":"re.DOTALL"},{"location":"api_docs/miscellaneous/#rex","text":"re. X","title":"re.X"},{"location":"api_docs/miscellaneous/#reverbose","text":"re. VERBOSE","title":"re.VERBOSE"},{"location":"api_docs/miscellaneous/#research","text":"re. search (pattern, string, flags=0)","title":"re.search"},{"location":"api_docs/miscellaneous/#rematch","text":"re. match (pattern, string, flags=0)","title":"re.match"},{"location":"api_docs/miscellaneous/#refullmatch","text":"re. fullmatch (pattern, string, flags=0)","title":"re.fullmatch"},{"location":"api_docs/miscellaneous/#resplit","text":"re. split (pattern, string, maxsplit=0, flags=0)","title":"re.split"},{"location":"api_docs/miscellaneous/#refindall","text":"re. findall (pattern, string, flags=0) The pattern argument should be a constant string for multi-group patterns (for Bodo to know the output will be a list of string tuples). An error is raised otherwise. Example Usage : >>> @bodo . jit ... def f ( pat , in_str ): ... return re . findall ( pat , in_str ) ... >>> f ( r \"\\w+\" , \"Words, words, words.\" ) [ 'Words' , 'words' , 'words' ] Constant multi-group pattern works: >>> @bodo . jit ... def f2 ( in_str ): ... return re . findall ( r \"(\\w+).*(\\d+)\" , in_str ) ... >>> f2 ( \"Words, 123\" ) [( 'Words' , '3' )] Non-constant multi-group pattern throws an error: >>> @bodo . jit ... def f ( pat , in_str ): ... return re . findall ( pat , in_str ) ... >>> f ( r \"(\\w+).*(\\d+)\" , \"Words, 123\" ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/dev/bodo/bodo/libs/re_ext.py\" , line 338 , in _pat_findall_impl raise ValueError ( ValueError : pattern string should be constant for 'findall' with multiple groups","title":"re.findall"},{"location":"api_docs/miscellaneous/#resub","text":"re. sub (pattern, repl, string, count=0, flags=0)","title":"re.sub"},{"location":"api_docs/miscellaneous/#resubn","text":"re. subn (pattern, repl, string, count=0, flags=0)","title":"re.subn"},{"location":"api_docs/miscellaneous/#reescape","text":"re. escape (pattern)","title":"re.escape"},{"location":"api_docs/miscellaneous/#repurge","text":"re. purge","title":"re.purge"},{"location":"api_docs/miscellaneous/#repatternsearch","text":"re.Pattern. search (string[, pos[, endpos]])","title":"re.Pattern.search"},{"location":"api_docs/miscellaneous/#repatternmatch","text":"re.Pattern. match (string[, pos[, endpos]])","title":"re.Pattern.match"},{"location":"api_docs/miscellaneous/#repatternfullmatch","text":"re.Pattern. fullmatch (string[, pos[, endpos]])","title":"re.Pattern.fullmatch"},{"location":"api_docs/miscellaneous/#repatternsplit","text":"re.Pattern. split (string, maxsplit=0)","title":"re.Pattern.split"},{"location":"api_docs/miscellaneous/#repatternfindall","text":"re.Pattern. findall (string[, pos[, endpos]]) This has the same limitation as re.findall .","title":"re.Pattern.findall"},{"location":"api_docs/miscellaneous/#repatternsub","text":"re.Pattern. sub (repl, string, count=0)","title":"re.Pattern.sub"},{"location":"api_docs/miscellaneous/#repatternsubn","text":"re.Pattern. subn (repl, string, count=0)","title":"re.Pattern.subn"},{"location":"api_docs/miscellaneous/#repatternflags","text":"re.Pattern. flags","title":"re.Pattern.flags"},{"location":"api_docs/miscellaneous/#repatterngroups","text":"re.Pattern. groups","title":"re.Pattern.groups"},{"location":"api_docs/miscellaneous/#repatterngroupindex","text":"re.Pattern. groupindex","title":"re.Pattern.groupindex"},{"location":"api_docs/miscellaneous/#repatternpattern","text":"re.Pattern. pattern","title":"re.Pattern.pattern"},{"location":"api_docs/miscellaneous/#rematchexpand","text":"re.Match. expand (template)","title":"re.Match.expand"},{"location":"api_docs/miscellaneous/#rematchgroup","text":"re.Match. group ([group1, ...])","title":"re.Match.group"},{"location":"api_docs/miscellaneous/#rematch__getitem__","text":"re.Match. __getitem__ (g)","title":"re.Match.__getitem__"},{"location":"api_docs/miscellaneous/#rematchgroups","text":"re.Match. groups (default=None)","title":"re.Match.groups"},{"location":"api_docs/miscellaneous/#rematchgroupdict","text":"re.Match. groupdict (default=None) (does not support default=None for groups that did not participate in the match)","title":"re.Match.groupdict"},{"location":"api_docs/miscellaneous/#rematchstart","text":"re.Match. start ([group])","title":"re.Match.start"},{"location":"api_docs/miscellaneous/#rematchend","text":"re.Match. end ([group])","title":"re.Match.end"},{"location":"api_docs/miscellaneous/#rematchspan","text":"re.Match. span ([group])","title":"re.Match.span"},{"location":"api_docs/miscellaneous/#rematchpos","text":"re.Match. pos","title":"re.Match.pos"},{"location":"api_docs/miscellaneous/#rematchendpos","text":"re.Match. endpos","title":"re.Match.endpos"},{"location":"api_docs/miscellaneous/#rematchlastindex","text":"re.Match. lastindex","title":"re.Match.lastindex"},{"location":"api_docs/miscellaneous/#rematchlastgroup","text":"re.Match. lastgroup","title":"re.Match.lastgroup"},{"location":"api_docs/miscellaneous/#rematchre","text":"re.Match. re","title":"re.Match.re"},{"location":"api_docs/miscellaneous/#rematchstring","text":"re.Match. string","title":"re.Match.string"},{"location":"api_docs/miscellaneous/#class-support-using-jitclass","text":"Bodo supports Python classes using the @bodo.jitclass decorator. It requires type annotation of the fields, as well as distributed annotation where applicable. For example, the example class below holds a distributed dataframe and a name filed. Types can either be specified directly using the imports in the bodo package or can be inferred from existing types using bodo.typeof . The %%init%% function is required, and has to initialize the attributes. In addition, subclasses are not supported in jitclass yet. Warning Class support is currently experimental and therefore we recommend refactoring computation into regular JIT functions instead if possible. @bodo . jitclass ( { \"df\" : bodo . typeof ( pd . DataFrame ({ \"A\" : [ 1 ], \"B\" : [ 0.1 ]})), \"name\" : bodo . string_type , }, distributed = [ \"df\" ], ) class MyClass : def %% init %% ( self , n , name ): self . df = pd . DataFrame ({ \"A\" : np . arange ( n ), \"B\" : np . ones ( n )}) self . name = name def sum ( self ): return self . df . A . sum () @property def sum_vals ( self ): return self . df . sum () . sum () def get_name ( self ): return self . name @staticmethod def add_one ( a ): return a + 1 This JIT class can be used in regular Python code, as well as other Bodo JIT code. # From a compiled function @bodo . jit def f (): my_instance = MyClass ( 32 , \"my_name_jit\" ) print ( my_instance . sum ()) print ( my_instance . sum_vals ) print ( my_instance . get_name ()) f () 496 528.0 my_name_jit # From regular Python my_instance = MyClass ( 32 , \"my_name_python\" ) print ( my_instance . sum ()) print ( my_instance . sum_vals ) print ( my_instance . get_name ()) print ( MyClass . add_one ( 8 )) 496 528.0 my_name_python 9 Bodo's jitclass is built on top of Numba's jitclass (see Numba jitclass for more details).","title":"Class Support using @jitclass"},{"location":"api_docs/numpy/","text":"Numpy Operations \u00b6 Below is the list of the data-parallel Numpy operators that Bodo can optimize and parallelize. Numpy element-wise array operations \u00b6 Unary operators \u00b6 + - ~ Binary operators \u00b6 + - * / /? % | >> ^ << & ** // Comparison operators \u00b6 == != < <= > >= Data-parallel math operations \u00b6 numpy.add numpy.subtract numpy.multiply numpy.divide numpy.logaddexp numpy.logaddexp2 numpy.true_divide numpy.floor_divide numpy.negative numpy.positive numpy.power numpy.remainder numpy.mod numpy.fmod numpy.abs numpy.absolute numpy.fabs numpy.rint numpy.sign numpy.conj numpy.exp numpy.exp2 numpy.log numpy.log2 numpy.log10 numpy.expm1 numpy.log1p numpy.sqrt numpy.square numpy.reciprocal numpy.gcd numpy.lcm numpy.conjugate Trigonometric functions \u00b6 numpy.sin numpy.cos numpy.tan numpy.arcsin numpy.arccos numpy.arctan numpy.arctan2 numpy.hypot numpy.sinh numpy.cosh numpy.tanh numpy.arcsinh numpy.arccosh numpy.arctanh numpy.deg2rad numpy.rad2deg numpy.degrees numpy.radians Bit manipulation functions \u00b6 numpy.bitwise_and numpy.bitwise_or numpy.bitwise_xor numpy.bitwise_not numpy.invert numpy.left_shift numpy.right_shift Comparison functions \u00b6 numpy.logical_and numpy.logical_or numpy.logical_xor numpy.logical_not Floating functions \u00b6 numpy.isfinite numpy.isinf numpy.signbit numpy.ldexp numpy.floor numpy.ceil numpy.trunc Numpy reduction functions \u00b6 numpy.sum numpy.prod numpy.min numpy.max numpy.argmin numpy.argmax numpy.all numpy.any Numpy array creation functions \u00b6 numpy.empty numpy.identity numpy.zeros numpy.ones numpy.empty_like numpy.zeros_like numpy.ones_like numpy.full_like numpy.array numpy.asarray numpy.copy numpy.arange numpy.linspace numpy.repeat only scalar num_repeats Numpy array manipulation functions \u00b6 numpy.shape numpy.reshape shape values cannot be -1. numpy.sort numpy.concatenate numpy.append numpy.unique The output is assumed to be \"small\" relative to input and is replicated. Use Series.drop_duplicates() if the output should remain distributed. numpy.where (1 and 3 arguments) numpy.select The default value for numeric/boolean types is 0/False . For all other types, the default is pd.NA . If any of the values in choicelist are nullable, or the default is pd.NA or None , the output will be a nullable pandas array instead of a numpy array. numpy.union1d numpy.intersect1d no distributed support yet numpy.setdiff1d no distributed support yet numpy.hstack concatenates elements on each rank without maintaining order Numpy mathematical and statistics functions \u00b6 numpy.cumsum numpy.diff numpy.percentile numpy.quantile numpy.median numpy.mean numpy.std Random number generator functions \u00b6 numpy.random.rand numpy.random.randn numpy.random.ranf numpy.random.random_sample numpy.random.sample numpy.random.random numpy.random.standard_normal numpy.random.multivariate_normal (must provide size) numpy.random.chisquare numpy.random.weibull numpy.random.power numpy.random.geometric numpy.random.exponential numpy.random.poisson numpy.random.rayleigh numpy.random.normal numpy.random.uniform numpy.random.beta numpy.random.binomial numpy.random.f numpy.random.gamma numpy.random.lognormal numpy.random.laplace numpy.random.randint numpy.random.triangular numpy.dot function \u00b6 numpy.dot between a matrix and a vector numpy.dot two vectors. Numpy I/O \u00b6 numpy.ndarray.tofile numpy.fromfile Our scalable I/O section contains example usage and more system specific instructions . Miscellaneous \u00b6 Numpy array comprehension : e.g. : A = np.array([i**2 for i in range(N)]) Note Optional arguments are not supported unless if explicitly mentioned here. For operations on multi-dimensional arrays, automatic broadcast of dimensions of size 1 is not supported. Numpy dot() Parallelization \u00b6 The np.dot function has different distribution rules based on the number of dimensions and the distributions of its input arrays. The example below demonstrates two cases: @bodo . jit def example_dot ( N , D ): X = np . random . ranf (( N , D )) Y = np . random . ranf ( N ) w = np . dot ( Y , X ) z = np . dot ( X , w ) return z . sum () example_dot ( 1024 , 10 ) example_dot . distributed_diagnostics () Here is the output of distributed_diagnostics() : Data distributions: $ X.130 1D_Block $ Y.131 1D_Block $ b.2.158 REP Parfor distributions: 0 1D_Block 1 1D_Block 3 1D_Block Distributed listing for function example_dot, ../tmp/dist_rep.py (4) <code><apihead></apihead><apihead></apihead><apihead></apihead><apihead></apihead><apihead></apihead><apihead></apihead><apihead></apihead><apihead></apihead>++| parfor_id/variable: distribution</code><br><br><br>@bodo.jit | def example_dot(N, D): | <pre><code>X = np.random.ranf((N, D))++++| #0: 1D_Block, $X.130: 1D_Block </code></pre><br><br><pre><code>Y = np.random.ranf(N)++++++++-| #1: 1D_Block, $Y.131: 1D_Block </code></pre><br><br><pre><code>w = np.dot(Y, X)++++++++++++++| $b.2.158: REP </code></pre><br><br><pre><code>z = np.dot(X, w)++++++++++++++| #3: 1D_Block </code></pre><br><br> return z.sum() | The first dot has a 1D array with 1D_Block distribution as first input Y ), while the second input is a 2D array with 1D_Block distribution ( X ). Hence, dot is a sum reduction across distributed datasets and therefore, the output ( w ) is on the reduce side and is assigned REP distribution. The second dot has a 2D array with 1D_Block distribution ( X ) as first input, while the second input is a REP array ( w ). Hence, the computation is data-parallel across rows of X , which implies a 1D_Block distribution for output ( z ). Variable z does not exist in the distribution report since the compiler optimizations were able to eliminate it. Its values are generated and consumed on-the-fly, without memory load/store overheads.","title":"Numpy"},{"location":"api_docs/numpy/#numpy","text":"Below is the list of the data-parallel Numpy operators that Bodo can optimize and parallelize.","title":"Numpy Operations"},{"location":"api_docs/numpy/#numpy-element-wise-array-operations","text":"","title":"Numpy element-wise array operations"},{"location":"api_docs/numpy/#unary-operators","text":"+ - ~","title":"Unary operators"},{"location":"api_docs/numpy/#binary-operators","text":"+ - * / /? % | >> ^ << & ** //","title":"Binary operators"},{"location":"api_docs/numpy/#comparison-operators","text":"== != < <= > >=","title":"Comparison operators"},{"location":"api_docs/numpy/#data-parallel-math-operations","text":"numpy.add numpy.subtract numpy.multiply numpy.divide numpy.logaddexp numpy.logaddexp2 numpy.true_divide numpy.floor_divide numpy.negative numpy.positive numpy.power numpy.remainder numpy.mod numpy.fmod numpy.abs numpy.absolute numpy.fabs numpy.rint numpy.sign numpy.conj numpy.exp numpy.exp2 numpy.log numpy.log2 numpy.log10 numpy.expm1 numpy.log1p numpy.sqrt numpy.square numpy.reciprocal numpy.gcd numpy.lcm numpy.conjugate","title":"Data-parallel math operations"},{"location":"api_docs/numpy/#trigonometric-functions","text":"numpy.sin numpy.cos numpy.tan numpy.arcsin numpy.arccos numpy.arctan numpy.arctan2 numpy.hypot numpy.sinh numpy.cosh numpy.tanh numpy.arcsinh numpy.arccosh numpy.arctanh numpy.deg2rad numpy.rad2deg numpy.degrees numpy.radians","title":"Trigonometric functions"},{"location":"api_docs/numpy/#bit-manipulation-functions","text":"numpy.bitwise_and numpy.bitwise_or numpy.bitwise_xor numpy.bitwise_not numpy.invert numpy.left_shift numpy.right_shift","title":"Bit manipulation functions"},{"location":"api_docs/numpy/#comparison-functions","text":"numpy.logical_and numpy.logical_or numpy.logical_xor numpy.logical_not","title":"Comparison functions"},{"location":"api_docs/numpy/#floating-functions","text":"numpy.isfinite numpy.isinf numpy.signbit numpy.ldexp numpy.floor numpy.ceil numpy.trunc","title":"Floating functions"},{"location":"api_docs/numpy/#numpy-reduction-functions","text":"numpy.sum numpy.prod numpy.min numpy.max numpy.argmin numpy.argmax numpy.all numpy.any","title":"Numpy reduction functions"},{"location":"api_docs/numpy/#numpy-array-creation-functions","text":"numpy.empty numpy.identity numpy.zeros numpy.ones numpy.empty_like numpy.zeros_like numpy.ones_like numpy.full_like numpy.array numpy.asarray numpy.copy numpy.arange numpy.linspace numpy.repeat only scalar num_repeats","title":"Numpy array creation functions"},{"location":"api_docs/numpy/#numpy-array-manipulation-functions","text":"numpy.shape numpy.reshape shape values cannot be -1. numpy.sort numpy.concatenate numpy.append numpy.unique The output is assumed to be \"small\" relative to input and is replicated. Use Series.drop_duplicates() if the output should remain distributed. numpy.where (1 and 3 arguments) numpy.select The default value for numeric/boolean types is 0/False . For all other types, the default is pd.NA . If any of the values in choicelist are nullable, or the default is pd.NA or None , the output will be a nullable pandas array instead of a numpy array. numpy.union1d numpy.intersect1d no distributed support yet numpy.setdiff1d no distributed support yet numpy.hstack concatenates elements on each rank without maintaining order","title":"Numpy array manipulation functions"},{"location":"api_docs/numpy/#numpy-mathematical-and-statistics-functions","text":"numpy.cumsum numpy.diff numpy.percentile numpy.quantile numpy.median numpy.mean numpy.std","title":"Numpy mathematical and statistics functions"},{"location":"api_docs/numpy/#random-number-generator-functions","text":"numpy.random.rand numpy.random.randn numpy.random.ranf numpy.random.random_sample numpy.random.sample numpy.random.random numpy.random.standard_normal numpy.random.multivariate_normal (must provide size) numpy.random.chisquare numpy.random.weibull numpy.random.power numpy.random.geometric numpy.random.exponential numpy.random.poisson numpy.random.rayleigh numpy.random.normal numpy.random.uniform numpy.random.beta numpy.random.binomial numpy.random.f numpy.random.gamma numpy.random.lognormal numpy.random.laplace numpy.random.randint numpy.random.triangular","title":"Random number generator functions"},{"location":"api_docs/numpy/#numpydot-function","text":"numpy.dot between a matrix and a vector numpy.dot two vectors.","title":"numpy.dot function"},{"location":"api_docs/numpy/#numpy-io","text":"numpy.ndarray.tofile numpy.fromfile Our scalable I/O section contains example usage and more system specific instructions .","title":"Numpy I/O"},{"location":"api_docs/numpy/#miscellaneous","text":"Numpy array comprehension : e.g. : A = np.array([i**2 for i in range(N)]) Note Optional arguments are not supported unless if explicitly mentioned here. For operations on multi-dimensional arrays, automatic broadcast of dimensions of size 1 is not supported.","title":"Miscellaneous"},{"location":"api_docs/numpy/#numpy-dot-parallelization","text":"The np.dot function has different distribution rules based on the number of dimensions and the distributions of its input arrays. The example below demonstrates two cases: @bodo . jit def example_dot ( N , D ): X = np . random . ranf (( N , D )) Y = np . random . ranf ( N ) w = np . dot ( Y , X ) z = np . dot ( X , w ) return z . sum () example_dot ( 1024 , 10 ) example_dot . distributed_diagnostics () Here is the output of distributed_diagnostics() : Data distributions: $ X.130 1D_Block $ Y.131 1D_Block $ b.2.158 REP Parfor distributions: 0 1D_Block 1 1D_Block 3 1D_Block Distributed listing for function example_dot, ../tmp/dist_rep.py (4) <code><apihead></apihead><apihead></apihead><apihead></apihead><apihead></apihead><apihead></apihead><apihead></apihead><apihead></apihead><apihead></apihead>++| parfor_id/variable: distribution</code><br><br><br>@bodo.jit | def example_dot(N, D): | <pre><code>X = np.random.ranf((N, D))++++| #0: 1D_Block, $X.130: 1D_Block </code></pre><br><br><pre><code>Y = np.random.ranf(N)++++++++-| #1: 1D_Block, $Y.131: 1D_Block </code></pre><br><br><pre><code>w = np.dot(Y, X)++++++++++++++| $b.2.158: REP </code></pre><br><br><pre><code>z = np.dot(X, w)++++++++++++++| #3: 1D_Block </code></pre><br><br> return z.sum() | The first dot has a 1D array with 1D_Block distribution as first input Y ), while the second input is a 2D array with 1D_Block distribution ( X ). Hence, dot is a sum reduction across distributed datasets and therefore, the output ( w ) is on the reduce side and is assigned REP distribution. The second dot has a 2D array with 1D_Block distribution ( X ) as first input, while the second input is a REP array ( w ). Hence, the computation is data-parallel across rows of X , which implies a 1D_Block distribution for output ( z ). Variable z does not exist in the distribution report since the compiler optimizations were able to eliminate it. Its values are generated and consumed on-the-fly, without memory load/store overheads.","title":"Numpy dot() Parallelization"},{"location":"api_docs/udfs/","text":"User-Defined Functions (UDFs) \u00b6 While Pandas and other APIs can be extremely expressive, many data science and data engineering use cases require additional functionality beyond what is directly offered. In these situations, many programmers create User Defined Functions , or UDFs, which are Python functions designed to compute on each row or groups of rows depending on the context. Using UDFs with Bodo \u00b6 Bodo users can construct UDFs either by defining a separate JIT function or by creating a function within a JIT function (either via a lambda or closure). For example, here are two ways to construct a UDF that advances each element of a Timestamp Series to the last day of the current month. import pandas as pd import bodo @bodo . jit def jit_udf ( x ): return x + pd . tseries . offsets . MonthEnd ( n = 0 , normalize = True ) @bodo . jit def jit_example ( S ): return S . map ( jit_udf ) @bodo . jit def lambda_example ( S ): return S . map ( lambda x : x + pd . tseries . offsets . MonthEnd ( n = 0 , normalize = True )) S = pd . Series ( pd . date_range ( start = '1/1/2021' , periods = 100 )) pd . testing . assert_series_equal ( jit_example ( S ), lambda_example ( S )) UDFs can be used to compute one value per row or group (map functions) or compute an aggregation (agg functions). Bodo provides APIs for both, which are summarized below. Please refer to supported Pandas API for more information. Map Functions \u00b6 Series.map Series.apply Series.pipe DataFrame.map DataFrame.apply DataFrame.pipe GroupBy.apply GroupBy.pipe GroupBy.transform Agg Functions \u00b6 GroupBy.agg GroupBy.aggregate UDF Performance \u00b6 Bodo offers support for UDFs without the significant runtime penalty generally incurred in Pandas. An example of this is shown in the quick started guide . Bodo achieves a drastic performance advantage on UDFs because UDFs can be optimized by similar to any other JIT code. In contrast, library based solutions are typically limited in their ability to optimize UDFs. Additional Arguments \u00b6 We recommend passing additional variables to UDFs explicitly, instead of directly using variables local to the function defining the UDF. The latter is called the \\\"captured\\\" variables case, which is often error-prone and may result in compilation errors. For example, consider a UDF that appends a variable suffix to each string in a Series of strings. The proper way to write this function is to use the args argument to Series.apply() . import pandas as pd import bodo @bodo . jit def add_suffix ( S , suffix ): return S . apply ( lambda x , suf : x + suf , args = ( suffix ,)) S = pd . Series ([ \"abc\" , \"edf\" , \"32\" , \"Vew3\" , \"er3r2\" ] * 10 ) suffix = \"_\" add_suffix ( S , suffix ) Alternatively, arguments can be passed by keyword. @bodo . jit def add_suffix ( S , suffix ): return S . apply ( lambda x , suf : x + suf , suf = suffix ) Note Not all APIs support additional arguments. Please refer to supported Pandas API for more information on intended API usage. Apply with Pandas Methods and Numpy ufuncs \u00b6 In addition to UDFs, the apply API can also be used to call Pandas methods and Numpy ufuncs. To execute a Pandas method, you can provide the method name as a string. import pandas as pd import bodo @bodo . jit def ex ( S ): return S . apply ( \"nunique\" ) S = pd . Series ( list ( np . arange ( 100 ) + list ( np . arange ( 100 )))) ex ( S ) Numpy ufuncs can either be provided with a string matching the name or with the function itself. import numpy as np import pandas as pd import bodo @bodo . jit def ex_str ( S ): return S . apply ( \"sin\" ) def ex_func ( S ): return S . apply ( np . sin ) S = pd . Series ( list ( np . arange ( 100 ) + list ( np . arange ( 100 )))) pd . testing . assert_series_equal ( ex_str ( S ), ex_func ( S )) Note Numpy ufuncs are not currently supported with DataFrames. Type Stability Restrictions \u00b6 Bodo's type stability requirements can encounter some limitations when either using DataFrame.apply with different column types or when returning a DataFrame. Differently Typed Columns \u00b6 DataFrame.apply maps user provided UDFs to each row of the DataFrame. In the situation where a DataFrame has columns of different types, the Series passed to the UDF will contain values with different types. Bodo internally represents these as a Heterogeneous Series. This representation has limitations in the Series operations it supports. Please refer to the supported operations for heterogeneous series for more information. Returning a DataFrame \u00b6 In Pandas, Series.apply or DataFrame.apply there are multiple ways to return a DataFrame instead of a Series. However, for type stability reasons, Bodo can only infer a DataFrame when returning a Series whose size can be inferred at compile time for each row. Note If you provide an Index, then all Index values must be compile time constants. Here is an example using Series.apply to return a DataFrame. import pandas as pd import bodo @bodo . jit def series_ex ( S ): return S . apply ( lambda x : pd . Series (( 1 , x ))) S = pd . Series ( list ( np . arange ( 100 ) + list ( np . arange ( 100 )))) series_ex ( S ) If using a UDF that returns a DataFrame in Pandas through another means, this behavior will not match in Bodo and may result in a compilation error. Please convert your solution to one of the supported methods if possible.","title":"User Defined Functions (UDFs)"},{"location":"api_docs/udfs/#udfs","text":"While Pandas and other APIs can be extremely expressive, many data science and data engineering use cases require additional functionality beyond what is directly offered. In these situations, many programmers create User Defined Functions , or UDFs, which are Python functions designed to compute on each row or groups of rows depending on the context.","title":"User-Defined Functions (UDFs)"},{"location":"api_docs/udfs/#using-udfs-with-bodo","text":"Bodo users can construct UDFs either by defining a separate JIT function or by creating a function within a JIT function (either via a lambda or closure). For example, here are two ways to construct a UDF that advances each element of a Timestamp Series to the last day of the current month. import pandas as pd import bodo @bodo . jit def jit_udf ( x ): return x + pd . tseries . offsets . MonthEnd ( n = 0 , normalize = True ) @bodo . jit def jit_example ( S ): return S . map ( jit_udf ) @bodo . jit def lambda_example ( S ): return S . map ( lambda x : x + pd . tseries . offsets . MonthEnd ( n = 0 , normalize = True )) S = pd . Series ( pd . date_range ( start = '1/1/2021' , periods = 100 )) pd . testing . assert_series_equal ( jit_example ( S ), lambda_example ( S )) UDFs can be used to compute one value per row or group (map functions) or compute an aggregation (agg functions). Bodo provides APIs for both, which are summarized below. Please refer to supported Pandas API for more information.","title":"Using UDFs with Bodo"},{"location":"api_docs/udfs/#map-functions","text":"Series.map Series.apply Series.pipe DataFrame.map DataFrame.apply DataFrame.pipe GroupBy.apply GroupBy.pipe GroupBy.transform","title":"Map Functions"},{"location":"api_docs/udfs/#agg-functions","text":"GroupBy.agg GroupBy.aggregate","title":"Agg Functions"},{"location":"api_docs/udfs/#udf-performance","text":"Bodo offers support for UDFs without the significant runtime penalty generally incurred in Pandas. An example of this is shown in the quick started guide . Bodo achieves a drastic performance advantage on UDFs because UDFs can be optimized by similar to any other JIT code. In contrast, library based solutions are typically limited in their ability to optimize UDFs.","title":"UDF Performance"},{"location":"api_docs/udfs/#additional-arguments","text":"We recommend passing additional variables to UDFs explicitly, instead of directly using variables local to the function defining the UDF. The latter is called the \\\"captured\\\" variables case, which is often error-prone and may result in compilation errors. For example, consider a UDF that appends a variable suffix to each string in a Series of strings. The proper way to write this function is to use the args argument to Series.apply() . import pandas as pd import bodo @bodo . jit def add_suffix ( S , suffix ): return S . apply ( lambda x , suf : x + suf , args = ( suffix ,)) S = pd . Series ([ \"abc\" , \"edf\" , \"32\" , \"Vew3\" , \"er3r2\" ] * 10 ) suffix = \"_\" add_suffix ( S , suffix ) Alternatively, arguments can be passed by keyword. @bodo . jit def add_suffix ( S , suffix ): return S . apply ( lambda x , suf : x + suf , suf = suffix ) Note Not all APIs support additional arguments. Please refer to supported Pandas API for more information on intended API usage.","title":"Additional Arguments"},{"location":"api_docs/udfs/#apply-with-pandas-methods-and-numpy-ufuncs","text":"In addition to UDFs, the apply API can also be used to call Pandas methods and Numpy ufuncs. To execute a Pandas method, you can provide the method name as a string. import pandas as pd import bodo @bodo . jit def ex ( S ): return S . apply ( \"nunique\" ) S = pd . Series ( list ( np . arange ( 100 ) + list ( np . arange ( 100 )))) ex ( S ) Numpy ufuncs can either be provided with a string matching the name or with the function itself. import numpy as np import pandas as pd import bodo @bodo . jit def ex_str ( S ): return S . apply ( \"sin\" ) def ex_func ( S ): return S . apply ( np . sin ) S = pd . Series ( list ( np . arange ( 100 ) + list ( np . arange ( 100 )))) pd . testing . assert_series_equal ( ex_str ( S ), ex_func ( S )) Note Numpy ufuncs are not currently supported with DataFrames.","title":"Apply with Pandas Methods and Numpy ufuncs"},{"location":"api_docs/udfs/#type-stability-restrictions","text":"Bodo's type stability requirements can encounter some limitations when either using DataFrame.apply with different column types or when returning a DataFrame.","title":"Type Stability Restrictions"},{"location":"api_docs/udfs/#differently-typed-columns","text":"DataFrame.apply maps user provided UDFs to each row of the DataFrame. In the situation where a DataFrame has columns of different types, the Series passed to the UDF will contain values with different types. Bodo internally represents these as a Heterogeneous Series. This representation has limitations in the Series operations it supports. Please refer to the supported operations for heterogeneous series for more information.","title":"Differently Typed Columns"},{"location":"api_docs/udfs/#returning-a-dataframe","text":"In Pandas, Series.apply or DataFrame.apply there are multiple ways to return a DataFrame instead of a Series. However, for type stability reasons, Bodo can only infer a DataFrame when returning a Series whose size can be inferred at compile time for each row. Note If you provide an Index, then all Index values must be compile time constants. Here is an example using Series.apply to return a DataFrame. import pandas as pd import bodo @bodo . jit def series_ex ( S ): return S . apply ( lambda x : pd . Series (( 1 , x ))) S = pd . Series ( list ( np . arange ( 100 ) + list ( np . arange ( 100 )))) series_ex ( S ) If using a UDF that returns a DataFrame in Pandas through another means, this behavior will not match in Bodo and may result in a compilation error. Please convert your solution to one of the supported methods if possible.","title":"Returning a DataFrame"},{"location":"api_docs/ml/","text":"Machine Learning \u00b6 Bodo natively supports use of scikit-learn and XGBoost libraries with large-scale distributed data inside bodo.jit decorated functions.","title":"Index"},{"location":"api_docs/ml/#ml","text":"Bodo natively supports use of scikit-learn and XGBoost libraries with large-scale distributed data inside bodo.jit decorated functions.","title":"Machine Learning"},{"location":"api_docs/ml/sklearn/","text":"Scikit-learn \u00b6 Bodo supports scikit-learn versions 1.0.* . Install scikit-learn in your Bodo environment: conda install scikit-learn = '1.0.*' -c conda-forge","title":"Scikit-learn"},{"location":"api_docs/ml/sklearn/#scikit-learn","text":"Bodo supports scikit-learn versions 1.0.* . Install scikit-learn in your Bodo environment: conda install scikit-learn = '1.0.*' -c conda-forge","title":"Scikit-learn"},{"location":"api_docs/ml/sklearn/cluster/","text":"sklearn.cluster: Clustering\u00b6 \u00b6 sklearn.cluster.KMeans \u00b6 sklearn.cluster. KMeans This class provides K-Means clustering model. Important Currently, this model works by gathering all the data in a single node and then generating K-Means model. Make sure you have enough memory on the first node in your hostfile. Methods \u00b6 sklearn.cluster.KMeans.fit \u00b6 sklearn.cluster.KMeans. fit (X, y=None, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. sample_weight : Numeric NumPy Array Note Bodo ignores y , which is consistent with scikit-learn. sklearn.cluster.KMeans.predict \u00b6 sklearn.cluster.KMeans. predict (X, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. sample_weight : Numeric NumPy Array sklearn.cluster.KMeans.score \u00b6 sklearn.cluster.KMeans. score (X, y=None, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. sample_weight : Numeric NumPy Array Note Bodo ignores y, which is consistent with scikit-learn. sklearn.cluster.KMeans.transform \u00b6 sklearn.cluster.KMeans. transform (X) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. Example Usage \u00b6 >>> import bodo >>> from sklearn.cluster import KMeans >>> import numpy as np >>> @bodo . jit >>> def test_kmeans ( X ): ... kmeans = KMeans ( n_clusters = 2 ) ... kmeans . fit ( X ) ... ans = kmeans . predict ([[ 0 , 0 ], [ 12 , 3 ]]) ... print ( ans ) ... >>> X = np . array ([[ 1 , 2 ], [ 1 , 4 ], [ 1 , 0 ], [ 10 , 2 ], [ 10 , 4 ], [ 10 , 0 ]]) >>> test_kmeans ( X ) [ 1 0 ]","title":"sklearn.cluster"},{"location":"api_docs/ml/sklearn/cluster/#sklearncluster-clustering","text":"","title":"sklearn.cluster: Clustering\u00b6"},{"location":"api_docs/ml/sklearn/cluster/#sklearnclusterkmeans","text":"sklearn.cluster. KMeans This class provides K-Means clustering model. Important Currently, this model works by gathering all the data in a single node and then generating K-Means model. Make sure you have enough memory on the first node in your hostfile.","title":"sklearn.cluster.KMeans"},{"location":"api_docs/ml/sklearn/cluster/#methods","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/cluster/#sklearnclusterkmeansfit","text":"sklearn.cluster.KMeans. fit (X, y=None, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. sample_weight : Numeric NumPy Array Note Bodo ignores y , which is consistent with scikit-learn.","title":"sklearn.cluster.KMeans.fit"},{"location":"api_docs/ml/sklearn/cluster/#sklearnclusterkmeanspredict","text":"sklearn.cluster.KMeans. predict (X, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. sample_weight : Numeric NumPy Array","title":"sklearn.cluster.KMeans.predict"},{"location":"api_docs/ml/sklearn/cluster/#sklearnclusterkmeansscore","text":"sklearn.cluster.KMeans. score (X, y=None, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. sample_weight : Numeric NumPy Array Note Bodo ignores y, which is consistent with scikit-learn.","title":"sklearn.cluster.KMeans.score"},{"location":"api_docs/ml/sklearn/cluster/#sklearnclusterkmeanstransform","text":"sklearn.cluster.KMeans. transform (X) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix.","title":"sklearn.cluster.KMeans.transform"},{"location":"api_docs/ml/sklearn/cluster/#example-usage","text":">>> import bodo >>> from sklearn.cluster import KMeans >>> import numpy as np >>> @bodo . jit >>> def test_kmeans ( X ): ... kmeans = KMeans ( n_clusters = 2 ) ... kmeans . fit ( X ) ... ans = kmeans . predict ([[ 0 , 0 ], [ 12 , 3 ]]) ... print ( ans ) ... >>> X = np . array ([[ 1 , 2 ], [ 1 , 4 ], [ 1 , 0 ], [ 10 , 2 ], [ 10 , 4 ], [ 10 , 0 ]]) >>> test_kmeans ( X ) [ 1 0 ]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/ensemble/","text":"sklearn.ensemble \u00b6 sklearn.ensemble.RandomForestClassifier \u00b6 sklearn.ensemble. RandomForestClassifier This class provides Random Forest Classifier, an ensemble learning model, for distributed large-scale learning. Important random_state value is ignored when running on a multi-node cluster. Methods \u00b6 sklearn.ensemble.RandomForestClassifier.fit \u00b6 sklearn.ensemble.RandomForestClassifier. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. y : NumPy Array sample_weight : Numeric NumPy Array (only if data is not distributed) sklearn.ensemble.RandomForestClassifier.predict \u00b6 sklearn.ensemble.RandomForestClassifier. predict (X) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. sklearn.ensemble.RandomForestClassifier.predict_log_proba \u00b6 sklearn.ensemble.RandomForestClassifier. predict_log_proba (X) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. sklearn.ensemble.RandomForestClassifier.predict_proba \u00b6 sklearn.ensemble.RandomForestClassifier. predict_proba (X) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. sklearn.ensemble.RandomForestClassifier.score \u00b6 sklearn.ensemble.RandomForestClassifier. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array sample_weight : Numeric NumPy Array Example Usage \u00b6 >>> import bodo >>> from sklearn.ensemble import RandomForestClassifier >>> from sklearn.datasets import make_classification >>> X , y = make_classification ( n_samples = 1000 , n_features = 4 , ... n_informative = 2 , n_redundant = 0 , ... random_state = 0 , shuffle = False ) >>> @bodo . jit >>> def test_random_forest_classifier ( X , y ): ... clf = RandomForestClassifier ( max_depth = 2 ) ... clf . fit ( X , y ) ... ans = clf . predict ( np . array ([[ 0 , 0 , 0 , 0 ]])) ... print ( ans ) ... >>> test_random_forest_classifier ( X , y ) [ 1 ] sklearn.ensemble.RandomForestRegressor \u00b6 sklearn.ensemble. RandomForestRegressor This class provides Random Forest Regressor, an ensemble learning model, for distributed large-scale learning. Important random_state value is ignored when running on a multi-node cluster. Methods \u00b6 sklearn.ensemble.RandomForestRegressor.fit \u00b6 sklearn.ensemble.RandomForestRegressor. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. y : NumPy Array sample_weight : Numeric NumPy Array (only if data is not distributed) sklearn.ensemble.RandomForestRegressor.predict \u00b6 sklearn.ensemble.RandomForestRegressor. predict (X) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. sklearn.ensemble.RandomForestRegressor.score \u00b6 sklearn.ensemble.RandomForestRegressor. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. y : NumPy Array sample_weight : Numeric NumPy Array Example Usage \u00b6 >>> import bodo >>> from sklearn.ensemble import RandomForestRegressor >>> from sklearn.datasets import make_regression >>> X , y = make_regression ( n_features = 4 , n_informative = 2 , ... random_state = 0 , shuffle = False ) >>> @bodo . jit >>> def test_random_forest_regressor ( X , y ): ... regr = RandomForestRegressor ( max_depth = 2 ) ... regr . fit ( X , y ) ... ans = regr . predict ( np . array ([[ 0 , 0 , 0 , 0 ]])) ... print ( ans ) ... >>> test_random_forest_regressor ( X , y ) [ - 6.7933243 ]","title":"sklearn.ensemble"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemble","text":"","title":"sklearn.ensemble"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifier","text":"sklearn.ensemble. RandomForestClassifier This class provides Random Forest Classifier, an ensemble learning model, for distributed large-scale learning. Important random_state value is ignored when running on a multi-node cluster.","title":"sklearn.ensemble.RandomForestClassifier"},{"location":"api_docs/ml/sklearn/ensemble/#methods","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifierfit","text":"sklearn.ensemble.RandomForestClassifier. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. y : NumPy Array sample_weight : Numeric NumPy Array (only if data is not distributed)","title":"sklearn.ensemble.RandomForestClassifier.fit"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifierpredict","text":"sklearn.ensemble.RandomForestClassifier. predict (X) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix.","title":"sklearn.ensemble.RandomForestClassifier.predict"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifierpredict_log_proba","text":"sklearn.ensemble.RandomForestClassifier. predict_log_proba (X) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix.","title":"sklearn.ensemble.RandomForestClassifier.predict_log_proba"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifierpredict_proba","text":"sklearn.ensemble.RandomForestClassifier. predict_proba (X) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix.","title":"sklearn.ensemble.RandomForestClassifier.predict_proba"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifierscore","text":"sklearn.ensemble.RandomForestClassifier. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array sample_weight : Numeric NumPy Array","title":"sklearn.ensemble.RandomForestClassifier.score"},{"location":"api_docs/ml/sklearn/ensemble/#example-usage","text":">>> import bodo >>> from sklearn.ensemble import RandomForestClassifier >>> from sklearn.datasets import make_classification >>> X , y = make_classification ( n_samples = 1000 , n_features = 4 , ... n_informative = 2 , n_redundant = 0 , ... random_state = 0 , shuffle = False ) >>> @bodo . jit >>> def test_random_forest_classifier ( X , y ): ... clf = RandomForestClassifier ( max_depth = 2 ) ... clf . fit ( X , y ) ... ans = clf . predict ( np . array ([[ 0 , 0 , 0 , 0 ]])) ... print ( ans ) ... >>> test_random_forest_classifier ( X , y ) [ 1 ]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestregressor","text":"sklearn.ensemble. RandomForestRegressor This class provides Random Forest Regressor, an ensemble learning model, for distributed large-scale learning. Important random_state value is ignored when running on a multi-node cluster.","title":"sklearn.ensemble.RandomForestRegressor"},{"location":"api_docs/ml/sklearn/ensemble/#methods_1","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestregressorfit","text":"sklearn.ensemble.RandomForestRegressor. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. y : NumPy Array sample_weight : Numeric NumPy Array (only if data is not distributed)","title":"sklearn.ensemble.RandomForestRegressor.fit"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestregressorpredict","text":"sklearn.ensemble.RandomForestRegressor. predict (X) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix.","title":"sklearn.ensemble.RandomForestRegressor.predict"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestregressorscore","text":"sklearn.ensemble.RandomForestRegressor. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array, Pandas Dataframes, or CSR sparse matrix. y : NumPy Array sample_weight : Numeric NumPy Array","title":"sklearn.ensemble.RandomForestRegressor.score"},{"location":"api_docs/ml/sklearn/ensemble/#example-usage_1","text":">>> import bodo >>> from sklearn.ensemble import RandomForestRegressor >>> from sklearn.datasets import make_regression >>> X , y = make_regression ( n_features = 4 , n_informative = 2 , ... random_state = 0 , shuffle = False ) >>> @bodo . jit >>> def test_random_forest_regressor ( X , y ): ... regr = RandomForestRegressor ( max_depth = 2 ) ... regr . fit ( X , y ) ... ans = regr . predict ( np . array ([[ 0 , 0 , 0 , 0 ]])) ... print ( ans ) ... >>> test_random_forest_regressor ( X , y ) [ - 6.7933243 ]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/feature_extraction/","text":"sklearn.feature_extraction \u00b6 sklearn.feature_extraction.text.CountVectorizer \u00b6 sklearn.feature_extraction.text. CountVectorizer This class provides CountVectorizer support to convert a collection of text documents to a matrix of token counts. Note Arguments max_df and min_df are not supported yet. Methods \u00b6 sklearn.feature_extraction.text.CountVectorizer.fit_transform \u00b6 sklearn.feature_extraction.text.CountVectorizer. fit_transform ( raw_documents, y=None ) Supported Arguments raw_documents : iterables ( list, tuple, or NumPy Array, or Pandas Series that contains string) Note Bodo ignores y , which is consistent with scikit-learn. sklearn.feature_extraction.text.CountVectorizer.get_feature_names_out \u00b6 sklearn.feature_extraction.text.CountVectorizer. get_feature_names_out () Example Usage \u00b6 >>> import bodo >>> from sklearn.feature_extraction.text import CountVectorizer >>> corpus = [ ... 'This is the first document.' , ... 'This document is the second document.' , ... 'And this is the third one.' , ... 'Is this the first document?' , ... ] >>> @bodo . jit >>> def test_count_vectorizer ( corpus ): >>> vectorizer = CountVectorizer () >>> X = vectorizer . fit_transform ( corpus ) >>> print ( vectorizer . get_feature_names_out ()) ... >>> test_count_vectorizer ( corpus ) [ 'and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this' ] sklearn.feature_extraction.text.HashingVectorizer \u00b6 sklearn.feature_extraction.text. HashingVectorizer This class provides HashingVectorizer support to convert a collection of text documents to a matrix of token occurrences. Methods \u00b6 sklearn.feature_extraction.text.HashingVectorizer.fit_transform \u00b6 sklearn.feature_extraction.text.HashingVectorizer. fit_transform (X, y=None) Supported Arguments X : iterables ( list, tuple, or NumPy Array, or Pandas Series that contains string) Note Bodo ignores y , which is consistent with scikit-learn. Example Usage \u00b6 >>> import bodo >>> from sklearn.feature_extraction.text import HashingVectorizer >>> corpus = [ ... 'This is the first document.' , ... 'This document is the second document.' , ... 'And this is the third one.' , ... 'Is this the first document?' , ... ] >>> @bodo . jit >>> def test_hashing_vectorizer ( corpus ): >>> vectorizer = HashingVectorizer ( n_features = 2 ** 4 ) >>> X = vectorizer . fit_transform ( corpus ) >>> print ( X . shape ) ... >>> test_hashing_vectorizer ( corpus ) ( 4 , 16 )","title":"sklearn.feature_extraction"},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extraction","text":"","title":"sklearn.feature_extraction"},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extractiontextcountvectorizer","text":"sklearn.feature_extraction.text. CountVectorizer This class provides CountVectorizer support to convert a collection of text documents to a matrix of token counts. Note Arguments max_df and min_df are not supported yet.","title":"sklearn.feature_extraction.text.CountVectorizer"},{"location":"api_docs/ml/sklearn/feature_extraction/#methods","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extractiontextcountvectorizerfit_transform","text":"sklearn.feature_extraction.text.CountVectorizer. fit_transform ( raw_documents, y=None ) Supported Arguments raw_documents : iterables ( list, tuple, or NumPy Array, or Pandas Series that contains string) Note Bodo ignores y , which is consistent with scikit-learn.","title":"sklearn.feature_extraction.text.CountVectorizer.fit_transform"},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extractiontextcountvectorizerget_feature_names_out","text":"sklearn.feature_extraction.text.CountVectorizer. get_feature_names_out ()","title":"sklearn.feature_extraction.text.CountVectorizer.get_feature_names_out"},{"location":"api_docs/ml/sklearn/feature_extraction/#example-usage","text":">>> import bodo >>> from sklearn.feature_extraction.text import CountVectorizer >>> corpus = [ ... 'This is the first document.' , ... 'This document is the second document.' , ... 'And this is the third one.' , ... 'Is this the first document?' , ... ] >>> @bodo . jit >>> def test_count_vectorizer ( corpus ): >>> vectorizer = CountVectorizer () >>> X = vectorizer . fit_transform ( corpus ) >>> print ( vectorizer . get_feature_names_out ()) ... >>> test_count_vectorizer ( corpus ) [ 'and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this' ]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extractiontexthashingvectorizer","text":"sklearn.feature_extraction.text. HashingVectorizer This class provides HashingVectorizer support to convert a collection of text documents to a matrix of token occurrences.","title":"sklearn.feature_extraction.text.HashingVectorizer"},{"location":"api_docs/ml/sklearn/feature_extraction/#methods_1","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extractiontexthashingvectorizerfit_transform","text":"sklearn.feature_extraction.text.HashingVectorizer. fit_transform (X, y=None) Supported Arguments X : iterables ( list, tuple, or NumPy Array, or Pandas Series that contains string) Note Bodo ignores y , which is consistent with scikit-learn.","title":"sklearn.feature_extraction.text.HashingVectorizer.fit_transform"},{"location":"api_docs/ml/sklearn/feature_extraction/#example-usage_1","text":">>> import bodo >>> from sklearn.feature_extraction.text import HashingVectorizer >>> corpus = [ ... 'This is the first document.' , ... 'This document is the second document.' , ... 'And this is the third one.' , ... 'Is this the first document?' , ... ] >>> @bodo . jit >>> def test_hashing_vectorizer ( corpus ): >>> vectorizer = HashingVectorizer ( n_features = 2 ** 4 ) >>> X = vectorizer . fit_transform ( corpus ) >>> print ( X . shape ) ... >>> test_hashing_vectorizer ( corpus ) ( 4 , 16 )","title":"Example Usage"},{"location":"api_docs/ml/sklearn/linear_model/","text":"sklearn.linear_model \u00b6 sklearn.linear_model.Lasso \u00b6 sklearn.linear_model. Lasso This class provides Lasso regression support. Methods \u00b6 sklearn.linear_model.Lasso.fit \u00b6 sklearn.linear_model.Lasso. fit (X, y, sample_weight=None, check_input=True) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed) sklearn.linear_model.Lasso.predict \u00b6 sklearn.linear_model.Lasso. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.linear_model.Lasso.score \u00b6 sklearn.linear_model.Lasso. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes. Example Usage \u00b6 >>> import bodo >>> from sklearn.linear_model import Lasso >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.datasets import make_regression >>> X , y = make_regression ( ... n_samples = 10 , ... n_features = 10 , ... n_informative = 5 , ... ) >>> @bodo . jit ... def test_lasso ( X , y ): ... scaler = StandardScaler () ... scaler . fit ( X ) ... X = scaler . transform ( X ) ... reg = Lasso ( alpha = 0.1 ) ... reg . fit ( X , y ) ... ans = reg . predict ( X ) ... print ( ans ) ... print ( \"score: \" , reg . score ( X , y )) ... >>> test_lasso ( X , y ) [ - 108.40717491 - 92.14977392 - 54.82835898 - 52.81762142 291.33173703 60.60660979 128.64172956 30.42129155 110.20607814 58.05321319 ] score : 0.9999971902794988 sklearn.linear_model.LinearRegression \u00b6 sklearn.linear_model. LinearRegression This class provides linear regression support. Note Multilabel targets are not currently supported. Methods \u00b6 sklearn.linear_model.LinearRegression.fit \u00b6 sklearn.linear_model.LinearRegression. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed) sklearn.linear_model.LinearRegression.predict \u00b6 sklearn.linear_model.LinearRegression. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.linear_model.LinearRegression.score \u00b6 sklearn.linear_model.LinearRegression. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes. Attributes \u00b6 sklearn.linear_model.LinearRegression.coef_ \u00b6 sklearn.linear_model.LinearRegression. coef_ Example Usage \u00b6 >>> import bodo >>> from sklearn.linear_model import LinearRegression >>> import numpy as np >>> X = np . array ([[ 1 , 1 ], [ 1 , 2 ], [ 2 , 2 ], [ 2 , 3 ]]) >>> y = np . dot ( X , np . array ([ 1 , 2 ])) + 3 >>> @bodo . jit ... def test_linear_reg ( X , y ): ... reg = LinearRegression () ... reg . fit ( X , y ) ... print ( \"score: \" , reg . score ( X , y )) ... print ( \"coef_: \" , reg . coef_ ) ... ans = reg . predict ( np . array ([[ 3 , 5 ]])) ... print ( ans ) ... >>> test_linear_reg ( X , y ) score : 1.0 coef_ : [ 1. 2. ] [ 16. ] sklearn.linear_model.LogisticRegression \u00b6 sklearn.linear_model. LogisticRegression This class provides logistic regression classifier. Note Bodo uses Stochastic Gradient Descent (SGD) to train linear models across multiple nodes in a distributed fashion. This produces models that have similar accuracy compared to their corresponding sequential version in most cases. To achieve that, it is highly recommended to scale your data using StandardScaler before training and/or testing the model. See scikit-learn for more tips on how to tune model parameters for SGD here . Methods \u00b6 sklearn.linear_model.LogisticRegression.fit \u00b6 sklearn.linear_model.LogisticRegression. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed) sklearn.linear_model.LogisticRegression.predict \u00b6 sklearn.linear_model.LogisticRegression. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.linear_model.LogisticRegression.predict_log_proba \u00b6 sklearn.linear_model.LogisticRegression. predict_log_proba (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.linear_model.LogisticRegression.predict_proba \u00b6 sklearn.linear_model.LogisticRegression. predict_proba (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.linear_model.LogisticRegression.score \u00b6 sklearn.linear_model.LogisticRegression. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes. Attributes \u00b6 sklearn.linear_model.LogisticRegression.coef_ \u00b6 sklearn.linear_model.LogisticRegression. coef_ Example Usage \u00b6 >>> import bodo >>> from sklearn.datasets import make_classification >>> from sklearn.linear_model import LogisticRegression >>> X , y = make_classification ( ... n_samples = 1000 , ... n_features = 10 , ... n_informative = 5 , ... n_redundant = 0 , ... random_state = 0 , ... shuffle = 0 , ... n_classes = 2 , ... n_clusters_per_class = 1 ... ) >>> @bodo . jit ... def test_logistic ( X , y ): ... clf = LogisticRegression () ... clf . fit ( X , y ) ... ans = clf . predict ( X ) ... print ( \"score: \" , clf . score ( X , y )) ... >>> test_logistic ( X , y ) score : 0.997 sklearn.linear_model.Ridge \u00b6 sklearn.linear_model. Ridge This class provides ridge regression support. Methods \u00b6 sklearn.linear_model.Ridge.fit \u00b6 sklearn.linear_model.Ridge. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed) sklearn.linear_model.Ridge.predict \u00b6 sklearn.linear_model.Ridge. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.linear_model.Ridge.score \u00b6 sklearn.linear_model.Ridge. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes. Attributes \u00b6 sklearn.linear_model.Ridge.coef_ \u00b6 sklearn.linear_model.Ridge. coef_ Example Usage \u00b6 >>> import bodo >>> from sklearn.linear_model import Ridge >>> from sklearn.datasets import make_regression >>> X , y = make_regression ( ... n_samples = 1000 , ... n_features = 10 , ... n_informative = 5 , ... ) >>> @bodo . jit ... def test_ridge ( X , y ): ... reg = Ridge ( alpha = 1.0 ) ... reg . fit ( X , y ) ... print ( \"score: \" , reg . score ( X , y )) ... print ( \"coef_: \" , reg . coef_ ) ... >>> test_ridge ( X , y ) score : 0.999998857191076 coef_ : [ 1.07963671e-03 2.35051611e+01 9.46672751e+01 8.01581769e-03 3.66612234e+01 5.82527987e-03 2.60885671e+01 - 3.49454103e-03 8.39573884e+01 - 7.52605483e-03 ] sklearn.linear_model.SGDClassifier \u00b6 sklearn.linear_model.SGDClassifier This class provides linear classification models with SGD optimization which allows distributed large-scale learning. Supported loss functions hinge and log . SGDClassifier(loss='hinge') is equivalent to SVM linear classifer . SGDClassifier(loss='log') is equivalent to logistic regression classifer . early_stopping is not supported yet. Methods \u00b6 sklearn.linear_model.SGDClassifier.fit \u00b6 sklearn.linear_model.SGDClassifier. fit (X, y, coef_init=None, intercept_init=None, sample_weight=None) Supported Arguments - X : NumPy Array or Pandas Dataframes. - y : NumPy Array. - sample_weight : Numeric NumPy Array (only if data is not distributed) sklearn.linear_model.SGDClassifier.predict \u00b6 sklearn.linear_model.SGDClassifier. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.linear_model.SGDClassifier.predict_log_proba \u00b6 sklearn.linear_model.SGDClassifier. predict_log_proba (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.linear_model.SGDClassifier.predict_proba \u00b6 sklearn.linear_model.SGDClassifier. predict_proba (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.linear_model.SGDClassifier.score \u00b6 sklearn.linear_model.SGDClassifier.score(X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes. Attributes \u00b6 sklearn.linear_model.SGDClassifier.coef_ \u00b6 sklearn.linear_model.SGDClassifier. coef_ Example Usage \u00b6 >>> import bodo >>> from sklearn.linear_model import SGDClassifier >>> from sklearn.preprocessing import StandardScaler >>> import numpy as np >>> X = np . array ([[ - 1 , - 1 ], [ - 2 , - 1 ], [ 1 , 1 ], [ 2 , 1 ]]) >>> y = np . array ([ 1 , 1 , 2 , 2 ]) >>> @bodo . jit ... def test_sgdclassifier ( X , y ): ... scaler = StandardScaler () ... scaler . fit ( X ) ... X = scaler . transform ( X ) ... clf = SGDClassifier ( loss = \"hinge\" , penalty = \"l2\" ) ... clf . fit ( X , y ) ... ans = clf . predict ( np . array ([[ - 0.8 , - 1 ]])) ... print ( ans ) ... print ( \"coef_: \" , clf . coef_ ) ... >>> test_sgdclassifier ( X , y ) [ 1 ] coef_ : [[ 6.18236102 9.77517107 ]] sklearn.linear_model.SGDRegressor \u00b6 sklearn.linear_model. SGDRegressor This class provides linear regression models with SGD optimization which allows distributed large-scale learning. Supported loss function is squared_error . early_stopping is not supported yet. SGDRegressor(loss='squared_error', penalty='None') is equivalent to linear regression . SGDRegressor(loss='squared_error', penalty='l2') is equivalent to Ridge regression . SGDRegressor(loss='squared_error', penalty='l1') is equivalent to Lasso regression . Methods \u00b6 sklearn.linear_model.SGDRegressor.fit \u00b6 sklearn.linear_model.SGDRegressor. fit (X, y, coef_init=None, intercept_init=None, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed) sklearn.linear_model.SGDRegressor.predict \u00b6 sklearn.linear_model.SGDRegressor. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.linear_model.SGDRegressor.score \u00b6 sklearn.linear_model.SGDRegressor. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes. Example Usage \u00b6 >>> import bodo >>> from sklearn.linear_model import SGDRegressor >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.datasets import make_regression >>> X , y = make_regression ( ... n_samples = 1000 , ... n_features = 10 , ... n_informative = 5 , ... ) >>> @bodo . jit ... def test_sgd_reg ( X , y ): ... scaler = StandardScaler () ... scaler . fit ( X ) ... X = scaler . transform ( X ) ... reg = SGDRegressor () ... reg . fit ( X , y ) ... print ( \"score: \" , reg . score ( X , y )) ... >>> test_sgd_reg ( X , y ) 0.9999999836265652","title":"sklearn.linear_model"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_model","text":"","title":"sklearn.linear_model"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellasso","text":"sklearn.linear_model. Lasso This class provides Lasso regression support.","title":"sklearn.linear_model.Lasso"},{"location":"api_docs/ml/sklearn/linear_model/#methods","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellassofit","text":"sklearn.linear_model.Lasso. fit (X, y, sample_weight=None, check_input=True) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed)","title":"sklearn.linear_model.Lasso.fit"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellassopredict","text":"sklearn.linear_model.Lasso. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.Lasso.predict"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellassoscore","text":"sklearn.linear_model.Lasso. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.Lasso.score"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage","text":">>> import bodo >>> from sklearn.linear_model import Lasso >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.datasets import make_regression >>> X , y = make_regression ( ... n_samples = 10 , ... n_features = 10 , ... n_informative = 5 , ... ) >>> @bodo . jit ... def test_lasso ( X , y ): ... scaler = StandardScaler () ... scaler . fit ( X ) ... X = scaler . transform ( X ) ... reg = Lasso ( alpha = 0.1 ) ... reg . fit ( X , y ) ... ans = reg . predict ( X ) ... print ( ans ) ... print ( \"score: \" , reg . score ( X , y )) ... >>> test_lasso ( X , y ) [ - 108.40717491 - 92.14977392 - 54.82835898 - 52.81762142 291.33173703 60.60660979 128.64172956 30.42129155 110.20607814 58.05321319 ] score : 0.9999971902794988","title":"Example Usage"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellinearregression","text":"sklearn.linear_model. LinearRegression This class provides linear regression support. Note Multilabel targets are not currently supported.","title":"sklearn.linear_model.LinearRegression"},{"location":"api_docs/ml/sklearn/linear_model/#methods_1","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellinearregressionfit","text":"sklearn.linear_model.LinearRegression. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed)","title":"sklearn.linear_model.LinearRegression.fit"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellinearregressionpredict","text":"sklearn.linear_model.LinearRegression. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.LinearRegression.predict"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellinearregressionscore","text":"sklearn.linear_model.LinearRegression. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.LinearRegression.score"},{"location":"api_docs/ml/sklearn/linear_model/#attributes","text":"","title":"Attributes"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellinearregressioncoef_","text":"sklearn.linear_model.LinearRegression. coef_","title":"sklearn.linear_model.LinearRegression.coef_"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage_1","text":">>> import bodo >>> from sklearn.linear_model import LinearRegression >>> import numpy as np >>> X = np . array ([[ 1 , 1 ], [ 1 , 2 ], [ 2 , 2 ], [ 2 , 3 ]]) >>> y = np . dot ( X , np . array ([ 1 , 2 ])) + 3 >>> @bodo . jit ... def test_linear_reg ( X , y ): ... reg = LinearRegression () ... reg . fit ( X , y ) ... print ( \"score: \" , reg . score ( X , y )) ... print ( \"coef_: \" , reg . coef_ ) ... ans = reg . predict ( np . array ([[ 3 , 5 ]])) ... print ( ans ) ... >>> test_linear_reg ( X , y ) score : 1.0 coef_ : [ 1. 2. ] [ 16. ]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregression","text":"sklearn.linear_model. LogisticRegression This class provides logistic regression classifier. Note Bodo uses Stochastic Gradient Descent (SGD) to train linear models across multiple nodes in a distributed fashion. This produces models that have similar accuracy compared to their corresponding sequential version in most cases. To achieve that, it is highly recommended to scale your data using StandardScaler before training and/or testing the model. See scikit-learn for more tips on how to tune model parameters for SGD here .","title":"sklearn.linear_model.LogisticRegression"},{"location":"api_docs/ml/sklearn/linear_model/#methods_2","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressionfit","text":"sklearn.linear_model.LogisticRegression. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed)","title":"sklearn.linear_model.LogisticRegression.fit"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressionpredict","text":"sklearn.linear_model.LogisticRegression. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.LogisticRegression.predict"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressionpredict_log_proba","text":"sklearn.linear_model.LogisticRegression. predict_log_proba (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.LogisticRegression.predict_log_proba"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressionpredict_proba","text":"sklearn.linear_model.LogisticRegression. predict_proba (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.LogisticRegression.predict_proba"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressionscore","text":"sklearn.linear_model.LogisticRegression. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.LogisticRegression.score"},{"location":"api_docs/ml/sklearn/linear_model/#attributes_1","text":"","title":"Attributes"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressioncoef_","text":"sklearn.linear_model.LogisticRegression. coef_","title":"sklearn.linear_model.LogisticRegression.coef_"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage_2","text":">>> import bodo >>> from sklearn.datasets import make_classification >>> from sklearn.linear_model import LogisticRegression >>> X , y = make_classification ( ... n_samples = 1000 , ... n_features = 10 , ... n_informative = 5 , ... n_redundant = 0 , ... random_state = 0 , ... shuffle = 0 , ... n_classes = 2 , ... n_clusters_per_class = 1 ... ) >>> @bodo . jit ... def test_logistic ( X , y ): ... clf = LogisticRegression () ... clf . fit ( X , y ) ... ans = clf . predict ( X ) ... print ( \"score: \" , clf . score ( X , y )) ... >>> test_logistic ( X , y ) score : 0.997","title":"Example Usage"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelridge","text":"sklearn.linear_model. Ridge This class provides ridge regression support.","title":"sklearn.linear_model.Ridge"},{"location":"api_docs/ml/sklearn/linear_model/#methods_3","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelridgefit","text":"sklearn.linear_model.Ridge. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed)","title":"sklearn.linear_model.Ridge.fit"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelridgepredict","text":"sklearn.linear_model.Ridge. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.Ridge.predict"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelridgescore","text":"sklearn.linear_model.Ridge. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.Ridge.score"},{"location":"api_docs/ml/sklearn/linear_model/#attributes_2","text":"","title":"Attributes"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelridgecoef_","text":"sklearn.linear_model.Ridge. coef_","title":"sklearn.linear_model.Ridge.coef_"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage_3","text":">>> import bodo >>> from sklearn.linear_model import Ridge >>> from sklearn.datasets import make_regression >>> X , y = make_regression ( ... n_samples = 1000 , ... n_features = 10 , ... n_informative = 5 , ... ) >>> @bodo . jit ... def test_ridge ( X , y ): ... reg = Ridge ( alpha = 1.0 ) ... reg . fit ( X , y ) ... print ( \"score: \" , reg . score ( X , y )) ... print ( \"coef_: \" , reg . coef_ ) ... >>> test_ridge ( X , y ) score : 0.999998857191076 coef_ : [ 1.07963671e-03 2.35051611e+01 9.46672751e+01 8.01581769e-03 3.66612234e+01 5.82527987e-03 2.60885671e+01 - 3.49454103e-03 8.39573884e+01 - 7.52605483e-03 ]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifier","text":"sklearn.linear_model.SGDClassifier This class provides linear classification models with SGD optimization which allows distributed large-scale learning. Supported loss functions hinge and log . SGDClassifier(loss='hinge') is equivalent to SVM linear classifer . SGDClassifier(loss='log') is equivalent to logistic regression classifer . early_stopping is not supported yet.","title":"sklearn.linear_model.SGDClassifier"},{"location":"api_docs/ml/sklearn/linear_model/#methods_4","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifierfit","text":"sklearn.linear_model.SGDClassifier. fit (X, y, coef_init=None, intercept_init=None, sample_weight=None) Supported Arguments - X : NumPy Array or Pandas Dataframes. - y : NumPy Array. - sample_weight : Numeric NumPy Array (only if data is not distributed)","title":"sklearn.linear_model.SGDClassifier.fit"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifierpredict","text":"sklearn.linear_model.SGDClassifier. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.SGDClassifier.predict"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifierpredict_log_proba","text":"sklearn.linear_model.SGDClassifier. predict_log_proba (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.SGDClassifier.predict_log_proba"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifierpredict_proba","text":"sklearn.linear_model.SGDClassifier. predict_proba (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.SGDClassifier.predict_proba"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifierscore","text":"sklearn.linear_model.SGDClassifier.score(X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.SGDClassifier.score"},{"location":"api_docs/ml/sklearn/linear_model/#attributes_3","text":"","title":"Attributes"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifiercoef_","text":"sklearn.linear_model.SGDClassifier. coef_","title":"sklearn.linear_model.SGDClassifier.coef_"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage_4","text":">>> import bodo >>> from sklearn.linear_model import SGDClassifier >>> from sklearn.preprocessing import StandardScaler >>> import numpy as np >>> X = np . array ([[ - 1 , - 1 ], [ - 2 , - 1 ], [ 1 , 1 ], [ 2 , 1 ]]) >>> y = np . array ([ 1 , 1 , 2 , 2 ]) >>> @bodo . jit ... def test_sgdclassifier ( X , y ): ... scaler = StandardScaler () ... scaler . fit ( X ) ... X = scaler . transform ( X ) ... clf = SGDClassifier ( loss = \"hinge\" , penalty = \"l2\" ) ... clf . fit ( X , y ) ... ans = clf . predict ( np . array ([[ - 0.8 , - 1 ]])) ... print ( ans ) ... print ( \"coef_: \" , clf . coef_ ) ... >>> test_sgdclassifier ( X , y ) [ 1 ] coef_ : [[ 6.18236102 9.77517107 ]]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdregressor","text":"sklearn.linear_model. SGDRegressor This class provides linear regression models with SGD optimization which allows distributed large-scale learning. Supported loss function is squared_error . early_stopping is not supported yet. SGDRegressor(loss='squared_error', penalty='None') is equivalent to linear regression . SGDRegressor(loss='squared_error', penalty='l2') is equivalent to Ridge regression . SGDRegressor(loss='squared_error', penalty='l1') is equivalent to Lasso regression .","title":"sklearn.linear_model.SGDRegressor"},{"location":"api_docs/ml/sklearn/linear_model/#methods_5","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdregressorfit","text":"sklearn.linear_model.SGDRegressor. fit (X, y, coef_init=None, intercept_init=None, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed)","title":"sklearn.linear_model.SGDRegressor.fit"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdregressorpredict","text":"sklearn.linear_model.SGDRegressor. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.SGDRegressor.predict"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdregressorscore","text":"sklearn.linear_model.SGDRegressor. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes.","title":"sklearn.linear_model.SGDRegressor.score"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage_5","text":">>> import bodo >>> from sklearn.linear_model import SGDRegressor >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.datasets import make_regression >>> X , y = make_regression ( ... n_samples = 1000 , ... n_features = 10 , ... n_informative = 5 , ... ) >>> @bodo . jit ... def test_sgd_reg ( X , y ): ... scaler = StandardScaler () ... scaler . fit ( X ) ... X = scaler . transform ( X ) ... reg = SGDRegressor () ... reg . fit ( X , y ) ... print ( \"score: \" , reg . score ( X , y )) ... >>> test_sgd_reg ( X , y ) 0.9999999836265652","title":"Example Usage"},{"location":"api_docs/ml/sklearn/metrics/","text":"sklearn.metrics \u00b6 sklearn.metrics.accuracy_score \u00b6 sklearn.metrics. accuracy_score (y_true, y_pred, normalize=True, sample_weight=None) Supported Arguments y_true : 1d array-like. y_pred : 1d array-like. normalize : bool. sample_weight : 1d numeric array-like or None. Note y_true , y_pred , and sample_weight (if provided) must be of same length. Example Usage >>> import bodo >>> import numpy as np >>> from sklearn.metrics import accuracy_score >>> y_pred = np . array ([ 0 , 2 , 1 , 3 ]) >>> y_true = np . array ([ 0 , 1 , 2 , 3 ]) >>> @bodo . jit >>> def test_accuracy_score ( y_true , y_pred ): ... print ( accuracy_score ( y_true , y_pred )) >>> test_accuracy_score ( y_true , y_pred ) 0.5 sklearn.metrics.confusion_matrix \u00b6 sklearn.metrics. confusion_matrix (y_true, y_pred, labels=None, sample_weight=None, normalize=None) Supported Arguments y_true : 1d array-like. y_pred : 1d array-like. labels : 1d array-like. sample_weight : 1d numeric array-like or None . normalize : Must be one of 'true' , 'pred' , 'all' , or None Note y_true , y_pred , and sample_weight (if provided) must be of same length. Example Usage >>> import bodo >>> from sklearn.metrics import confusion_matrix >>> y_true = [ 2 , 0 , 2 , 2 , 0 , 1 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 0 , 2 ] >>> @bodo . jit >>> def test_confusion_matirx ( y_true , y_pred ): ... print ( confusion_matrix ( y_true , y_pred )) >>> test_confusion_matrix ( y_true , y_pred ) [[ 2 0 0 ] [ 0 0 1 ] [ 1 0 2 ]] sklearn.metrics.f1_score \u00b6 sklearn.metrics. f1_score (y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn') Supported Arguments y_true : 1d array-like. y_pred : 1d array-like. average : Must be one of 'micro' , 'macro' , 'samples' , 'weighted' , 'binary' , or None. Note y_true and y_pred must be of same length. Example Usage >>> import bodo >>> from sklearn.metrics import f1_score >>> y_true = [ 0 , 1 , 2 , 0 , 1 , 2 ] >>> y_pred = [ 0 , 2 , 1 , 0 , 0 , 1 ] >>> @bodo . jit >>> def test_f1_score ( y_true , y_pred ): ... print ( f1_score ( y_true , y_pred , average = 'macro' )) >>> test_f1_score ( y_true , y_pred ) 0.26666666666666666 sklearn.metrics.mean_absolute_error \u00b6 sklearn.metrics. mean_absolute_error (y_true, y_pred, sample_weight=None, multioutput='uniform_average') Supported Arguments y_true : NumPy array. y_pred : NumPy array. sample_weight : Numeric NumPy array or None. multioutput : Must be one of 'raw_values' , 'uniform_average' , or array-like. Note y_true , y_pred , and sample_weight (if provided) must be of same length. Example Usage >>> import bodo >>> import numpy as np >>> from sklearn.metrics import mean_absolute_error >>> y_true = np . array ([[ 0.5 , 1 ], [ - 1 , 1 ], [ 7 , - 6 ]]) >>> y_pred = np . array ([[ 0 , 2 ], [ - 1 , 2 ], [ 8 , - 5 ]]) >>> @bodo . jit >>> def test_mean_absolute_error ( y_true , y_pred ): ... print ( mean_absolute_error ( y_true , y_pred , multioutput = [ 0.3 , 0.7 ])) >>> test_mean_absolute_error ( y_true , y_pred ) 0.85 sklearn.metrics.mean_squared_error \u00b6 sklearn.metrics. mean_squared_error (y_true, y_pred, sample_weight=None, multioutput='uniform_average', squared=True) Supported Arguments y_true : NumPy array. y_pred : NumPy array. sample_weight : Numeric NumPy array or None. multioutput : Must be one of 'raw_values' , 'uniform_average' , or array-like. Note y_true , y_pred , and sample_weight (if provided) must be of same length. Example Usage >>> import bodo >>> import numpy as np >>> from sklearn.metrics import mean_squared_error >>> y_true = np . array ([[ 0.5 , 1 ], [ - 1 , 1 ], [ 7 , - 6 ]]) >>> y_pred = np . array ([[ 0 , 2 ], [ - 1 , 2 ], [ 8 , - 5 ]]) >>> @bodo . jit >>> def test_mean_squared_error ( y_true , y_pred ): ... print ( mean_squared_error ( y_true , y_pred , multioutput = [ 0.3 , 0.7 ])) >>> test_mean_squared_error ( y_true , y_pred ) 0.825 sklearn.metrics.precision_score \u00b6 sklearn.metrics. precision_score (y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn') Supported Arguments y_true : 1d array-like. y_pred : 1d array-like. average : Must be one of 'micro' , 'macro' , 'samples' , 'weighted' , 'binary' , or None . Note y_true and y_pred must be of same length. Example Usage >>> import bodo >>> from sklearn.metrics import precision_score >>> y_true = [ 0 , 1 , 2 , 0 , 1 , 2 ] >>> y_pred = [ 0 , 2 , 1 , 0 , 0 , 1 ] >>> @bodo . jit >>> def test_precision_score ( y_true , y_pred ): ... print ( precision_score ( y_true , y_pred , average = 'macro' )) >>> test_precision_score ( y_true , y_pred ) 0.2222222222222222 sklearn.metrics.r2_score \u00b6 sklearn.metrics. r2_score (y_true, y_pred, sample_weight=None, multioutput='uniform_average') Supported Arguments y_true : NumPy array. y_pred : NumPy array. sample_weight : Numeric NumPy array or None . multioutput : Must be one of 'raw_values' , 'uniform_average' , 'variance_weighted' , None , or array-like. Note y_true , y_pred , and sample_weight (if provided) must be of same length. Example Usage >>> import bodo >>> import numpy as np >>> from sklearn.metrics import r2_score >>> y_true = np . array ([[ 0.5 , 1 ], [ - 1 , 1 ], [ 7 , - 6 ]]) >>> y_pred = np . array ([[ 0 , 2 ], [ - 1 , 2 ], [ 8 , - 5 ]]) >>> @bodo . jit >>> def test_r2_score ( y_true , y_pred ): ... print ( r2_score ( y_true , y_pred , multioutput = [ 0.3 , 0.7 ])) >>> test_r2_score ( y_true , y_pred ) 0.9253456221198156 sklearn.metrics.recall_score \u00b6 sklearn.metrics. recall_score (y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn') Supported Arguments y_true : 1d array-like. y_pred : 1d array-like. average : Must be one of 'micro' , 'macro' , 'samples' , 'weighted' , 'binary' , or None . Note y_true and y_pred must be of same length. Example Usage >>> import bodo >>> from sklearn.metrics import recall_score >>> y_true = [ 0 , 1 , 2 , 0 , 1 , 2 ] >>> y_pred = [ 0 , 2 , 1 , 0 , 0 , 1 ] >>> @bodo . jit >>> def test_recall_score ( y_true , y_pred ): ... print ( recall_score ( y_true , y_pred , average = 'macro' )) >>> test_recall_score ( y_true , y_pred ) 0.3333333333333333","title":"sklearn.metrics"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetrics","text":"","title":"sklearn.metrics"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsaccuracy_score","text":"sklearn.metrics. accuracy_score (y_true, y_pred, normalize=True, sample_weight=None) Supported Arguments y_true : 1d array-like. y_pred : 1d array-like. normalize : bool. sample_weight : 1d numeric array-like or None. Note y_true , y_pred , and sample_weight (if provided) must be of same length. Example Usage >>> import bodo >>> import numpy as np >>> from sklearn.metrics import accuracy_score >>> y_pred = np . array ([ 0 , 2 , 1 , 3 ]) >>> y_true = np . array ([ 0 , 1 , 2 , 3 ]) >>> @bodo . jit >>> def test_accuracy_score ( y_true , y_pred ): ... print ( accuracy_score ( y_true , y_pred )) >>> test_accuracy_score ( y_true , y_pred ) 0.5","title":"sklearn.metrics.accuracy_score"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsconfusion_matrix","text":"sklearn.metrics. confusion_matrix (y_true, y_pred, labels=None, sample_weight=None, normalize=None) Supported Arguments y_true : 1d array-like. y_pred : 1d array-like. labels : 1d array-like. sample_weight : 1d numeric array-like or None . normalize : Must be one of 'true' , 'pred' , 'all' , or None Note y_true , y_pred , and sample_weight (if provided) must be of same length. Example Usage >>> import bodo >>> from sklearn.metrics import confusion_matrix >>> y_true = [ 2 , 0 , 2 , 2 , 0 , 1 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 0 , 2 ] >>> @bodo . jit >>> def test_confusion_matirx ( y_true , y_pred ): ... print ( confusion_matrix ( y_true , y_pred )) >>> test_confusion_matrix ( y_true , y_pred ) [[ 2 0 0 ] [ 0 0 1 ] [ 1 0 2 ]]","title":"sklearn.metrics.confusion_matrix"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsf1_score","text":"sklearn.metrics. f1_score (y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn') Supported Arguments y_true : 1d array-like. y_pred : 1d array-like. average : Must be one of 'micro' , 'macro' , 'samples' , 'weighted' , 'binary' , or None. Note y_true and y_pred must be of same length. Example Usage >>> import bodo >>> from sklearn.metrics import f1_score >>> y_true = [ 0 , 1 , 2 , 0 , 1 , 2 ] >>> y_pred = [ 0 , 2 , 1 , 0 , 0 , 1 ] >>> @bodo . jit >>> def test_f1_score ( y_true , y_pred ): ... print ( f1_score ( y_true , y_pred , average = 'macro' )) >>> test_f1_score ( y_true , y_pred ) 0.26666666666666666","title":"sklearn.metrics.f1_score"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsmean_absolute_error","text":"sklearn.metrics. mean_absolute_error (y_true, y_pred, sample_weight=None, multioutput='uniform_average') Supported Arguments y_true : NumPy array. y_pred : NumPy array. sample_weight : Numeric NumPy array or None. multioutput : Must be one of 'raw_values' , 'uniform_average' , or array-like. Note y_true , y_pred , and sample_weight (if provided) must be of same length. Example Usage >>> import bodo >>> import numpy as np >>> from sklearn.metrics import mean_absolute_error >>> y_true = np . array ([[ 0.5 , 1 ], [ - 1 , 1 ], [ 7 , - 6 ]]) >>> y_pred = np . array ([[ 0 , 2 ], [ - 1 , 2 ], [ 8 , - 5 ]]) >>> @bodo . jit >>> def test_mean_absolute_error ( y_true , y_pred ): ... print ( mean_absolute_error ( y_true , y_pred , multioutput = [ 0.3 , 0.7 ])) >>> test_mean_absolute_error ( y_true , y_pred ) 0.85","title":"sklearn.metrics.mean_absolute_error"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsmean_squared_error","text":"sklearn.metrics. mean_squared_error (y_true, y_pred, sample_weight=None, multioutput='uniform_average', squared=True) Supported Arguments y_true : NumPy array. y_pred : NumPy array. sample_weight : Numeric NumPy array or None. multioutput : Must be one of 'raw_values' , 'uniform_average' , or array-like. Note y_true , y_pred , and sample_weight (if provided) must be of same length. Example Usage >>> import bodo >>> import numpy as np >>> from sklearn.metrics import mean_squared_error >>> y_true = np . array ([[ 0.5 , 1 ], [ - 1 , 1 ], [ 7 , - 6 ]]) >>> y_pred = np . array ([[ 0 , 2 ], [ - 1 , 2 ], [ 8 , - 5 ]]) >>> @bodo . jit >>> def test_mean_squared_error ( y_true , y_pred ): ... print ( mean_squared_error ( y_true , y_pred , multioutput = [ 0.3 , 0.7 ])) >>> test_mean_squared_error ( y_true , y_pred ) 0.825","title":"sklearn.metrics.mean_squared_error"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsprecision_score","text":"sklearn.metrics. precision_score (y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn') Supported Arguments y_true : 1d array-like. y_pred : 1d array-like. average : Must be one of 'micro' , 'macro' , 'samples' , 'weighted' , 'binary' , or None . Note y_true and y_pred must be of same length. Example Usage >>> import bodo >>> from sklearn.metrics import precision_score >>> y_true = [ 0 , 1 , 2 , 0 , 1 , 2 ] >>> y_pred = [ 0 , 2 , 1 , 0 , 0 , 1 ] >>> @bodo . jit >>> def test_precision_score ( y_true , y_pred ): ... print ( precision_score ( y_true , y_pred , average = 'macro' )) >>> test_precision_score ( y_true , y_pred ) 0.2222222222222222","title":"sklearn.metrics.precision_score"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsr2_score","text":"sklearn.metrics. r2_score (y_true, y_pred, sample_weight=None, multioutput='uniform_average') Supported Arguments y_true : NumPy array. y_pred : NumPy array. sample_weight : Numeric NumPy array or None . multioutput : Must be one of 'raw_values' , 'uniform_average' , 'variance_weighted' , None , or array-like. Note y_true , y_pred , and sample_weight (if provided) must be of same length. Example Usage >>> import bodo >>> import numpy as np >>> from sklearn.metrics import r2_score >>> y_true = np . array ([[ 0.5 , 1 ], [ - 1 , 1 ], [ 7 , - 6 ]]) >>> y_pred = np . array ([[ 0 , 2 ], [ - 1 , 2 ], [ 8 , - 5 ]]) >>> @bodo . jit >>> def test_r2_score ( y_true , y_pred ): ... print ( r2_score ( y_true , y_pred , multioutput = [ 0.3 , 0.7 ])) >>> test_r2_score ( y_true , y_pred ) 0.9253456221198156","title":"sklearn.metrics.r2_score"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsrecall_score","text":"sklearn.metrics. recall_score (y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn') Supported Arguments y_true : 1d array-like. y_pred : 1d array-like. average : Must be one of 'micro' , 'macro' , 'samples' , 'weighted' , 'binary' , or None . Note y_true and y_pred must be of same length. Example Usage >>> import bodo >>> from sklearn.metrics import recall_score >>> y_true = [ 0 , 1 , 2 , 0 , 1 , 2 ] >>> y_pred = [ 0 , 2 , 1 , 0 , 0 , 1 ] >>> @bodo . jit >>> def test_recall_score ( y_true , y_pred ): ... print ( recall_score ( y_true , y_pred , average = 'macro' )) >>> test_recall_score ( y_true , y_pred ) 0.3333333333333333","title":"sklearn.metrics.recall_score"},{"location":"api_docs/ml/sklearn/model_selection/","text":"sklearn.model_selection \u00b6 sklearn.model_selection.train_test_split \u00b6 sklearn.model_selection. train_test_split (X, y, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None) Supported Arguments X : NumPy array or Pandas Dataframes. y : NumPy array or Pandas Dataframes. train_size : float between 0.0 and 1.0 or None only. test_size : float between 0.0 and 1.0 or None only. random_state : int, RandomState, or None. shuffle : bool. Example Usage >>> import bodo >>> import numpy as np >>> from sklearn.model_selection import train_test_split >>> @bodo . jit >>> def test_split (): ... X , y = np . arange ( 10 ) . reshape ( 5 , 2 ), np . arange ( 5 ) ... X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.33 , random_state = 42 ) ... print ( X_train ) ... print ( y_train ) X_train : [[ 4 5 ] [ 6 7 ] [ 8 9 ]] y_train : [ 2 3 4 ] X_test : [[ 2 3 ] [ 0 1 ]] y_test : [ 1 0 ]","title":"sklearn.model_selection"},{"location":"api_docs/ml/sklearn/model_selection/#sklearnmodel_selection","text":"","title":"sklearn.model_selection"},{"location":"api_docs/ml/sklearn/model_selection/#sklearnmodel_selectiontrain_test_split","text":"sklearn.model_selection. train_test_split (X, y, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None) Supported Arguments X : NumPy array or Pandas Dataframes. y : NumPy array or Pandas Dataframes. train_size : float between 0.0 and 1.0 or None only. test_size : float between 0.0 and 1.0 or None only. random_state : int, RandomState, or None. shuffle : bool. Example Usage >>> import bodo >>> import numpy as np >>> from sklearn.model_selection import train_test_split >>> @bodo . jit >>> def test_split (): ... X , y = np . arange ( 10 ) . reshape ( 5 , 2 ), np . arange ( 5 ) ... X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.33 , random_state = 42 ) ... print ( X_train ) ... print ( y_train ) X_train : [[ 4 5 ] [ 6 7 ] [ 8 9 ]] y_train : [ 2 3 4 ] X_test : [[ 2 3 ] [ 0 1 ]] y_test : [ 1 0 ]","title":"sklearn.model_selection.train_test_split"},{"location":"api_docs/ml/sklearn/naive_bayes/","text":"sklearn.naive_bayes \u00b6 sklearn.naive_bayes.MultinomialNB \u00b6 sklearn.naive_bayes. MultinomialNB This class provides Naive Bayes classifier for multinomial models with distributed large-scale learning. Methods \u00b6 sklearn.naive_bayes.MultinomialNB.fit \u00b6 sklearn.naive_bayes.MultinomialNB. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sklearn.naive_bayes.MultinomialNB.predict \u00b6 sklearn.naive_bayes.MultinomialNB. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.naive_bayes.MultinomialNB.score \u00b6 sklearn.naive_bayes.MultinomialNB. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes. Example Usage \u00b6 >>> import bodo >>> import numpy as np >>> from sklearn.naive_bayes import MultinomialNB >>> rng = np . random . RandomState ( 1 ) >>> X = rng . randint ( 5 , size = ( 6 , 100 )) >>> y = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) >>> X_test = rng . randint ( 5 , size = ( 1 , 100 )) >>> @bodo . jit ... def test_mnb ( X , y , X_test ): ... clf = MultinomialNB () ... clf . fit ( X , y ) ... ans = clf . predict ( X_test ) ... print ( ans ) ... >>> test_mnb ( X , y , X_test ) [ 5 ]","title":"sklearn.naive_bayes"},{"location":"api_docs/ml/sklearn/naive_bayes/#sklearnnaive_bayes","text":"","title":"sklearn.naive_bayes"},{"location":"api_docs/ml/sklearn/naive_bayes/#sklearnnaive_bayesmultinomialnb","text":"sklearn.naive_bayes. MultinomialNB This class provides Naive Bayes classifier for multinomial models with distributed large-scale learning.","title":"sklearn.naive_bayes.MultinomialNB"},{"location":"api_docs/ml/sklearn/naive_bayes/#methods","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/naive_bayes/#sklearnnaive_bayesmultinomialnbfit","text":"sklearn.naive_bayes.MultinomialNB. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes.","title":"sklearn.naive_bayes.MultinomialNB.fit"},{"location":"api_docs/ml/sklearn/naive_bayes/#sklearnnaive_bayesmultinomialnbpredict","text":"sklearn.naive_bayes.MultinomialNB. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.naive_bayes.MultinomialNB.predict"},{"location":"api_docs/ml/sklearn/naive_bayes/#sklearnnaive_bayesmultinomialnbscore","text":"sklearn.naive_bayes.MultinomialNB. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes.","title":"sklearn.naive_bayes.MultinomialNB.score"},{"location":"api_docs/ml/sklearn/naive_bayes/#example-usage","text":">>> import bodo >>> import numpy as np >>> from sklearn.naive_bayes import MultinomialNB >>> rng = np . random . RandomState ( 1 ) >>> X = rng . randint ( 5 , size = ( 6 , 100 )) >>> y = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) >>> X_test = rng . randint ( 5 , size = ( 1 , 100 )) >>> @bodo . jit ... def test_mnb ( X , y , X_test ): ... clf = MultinomialNB () ... clf . fit ( X , y ) ... ans = clf . predict ( X_test ) ... print ( ans ) ... >>> test_mnb ( X , y , X_test ) [ 5 ]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/preprocessing/","text":"sklearn.preprocessing \u00b6 sklearn.preprocessing.LabelEncoder \u00b6 sklearn.preprocessing. LabelEncoder This class provides LabelEncoder support to encode target labels y with values between 0 and n-classes-1. Methods \u00b6 sklearn.preprocessing.LabelEncoder.fit \u00b6 sklearn.preprocessing.LabelEncoder. fit (y) Supported Arguments y : 1d array-like. sklearn.preprocessing.LabelEncoder.fit_transform \u00b6 sklearn.preprocessing.LabelEncoder. fit_transform (y) Supported Arguments y : 1d array-like. sklearn.preprocessing.LabelEncoder.transform \u00b6 sklearn.preprocessing.LabelEncoder. transform (y) Supported Arguments y : 1d array-like. Example Usage \u00b6 >>> import bodo >>> import numpy as np >>> from sklearn.preprocessing import LabelEncoder >>> @bodo . jit ... def test_le (): ... le = LabelEncoder () ... le . fit ([ 1 , 2 , 2 , 6 ]) ... print ( le . transform ([ 1 , 1 , 2 , 6 ])) ... >>> test_le () [ 0 0 1 2 ] sklearn.preprocessing.MinMaxScaler \u00b6 sklearn.preprocessing. MinMaxScaler This class provides MinMax Scaler support to scale your data based on the range of its features. Methods \u00b6 sklearn.preprocessing.MinMaxScaler.fit \u00b6 sklearn.preprocessing.MinMaxScaler. fit (X, y=None) Supported Arguments X : NumPy array or Pandas Dataframes. sklearn.preprocessing.MinMaxScaler.inverse_transform \u00b6 sklearn.preprocessing.MinMaxScaler. inverse_transform (X) Supported Arguments X : NumPy array or Pandas Dataframes. sklearn.preprocessing.MinMaxScaler.transform \u00b6 sklearn.preprocessing.MinMaxScaler. transform (X) Supported Arguments X : NumPy array or Pandas Dataframes. Example Usage \u00b6 >>> import bodo >>> import numpy as np >>> from sklearn.preprocessing import MinMaxScaler >>> data = np . array ([[ - 1 , 2 ], [ - 0.5 , 6 ], [ 0 , 10 ], [ 1 , 18 ]]) >>> @bodo . jit ... def test_minmax ( data ): ... scaler = MinMaxScaler () ... scaler . fit ( data ) ... print ( scaler . transform ( data )) ... >>> test_minmax ( data ) [[ 0. 0. ] [ 0.25 0.25 ] [ 0.5 0.5 ] [ 1. 1. ]] sklearn.preprocessing.StandardScaler \u00b6 sklearn.preprocessing. StandardScaler This class provides Standard Scaler support to center your data and to scale it to achieve unit variance. Methods \u00b6 sklearn.preprocessing.StandardScaler.fit \u00b6 sklearn.preprocessing.StandardScaler. fit (X, y=None, sample_weight=None) Supported Arguments - X : NumPy Array or Pandas Dataframes. - y : NumPy Array. - sample_weight : Numeric NumPy Array (only if data is not distributed) sklearn.preprocessing.StandardScaler.inverse_transform \u00b6 sklearn.preprocessing.StandardScaler. inverse_transform (X, copy=None) Supported Arguments X : NumPy Array or Pandas Dataframes. copy : bool or None. sklearn.preprocessing.StandardScaler.transform \u00b6 sklearn.preprocessing.StandardScaler. transform (X, copy=None) Supported Arguments X : NumPy Array or Pandas Dataframes. copy : bool or None. Example Usage \u00b6 >>> import bodo >>> import numpy as np >>> from sklearn.svm import LinearSVC >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.datasets import make_classification >>> X , y = make_classification ( n_features = 4 , random_state = 0 ) >>> @bodo . jit ... def test_linearsvc ( X , y ): ... scaler = StandardScaler () ... scaler . fit ( X ) ... X = scaler . transform ( X ) ... clf = LinearSVC () ... clf . fit ( X , y ) ... ans = clf . predict ( np . array ([[ 0 , 0 , 0 , 0 ]])) ... print ( ans ) ... >>> test_linearsvc ( X , y ) [ 1 ]","title":"sklearn.preprocessing"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessing","text":"","title":"sklearn.preprocessing"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessinglabelencoder","text":"sklearn.preprocessing. LabelEncoder This class provides LabelEncoder support to encode target labels y with values between 0 and n-classes-1.","title":"sklearn.preprocessing.LabelEncoder"},{"location":"api_docs/ml/sklearn/preprocessing/#methods","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessinglabelencoderfit","text":"sklearn.preprocessing.LabelEncoder. fit (y) Supported Arguments y : 1d array-like.","title":"sklearn.preprocessing.LabelEncoder.fit"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessinglabelencoderfit_transform","text":"sklearn.preprocessing.LabelEncoder. fit_transform (y) Supported Arguments y : 1d array-like.","title":"sklearn.preprocessing.LabelEncoder.fit_transform"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessinglabelencodertransform","text":"sklearn.preprocessing.LabelEncoder. transform (y) Supported Arguments y : 1d array-like.","title":"sklearn.preprocessing.LabelEncoder.transform"},{"location":"api_docs/ml/sklearn/preprocessing/#example-usage","text":">>> import bodo >>> import numpy as np >>> from sklearn.preprocessing import LabelEncoder >>> @bodo . jit ... def test_le (): ... le = LabelEncoder () ... le . fit ([ 1 , 2 , 2 , 6 ]) ... print ( le . transform ([ 1 , 1 , 2 , 6 ])) ... >>> test_le () [ 0 0 1 2 ]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingminmaxscaler","text":"sklearn.preprocessing. MinMaxScaler This class provides MinMax Scaler support to scale your data based on the range of its features.","title":"sklearn.preprocessing.MinMaxScaler"},{"location":"api_docs/ml/sklearn/preprocessing/#methods_1","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingminmaxscalerfit","text":"sklearn.preprocessing.MinMaxScaler. fit (X, y=None) Supported Arguments X : NumPy array or Pandas Dataframes.","title":"sklearn.preprocessing.MinMaxScaler.fit"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingminmaxscalerinverse_transform","text":"sklearn.preprocessing.MinMaxScaler. inverse_transform (X) Supported Arguments X : NumPy array or Pandas Dataframes.","title":"sklearn.preprocessing.MinMaxScaler.inverse_transform"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingminmaxscalertransform","text":"sklearn.preprocessing.MinMaxScaler. transform (X) Supported Arguments X : NumPy array or Pandas Dataframes.","title":"sklearn.preprocessing.MinMaxScaler.transform"},{"location":"api_docs/ml/sklearn/preprocessing/#example-usage_1","text":">>> import bodo >>> import numpy as np >>> from sklearn.preprocessing import MinMaxScaler >>> data = np . array ([[ - 1 , 2 ], [ - 0.5 , 6 ], [ 0 , 10 ], [ 1 , 18 ]]) >>> @bodo . jit ... def test_minmax ( data ): ... scaler = MinMaxScaler () ... scaler . fit ( data ) ... print ( scaler . transform ( data )) ... >>> test_minmax ( data ) [[ 0. 0. ] [ 0.25 0.25 ] [ 0.5 0.5 ] [ 1. 1. ]]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingstandardscaler","text":"sklearn.preprocessing. StandardScaler This class provides Standard Scaler support to center your data and to scale it to achieve unit variance.","title":"sklearn.preprocessing.StandardScaler"},{"location":"api_docs/ml/sklearn/preprocessing/#methods_2","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingstandardscalerfit","text":"sklearn.preprocessing.StandardScaler. fit (X, y=None, sample_weight=None) Supported Arguments - X : NumPy Array or Pandas Dataframes. - y : NumPy Array. - sample_weight : Numeric NumPy Array (only if data is not distributed)","title":"sklearn.preprocessing.StandardScaler.fit"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingstandardscalerinverse_transform","text":"sklearn.preprocessing.StandardScaler. inverse_transform (X, copy=None) Supported Arguments X : NumPy Array or Pandas Dataframes. copy : bool or None.","title":"sklearn.preprocessing.StandardScaler.inverse_transform"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingstandardscalertransform","text":"sklearn.preprocessing.StandardScaler. transform (X, copy=None) Supported Arguments X : NumPy Array or Pandas Dataframes. copy : bool or None.","title":"sklearn.preprocessing.StandardScaler.transform"},{"location":"api_docs/ml/sklearn/preprocessing/#example-usage_2","text":">>> import bodo >>> import numpy as np >>> from sklearn.svm import LinearSVC >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.datasets import make_classification >>> X , y = make_classification ( n_features = 4 , random_state = 0 ) >>> @bodo . jit ... def test_linearsvc ( X , y ): ... scaler = StandardScaler () ... scaler . fit ( X ) ... X = scaler . transform ( X ) ... clf = LinearSVC () ... clf . fit ( X , y ) ... ans = clf . predict ( np . array ([[ 0 , 0 , 0 , 0 ]])) ... print ( ans ) ... >>> test_linearsvc ( X , y ) [ 1 ]","title":"Example Usage"},{"location":"api_docs/ml/sklearn/svm/","text":"sklearn.svm \u00b6 sklearn.svm.LinearSVC \u00b6 sklearn.svm. LinearSVC This class provides Linear Support Vector Classification. Methods \u00b6 sklearn.svm.LinearSVC.fit \u00b6 sklearn.svm.LinearSVC. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed) sklearn.svm.LinearSVC.predict \u00b6 sklearn.svm.LinearSVC. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes. sklearn.svm.LinearSVC.score \u00b6 sklearn.svm.LinearSVC. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes. Example Usage: \u00b6 >>> import bodo >>> import numpy as np >>> from sklearn.svm import LinearSVC >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.datasets import make_classification >>> X , y = make_classification ( n_features = 4 , random_state = 0 ) >>> @bodo . jit ... def test_linearsvc ( X , y ): ... scaler = StandardScaler () ... scaler . fit ( X ) ... X = scaler . transform ( X ) ... clf = LinearSVC () ... clf . fit ( X , y ) ... ans = clf . predict ( np . array ([[ 0 , 0 , 0 , 0 ]])) ... print ( ans ) ... >>> test_linearsvc ( X , y ) [ 1 ]","title":"sklearn.svm"},{"location":"api_docs/ml/sklearn/svm/#sklearnsvm","text":"","title":"sklearn.svm"},{"location":"api_docs/ml/sklearn/svm/#sklearnsvmlinearsvc","text":"sklearn.svm. LinearSVC This class provides Linear Support Vector Classification.","title":"sklearn.svm.LinearSVC"},{"location":"api_docs/ml/sklearn/svm/#methods","text":"","title":"Methods"},{"location":"api_docs/ml/sklearn/svm/#sklearnsvmlinearsvcfit","text":"sklearn.svm.LinearSVC. fit (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array. sample_weight : Numeric NumPy Array (only if data is not distributed)","title":"sklearn.svm.LinearSVC.fit"},{"location":"api_docs/ml/sklearn/svm/#sklearnsvmlinearsvcpredict","text":"sklearn.svm.LinearSVC. predict (X) Supported Arguments X : NumPy Array or Pandas Dataframes.","title":"sklearn.svm.LinearSVC.predict"},{"location":"api_docs/ml/sklearn/svm/#sklearnsvmlinearsvcscore","text":"sklearn.svm.LinearSVC. score (X, y, sample_weight=None) Supported Arguments X : NumPy Array or Pandas Dataframes. y : NumPy Array or Pandas Dataframes. sample_weight : Numeric NumPy Array or Pandas Dataframes.","title":"sklearn.svm.LinearSVC.score"},{"location":"api_docs/ml/sklearn/svm/#example-usage","text":">>> import bodo >>> import numpy as np >>> from sklearn.svm import LinearSVC >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.datasets import make_classification >>> X , y = make_classification ( n_features = 4 , random_state = 0 ) >>> @bodo . jit ... def test_linearsvc ( X , y ): ... scaler = StandardScaler () ... scaler . fit ( X ) ... X = scaler . transform ( X ) ... clf = LinearSVC () ... clf . fit ( X , y ) ... ans = clf . predict ( np . array ([[ 0 , 0 , 0 , 0 ]])) ... print ( ans ) ... >>> test_linearsvc ( X , y ) [ 1 ]","title":"Example Usage:"},{"location":"api_docs/pandas/","text":"Pandas \u00b6 Datatypes General Functions Dataframe API Groupby Series API Window Date Offsets Input/Output Index Objects TimeDelta Timestamp Integer NA Issues Type Inference for Object Data","title":"Index"},{"location":"api_docs/pandas/#pandas","text":"Datatypes General Functions Dataframe API Groupby Series API Window Date Offsets Input/Output Index Objects TimeDelta Timestamp Integer NA Issues Type Inference for Object Data","title":"Pandas"},{"location":"api_docs/pandas/dataframe/","text":"DataFrame \u00b6 Bodo provides extensive DataFrame support documented below. pd.Dataframe \u00b6 pandas. DataFrame (data=None, index=None, columns=None, dtype=None, copy=None) Supported Arguments data : constant key dictionary, 2D Numpy array columns argument is required when using a 2D Numpy array index : List, Tuple, Pandas index types, Pandas array types, Pandas series types, Numpy array types columns : Constant list of String, Constant tuple of String Must be constant at Compile Time dtype : All values supported with dataframe.astype (see below) copy : boolean Must be constant at Compile Time Attributes and underlying data \u00b6 `pd.DataFrame.columns++ pandas.DataFrame. columns Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"X\" , \"Y\" , \"Z\" ], \"C\" : [ pd . Timedelta ( 10 , unit = \"D\" ), pd . Timedelta ( 10 , unit = \"H\" ), pd . Timedelta ( 10 , unit = \"S\" )]}) ... return df . columns >>> f () Index ([ 'A' , 'B' , 'C' ], dtype = 'object' ) pd.DataFrame.dtypes \u00b6 pandas.DataFrame. dtypes Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"X\" , \"Y\" , \"Z\" ], \"C\" : [ pd . Timedelta ( 10 , unit = \"D\" ), pd . Timedelta ( 10 , unit = \"H\" ), pd . Timedelta ( 10 , unit = \"S\" )]}) ... return df . dtypes >>> f () A int64 B string C timedelta64 [ ns ] dtype : object pd.DataFrame.empty \u00b6 pandas.DataFrame. empty Example Usage >>> @bodo . jit ... def f (): ... df1 = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) ... df2 = pd . DataFrame () ... return df1 . empty , df2 . empty >>> f () ( False , True ) pd.DataFrame.index \u00b6 pandas.DataFrame. index Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}, index = [ \"x\" , \"y\" , \"z\" ]) ... return df . index >>> f () Index ([ 'x' , 'y' , 'z' ], dtype = 'object' ) pd.DataFrame.ndim \u00b6 pandas.DataFrame. ndim Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"X\" , \"Y\" , \"Z\" ], \"C\" : [ pd . Timedelta ( 10 , unit = \"D\" ), pd . Timedelta ( 10 , unit = \"H\" ), pd . Timedelta ( 10 , unit = \"S\" )]}) ... return df . ndim >>> f () 2 pd.DataFrame.select_dtypes \u00b6 pandas.DataFrame. select_dtypes (include=None, exclude=None) Supported Arguments include : string, type, List or tuple of string/type Must be constant at Compile Time exclude : string, type, List or tuple of string/type Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 ], \"B\" : [ \"X\" ], \"C\" : [ pd . Timedelta ( 10 , unit = \"D\" )], \"D\" : [ True ], \"E\" : [ 3.1 ]}) ... out_1 = df_l . select_dtypes ( exclude = [ np . float64 , \"bool\" ]) ... out_2 = df_l . select_dtypes ( include = \"int\" ) ... out_3 = df_l . select_dtypes ( include = np . bool_ , exclude = ( np . int64 , \"timedelta64[ns]\" )) ... formated_out = \" \\n \" . join ([ out_1 . to_string (), out_2 . to_string (), out_3 . to_string ()]) ... return formated_out >>> f () A B C 0 1 X 10 days A 0 1 D 0 True pd.DataFrame.filter \u00b6 pandas.DataFrame. filter (items=None, like=None, regex=None, axis=None) Supported Arguments items : Constant list of String like : Constant string regex : Constant String axis (only supports the \"column\" axis): Constant String, Constant integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"ababab\" : [ 1 ], \"hello world\" : [ 2 ], \"A\" : [ 3 ]}) ... filtered_df_1 = pd . DataFrame ({ \"ababab\" : [ 1 ], \"hello world\" : [ 2 ], \"A\" : [ 3 ]}) . filter ( items = [ \"A\" ]) ... filtered_df_2 = pd . DataFrame ({ \"ababab\" : [ 1 ], \"hello world\" : [ 2 ], \"A\" : [ 3 ]}) . filter ( like = \"hello\" , axis = \"columns\" ) ... filtered_df_3 = pd . DataFrame ({ \"ababab\" : [ 1 ], \"hello world\" : [ 2 ], \"A\" : [ 3 ]}) . filter ( regex = \"(ab) {3} \" , axis = 1 ) ... formated_out = \" \\n \" . join ([ filtered_df_1 . to_string (), filtered_df_2 . to_string (), filtered_df_3 . to_string ()]) ... return formated_out >>> f () A 0 3 hello world 0 2 ababab 0 1 pd.DataFrame.shape \u00b6 pandas.DataFrame. shape Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 3 , 4 , 5 ]}) ... return df . shape >>> f () ( 3 , 2 ) pd.DataFrame.size \u00b6 pandas.DataFrame. size Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 3 , 4 , 5 ]}) ... return df . size >>> f () 6 pd.DataFrame.to_numpy \u00b6 pandas.DataFrame. to_numpy (dtype=None, copy=False, na_value=NoDefault.no_default) Supported Arguments copy : boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 3.1 , 4.2 , 5.3 ]}) ... return df . to_numpy () >>> f () [[ 1. 3.1 ] [ 2. 4.2 ] [ 3. 5.3 ]] pd.DataFrame.values \u00b6 pandas.DataFrame. values (only for numeric dataframes) Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 3.1 , 4.2 , 5.3 ]}) ... return df . values >>> f () [[ 1. 3.1 ] [ 2. 4.2 ] [ 3. 5.3 ]] Conversion \u00b6 pd.DataFrame.astype \u00b6 pandas.DataFrame. astype (dtype, copy=True, errors='raise') Supported Arguments dtype : dict of string column names keys, and Strings/types values. String (string must be parsable by np.dtype ), Valid type (see types), The following functions: float, int, bool, str Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 3.1 , 4.2 , 5.3 ]}) ... return df . astype ({ \"A\" : float , \"B\" : \"datetime64[ns]\" }) >>> f () A B 0 1.0 1970 - 01 - 01 00 : 00 : 00.000000003 1 2.0 1970 - 01 - 01 00 : 00 : 00.000000004 2 3.0 1970 - 01 - 01 00 : 00 : 00.000000005 pd.DataFrame.copy \u00b6 pandas.DataFrame. copy (deep=True) Supported Arguments copy : boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) ... shallow_df = df . copy ( deep = False ) ... deep_df = df . copy () ... shallow_df [ \"A\" ][ 0 ] = - 1 ... formated_out = \" \\n \" . join ([ df . to_string (), shallow_df . to_string (), deep_df . to_string ()]) ... return formated_out >>> f () A 0 - 1 1 2 2 3 A 0 - 1 1 2 2 3 A 0 1 1 2 2 3 pd.DataFrame.isna \u00b6 pandas.DataFrame. isna () Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , None , 3 ]}) ... return df . isna () >>> f () A 0 False 1 True 2 False pd.DataFrame.isnull \u00b6 pandas.DataFrame. isnull () Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , None , 3 ]}) ... return df . isnull () >>> f () A 0 False 1 True 2 False pd.DataFrame.notna \u00b6 pandas.DataFrame. notna () Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , None , 3 ]}) ... return df . notna () >>> f () A 0 True 1 False 2 True pd.DataFrame.notnull \u00b6 pandas.DataFrame. notnull () Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , None , 3 ]}) ... return df . notnull () >>> f () A 0 True 1 False 2 True pd.DataFrame.info \u00b6 pandas.DataFrame. info (verbose=None, buf=None, max_cols=None, memory_usage=None, show_counts=None, null_counts=None) Supported Arguments : None Example Usage >>> @bodo.jit ... def f(): ... df = pd.DataFrame({\"A\": [1,2,3], \"B\": [\"X\", \"Y\", \"Z\"], \"C\": [pd.Timedelta(10, unit=\"D\"), pd.Timedelta(10, unit=\"H\"), pd.Timedelta(10, unit=\"S\")]}) ... return df.info() >>> f() <class 'DataFrameType'> RangeIndexType(none): 3 entries, 0 to 2 Data columns (total 3 columns): # Column Non-Null Count Dtype 0 A 3 non-null int64 1 B 3 non-null unicode_type 2 C 3 non-null timedelta64[ns] dtypes: int64(1), timedelta64[ns](1), unicode_type(1) memory usage: 108.0 bytes Note The exact output string may vary slightly from Pandas. pd.DataFrame.infer_objects \u00b6 pandas.DataFrame. infer_objects () Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) ... return df . infer_objects () A 0 1 1 2 2 3 Note Bodo does not internally use the object dtype, so types are never inferred. As a result, this API just produces a deep copy, consistent with Pandas. Indexing, iteration \u00b6 pd.DataFrame.head \u00b6 pandas.DataFrame. head (n=5) Supported Arguments head : integer Example Usage >>> @bodo . jit ... def f (): ... return pd . DataFrame ({ \"A\" : np . arange ( 1000 )}) . head ( 3 ) A 0 0 1 1 2 2 pd.DataFrame.iat \u00b6 pandas.DataFrame. iat Note We only support indexing using iat using a pair of integers. We require that the second int (the column integer) is a compile time constant Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... df . iat [ 0 , 0 ] = df . iat [ 2 , 2 ] ... return df >>> f () A B C 0 9 4 7 1 2 5 8 2 3 6 9 pd.DataFrame.iloc \u00b6 pandas.DataFrame. iloc getitem : df.iloc supports single integer indexing (returns row as series) df.iloc[0] df.iloc supports single list/array/series of integers/bool df.iloc[[0,1,2]] for tuples indexing df.iloc[row_idx, col_idx] we allow: row_idx to be int list/array/series of integers/bool slice col_idx to be constant int, constant list of integers, or constant slice e.g.: df.iloc[[0,1,2], :] setitem : df.iloc only supports scalar setitem df.iloc only supports tuple indexing df.iloc[row_idx, col_idx] row_idx can be anything supported for series setitem: int list/array/series of integers/bool slice col_idx can be: constant int, constant list/tuple of integers Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... df . iloc [ 0 , 0 ] = df . iloc [ 2 , 2 ] ... df . iloc [ 1 , [ 1 , 2 ]] = df . iloc [ 0 , 1 ] ... df [ \"D\" ] = df . iloc [ 0 ] ... return df >>> f () A B C D 0 9 4 7 7 1 2 4 4 4 2 3 6 9 9 pd.DataFrame.insert \u00b6 pandas.DataFrame. insert (loc, column, value, allow_duplicates=False) Supported Arguments loc : constant integer column : constant string value : scalar, list/tuple, Pandas/Numpy array, Pandas index types, series allow_duplicates : constant boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... df . insert ( 3 , \"D\" , [ - 1 , - 2 , - 3 ]) ... return df >>> f () A B C D 0 1 4 7 - 1 1 2 5 8 - 2 2 3 6 9 - 3 pd.DataFrame.isin \u00b6 pandas.DataFrame. isin (values) Supported Arguments values : DataFrame (must have same indices) + iterable type, Numpy array types, Pandas array types, List/Tuple, Pandas Index Types (excluding interval Index and MultiIndex) Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... isin_1 = df . isin ([ 1 , 5 , 9 ]) ... isin_2 = df . isin ( pd . DataFrame ({ \"A\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]})) ... formated_out = \" \\n \" . join ([ isin_1 . to_string (), isin_2 . to_string ()]) ... return formated_out >>> f () A B C 0 True False False 1 False True False 2 False False True A B C 0 False False True 1 False False True 2 False False True Note DataFrame.isin ignores DataFrame indices. For example: >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . isin ( pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}, index = [ \"A\" , \"B\" , \"C\" ])) >>> f () A B C 0 True False False 1 True False False 2 True False False >>> def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . isin ( pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}, index = [ \"A\" , \"B\" , \"C\" ])) >>> f () A B C 0 False False False 1 False False False 2 False False False pd.DataFrame.itertuples \u00b6 pandas.DataFrame. itertuples (index=True, name='Pandas') Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... for x in pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) . itertuples (): ... print ( x ) ... print ( x [ 0 ]) ... print ( x [ 2 :]) >>> f () Pandas ( Index = 0 , A = 1 , B = 4 , C = 7 ) 0 ( 4 , 7 ) Pandas ( Index = 1 , A = 2 , B = 5 , C = 8 ) 1 ( 5 , 8 ) Pandas ( Index = 2 , A = 3 , B = 6 , C = 9 ) 2 ( 6 , 9 ) pd.DataFrame.query \u00b6 pandas.DataFrame. query (expr, inplace=False, **kwargs) Supported Arguments expr : Constant String Example Usage >>> @bodo . jit ... def f ( a ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . query ( 'A > @a' ) >>> f ( 1 ) A B C 1 2 5 8 2 3 6 9 Note The output of the query must evaluate to a 1d boolean array. Cannot refer to the index by name in the query string. Query must be one line. If using environment variables, they should be passed as arguments to the function. pd.DataFrame.tail \u00b6 pandas.DataFrame. tail (n=5) Supported Arguments n : Integer Example Usage >>> @bodo . jit ... def f (): ... return pd . DataFrame ({ \"A\" : np . arange ( 1000 )}) . tail ( 3 ) >>> f () A 997 997 998 998 999 999 pandas.DataFrame. where (cond, other=np.nan, inplace=False, axis=1, level=None, errors='raise', try_cast=NoDefault.no_default) Supported Arguments cond : Boolean DataFrame, Boolean Series, Boolean Array If 1-dimensional array or Series is provided, equivalent to Pandas df.where with axis=1 . other : Scalar, DataFrame, Series, 1 or 2-D Array, None Data types in other must match corresponding entries in DataFrame. None or omitting argument defaults to the respective NA value for each type. Note DataFrame can contain categorical data if other is a scalar. Example Usage >>> @bodo . jit ... def f ( df , cond , other ): ... return df . where ( cond , other ) >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4.3 , 2.4 , 1.2 ]}) >>> cond = df > 2 >>> other = df + 100 >>> f ( df , cond , other ) A B 0 101 4.3 1 102 2.4 2 3 101.2 pandas.DataFrame. mask (cond, other=np.nan, inplace=False, axis=1, level=None, errors='raise', try_cast=NoDefault.no_default) Supported Arguments cond : Boolean DataFrame,Boolean Series,Boolean Array If 1-dimensional array or Series is provided, equivalent to Pandas df.mask with axis=1 . other : Scalar, DataFrame, Series, 1 or 2-D Array None , - Data types in other must match corresponding entries in DataFrame. None or omitting argument defaults to the respective NA value for each type. Note DataFrame can contain categorical data if other is a scalar. Example Usage >>> @bodo . jit ... def f ( df , cond , other ): ... return df . mask ( cond , other ) >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4.3 , 2.4 , 1.2 ]}) >>> cond = df > 2 >>> other = df + 100 >>> f ( df , cond , other ) A B 0 1 104.3 1 2 102.4 2 103 1.2 Function application, GroupBy & Window \u00b6 pd.DataFrame.apply \u00b6 pandas.DataFrame. apply (func, axis=0, raw=False, result_type=None, args=(), _bodo_inline=False, **kwargs) Supported Arguments func : function (e.g. lambda) (axis must = 1), jit function (axis must = 1), String which refers to a supported DataFrame method Must be constant at Compile Time axis : Integer (0, 1), String (only if the method takes axis as an argument ) Must be constant at Compile Time _bodo_inline : boolean Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . apply ( lambda x : x [ \"A\" ] * ( x [ \"B\" ] + x [ \"C\" ])) >>> f () 0 11 1 26 2 45 dtype : int64 Note Supports extra _bodo_inline boolean argument to manually control bodo's inlining behavior. Inlining user-defined functions (UDFs) can potentially improve performance at the expense of extra compilation time. Bodo uses heuristics to make a decision automatically if _bodo_inline is not provided. pd.DataFrame.groupby \u00b6 pandas.DataFrame. groupby (by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) Supported Arguments by : String column label, List/Tuple of column labels Must be constant at Compile Time as_index : boolean Must be constant at Compile Time dropna : boolean Must be constant at Compile Time Note sort=False and observed=True are set by default. These are the only support values for sort and observed. For more information on using groupby, see the groupby section . Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 2 , 2 ], \"B\" : [ - 2 , - 2 , 2 , 2 ]}) ... return df . groupby ( \"A\" ) . sum () >>> f () B A 1 - 4 2 4 pd.DataFrame.rolling \u00b6 pandas.DataFrame. rolling (window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None, method='single') Supported Arguments window : Integer, String (must be parsable as a time offset), datetime.timedelta ,pd.Timedelta`, List/Tuple of column labels min_periods : Integer center : boolean on : Scalar column label Must be constant at Compile Time dropna :boolean Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 ]}) ... return df . rolling ( 3 , center = True ) . mean () >>> f () A 0 NaN 1 2.0 2 3.0 3 4.0 4 NaN For more information, please see the Window section . Computations / Descriptive Stats \u00b6 pd.DataFrame.abs \u00b6 pandas.DataFrame. abs () Note Only supported for dataframes containing numerical data and Timedeltas Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , - 2 ], \"B\" : [ 3.1 , - 4.2 ], \"C\" : [ pd . Timedelta ( 10 , unit = \"D\" ), pd . Timedelta ( - 10 , unit = \"D\" )]}) ... return df . abs () >>> f () A B C 0 1 3.1 10 days 1 2 4.2 10 days pd.DataFrame.corr \u00b6 pandas.DataFrame. corr (method='pearson', min_periods=1) Supported Arguments min_periods : Integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ .9 , .8 , .7 , .4 ], \"B\" : [ - .8 , - .9 , - .8 , - .4 ], \"c\" : [ .7 , .7 , .7 , .4 ]}) ... return df . corr () >>> f () A B c A 1.000000 - 0.904656 0.92582 B - 0.904656 1.000000 - 0.97714 c 0.925820 - 0.977140 1.00000 pd.DataFrame.count \u00b6 pandas.DataFrame. count (axis=0, level=None, numeric_only=False) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , None , 3 ], \"B\" : [ None , 2 , None ]}) ... return df . count () >>> f () A 2 B 1 pd.DataFrame.cov \u00b6 pandas.DataFrame. cov (min_periods=None, ddof=1) Supported Arguments min_periods : Integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 0.695 , 0.478 , 0.628 ], \"B\" : [ - 0.695 , - 0.478 , - 0.628 ], \"C\" : [ 0.07 , - 0.68 , 0.193 ]}) ... return df . cov () >>> f () A B C A 0.012346 - 0.012346 0.047577 B - 0.012346 0.012346 - 0.047577 C 0.047577 - 0.047577 0.223293 pd.DataFrame.cumprod \u00b6 pandas.DataFrame. cumprod (axis=None, skipna=True) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ .1 , np . NaN , 12.3 ],}) ... return df . cumprod () >>> f () A B 0 1 0.1 1 2 NaN 2 6 NaN Note Not supported for dataframe with nullable integer. pd.DataFrame.cumsum \u00b6 pandas.DataFrame. cumsum (axis=None, skipna=True) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ .1 , np . NaN , 12.3 ],}) ... return df . cumsum () >>> f () A B 0 1 0.1 1 3 NaN 2 6 NaN Note Not supported for dataframe with nullable integer. pd.DataFrame.describe \u00b6 pandas.DataFrame. describe (percentiles=None, include=None, exclude=None, datetime_is_numeric=False) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ pd . Timestamp ( 2000 , 10 , 2 ), pd . Timestamp ( 2001 , 9 , 5 ), pd . Timestamp ( 2002 , 3 , 11 )]}) ... return df . describe () >>> f () A B count 3.0 3 mean 2.0 2001 - 07 - 16 16 : 00 : 00 min 1.0 2000 - 10 - 02 00 : 00 : 00 25 % 1.5 2001 - 03 - 20 00 : 00 : 00 50 % 2.0 2001 - 09 - 05 00 : 00 : 00 75 % 2.5 2001 - 12 - 07 12 : 00 : 00 max 3.0 2002 - 03 - 11 00 : 00 : 00 std 1.0 NaN Note Only supported for dataframes containing numeric data, and datetime data. Datetime_is_numeric defaults to True in JIT code. pd.DataFrame.diff \u00b6 pandas.DataFrame. diff (periods=1, axis=0) Supported Arguments periods : Integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ pd . Timestamp ( 2000 , 10 , 2 ), pd . Timestamp ( 2001 , 9 , 5 ), pd . Timestamp ( 2002 , 3 , 11 )]}) ... return df . diff ( 1 ) >>> f () A B 0 NaN NaT 1 1.0 338 days 2 1.0 187 days Note Only supported for dataframes containing float, non-null int, and datetime64ns values pd.DataFrame.max \u00b6 pandas.DataFrame. max (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . max ( axis = 1 ) >>> f () 0 7 1 8 2 9 Note Only supported for dataframes containing float, non-null int, and datetime64ns values. pd.DataFrame.mean \u00b6 pandas.DataFrame. mean (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . mean ( axis = 1 ) >>> f () 0 4.0 1 5.0 2 6.0 Note Only supported for dataframes containing float, non-null int, and datetime64ns values. pd.DataFrame.median \u00b6 pandas.DataFrame. median (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . median ( axis = 1 ) >>> f () 0 4.0 1 5.0 2 6.0 Note Only supported for dataframes containing float, non-null int, and datetime64ns values. pd.DataFrame.min \u00b6 pandas.DataFrame. min (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . min ( axis = 1 ) >>> f () 0 1 1 2 2 3 Note Only supported for dataframes containing float, non-null int, and datetime64ns values. pd.DataFrame.nunique \u00b6 pandas.DataFrame. nunique (axis=0, dropna=True) Supported Arguments dropna : boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 1 , 1 , 1 ], \"C\" : [ 4 , None , 6 ]}) ... return df . nunique () >>> f () A 3 B 1 C 2 pd.DataFrame.pct_change \u00b6 pandas.DataFrame. pct_change (periods=1, fill_method='pad', limit=None, freq=None) Supported Arguments periods : Integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 10 , 100 , 1000 , 10000 ]}) ... return df . pct_change () >>> f () A 0 NaN 1 9.0 2 9.0 3 9.0 pd.DataFrame.pipe \u00b6 pandas.DataFrame. pipe (func, *args, **kwargs) Supported Arguments func : JIT function or callable defined within a JIT function. Additional arguments for func can be passed as additional arguments. Note func cannot be a tuple Example Usage >>> @bodo . jit ... def f (): ... def g ( df , axis ): ... return df . max ( axis ) ... df = pd . DataFrame ({ \"A\" : [ 10 , 100 , 1000 , 10000 ]}) ... return df . pipe ( g , axis = 0 ) ... >>> f () A 10000 dtype : int64 pd.DataFrame.prod \u00b6 pandas.DataFrame. prod (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . prod ( axis = 1 ) >>> f () A 6 B 120 C 504 dtype : int64 pd.DataFrame.product \u00b6 pandas.DataFrame. product (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . product ( axis = 1 ) >>> f () A 6 B 120 C 504 dtype : int64 pd.DataFrame.quantile \u00b6 pandas.DataFrame. quantile (q=0.5, axis=0, numeric_only=True, interpolation='linear') Supported Arguments q : Float or Int must be 0<= q <= 1 axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . quantile () >>> f () A 2.0 B 5.0 C 8.0 dtype : float64 dtype : int64 pd.DataFrame.std \u00b6 pandas.DataFrame. std (axis=None, skipna=None, level=None, ddof=1, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . std ( axis = 1 ) >>> f () 0 3.0 1 3.0 2 3.0 dtype : float64 pd.DataFrame.sum \u00b6 pandas.DataFrame. sum (axis=None, skipna=None, level=None, numeric_only=None, min_count=0) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . sum ( axis = 1 ) >>> f () 0 12 1 15 2 18 dtype : int64 pd.DataFrame.var \u00b6 pandas.DataFrame. var (axis=None, skipna=None, level=None, ddof=1, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . var ( axis = 1 ) >>> f () 0 9.0 1 9.0 2 9.0 dtype : float64 pd.DataFrame.memory_usage \u00b6 pandas.DataFrame. memory_usage (index=True, deep=False) Supported Arguments index : boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : np . array ([ 1 , 2 , 3 ], dtype = np . int64 ), \"B\" : np . array ([ 1 , 2 , 3 ], dtype = np . int32 ), \"C\" : [ \"1\" , \"2\" , \"3456689\" ]}) ... return df . memory_usage () >>> f () Index 24 A 24 B 12 C 42 dtype : int64 Reindexing / Selection / Label manipulation \u00b6 pd.DataFrame.drop \u00b6 pandas.DataFrame. drop (labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') Only dropping columns supported, either using columns argument or setting axis=1 and using the labels argument labels and columns require constant string, or constant list/tuple of string values inplace supported with a constant boolean value All other arguments are unsupported Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... df . drop ( columns = [ \"B\" , \"C\" ], inplace = True ) ... return df >>> f () A 0 1 1 2 2 3 pd.DataFrame.drop_duplicates \u00b6 pandas.DataFrame. drop_duplicates (subset=None, keep='first', inplace=False, ignore_index=False) Supported Arguments subset : Constant list/tuple of String column names, Constant list/tuple of Integer column names, Constant String column names, Constant Integer column names Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 3 , 4 ], \"B\" : [ 1 , 1 , 3 , 3 ], \"C\" : [ 7 , 8 , 9 , 10 ]}) ... return df . drop_duplicates ( subset = [ \"A\" , \"B\" ]) >>> f () A B C 0 1 1 7 2 3 3 9 3 4 3 10 pd.DataFrame.duplicated \u00b6 pandas.DataFrame. duplicated (subset=None, keep='first') Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 3 , 4 ], \"B\" : [ 1 , 1 , 3 , 3 ]}) ... return df . duplicated () >>> f () 0 False 1 True 2 False 3 False dtype : bool pd.DataFrame.first \u00b6 pandas.DataFrame. first (offset) Supported Arguments offset : String or Offset type String argument must be a valid frequency alias . Note DataFrame must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted. Example Usage >>> @bodo . jit ... def f ( df , offset ): ... return df . first ( offset ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : np . arange ( 100 , 200 )}, index = pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 100 )) >>> f ( df , \"2M\" ) A B 2022 - 01 - 01 00 : 00 : 00.000000000 0 100 2022 - 01 - 12 01 : 27 : 16.363636363 1 101 2022 - 01 - 23 02 : 54 : 32.727272727 2 102 2022 - 02 - 03 04 : 21 : 49.090909091 3 103 2022 - 02 - 14 05 : 49 : 05.454545454 4 104 2022 - 02 - 25 07 : 16 : 21.818181818 5 105 pd.DataFrame.idxmax \u00b6 pandas.DataFrame. idxmax (axis=0, skipna=True) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . idxmax () >>> f () A 2 B 2 C 2 dtype : int64 pd.DataFrame.idxmin \u00b6 pandas.DataFrame. idxmin (axis=0, skipna=True) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . idxmax () >>> f () A 0 B 0 C 20 dtype : int64 pd.DataFrame.last \u00b6 pandas.DataFrame. last (offset) Supported Arguments offset : String or Offset type String argument must be a valid frequency alias Note DataFrame must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted. Example Usage >>> @bodo . jit ... def f ( df , offset ): ... return df . last ( offset ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : np . arange ( 100 , 200 )}, index = pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 100 )) >>> f ( df , \"2M\" ) A B 2024 - 11 - 05 16 : 43 : 38.181818176 94 194 2024 - 11 - 16 18 : 10 : 54.545454544 95 195 2024 - 11 - 27 19 : 38 : 10.909090912 96 196 2024 - 12 - 08 21 : 05 : 27.272727264 97 197 2024 - 12 - 19 22 : 32 : 43.636363632 98 198 2024 - 12 - 31 00 : 00 : 00.000000000 99 199 pd.DataFrame.rename \u00b6 pandas.DataFrame. rename (mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors='ignore') Supported Arguments mapper : must be constant dictionary. Can only be used alongside axis=1 columns : must be constant dictionary axis : Integer Can only be used alongside mapper argument copy : boolean inplace : must be constant boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . rename ( columns = { \"A\" : \"X\" , \"B\" : \"Y\" , \"C\" : \"Z\" }) >>> f () X Y Z 0 1 4 7 1 2 5 8 2 3 6 9 pd.DataFrame.reset_index \u00b6 pandas.DataFrame. reset_index (level=None, drop=False, inplace=False, col_level=0, col_fill='') Supported Arguments level : Integer If specified, must drop all levels. drop : Constant boolean inplace : Constant boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}, index = [ \"X\" , \"Y\" , \"Z\" ]) ... return df . reset_index () >>> f () index A B C 0 X 1 4 7 1 Y 2 5 8 2 Z 3 6 9 pd.DataFrame.set_index \u00b6 pandas.DataFrame. set_index (keys, drop=True, append=False, inplace=False, verify_integrity=False) Supported Arguments keys: must be a constant string Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}, index = [ \"X\" , \"Y\" , \"Z\" ]) ... return df . set_index ( \"C\" ) >>> f () A B C 7 1 4 8 2 5 9 3 6 pd.DataFrame.take \u00b6 pandas.DataFrame. take (indices, axis=0, is_copy=None) Supported Arguments indices: scalar Integer, Pandas Integer Array, Numpy Integer Array, Integer Series Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . take ( pd . Series ([ - 1 , - 2 ])) >>> f () A B C 2 3 6 9 1 2 5 8 Missing data handling \u00b6 pd.DataFrame.dropna \u00b6 pandas.DataFrame. dropna (axis=0, how='any', thresh=None, subset=None, inplace=False) Supported Arguments how : Constant String: either \"all\" or \"any\" thresh : Integer subset : Constant list/tuple of String column names, Constant list/tuple of Integer column names, Constant String column names, Constant Integer column names Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , None ], \"B\" : [ 4 , 5 , None , None ], \"C\" : [ 6 , None , None , None ]}) ... df_1 = df . dropna ( how = \"all\" , subset = [ \"B\" , \"C\" ]) ... df_2 = df . dropna ( thresh = 3 ) ... formated_out = \" \\n \" . join ([ df_1 . to_string (), df_2 . to_string ()]) ... return formated_out >>> f () A B C 0 1 4 6 1 2 5 < NA > A B C 0 1 4 6 pd.DataFrame.fillna \u00b6 pandas.DataFrame. fillna (value=None, method=None, axis=None, inplace=False, limit=None, downcast=None) Supported Arguments value : various scalars Must be of the same type as the filled column inplace : Constant boolean inplace is not supported alongside method method : One of bfill , backfill , ffill , or pad Must be constant at Compile Time inplace is not supported alongside method Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , None ], \"B\" : [ 4 , 5 , None , None ], \"C\" : [ 6 , None , None , None ]}) ... return df . fillna ( - 1 ) >>> f () pd.DataFrame.replace \u00b6 pandas.DataFrame. replace (to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad') Supported Arguments to_replace : various scalars Required argument value : various scalars Must be of the same type as to_replace Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . replace ( 1 , - 1 ) >>> f () A B C 0 - 1 4 7 1 2 5 8 2 3 6 9 Reshaping, sorting, transposing \u00b6 pd.DataFrame.explode \u00b6 pandas.DataFrame. explode (column, ignore_index=False) Supported Arguments column : Constant Column label or list of labels Example Usage >>> @bodo . jit ... def f ( df , cols ): ... return df . explode ( cols ) >>> df = pd . DataFrame ({ \"A\" : [[ 0 , 1 , 2 ], [ 5 ], [], [ 3 , 4 ]], \"B\" : [ 1 , 7 , 2 , 4 ], \"C\" : [[ 1 , 2 , 3 ], np . nan , [], [ 1 , 2 ]]}) >>> f ( df , [ \"A\" , \"C\" ]) A B C 0 0 1 1 0 1 1 2 0 2 1 3 1 5 7 < NA > 2 < NA > 2 < NA > 3 3 4 1 3 4 4 2 pd.DataFrame.pivot \u00b6 pandas.DataFrame. pivot (values=None, index=None, columns=None) Supported Arguments values : Constant Column Label or list of labels index : Constant Column Label or list of labels columns : Constant Column Label Note The the number of columns and names of the output DataFrame won't be known at compile time. To update typing information on DataFrame you should pass it back to Python. Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ \"X\" , \"X\" , \"X\" , \"X\" , \"Y\" , \"Y\" ], \"B\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], \"C\" : [ 10 , 11 , 12 , 20 , 21 , 22 ]}) ... pivoted_tbl = df . pivot ( columns = \"A\" , index = \"B\" , values = \"C\" ) ... return pivoted_tbl >>> f () A X Y B 1 10.0 NaN 2 11.0 NaN 3 12.0 NaN 4 20.0 NaN 5 NaN 21.0 6 NaN 22.0 pd.DataFrame.pivot_table \u00b6 pandas.DataFrame. pivot_table (values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True) Supported Arguments values : Constant Column Label or list of labels index : Constant Column Label or list of labels columns : Constant Column Label aggfunc : String Constant Note This code takes two different paths depending on if pivot values are annotated. When pivot values are annotated then output columns are set to the annotated values. For example, @bodo.jit(pivots={'pt': ['small', 'large']}) declares the output pivot table pt will have columns called small and large . If pivot values are not annotated, then the number of columns and names of the output DataFrame won't be known at compile time. To update typing information on DataFrame you should pass it back to Python. Example Usage >>> @bodo . jit ( pivots = { 'pivoted_tbl' : [ 'X' , 'Y' ]}) ... def f (): ... df = pd . DataFrame ({ \"A\" : [ \"X\" , \"X\" , \"X\" , \"X\" , \"Y\" , \"Y\" ], \"B\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], \"C\" : [ 10 , 11 , 12 , 20 , 21 , 22 ]}) ... pivoted_tbl = df . pivot_table ( columns = \"A\" , index = \"B\" , values = \"C\" , aggfunc = \"mean\" ) ... return pivoted_tbl >>> f () X Y B 1 10.0 NaN 2 11.0 NaN 3 12.0 NaN 4 20.0 NaN 5 NaN 21.0 6 NaN 22.0 pd.DataFrame.sample \u00b6 pandas.DataFrame. sample (n=None, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False) Supported Arguments n : Integer frac : Float replace : boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . sample ( 1 ) >>> f () A B C 2 3 6 9 pd.DataFrame.sort_index \u00b6 pandas.DataFrame. sort_index (axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None) Supported Arguments ascending : boolean na_position :constant String (\"first\" or \"last\") Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}, index = [ 1 , None , 3 ]) ... return df . sort_index ( ascending = False , na_position = \"last\" ) >>> f () A 3 3 1 1 NaN 2 pd.DataFrame.sort_values \u00b6 pandas.DataFrame. sort_values (by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None) Supported Arguments by : constant String or constant list of strings ascending : boolean, list/tuple of boolean, with length equal to the number of key columns inplace : Constant boolean na_position : constant String (\"first\" or \"last\"), constant list/tuple of String, with length equal to the number of key columns Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 2 , None ], \"B\" : [ 4 , 5 , 6 , None ]}) ... df . sort_values ( by = [ \"A\" , \"B\" ], ascending = [ True , False ], na_position = [ \"first\" , \"last\" ], inplace = True ) ... return df >>> f () A B 3 < NA > < NA > 0 1 4 2 2 6 1 2 5 pd.DataFrame.to_string \u00b6 pandas.DataFrame. to_string (buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, min_rows=None, max_cols=None, show_dimensions=False, decimal='.', line_width=None, max_colwidth=None, encoding=None) Supported Arguments buf columns col_space header index na_rep formatters float_format sparsify index_names justify max_rows min_rows max_cols how_dimensions decimal line_width max_colwidth encoding Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) ... return df . to_string () >>> f () A 0 1 1 2 2 3 Note This function is not optimized. When called on a distributed dataframe, the string returned for each rank will be reflective of the dataframe for that rank. Combining / joining / merging \u00b6 pd.DataFrame.append \u00b6 pandas.DataFrame. append (other, ignore_index=False, verify_integrity=False, sort=False) Supported Arguments other : DataFrame, list/tuple of DataFrame ignore_index : constant boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) ... return df . append ( pd . DataFrame ({ \"A\" : [ - 1 , - 2 , - 3 ], \"C\" : [ 4 , 5 , 6 ]})) >>> f () A B C 0 1 4.0 NaN 1 2 5.0 NaN 2 3 6.0 NaN 0 - 1 NaN 4.0 1 - 2 NaN 5.0 2 - 3 NaN 6.0 pd.DataFrame.assign \u00b6 pandas.DataFrame. assign (**kwargs) Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) ... df2 = df . assign ( C = 2 * df [ \"B\" ], D = lambda x : x . C - 1 ) ... return df2 >>> f () A B C D 0 1 4 8 - 8 1 2 5 10 - 10 2 3 6 12 - 12 Note arguments can be JIT functions, lambda functions, or values that can be used to initialize a Pandas Series. pd.DataFrame.join \u00b6 pandas.DataFrame. join (other, on=None, how='left', lsuffix='', rsuffix='', sort=False) Supported Arguments other : DataFrame on : constant string column name, constant list/tuple of column names Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) ... return df . join ( on = \"A\" , other = pd . DataFrame ({ \"C\" : [ - 1 , - 2 , - 3 ], \"D\" : [ 4 , 5 , 6 ]})) >>> f () A B C D 0 1 4 - 2 5 1 1 5 - 2 5 2 3 6 < NA > < NA > Note Joined dataframes cannot have common columns. The output dataframe is not sorted by default for better parallel performance pd.DataFrame.merge \u00b6 pandas.DataFrame. merge (right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None) Note See pd.merge for full list of supported arguments, and more examples. Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) ... return df . merge ( pd . DataFrame ({ \"C\" : [ - 1 , - 2 , - 3 ], \"D\" : [ 4 , 4 , 6 ]}), left_on = \"B\" , right_on = \"D\" ) >>> f () A B C D 0 1 4 - 1 4 1 1 4 - 2 4 2 3 6 - 3 6 Time series-related \u00b6 pd.DataFrame.shift \u00b6 pandas.DataFrame. shift (periods=1, freq=None, axis=0, fill_value=NoDefault.no_default) Supported Arguments periods : Integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) ... return df . shift ( 1 ) >>> f () A B 0 NaN NaN 1 1.0 4.0 2 1.0 5.0 Note Only supported for dataframes containing numeric, boolean, datetime.date and string types. Serialization, IO, Conversion \u00b6 Also see S3 and HDFS configuration requirements and more on Scalable File I/O . pd.DataFrame.to_csv \u00b6 pandas.DataFrame. to_csv compression argument defaults to None in JIT code. This is the only supported value of this argument. mode argument supports only the default value \"w\" . errors argument supports only the default value strict . storage_options argument supports only the default value None . pd.DataFrame.to_json \u00b6 pandas.DataFrame. to_json pd.DataFrame.to_parquet \u00b6 pandas.DataFrame. to_parquet pd.DataFrame.to_sql \u00b6 pandas.DataFrame. to_sql See Example Usage and more system specific instructions . Argument con is supported but only as a string form. SQLalchemy connectable is not supported. Argument name , schema , if_exists , index , index_label , dtype , method are supported. Argument chunksize is not supported. Plotting \u00b6 pd.DataFrame.plot \u00b6 pandas.DataFrame. plot (x=None, y=None, kind=\"line\", figsize=None, xlabel=None, ylabel=None, title=None, legend=True, fontsize=None, xticks=None, yticks=None, ax=None) Supported Arguments x : Constant String column name, Constant integer y : Constant String column name, Constant integer kind : constant String (\"line\" or \"scatter\") figsize : constant numeric tuple (width, height) xlabel : constant String ylabel : constant String title : constant String legend : boolean fontsize : integer xticks : Constant Tuple yticks : Constant Tuple ax : Matplotlib Axes Object","title":"Dataframe"},{"location":"api_docs/pandas/dataframe/#dataframe","text":"Bodo provides extensive DataFrame support documented below.","title":"DataFrame"},{"location":"api_docs/pandas/dataframe/#pddataframe","text":"pandas. DataFrame (data=None, index=None, columns=None, dtype=None, copy=None) Supported Arguments data : constant key dictionary, 2D Numpy array columns argument is required when using a 2D Numpy array index : List, Tuple, Pandas index types, Pandas array types, Pandas series types, Numpy array types columns : Constant list of String, Constant tuple of String Must be constant at Compile Time dtype : All values supported with dataframe.astype (see below) copy : boolean Must be constant at Compile Time","title":"pd.Dataframe"},{"location":"api_docs/pandas/dataframe/#attributes-and-underlying-data","text":"","title":"Attributes and underlying data"},{"location":"api_docs/pandas/dataframe/#pddataframedtypes","text":"pandas.DataFrame. dtypes Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"X\" , \"Y\" , \"Z\" ], \"C\" : [ pd . Timedelta ( 10 , unit = \"D\" ), pd . Timedelta ( 10 , unit = \"H\" ), pd . Timedelta ( 10 , unit = \"S\" )]}) ... return df . dtypes >>> f () A int64 B string C timedelta64 [ ns ] dtype : object","title":"pd.DataFrame.dtypes"},{"location":"api_docs/pandas/dataframe/#pddataframeempty","text":"pandas.DataFrame. empty Example Usage >>> @bodo . jit ... def f (): ... df1 = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) ... df2 = pd . DataFrame () ... return df1 . empty , df2 . empty >>> f () ( False , True )","title":"pd.DataFrame.empty"},{"location":"api_docs/pandas/dataframe/#pddataframeindex","text":"pandas.DataFrame. index Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}, index = [ \"x\" , \"y\" , \"z\" ]) ... return df . index >>> f () Index ([ 'x' , 'y' , 'z' ], dtype = 'object' )","title":"pd.DataFrame.index"},{"location":"api_docs/pandas/dataframe/#pddataframendim","text":"pandas.DataFrame. ndim Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ \"X\" , \"Y\" , \"Z\" ], \"C\" : [ pd . Timedelta ( 10 , unit = \"D\" ), pd . Timedelta ( 10 , unit = \"H\" ), pd . Timedelta ( 10 , unit = \"S\" )]}) ... return df . ndim >>> f () 2","title":"pd.DataFrame.ndim"},{"location":"api_docs/pandas/dataframe/#pddataframeselect_dtypes","text":"pandas.DataFrame. select_dtypes (include=None, exclude=None) Supported Arguments include : string, type, List or tuple of string/type Must be constant at Compile Time exclude : string, type, List or tuple of string/type Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 ], \"B\" : [ \"X\" ], \"C\" : [ pd . Timedelta ( 10 , unit = \"D\" )], \"D\" : [ True ], \"E\" : [ 3.1 ]}) ... out_1 = df_l . select_dtypes ( exclude = [ np . float64 , \"bool\" ]) ... out_2 = df_l . select_dtypes ( include = \"int\" ) ... out_3 = df_l . select_dtypes ( include = np . bool_ , exclude = ( np . int64 , \"timedelta64[ns]\" )) ... formated_out = \" \\n \" . join ([ out_1 . to_string (), out_2 . to_string (), out_3 . to_string ()]) ... return formated_out >>> f () A B C 0 1 X 10 days A 0 1 D 0 True","title":"pd.DataFrame.select_dtypes"},{"location":"api_docs/pandas/dataframe/#pddataframefilter","text":"pandas.DataFrame. filter (items=None, like=None, regex=None, axis=None) Supported Arguments items : Constant list of String like : Constant string regex : Constant String axis (only supports the \"column\" axis): Constant String, Constant integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"ababab\" : [ 1 ], \"hello world\" : [ 2 ], \"A\" : [ 3 ]}) ... filtered_df_1 = pd . DataFrame ({ \"ababab\" : [ 1 ], \"hello world\" : [ 2 ], \"A\" : [ 3 ]}) . filter ( items = [ \"A\" ]) ... filtered_df_2 = pd . DataFrame ({ \"ababab\" : [ 1 ], \"hello world\" : [ 2 ], \"A\" : [ 3 ]}) . filter ( like = \"hello\" , axis = \"columns\" ) ... filtered_df_3 = pd . DataFrame ({ \"ababab\" : [ 1 ], \"hello world\" : [ 2 ], \"A\" : [ 3 ]}) . filter ( regex = \"(ab) {3} \" , axis = 1 ) ... formated_out = \" \\n \" . join ([ filtered_df_1 . to_string (), filtered_df_2 . to_string (), filtered_df_3 . to_string ()]) ... return formated_out >>> f () A 0 3 hello world 0 2 ababab 0 1","title":"pd.DataFrame.filter"},{"location":"api_docs/pandas/dataframe/#pddataframeshape","text":"pandas.DataFrame. shape Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 3 , 4 , 5 ]}) ... return df . shape >>> f () ( 3 , 2 )","title":"pd.DataFrame.shape"},{"location":"api_docs/pandas/dataframe/#pddataframesize","text":"pandas.DataFrame. size Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 3 , 4 , 5 ]}) ... return df . size >>> f () 6","title":"pd.DataFrame.size"},{"location":"api_docs/pandas/dataframe/#pddataframeto_numpy","text":"pandas.DataFrame. to_numpy (dtype=None, copy=False, na_value=NoDefault.no_default) Supported Arguments copy : boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 3.1 , 4.2 , 5.3 ]}) ... return df . to_numpy () >>> f () [[ 1. 3.1 ] [ 2. 4.2 ] [ 3. 5.3 ]]","title":"pd.DataFrame.to_numpy"},{"location":"api_docs/pandas/dataframe/#pddataframevalues","text":"pandas.DataFrame. values (only for numeric dataframes) Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 3.1 , 4.2 , 5.3 ]}) ... return df . values >>> f () [[ 1. 3.1 ] [ 2. 4.2 ] [ 3. 5.3 ]]","title":"pd.DataFrame.values"},{"location":"api_docs/pandas/dataframe/#conversion","text":"","title":"Conversion"},{"location":"api_docs/pandas/dataframe/#pddataframeastype","text":"pandas.DataFrame. astype (dtype, copy=True, errors='raise') Supported Arguments dtype : dict of string column names keys, and Strings/types values. String (string must be parsable by np.dtype ), Valid type (see types), The following functions: float, int, bool, str Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 3.1 , 4.2 , 5.3 ]}) ... return df . astype ({ \"A\" : float , \"B\" : \"datetime64[ns]\" }) >>> f () A B 0 1.0 1970 - 01 - 01 00 : 00 : 00.000000003 1 2.0 1970 - 01 - 01 00 : 00 : 00.000000004 2 3.0 1970 - 01 - 01 00 : 00 : 00.000000005","title":"pd.DataFrame.astype"},{"location":"api_docs/pandas/dataframe/#pddataframecopy","text":"pandas.DataFrame. copy (deep=True) Supported Arguments copy : boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) ... shallow_df = df . copy ( deep = False ) ... deep_df = df . copy () ... shallow_df [ \"A\" ][ 0 ] = - 1 ... formated_out = \" \\n \" . join ([ df . to_string (), shallow_df . to_string (), deep_df . to_string ()]) ... return formated_out >>> f () A 0 - 1 1 2 2 3 A 0 - 1 1 2 2 3 A 0 1 1 2 2 3","title":"pd.DataFrame.copy"},{"location":"api_docs/pandas/dataframe/#pddataframeisna","text":"pandas.DataFrame. isna () Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , None , 3 ]}) ... return df . isna () >>> f () A 0 False 1 True 2 False","title":"pd.DataFrame.isna"},{"location":"api_docs/pandas/dataframe/#pddataframeisnull","text":"pandas.DataFrame. isnull () Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , None , 3 ]}) ... return df . isnull () >>> f () A 0 False 1 True 2 False","title":"pd.DataFrame.isnull"},{"location":"api_docs/pandas/dataframe/#pddataframenotna","text":"pandas.DataFrame. notna () Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , None , 3 ]}) ... return df . notna () >>> f () A 0 True 1 False 2 True","title":"pd.DataFrame.notna"},{"location":"api_docs/pandas/dataframe/#pddataframenotnull","text":"pandas.DataFrame. notnull () Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , None , 3 ]}) ... return df . notnull () >>> f () A 0 True 1 False 2 True","title":"pd.DataFrame.notnull"},{"location":"api_docs/pandas/dataframe/#pddataframeinfo","text":"pandas.DataFrame. info (verbose=None, buf=None, max_cols=None, memory_usage=None, show_counts=None, null_counts=None) Supported Arguments : None Example Usage >>> @bodo.jit ... def f(): ... df = pd.DataFrame({\"A\": [1,2,3], \"B\": [\"X\", \"Y\", \"Z\"], \"C\": [pd.Timedelta(10, unit=\"D\"), pd.Timedelta(10, unit=\"H\"), pd.Timedelta(10, unit=\"S\")]}) ... return df.info() >>> f() <class 'DataFrameType'> RangeIndexType(none): 3 entries, 0 to 2 Data columns (total 3 columns): # Column Non-Null Count Dtype 0 A 3 non-null int64 1 B 3 non-null unicode_type 2 C 3 non-null timedelta64[ns] dtypes: int64(1), timedelta64[ns](1), unicode_type(1) memory usage: 108.0 bytes Note The exact output string may vary slightly from Pandas.","title":"pd.DataFrame.info"},{"location":"api_docs/pandas/dataframe/#pddataframeinfer_objects","text":"pandas.DataFrame. infer_objects () Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) ... return df . infer_objects () A 0 1 1 2 2 3 Note Bodo does not internally use the object dtype, so types are never inferred. As a result, this API just produces a deep copy, consistent with Pandas.","title":"pd.DataFrame.infer_objects"},{"location":"api_docs/pandas/dataframe/#indexing-iteration","text":"","title":"Indexing, iteration"},{"location":"api_docs/pandas/dataframe/#pddataframehead","text":"pandas.DataFrame. head (n=5) Supported Arguments head : integer Example Usage >>> @bodo . jit ... def f (): ... return pd . DataFrame ({ \"A\" : np . arange ( 1000 )}) . head ( 3 ) A 0 0 1 1 2 2","title":"pd.DataFrame.head"},{"location":"api_docs/pandas/dataframe/#pddataframeiat","text":"pandas.DataFrame. iat Note We only support indexing using iat using a pair of integers. We require that the second int (the column integer) is a compile time constant Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... df . iat [ 0 , 0 ] = df . iat [ 2 , 2 ] ... return df >>> f () A B C 0 9 4 7 1 2 5 8 2 3 6 9","title":"pd.DataFrame.iat"},{"location":"api_docs/pandas/dataframe/#pddataframeiloc","text":"pandas.DataFrame. iloc getitem : df.iloc supports single integer indexing (returns row as series) df.iloc[0] df.iloc supports single list/array/series of integers/bool df.iloc[[0,1,2]] for tuples indexing df.iloc[row_idx, col_idx] we allow: row_idx to be int list/array/series of integers/bool slice col_idx to be constant int, constant list of integers, or constant slice e.g.: df.iloc[[0,1,2], :] setitem : df.iloc only supports scalar setitem df.iloc only supports tuple indexing df.iloc[row_idx, col_idx] row_idx can be anything supported for series setitem: int list/array/series of integers/bool slice col_idx can be: constant int, constant list/tuple of integers Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... df . iloc [ 0 , 0 ] = df . iloc [ 2 , 2 ] ... df . iloc [ 1 , [ 1 , 2 ]] = df . iloc [ 0 , 1 ] ... df [ \"D\" ] = df . iloc [ 0 ] ... return df >>> f () A B C D 0 9 4 7 7 1 2 4 4 4 2 3 6 9 9","title":"pd.DataFrame.iloc"},{"location":"api_docs/pandas/dataframe/#pddataframeinsert","text":"pandas.DataFrame. insert (loc, column, value, allow_duplicates=False) Supported Arguments loc : constant integer column : constant string value : scalar, list/tuple, Pandas/Numpy array, Pandas index types, series allow_duplicates : constant boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... df . insert ( 3 , \"D\" , [ - 1 , - 2 , - 3 ]) ... return df >>> f () A B C D 0 1 4 7 - 1 1 2 5 8 - 2 2 3 6 9 - 3","title":"pd.DataFrame.insert"},{"location":"api_docs/pandas/dataframe/#pddataframeisin","text":"pandas.DataFrame. isin (values) Supported Arguments values : DataFrame (must have same indices) + iterable type, Numpy array types, Pandas array types, List/Tuple, Pandas Index Types (excluding interval Index and MultiIndex) Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... isin_1 = df . isin ([ 1 , 5 , 9 ]) ... isin_2 = df . isin ( pd . DataFrame ({ \"A\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]})) ... formated_out = \" \\n \" . join ([ isin_1 . to_string (), isin_2 . to_string ()]) ... return formated_out >>> f () A B C 0 True False False 1 False True False 2 False False True A B C 0 False False True 1 False False True 2 False False True Note DataFrame.isin ignores DataFrame indices. For example: >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . isin ( pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}, index = [ \"A\" , \"B\" , \"C\" ])) >>> f () A B C 0 True False False 1 True False False 2 True False False >>> def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . isin ( pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}, index = [ \"A\" , \"B\" , \"C\" ])) >>> f () A B C 0 False False False 1 False False False 2 False False False","title":"pd.DataFrame.isin"},{"location":"api_docs/pandas/dataframe/#pddataframeitertuples","text":"pandas.DataFrame. itertuples (index=True, name='Pandas') Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... for x in pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) . itertuples (): ... print ( x ) ... print ( x [ 0 ]) ... print ( x [ 2 :]) >>> f () Pandas ( Index = 0 , A = 1 , B = 4 , C = 7 ) 0 ( 4 , 7 ) Pandas ( Index = 1 , A = 2 , B = 5 , C = 8 ) 1 ( 5 , 8 ) Pandas ( Index = 2 , A = 3 , B = 6 , C = 9 ) 2 ( 6 , 9 )","title":"pd.DataFrame.itertuples"},{"location":"api_docs/pandas/dataframe/#pddataframequery","text":"pandas.DataFrame. query (expr, inplace=False, **kwargs) Supported Arguments expr : Constant String Example Usage >>> @bodo . jit ... def f ( a ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . query ( 'A > @a' ) >>> f ( 1 ) A B C 1 2 5 8 2 3 6 9 Note The output of the query must evaluate to a 1d boolean array. Cannot refer to the index by name in the query string. Query must be one line. If using environment variables, they should be passed as arguments to the function.","title":"pd.DataFrame.query"},{"location":"api_docs/pandas/dataframe/#pddataframetail","text":"pandas.DataFrame. tail (n=5) Supported Arguments n : Integer Example Usage >>> @bodo . jit ... def f (): ... return pd . DataFrame ({ \"A\" : np . arange ( 1000 )}) . tail ( 3 ) >>> f () A 997 997 998 998 999 999 pandas.DataFrame. where (cond, other=np.nan, inplace=False, axis=1, level=None, errors='raise', try_cast=NoDefault.no_default) Supported Arguments cond : Boolean DataFrame, Boolean Series, Boolean Array If 1-dimensional array or Series is provided, equivalent to Pandas df.where with axis=1 . other : Scalar, DataFrame, Series, 1 or 2-D Array, None Data types in other must match corresponding entries in DataFrame. None or omitting argument defaults to the respective NA value for each type. Note DataFrame can contain categorical data if other is a scalar. Example Usage >>> @bodo . jit ... def f ( df , cond , other ): ... return df . where ( cond , other ) >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4.3 , 2.4 , 1.2 ]}) >>> cond = df > 2 >>> other = df + 100 >>> f ( df , cond , other ) A B 0 101 4.3 1 102 2.4 2 3 101.2 pandas.DataFrame. mask (cond, other=np.nan, inplace=False, axis=1, level=None, errors='raise', try_cast=NoDefault.no_default) Supported Arguments cond : Boolean DataFrame,Boolean Series,Boolean Array If 1-dimensional array or Series is provided, equivalent to Pandas df.mask with axis=1 . other : Scalar, DataFrame, Series, 1 or 2-D Array None , - Data types in other must match corresponding entries in DataFrame. None or omitting argument defaults to the respective NA value for each type. Note DataFrame can contain categorical data if other is a scalar. Example Usage >>> @bodo . jit ... def f ( df , cond , other ): ... return df . mask ( cond , other ) >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4.3 , 2.4 , 1.2 ]}) >>> cond = df > 2 >>> other = df + 100 >>> f ( df , cond , other ) A B 0 1 104.3 1 2 102.4 2 103 1.2","title":"pd.DataFrame.tail"},{"location":"api_docs/pandas/dataframe/#function-application-groupby-window","text":"","title":"Function application, GroupBy &amp; Window"},{"location":"api_docs/pandas/dataframe/#pddataframeapply","text":"pandas.DataFrame. apply (func, axis=0, raw=False, result_type=None, args=(), _bodo_inline=False, **kwargs) Supported Arguments func : function (e.g. lambda) (axis must = 1), jit function (axis must = 1), String which refers to a supported DataFrame method Must be constant at Compile Time axis : Integer (0, 1), String (only if the method takes axis as an argument ) Must be constant at Compile Time _bodo_inline : boolean Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . apply ( lambda x : x [ \"A\" ] * ( x [ \"B\" ] + x [ \"C\" ])) >>> f () 0 11 1 26 2 45 dtype : int64 Note Supports extra _bodo_inline boolean argument to manually control bodo's inlining behavior. Inlining user-defined functions (UDFs) can potentially improve performance at the expense of extra compilation time. Bodo uses heuristics to make a decision automatically if _bodo_inline is not provided.","title":"pd.DataFrame.apply"},{"location":"api_docs/pandas/dataframe/#pddataframegroupby","text":"pandas.DataFrame. groupby (by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) Supported Arguments by : String column label, List/Tuple of column labels Must be constant at Compile Time as_index : boolean Must be constant at Compile Time dropna : boolean Must be constant at Compile Time Note sort=False and observed=True are set by default. These are the only support values for sort and observed. For more information on using groupby, see the groupby section . Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 2 , 2 ], \"B\" : [ - 2 , - 2 , 2 , 2 ]}) ... return df . groupby ( \"A\" ) . sum () >>> f () B A 1 - 4 2 4","title":"pd.DataFrame.groupby"},{"location":"api_docs/pandas/dataframe/#pddataframerolling","text":"pandas.DataFrame. rolling (window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None, method='single') Supported Arguments window : Integer, String (must be parsable as a time offset), datetime.timedelta ,pd.Timedelta`, List/Tuple of column labels min_periods : Integer center : boolean on : Scalar column label Must be constant at Compile Time dropna :boolean Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 ]}) ... return df . rolling ( 3 , center = True ) . mean () >>> f () A 0 NaN 1 2.0 2 3.0 3 4.0 4 NaN For more information, please see the Window section .","title":"pd.DataFrame.rolling"},{"location":"api_docs/pandas/dataframe/#computations-descriptive-stats","text":"","title":"Computations / Descriptive Stats"},{"location":"api_docs/pandas/dataframe/#pddataframeabs","text":"pandas.DataFrame. abs () Note Only supported for dataframes containing numerical data and Timedeltas Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , - 2 ], \"B\" : [ 3.1 , - 4.2 ], \"C\" : [ pd . Timedelta ( 10 , unit = \"D\" ), pd . Timedelta ( - 10 , unit = \"D\" )]}) ... return df . abs () >>> f () A B C 0 1 3.1 10 days 1 2 4.2 10 days","title":"pd.DataFrame.abs"},{"location":"api_docs/pandas/dataframe/#pddataframecorr","text":"pandas.DataFrame. corr (method='pearson', min_periods=1) Supported Arguments min_periods : Integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ .9 , .8 , .7 , .4 ], \"B\" : [ - .8 , - .9 , - .8 , - .4 ], \"c\" : [ .7 , .7 , .7 , .4 ]}) ... return df . corr () >>> f () A B c A 1.000000 - 0.904656 0.92582 B - 0.904656 1.000000 - 0.97714 c 0.925820 - 0.977140 1.00000","title":"pd.DataFrame.corr"},{"location":"api_docs/pandas/dataframe/#pddataframecount","text":"pandas.DataFrame. count (axis=0, level=None, numeric_only=False) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , None , 3 ], \"B\" : [ None , 2 , None ]}) ... return df . count () >>> f () A 2 B 1","title":"pd.DataFrame.count"},{"location":"api_docs/pandas/dataframe/#pddataframecov","text":"pandas.DataFrame. cov (min_periods=None, ddof=1) Supported Arguments min_periods : Integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 0.695 , 0.478 , 0.628 ], \"B\" : [ - 0.695 , - 0.478 , - 0.628 ], \"C\" : [ 0.07 , - 0.68 , 0.193 ]}) ... return df . cov () >>> f () A B C A 0.012346 - 0.012346 0.047577 B - 0.012346 0.012346 - 0.047577 C 0.047577 - 0.047577 0.223293","title":"pd.DataFrame.cov"},{"location":"api_docs/pandas/dataframe/#pddataframecumprod","text":"pandas.DataFrame. cumprod (axis=None, skipna=True) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ .1 , np . NaN , 12.3 ],}) ... return df . cumprod () >>> f () A B 0 1 0.1 1 2 NaN 2 6 NaN Note Not supported for dataframe with nullable integer.","title":"pd.DataFrame.cumprod"},{"location":"api_docs/pandas/dataframe/#pddataframecumsum","text":"pandas.DataFrame. cumsum (axis=None, skipna=True) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ .1 , np . NaN , 12.3 ],}) ... return df . cumsum () >>> f () A B 0 1 0.1 1 3 NaN 2 6 NaN Note Not supported for dataframe with nullable integer.","title":"pd.DataFrame.cumsum"},{"location":"api_docs/pandas/dataframe/#pddataframedescribe","text":"pandas.DataFrame. describe (percentiles=None, include=None, exclude=None, datetime_is_numeric=False) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ pd . Timestamp ( 2000 , 10 , 2 ), pd . Timestamp ( 2001 , 9 , 5 ), pd . Timestamp ( 2002 , 3 , 11 )]}) ... return df . describe () >>> f () A B count 3.0 3 mean 2.0 2001 - 07 - 16 16 : 00 : 00 min 1.0 2000 - 10 - 02 00 : 00 : 00 25 % 1.5 2001 - 03 - 20 00 : 00 : 00 50 % 2.0 2001 - 09 - 05 00 : 00 : 00 75 % 2.5 2001 - 12 - 07 12 : 00 : 00 max 3.0 2002 - 03 - 11 00 : 00 : 00 std 1.0 NaN Note Only supported for dataframes containing numeric data, and datetime data. Datetime_is_numeric defaults to True in JIT code.","title":"pd.DataFrame.describe"},{"location":"api_docs/pandas/dataframe/#pddataframediff","text":"pandas.DataFrame. diff (periods=1, axis=0) Supported Arguments periods : Integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ pd . Timestamp ( 2000 , 10 , 2 ), pd . Timestamp ( 2001 , 9 , 5 ), pd . Timestamp ( 2002 , 3 , 11 )]}) ... return df . diff ( 1 ) >>> f () A B 0 NaN NaT 1 1.0 338 days 2 1.0 187 days Note Only supported for dataframes containing float, non-null int, and datetime64ns values","title":"pd.DataFrame.diff"},{"location":"api_docs/pandas/dataframe/#pddataframemax","text":"pandas.DataFrame. max (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . max ( axis = 1 ) >>> f () 0 7 1 8 2 9 Note Only supported for dataframes containing float, non-null int, and datetime64ns values.","title":"pd.DataFrame.max"},{"location":"api_docs/pandas/dataframe/#pddataframemean","text":"pandas.DataFrame. mean (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . mean ( axis = 1 ) >>> f () 0 4.0 1 5.0 2 6.0 Note Only supported for dataframes containing float, non-null int, and datetime64ns values.","title":"pd.DataFrame.mean"},{"location":"api_docs/pandas/dataframe/#pddataframemedian","text":"pandas.DataFrame. median (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . median ( axis = 1 ) >>> f () 0 4.0 1 5.0 2 6.0 Note Only supported for dataframes containing float, non-null int, and datetime64ns values.","title":"pd.DataFrame.median"},{"location":"api_docs/pandas/dataframe/#pddataframemin","text":"pandas.DataFrame. min (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . min ( axis = 1 ) >>> f () 0 1 1 2 2 3 Note Only supported for dataframes containing float, non-null int, and datetime64ns values.","title":"pd.DataFrame.min"},{"location":"api_docs/pandas/dataframe/#pddataframenunique","text":"pandas.DataFrame. nunique (axis=0, dropna=True) Supported Arguments dropna : boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 1 , 1 , 1 ], \"C\" : [ 4 , None , 6 ]}) ... return df . nunique () >>> f () A 3 B 1 C 2","title":"pd.DataFrame.nunique"},{"location":"api_docs/pandas/dataframe/#pddataframepct_change","text":"pandas.DataFrame. pct_change (periods=1, fill_method='pad', limit=None, freq=None) Supported Arguments periods : Integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 10 , 100 , 1000 , 10000 ]}) ... return df . pct_change () >>> f () A 0 NaN 1 9.0 2 9.0 3 9.0","title":"pd.DataFrame.pct_change"},{"location":"api_docs/pandas/dataframe/#pddataframepipe","text":"pandas.DataFrame. pipe (func, *args, **kwargs) Supported Arguments func : JIT function or callable defined within a JIT function. Additional arguments for func can be passed as additional arguments. Note func cannot be a tuple Example Usage >>> @bodo . jit ... def f (): ... def g ( df , axis ): ... return df . max ( axis ) ... df = pd . DataFrame ({ \"A\" : [ 10 , 100 , 1000 , 10000 ]}) ... return df . pipe ( g , axis = 0 ) ... >>> f () A 10000 dtype : int64","title":"pd.DataFrame.pipe"},{"location":"api_docs/pandas/dataframe/#pddataframeprod","text":"pandas.DataFrame. prod (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . prod ( axis = 1 ) >>> f () A 6 B 120 C 504 dtype : int64","title":"pd.DataFrame.prod"},{"location":"api_docs/pandas/dataframe/#pddataframeproduct","text":"pandas.DataFrame. product (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . product ( axis = 1 ) >>> f () A 6 B 120 C 504 dtype : int64","title":"pd.DataFrame.product"},{"location":"api_docs/pandas/dataframe/#pddataframequantile","text":"pandas.DataFrame. quantile (q=0.5, axis=0, numeric_only=True, interpolation='linear') Supported Arguments q : Float or Int must be 0<= q <= 1 axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . quantile () >>> f () A 2.0 B 5.0 C 8.0 dtype : float64 dtype : int64","title":"pd.DataFrame.quantile"},{"location":"api_docs/pandas/dataframe/#pddataframestd","text":"pandas.DataFrame. std (axis=None, skipna=None, level=None, ddof=1, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . std ( axis = 1 ) >>> f () 0 3.0 1 3.0 2 3.0 dtype : float64","title":"pd.DataFrame.std"},{"location":"api_docs/pandas/dataframe/#pddataframesum","text":"pandas.DataFrame. sum (axis=None, skipna=None, level=None, numeric_only=None, min_count=0) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . sum ( axis = 1 ) >>> f () 0 12 1 15 2 18 dtype : int64","title":"pd.DataFrame.sum"},{"location":"api_docs/pandas/dataframe/#pddataframevar","text":"pandas.DataFrame. var (axis=None, skipna=None, level=None, ddof=1, numeric_only=None) Supported Arguments axis : Integer (0 or 1) Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . var ( axis = 1 ) >>> f () 0 9.0 1 9.0 2 9.0 dtype : float64","title":"pd.DataFrame.var"},{"location":"api_docs/pandas/dataframe/#pddataframememory_usage","text":"pandas.DataFrame. memory_usage (index=True, deep=False) Supported Arguments index : boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : np . array ([ 1 , 2 , 3 ], dtype = np . int64 ), \"B\" : np . array ([ 1 , 2 , 3 ], dtype = np . int32 ), \"C\" : [ \"1\" , \"2\" , \"3456689\" ]}) ... return df . memory_usage () >>> f () Index 24 A 24 B 12 C 42 dtype : int64","title":"pd.DataFrame.memory_usage"},{"location":"api_docs/pandas/dataframe/#reindexing-selection-label-manipulation","text":"","title":"Reindexing / Selection / Label manipulation"},{"location":"api_docs/pandas/dataframe/#pddataframedrop","text":"pandas.DataFrame. drop (labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') Only dropping columns supported, either using columns argument or setting axis=1 and using the labels argument labels and columns require constant string, or constant list/tuple of string values inplace supported with a constant boolean value All other arguments are unsupported Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... df . drop ( columns = [ \"B\" , \"C\" ], inplace = True ) ... return df >>> f () A 0 1 1 2 2 3","title":"pd.DataFrame.drop"},{"location":"api_docs/pandas/dataframe/#pddataframedrop_duplicates","text":"pandas.DataFrame. drop_duplicates (subset=None, keep='first', inplace=False, ignore_index=False) Supported Arguments subset : Constant list/tuple of String column names, Constant list/tuple of Integer column names, Constant String column names, Constant Integer column names Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 3 , 4 ], \"B\" : [ 1 , 1 , 3 , 3 ], \"C\" : [ 7 , 8 , 9 , 10 ]}) ... return df . drop_duplicates ( subset = [ \"A\" , \"B\" ]) >>> f () A B C 0 1 1 7 2 3 3 9 3 4 3 10","title":"pd.DataFrame.drop_duplicates"},{"location":"api_docs/pandas/dataframe/#pddataframeduplicated","text":"pandas.DataFrame. duplicated (subset=None, keep='first') Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 3 , 4 ], \"B\" : [ 1 , 1 , 3 , 3 ]}) ... return df . duplicated () >>> f () 0 False 1 True 2 False 3 False dtype : bool","title":"pd.DataFrame.duplicated"},{"location":"api_docs/pandas/dataframe/#pddataframefirst","text":"pandas.DataFrame. first (offset) Supported Arguments offset : String or Offset type String argument must be a valid frequency alias . Note DataFrame must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted. Example Usage >>> @bodo . jit ... def f ( df , offset ): ... return df . first ( offset ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : np . arange ( 100 , 200 )}, index = pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 100 )) >>> f ( df , \"2M\" ) A B 2022 - 01 - 01 00 : 00 : 00.000000000 0 100 2022 - 01 - 12 01 : 27 : 16.363636363 1 101 2022 - 01 - 23 02 : 54 : 32.727272727 2 102 2022 - 02 - 03 04 : 21 : 49.090909091 3 103 2022 - 02 - 14 05 : 49 : 05.454545454 4 104 2022 - 02 - 25 07 : 16 : 21.818181818 5 105","title":"pd.DataFrame.first"},{"location":"api_docs/pandas/dataframe/#pddataframeidxmax","text":"pandas.DataFrame. idxmax (axis=0, skipna=True) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . idxmax () >>> f () A 2 B 2 C 2 dtype : int64","title":"pd.DataFrame.idxmax"},{"location":"api_docs/pandas/dataframe/#pddataframeidxmin","text":"pandas.DataFrame. idxmin (axis=0, skipna=True) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . idxmax () >>> f () A 0 B 0 C 20 dtype : int64","title":"pd.DataFrame.idxmin"},{"location":"api_docs/pandas/dataframe/#pddataframelast","text":"pandas.DataFrame. last (offset) Supported Arguments offset : String or Offset type String argument must be a valid frequency alias Note DataFrame must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted. Example Usage >>> @bodo . jit ... def f ( df , offset ): ... return df . last ( offset ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : np . arange ( 100 , 200 )}, index = pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 100 )) >>> f ( df , \"2M\" ) A B 2024 - 11 - 05 16 : 43 : 38.181818176 94 194 2024 - 11 - 16 18 : 10 : 54.545454544 95 195 2024 - 11 - 27 19 : 38 : 10.909090912 96 196 2024 - 12 - 08 21 : 05 : 27.272727264 97 197 2024 - 12 - 19 22 : 32 : 43.636363632 98 198 2024 - 12 - 31 00 : 00 : 00.000000000 99 199","title":"pd.DataFrame.last"},{"location":"api_docs/pandas/dataframe/#pddataframerename","text":"pandas.DataFrame. rename (mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors='ignore') Supported Arguments mapper : must be constant dictionary. Can only be used alongside axis=1 columns : must be constant dictionary axis : Integer Can only be used alongside mapper argument copy : boolean inplace : must be constant boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . rename ( columns = { \"A\" : \"X\" , \"B\" : \"Y\" , \"C\" : \"Z\" }) >>> f () X Y Z 0 1 4 7 1 2 5 8 2 3 6 9","title":"pd.DataFrame.rename"},{"location":"api_docs/pandas/dataframe/#pddataframereset_index","text":"pandas.DataFrame. reset_index (level=None, drop=False, inplace=False, col_level=0, col_fill='') Supported Arguments level : Integer If specified, must drop all levels. drop : Constant boolean inplace : Constant boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}, index = [ \"X\" , \"Y\" , \"Z\" ]) ... return df . reset_index () >>> f () index A B C 0 X 1 4 7 1 Y 2 5 8 2 Z 3 6 9","title":"pd.DataFrame.reset_index"},{"location":"api_docs/pandas/dataframe/#pddataframeset_index","text":"pandas.DataFrame. set_index (keys, drop=True, append=False, inplace=False, verify_integrity=False) Supported Arguments keys: must be a constant string Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}, index = [ \"X\" , \"Y\" , \"Z\" ]) ... return df . set_index ( \"C\" ) >>> f () A B C 7 1 4 8 2 5 9 3 6","title":"pd.DataFrame.set_index"},{"location":"api_docs/pandas/dataframe/#pddataframetake","text":"pandas.DataFrame. take (indices, axis=0, is_copy=None) Supported Arguments indices: scalar Integer, Pandas Integer Array, Numpy Integer Array, Integer Series Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . take ( pd . Series ([ - 1 , - 2 ])) >>> f () A B C 2 3 6 9 1 2 5 8","title":"pd.DataFrame.take"},{"location":"api_docs/pandas/dataframe/#missing-data-handling","text":"","title":"Missing data handling"},{"location":"api_docs/pandas/dataframe/#pddataframedropna","text":"pandas.DataFrame. dropna (axis=0, how='any', thresh=None, subset=None, inplace=False) Supported Arguments how : Constant String: either \"all\" or \"any\" thresh : Integer subset : Constant list/tuple of String column names, Constant list/tuple of Integer column names, Constant String column names, Constant Integer column names Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , None ], \"B\" : [ 4 , 5 , None , None ], \"C\" : [ 6 , None , None , None ]}) ... df_1 = df . dropna ( how = \"all\" , subset = [ \"B\" , \"C\" ]) ... df_2 = df . dropna ( thresh = 3 ) ... formated_out = \" \\n \" . join ([ df_1 . to_string (), df_2 . to_string ()]) ... return formated_out >>> f () A B C 0 1 4 6 1 2 5 < NA > A B C 0 1 4 6","title":"pd.DataFrame.dropna"},{"location":"api_docs/pandas/dataframe/#pddataframefillna","text":"pandas.DataFrame. fillna (value=None, method=None, axis=None, inplace=False, limit=None, downcast=None) Supported Arguments value : various scalars Must be of the same type as the filled column inplace : Constant boolean inplace is not supported alongside method method : One of bfill , backfill , ffill , or pad Must be constant at Compile Time inplace is not supported alongside method Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , None ], \"B\" : [ 4 , 5 , None , None ], \"C\" : [ 6 , None , None , None ]}) ... return df . fillna ( - 1 ) >>> f ()","title":"pd.DataFrame.fillna"},{"location":"api_docs/pandas/dataframe/#pddataframereplace","text":"pandas.DataFrame. replace (to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad') Supported Arguments to_replace : various scalars Required argument value : various scalars Must be of the same type as to_replace Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . replace ( 1 , - 1 ) >>> f () A B C 0 - 1 4 7 1 2 5 8 2 3 6 9","title":"pd.DataFrame.replace"},{"location":"api_docs/pandas/dataframe/#reshaping-sorting-transposing","text":"","title":"Reshaping, sorting, transposing"},{"location":"api_docs/pandas/dataframe/#pddataframeexplode","text":"pandas.DataFrame. explode (column, ignore_index=False) Supported Arguments column : Constant Column label or list of labels Example Usage >>> @bodo . jit ... def f ( df , cols ): ... return df . explode ( cols ) >>> df = pd . DataFrame ({ \"A\" : [[ 0 , 1 , 2 ], [ 5 ], [], [ 3 , 4 ]], \"B\" : [ 1 , 7 , 2 , 4 ], \"C\" : [[ 1 , 2 , 3 ], np . nan , [], [ 1 , 2 ]]}) >>> f ( df , [ \"A\" , \"C\" ]) A B C 0 0 1 1 0 1 1 2 0 2 1 3 1 5 7 < NA > 2 < NA > 2 < NA > 3 3 4 1 3 4 4 2","title":"pd.DataFrame.explode"},{"location":"api_docs/pandas/dataframe/#pddataframepivot","text":"pandas.DataFrame. pivot (values=None, index=None, columns=None) Supported Arguments values : Constant Column Label or list of labels index : Constant Column Label or list of labels columns : Constant Column Label Note The the number of columns and names of the output DataFrame won't be known at compile time. To update typing information on DataFrame you should pass it back to Python. Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ \"X\" , \"X\" , \"X\" , \"X\" , \"Y\" , \"Y\" ], \"B\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], \"C\" : [ 10 , 11 , 12 , 20 , 21 , 22 ]}) ... pivoted_tbl = df . pivot ( columns = \"A\" , index = \"B\" , values = \"C\" ) ... return pivoted_tbl >>> f () A X Y B 1 10.0 NaN 2 11.0 NaN 3 12.0 NaN 4 20.0 NaN 5 NaN 21.0 6 NaN 22.0","title":"pd.DataFrame.pivot"},{"location":"api_docs/pandas/dataframe/#pddataframepivot_table","text":"pandas.DataFrame. pivot_table (values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True) Supported Arguments values : Constant Column Label or list of labels index : Constant Column Label or list of labels columns : Constant Column Label aggfunc : String Constant Note This code takes two different paths depending on if pivot values are annotated. When pivot values are annotated then output columns are set to the annotated values. For example, @bodo.jit(pivots={'pt': ['small', 'large']}) declares the output pivot table pt will have columns called small and large . If pivot values are not annotated, then the number of columns and names of the output DataFrame won't be known at compile time. To update typing information on DataFrame you should pass it back to Python. Example Usage >>> @bodo . jit ( pivots = { 'pivoted_tbl' : [ 'X' , 'Y' ]}) ... def f (): ... df = pd . DataFrame ({ \"A\" : [ \"X\" , \"X\" , \"X\" , \"X\" , \"Y\" , \"Y\" ], \"B\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], \"C\" : [ 10 , 11 , 12 , 20 , 21 , 22 ]}) ... pivoted_tbl = df . pivot_table ( columns = \"A\" , index = \"B\" , values = \"C\" , aggfunc = \"mean\" ) ... return pivoted_tbl >>> f () X Y B 1 10.0 NaN 2 11.0 NaN 3 12.0 NaN 4 20.0 NaN 5 NaN 21.0 6 NaN 22.0","title":"pd.DataFrame.pivot_table"},{"location":"api_docs/pandas/dataframe/#pddataframesample","text":"pandas.DataFrame. sample (n=None, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False) Supported Arguments n : Integer frac : Float replace : boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ], \"C\" : [ 7 , 8 , 9 ]}) ... return df . sample ( 1 ) >>> f () A B C 2 3 6 9","title":"pd.DataFrame.sample"},{"location":"api_docs/pandas/dataframe/#pddataframesort_index","text":"pandas.DataFrame. sort_index (axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None) Supported Arguments ascending : boolean na_position :constant String (\"first\" or \"last\") Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}, index = [ 1 , None , 3 ]) ... return df . sort_index ( ascending = False , na_position = \"last\" ) >>> f () A 3 3 1 1 NaN 2","title":"pd.DataFrame.sort_index"},{"location":"api_docs/pandas/dataframe/#pddataframesort_values","text":"pandas.DataFrame. sort_values (by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None) Supported Arguments by : constant String or constant list of strings ascending : boolean, list/tuple of boolean, with length equal to the number of key columns inplace : Constant boolean na_position : constant String (\"first\" or \"last\"), constant list/tuple of String, with length equal to the number of key columns Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 2 , None ], \"B\" : [ 4 , 5 , 6 , None ]}) ... df . sort_values ( by = [ \"A\" , \"B\" ], ascending = [ True , False ], na_position = [ \"first\" , \"last\" ], inplace = True ) ... return df >>> f () A B 3 < NA > < NA > 0 1 4 2 2 6 1 2 5","title":"pd.DataFrame.sort_values"},{"location":"api_docs/pandas/dataframe/#pddataframeto_string","text":"pandas.DataFrame. to_string (buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, min_rows=None, max_cols=None, show_dimensions=False, decimal='.', line_width=None, max_colwidth=None, encoding=None) Supported Arguments buf columns col_space header index na_rep formatters float_format sparsify index_names justify max_rows min_rows max_cols how_dimensions decimal line_width max_colwidth encoding Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) ... return df . to_string () >>> f () A 0 1 1 2 2 3 Note This function is not optimized. When called on a distributed dataframe, the string returned for each rank will be reflective of the dataframe for that rank.","title":"pd.DataFrame.to_string"},{"location":"api_docs/pandas/dataframe/#combining-joining-merging","text":"","title":"Combining / joining / merging"},{"location":"api_docs/pandas/dataframe/#pddataframeappend","text":"pandas.DataFrame. append (other, ignore_index=False, verify_integrity=False, sort=False) Supported Arguments other : DataFrame, list/tuple of DataFrame ignore_index : constant boolean Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) ... return df . append ( pd . DataFrame ({ \"A\" : [ - 1 , - 2 , - 3 ], \"C\" : [ 4 , 5 , 6 ]})) >>> f () A B C 0 1 4.0 NaN 1 2 5.0 NaN 2 3 6.0 NaN 0 - 1 NaN 4.0 1 - 2 NaN 5.0 2 - 3 NaN 6.0","title":"pd.DataFrame.append"},{"location":"api_docs/pandas/dataframe/#pddataframeassign","text":"pandas.DataFrame. assign (**kwargs) Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) ... df2 = df . assign ( C = 2 * df [ \"B\" ], D = lambda x : x . C - 1 ) ... return df2 >>> f () A B C D 0 1 4 8 - 8 1 2 5 10 - 10 2 3 6 12 - 12 Note arguments can be JIT functions, lambda functions, or values that can be used to initialize a Pandas Series.","title":"pd.DataFrame.assign"},{"location":"api_docs/pandas/dataframe/#pddataframejoin","text":"pandas.DataFrame. join (other, on=None, how='left', lsuffix='', rsuffix='', sort=False) Supported Arguments other : DataFrame on : constant string column name, constant list/tuple of column names Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) ... return df . join ( on = \"A\" , other = pd . DataFrame ({ \"C\" : [ - 1 , - 2 , - 3 ], \"D\" : [ 4 , 5 , 6 ]})) >>> f () A B C D 0 1 4 - 2 5 1 1 5 - 2 5 2 3 6 < NA > < NA > Note Joined dataframes cannot have common columns. The output dataframe is not sorted by default for better parallel performance","title":"pd.DataFrame.join"},{"location":"api_docs/pandas/dataframe/#pddataframemerge","text":"pandas.DataFrame. merge (right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None) Note See pd.merge for full list of supported arguments, and more examples. Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) ... return df . merge ( pd . DataFrame ({ \"C\" : [ - 1 , - 2 , - 3 ], \"D\" : [ 4 , 4 , 6 ]}), left_on = \"B\" , right_on = \"D\" ) >>> f () A B C D 0 1 4 - 1 4 1 1 4 - 2 4 2 3 6 - 3 6","title":"pd.DataFrame.merge"},{"location":"api_docs/pandas/dataframe/#time-series-related","text":"","title":"Time series-related"},{"location":"api_docs/pandas/dataframe/#pddataframeshift","text":"pandas.DataFrame. shift (periods=1, freq=None, axis=0, fill_value=NoDefault.no_default) Supported Arguments periods : Integer Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) ... return df . shift ( 1 ) >>> f () A B 0 NaN NaN 1 1.0 4.0 2 1.0 5.0 Note Only supported for dataframes containing numeric, boolean, datetime.date and string types.","title":"pd.DataFrame.shift"},{"location":"api_docs/pandas/dataframe/#serialization-io-conversion","text":"Also see S3 and HDFS configuration requirements and more on Scalable File I/O .","title":"Serialization, IO, Conversion"},{"location":"api_docs/pandas/dataframe/#pddataframeto_csv","text":"pandas.DataFrame. to_csv compression argument defaults to None in JIT code. This is the only supported value of this argument. mode argument supports only the default value \"w\" . errors argument supports only the default value strict . storage_options argument supports only the default value None .","title":"pd.DataFrame.to_csv"},{"location":"api_docs/pandas/dataframe/#pddataframeto_json","text":"pandas.DataFrame. to_json","title":"pd.DataFrame.to_json"},{"location":"api_docs/pandas/dataframe/#pddataframeto_parquet","text":"pandas.DataFrame. to_parquet","title":"pd.DataFrame.to_parquet"},{"location":"api_docs/pandas/dataframe/#pddataframeto_sql","text":"pandas.DataFrame. to_sql See Example Usage and more system specific instructions . Argument con is supported but only as a string form. SQLalchemy connectable is not supported. Argument name , schema , if_exists , index , index_label , dtype , method are supported. Argument chunksize is not supported.","title":"pd.DataFrame.to_sql"},{"location":"api_docs/pandas/dataframe/#plotting","text":"","title":"Plotting"},{"location":"api_docs/pandas/dataframe/#pddataframeplot","text":"pandas.DataFrame. plot (x=None, y=None, kind=\"line\", figsize=None, xlabel=None, ylabel=None, title=None, legend=True, fontsize=None, xticks=None, yticks=None, ax=None) Supported Arguments x : Constant String column name, Constant integer y : Constant String column name, Constant integer kind : constant String (\"line\" or \"scatter\") figsize : constant numeric tuple (width, height) xlabel : constant String ylabel : constant String title : constant String legend : boolean fontsize : integer xticks : Constant Tuple yticks : Constant Tuple ax : Matplotlib Axes Object","title":"pd.DataFrame.plot"},{"location":"api_docs/pandas/datatypes/","text":"Data Types \u00b6 Bodo supports the following data types as values in Pandas Dataframe and Series data structures. This represents all Pandas data types except TZ-aware datetime , Period , Interval , and Sparse (which will be supported in the future). Comparing to Spark, equivalents of all Spark data types are supported. Numpy booleans: np.bool_ . Numpy integer data types: np.int8 , np.int16 , np.int32 , np.int64 , np.uint8 , np.uint16 , np.uint32 , np.uint64 . Numpy floating point data types: np.float32 , np.float64 . Numpy datetime data types: np.dtype(\"datetime64[ns]\") and np.dtype(\"timedelta[ns]\") . The resolution has to be ns currently, which covers most practical use cases. Numpy complex data types: np.complex64 and np.complex128 . Strings (including nulls). datetime.date values (including nulls). datetime.timedelta values (including nulls). Pandas nullable integers . Pandas nullable booleans . Pandas Categoricals . Lists of other data types. Tuples of other data types. Structs of other data types. Maps of other data types (each map is a set of key-value pairs). All keys should have the same type to ensure type stability. All values should have the same type as well. decimal.Decimal values (including nulls). The decimal values are stored as fixed-precision Apache Arrow Decimal128 format, which is also similar to PySpark decimals . The decimal type has a precision (the maximum total number of digits) and a scale (the number of digits on the right of dot) attribute, specifying how the stored data is interpreted. For example, the (4, 2) case can store from -999.99 to 999.99. The precision can be up to 38, and the scale must be less or equal to precision. Arbitrary-precision Python decimal.Decimal values are converted with precision of 38 and scale of 18. In addition, it may be desirable to specify type annotations in some cases ( e.g. , file I/O array input types ). Typically these types are array types and they all can be accessed directly from the bodo module. The following table can be used to select the necessary Bodo Type based upon the desired Python, Numpy, or Pandas type. Bodo Type Name Equivalent Python, Numpy, or Pandas type bodo.bool_[:] , bodo.int8[:] , ..., bodo.int64[:] , bodo.uint8[:] , ..., bodo.uint64[:] , bodo.float32[:] , bodo.float64[:] One-dimensional Numpy array of the given type. A full list of supported Numpy types can be found here . A multidimensional can be specified by adding additional colons ( e.g. , bodo.int32[:, :, :] for a three-dimensional array). bodo.string_array_type Array of nullable strings bodo.IntegerArrayType(integer_type) Array of Pandas nullable integers of the given integer type. e.g. , bodo.IntegerArrayType(bodo.int64) bodo.boolean_array Array of Pandas nullable booleans bodo.datetime64ns[:] Array of Numpy datetime64 values bodo.timedelta64ns[:] Array of Numpy timedelta64 values bodo.datetime_date_array_type Array of datetime.date types bodo.datetime_timedelta_array_type Array of datetime.timedelta types bodo.DecimalArrayType(precision, scale) Array of Apache Arrow Decimal128 values with the given precision and scale. e.g. , bodo.DecimalArrayType(38, 18) bodo.binary_array_type Array of nullable bytes values bodo.StructArrayType(data_types, field_names) Array of a user defined struct with the given tuple of data types and field names. e.g. , bodo.StructArrayType((bodo.int32[:], bodo.datetime64ns[:]), (\"a\", \"b\")) bodo.TupleArrayType(data_types) Array of a user defined tuple with the given tuple of data types. e.g. , bodo.TupleArrayType((bodo.int32[:], bodo.datetime64ns[:])) bodo.MapArrayType(key_arr_type, value_arr_type) Array of Python dictionaries with the given key and value array types. e.g. , bodo.MapArrayType(bodo.uint16[:], bodo.string_array_type) bodo.PDCategoricalDtype(cat_tuple, cat_elem_type, is_ordered_cat) Pandas categorical type with the possible categories, each category's type, and if the categories are ordered. e.g. , bodo.PDCategoricalDtype((\"A\", \"B\", \"AA\"), bodo.string_type, True) bodo.CategoricalArrayType(categorical_type) Array of Pandas categorical values. e.g. , bodo.CategoricalArrayType(bodo.PDCategoricalDtype((\"A\", \"B\", \"AA\"), bodo.string_type, True)) bodo.DatetimeIndexType(name_type) Index of datetime64 values with a given name type. e.g. , bodo.DatetimeIndexType(bodo.string_type) bodo.NumericIndexType(data_type, name_type) Index of pd.Int64 , pd.Uint64 , or Float64 objects, based upon the given data_type and name type. e.g. , bodo.NumericIndexType(bodo.float64, bodo.string_type) bodo.PeriodIndexType(freq, name_type) pd.PeriodIndex with a given frequency and name type. e.g. , bodo.PeriodIndexType('A', bodo.string_type) bodo.RangeIndexType(name_type) RangeIndex with a given name type. e.g. , bodo.RangeIndexType(bodo.string_type) bodo.StringIndexType(name_type) Index of strings with a given name type. e.g. , bodo.StringIndexType(bodo.string_type) bodo.BinaryIndexType(name_type) Index of binary values with a given name type. e.g. , bodo.BinaryIndexType(bodo.string_type) bodo.TimedeltaIndexType(name_type) Index of timedelta64 values with a given name type. e.g. , bodo.TimedeltaIndexType(bodo.string_type) bodo.SeriesType(dtype=data_type, index=index_type, name_typ=name_type) Series with a given data type, index type, and name type. e.g. , bodo.SeriesType(bodo.float32, bodo.DatetimeIndexType(bodo.string_type), bodo.string_type) bodo.DataFrameType(data_types_tuple, index_type, column_names) DataFrame with a tuple of data types, an index type, and the names of the columns. e.g. , bodo.DataFrameType((bodo.int64[::1], bodo.float64[::1]), bodo.RangeIndexType(bodo.none), (\"A\", \"B\"))","title":"Datatypes"},{"location":"api_docs/pandas/datatypes/#pandas-dtype","text":"Bodo supports the following data types as values in Pandas Dataframe and Series data structures. This represents all Pandas data types except TZ-aware datetime , Period , Interval , and Sparse (which will be supported in the future). Comparing to Spark, equivalents of all Spark data types are supported. Numpy booleans: np.bool_ . Numpy integer data types: np.int8 , np.int16 , np.int32 , np.int64 , np.uint8 , np.uint16 , np.uint32 , np.uint64 . Numpy floating point data types: np.float32 , np.float64 . Numpy datetime data types: np.dtype(\"datetime64[ns]\") and np.dtype(\"timedelta[ns]\") . The resolution has to be ns currently, which covers most practical use cases. Numpy complex data types: np.complex64 and np.complex128 . Strings (including nulls). datetime.date values (including nulls). datetime.timedelta values (including nulls). Pandas nullable integers . Pandas nullable booleans . Pandas Categoricals . Lists of other data types. Tuples of other data types. Structs of other data types. Maps of other data types (each map is a set of key-value pairs). All keys should have the same type to ensure type stability. All values should have the same type as well. decimal.Decimal values (including nulls). The decimal values are stored as fixed-precision Apache Arrow Decimal128 format, which is also similar to PySpark decimals . The decimal type has a precision (the maximum total number of digits) and a scale (the number of digits on the right of dot) attribute, specifying how the stored data is interpreted. For example, the (4, 2) case can store from -999.99 to 999.99. The precision can be up to 38, and the scale must be less or equal to precision. Arbitrary-precision Python decimal.Decimal values are converted with precision of 38 and scale of 18. In addition, it may be desirable to specify type annotations in some cases ( e.g. , file I/O array input types ). Typically these types are array types and they all can be accessed directly from the bodo module. The following table can be used to select the necessary Bodo Type based upon the desired Python, Numpy, or Pandas type. Bodo Type Name Equivalent Python, Numpy, or Pandas type bodo.bool_[:] , bodo.int8[:] , ..., bodo.int64[:] , bodo.uint8[:] , ..., bodo.uint64[:] , bodo.float32[:] , bodo.float64[:] One-dimensional Numpy array of the given type. A full list of supported Numpy types can be found here . A multidimensional can be specified by adding additional colons ( e.g. , bodo.int32[:, :, :] for a three-dimensional array). bodo.string_array_type Array of nullable strings bodo.IntegerArrayType(integer_type) Array of Pandas nullable integers of the given integer type. e.g. , bodo.IntegerArrayType(bodo.int64) bodo.boolean_array Array of Pandas nullable booleans bodo.datetime64ns[:] Array of Numpy datetime64 values bodo.timedelta64ns[:] Array of Numpy timedelta64 values bodo.datetime_date_array_type Array of datetime.date types bodo.datetime_timedelta_array_type Array of datetime.timedelta types bodo.DecimalArrayType(precision, scale) Array of Apache Arrow Decimal128 values with the given precision and scale. e.g. , bodo.DecimalArrayType(38, 18) bodo.binary_array_type Array of nullable bytes values bodo.StructArrayType(data_types, field_names) Array of a user defined struct with the given tuple of data types and field names. e.g. , bodo.StructArrayType((bodo.int32[:], bodo.datetime64ns[:]), (\"a\", \"b\")) bodo.TupleArrayType(data_types) Array of a user defined tuple with the given tuple of data types. e.g. , bodo.TupleArrayType((bodo.int32[:], bodo.datetime64ns[:])) bodo.MapArrayType(key_arr_type, value_arr_type) Array of Python dictionaries with the given key and value array types. e.g. , bodo.MapArrayType(bodo.uint16[:], bodo.string_array_type) bodo.PDCategoricalDtype(cat_tuple, cat_elem_type, is_ordered_cat) Pandas categorical type with the possible categories, each category's type, and if the categories are ordered. e.g. , bodo.PDCategoricalDtype((\"A\", \"B\", \"AA\"), bodo.string_type, True) bodo.CategoricalArrayType(categorical_type) Array of Pandas categorical values. e.g. , bodo.CategoricalArrayType(bodo.PDCategoricalDtype((\"A\", \"B\", \"AA\"), bodo.string_type, True)) bodo.DatetimeIndexType(name_type) Index of datetime64 values with a given name type. e.g. , bodo.DatetimeIndexType(bodo.string_type) bodo.NumericIndexType(data_type, name_type) Index of pd.Int64 , pd.Uint64 , or Float64 objects, based upon the given data_type and name type. e.g. , bodo.NumericIndexType(bodo.float64, bodo.string_type) bodo.PeriodIndexType(freq, name_type) pd.PeriodIndex with a given frequency and name type. e.g. , bodo.PeriodIndexType('A', bodo.string_type) bodo.RangeIndexType(name_type) RangeIndex with a given name type. e.g. , bodo.RangeIndexType(bodo.string_type) bodo.StringIndexType(name_type) Index of strings with a given name type. e.g. , bodo.StringIndexType(bodo.string_type) bodo.BinaryIndexType(name_type) Index of binary values with a given name type. e.g. , bodo.BinaryIndexType(bodo.string_type) bodo.TimedeltaIndexType(name_type) Index of timedelta64 values with a given name type. e.g. , bodo.TimedeltaIndexType(bodo.string_type) bodo.SeriesType(dtype=data_type, index=index_type, name_typ=name_type) Series with a given data type, index type, and name type. e.g. , bodo.SeriesType(bodo.float32, bodo.DatetimeIndexType(bodo.string_type), bodo.string_type) bodo.DataFrameType(data_types_tuple, index_type, column_names) DataFrame with a tuple of data types, an index type, and the names of the columns. e.g. , bodo.DataFrameType((bodo.int64[::1], bodo.float64[::1]), bodo.RangeIndexType(bodo.none), (\"A\", \"B\"))","title":"Data Types"},{"location":"api_docs/pandas/dateoffsets/","text":"Date Offsets \u00b6 Bodo supports a subset of the offset types in pandas.tseries.offsets : DateOffset \u00b6 pd.tseries.offsets.DateOffset \u00b6 pandas.tseries.offsets. DateOffset (n=1, normalize=False, years=None, months=None, weeks=None, days=None, hours=None, minutes=None, seconds=None, microseconds=None, nanoseconds=None, year=None, month=None, day=None, weekday=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None) Supported Arguments n : integer normalize : boolean years : integer months : integer weeks : integer days : integer hours : integer minutes : integer seconds : integer microseconds : integer nanoseconds : integer year : integer month : integer weekday : integer day : integer hour : integer minute : integer second : integer microsecond : integer nanosecond : integer Example Usage >>> @bodo . jit >>> def f ( ts ): ... return ts + pd . tseries . offsets . DateOffset ( n = 4 , normalize = True , weeks = 11 , hour = 2 ) >>> ts = pd . Timestamp ( year = 2020 , month = 10 , day = 30 , hour = 22 ) >>> f ( ts ) Timestamp ( '2021-09-03 02:00:00' ) Properties \u00b6 pd.tseries.offsets.DateOffset.normalize` \u00b6 pandas.tseries.offsets.DateOffset. normalize pd.tseries.offsets.DateOffset.n \u00b6 pandas.tseries.offsets. DateOffset .n MonthBegin \u00b6 pd.tseries.offsets.MonthBegin \u00b6 pandas.tseries.offsets. MonthBegin (n=1, normalize=False) Supported Arguments n : integer normalize : boolean Example Usage >>> @bodo . jit >>> def f ( ts ): ... return ts + pd . tseries . offsets . MonthBegin ( n = 4 , normalize = True ) >>> ts = pd . Timestamp ( year = 2020 , month = 10 , day = 30 , hour = 22 ) >>> f ( ts ) Timestamp ( '2021-02-01 00:00:00' ) MonthEnd \u00b6 pd.tseries.offsets.MonthEnd \u00b6 pandas.tseries.offsets. MonthEnd (n=1, normalize=False) Supported Arguments n : integer normalize : boolean Example Usage >>> @bodo . jit >>> def f ( ts ): ... return ts + pd . tseries . offsets . MonthEnd ( n = 4 , normalize = False ) >>> ts = pd . Timestamp ( year = 2020 , month = 10 , day = 30 , hour = 22 ) >>> f ( ts ) Timestamp ( '2021-01-31 22:00:00' ) Week \u00b6 pd.tseries.offsets.Week \u00b6 pandas.tseries.offsets. Week (n=1, normalize=False, weekday=None) Supported Arguments n : integer normalize : boolean weekday : integer Example Usage >>> @bodo . jit >>> def f ( ts ): ... return ts + pd . tseries . offsets . Week ( n = 4 , normalize = True , weekday = 5 ) >>> ts = pd . Timestamp ( year = 2020 , month = 10 , day = 30 , hour = 22 ) >>> f ( ts ) Timestamp ( '2020-11-21 00:00:00' ) Binary Operations \u00b6 For all offsets, addition and subtraction with a scalar datetime.date , datetime.datetime or pandas.Timestamp is supported. Multiplication is also supported with a scalar integer.","title":"DateOffsets"},{"location":"api_docs/pandas/dateoffsets/#date-offsets","text":"Bodo supports a subset of the offset types in pandas.tseries.offsets :","title":"Date Offsets"},{"location":"api_docs/pandas/dateoffsets/#dateoffset","text":"","title":"DateOffset"},{"location":"api_docs/pandas/dateoffsets/#pdtseriesoffsetsdateoffset","text":"pandas.tseries.offsets. DateOffset (n=1, normalize=False, years=None, months=None, weeks=None, days=None, hours=None, minutes=None, seconds=None, microseconds=None, nanoseconds=None, year=None, month=None, day=None, weekday=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None) Supported Arguments n : integer normalize : boolean years : integer months : integer weeks : integer days : integer hours : integer minutes : integer seconds : integer microseconds : integer nanoseconds : integer year : integer month : integer weekday : integer day : integer hour : integer minute : integer second : integer microsecond : integer nanosecond : integer Example Usage >>> @bodo . jit >>> def f ( ts ): ... return ts + pd . tseries . offsets . DateOffset ( n = 4 , normalize = True , weeks = 11 , hour = 2 ) >>> ts = pd . Timestamp ( year = 2020 , month = 10 , day = 30 , hour = 22 ) >>> f ( ts ) Timestamp ( '2021-09-03 02:00:00' )","title":"pd.tseries.offsets.DateOffset"},{"location":"api_docs/pandas/dateoffsets/#properties","text":"","title":"Properties"},{"location":"api_docs/pandas/dateoffsets/#pdtseriesoffsetsdateoffsetnormalize","text":"pandas.tseries.offsets.DateOffset. normalize","title":"pd.tseries.offsets.DateOffset.normalize`"},{"location":"api_docs/pandas/dateoffsets/#pdtseriesoffsetsdateoffsetn","text":"pandas.tseries.offsets. DateOffset .n","title":"pd.tseries.offsets.DateOffset.n"},{"location":"api_docs/pandas/dateoffsets/#monthbegin","text":"","title":"MonthBegin"},{"location":"api_docs/pandas/dateoffsets/#pdtseriesoffsetsmonthbegin","text":"pandas.tseries.offsets. MonthBegin (n=1, normalize=False) Supported Arguments n : integer normalize : boolean Example Usage >>> @bodo . jit >>> def f ( ts ): ... return ts + pd . tseries . offsets . MonthBegin ( n = 4 , normalize = True ) >>> ts = pd . Timestamp ( year = 2020 , month = 10 , day = 30 , hour = 22 ) >>> f ( ts ) Timestamp ( '2021-02-01 00:00:00' )","title":"pd.tseries.offsets.MonthBegin"},{"location":"api_docs/pandas/dateoffsets/#monthend","text":"","title":"MonthEnd"},{"location":"api_docs/pandas/dateoffsets/#pdtseriesoffsetsmonthend","text":"pandas.tseries.offsets. MonthEnd (n=1, normalize=False) Supported Arguments n : integer normalize : boolean Example Usage >>> @bodo . jit >>> def f ( ts ): ... return ts + pd . tseries . offsets . MonthEnd ( n = 4 , normalize = False ) >>> ts = pd . Timestamp ( year = 2020 , month = 10 , day = 30 , hour = 22 ) >>> f ( ts ) Timestamp ( '2021-01-31 22:00:00' )","title":"pd.tseries.offsets.MonthEnd"},{"location":"api_docs/pandas/dateoffsets/#week","text":"","title":"Week"},{"location":"api_docs/pandas/dateoffsets/#pdtseriesoffsetsweek","text":"pandas.tseries.offsets. Week (n=1, normalize=False, weekday=None) Supported Arguments n : integer normalize : boolean weekday : integer Example Usage >>> @bodo . jit >>> def f ( ts ): ... return ts + pd . tseries . offsets . Week ( n = 4 , normalize = True , weekday = 5 ) >>> ts = pd . Timestamp ( year = 2020 , month = 10 , day = 30 , hour = 22 ) >>> f ( ts ) Timestamp ( '2020-11-21 00:00:00' )","title":"pd.tseries.offsets.Week"},{"location":"api_docs/pandas/dateoffsets/#binary-operations","text":"For all offsets, addition and subtraction with a scalar datetime.date , datetime.datetime or pandas.Timestamp is supported. Multiplication is also supported with a scalar integer.","title":"Binary Operations"},{"location":"api_docs/pandas/general/","text":"General functions \u00b6 Data manipulations \u00b6 pd.pivot \u00b6 pandas. pivot (data, values=None, index=None, columns=None) Supported Arguments argument datatypes data DataFrame values Constant Column Label or list of labels index Constant Column Label or list of labels columns Constant Column Label Note The the number of columns and names of the output DataFrame won't be known at compile time. To update typing information on DataFrame you should pass it back to Python. Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ \"X\" , \"X\" , \"X\" , \"X\" , \"Y\" , \"Y\" ], \"B\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], \"C\" : [ 10 , 11 , 12 , 20 , 21 , 22 ]}) ... pivoted_tbl = pd . pivot ( data , columns = \"A\" , index = \"B\" , values = \"C\" ) ... return pivoted_tbl >>> f () A X Y B 1 10.0 NaN 2 11.0 NaN 3 12.0 NaN 4 20.0 NaN 5 NaN 21.0 6 NaN 22.0 pd.pivot_table \u00b6 pandas. pivot_table (data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True) Supported Arguments argument datatypes data DataFrame values Constant Column Label or list of labels index Constant Column Label or list of labels columns Constant Column Label aggfunc String Constant Note This code takes two different paths depending on if pivot values are annotated. When pivot values are annotated then output columns are set to the annotated values. For example, @bodo.jit(pivots={'pt': ['small', 'large']}) declares the output pivot table pt will have columns called small and large . If pivot values are not annotated, then the number of columns and names of the output DataFrame won't be known at compile time. To update typing information on DataFrame you should pass it back to Python. Example Usage >>> @bodo . jit ( pivots = { 'pivoted_tbl' : [ 'X' , 'Y' ]}) ... def f (): ... df = pd . DataFrame ({ \"A\" : [ \"X\" , \"X\" , \"X\" , \"X\" , \"Y\" , \"Y\" ], \"B\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], \"C\" : [ 10 , 11 , 12 , 20 , 21 , 22 ]}) ... pivoted_tbl = pd . pivot_table ( df , columns = \"A\" , index = \"B\" , values = \"C\" , aggfunc = \"mean\" ) ... return pivoted_tbl >>> f () X Y B 1 10.0 NaN 2 11.0 NaN 3 12.0 NaN 4 20.0 NaN 5 NaN 21.0 6 NaN 22.0 pd.crosstab \u00b6 pandas. crosstab (index, columns, values=None, rownames=None, colnames=None, aggfunc=None, margins=False, margins_name='All', dropna=True, normalize=False) Supported Arguments argument datatypes index SeriesType columns SeriesType Note Annotation of pivot values is required. For example, @bodo.jit(pivots={'pt': ['small', 'large']}) declares the output table pt will have columns called small and large . Example Usage >>> @bodo . jit ( pivots = { \"pt\" : [ \"small\" , \"large\" ]}) ... def f ( df ): ... pt = pd . crosstab ( df . A , df . C ) ... return pt >>> list_A = [ \"foo\" , \"foo\" , \"bar\" , \"bar\" , \"bar\" , \"bar\" ] >>> list_C = [ \"small\" , \"small\" , \"large\" , \"small\" , \"small\" , \"middle\" ] >>> df = pd . DataFrame ({ \"A\" : list_A , \"C\" : list_C }) >>> f ( df ) small large index foo 2 0 bar 2 1 pd.cut \u00b6 pandas. cut (x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates=\"raise\", ordered=True) Supported Arguments argument datatypes x Series or Array like bins Integer or Array like include_lowest Boolean Example Usage >>> @bodo . jit ... def f ( S ): ... bins = 4 ... include_lowest = True ... return pd . cut ( S , bins , include_lowest = include_lowest ) >>> S = pd . Series ( ... [ - 2 , 1 , 3 , 4 , 5 , 11 , 15 , 20 , 22 ], ... [ \"a1\" , \"a2\" , \"a3\" , \"a4\" , \"a5\" , \"a6\" , \"a7\" , \"a8\" , \"a9\" ], ... name = \"ABC\" , ... ) >>> f ( S ) a1 ( - 2.025 , 4.0 ] a2 ( - 2.025 , 4.0 ] a3 ( - 2.025 , 4.0 ] a4 ( - 2.025 , 4.0 ] a5 ( 4.0 , 10.0 ] a6 ( 10.0 , 16.0 ] a7 ( 10.0 , 16.0 ] a8 ( 16.0 , 22.0 ] a9 ( 16.0 , 22.0 ] Name : ABC , dtype : category Categories ( 4 , interval [ float64 , right ]): [( - 2.025 , 4.0 ] < ( 4.0 , 10.0 ] < ( 10.0 , 16.0 ] < ( 16.0 , 22.0 ]] pd.qcut \u00b6 pandas. qcut (x, q, labels=None, retbins=False, precision=3, duplicates=\"raise\") Supported Arguments argument datatypes x Series or Array like q Integer or Array like of floats Example Usage >>> @bodo . jit ... def f ( S ): ... q = 4 ... return pd . qcut ( S , q ) >>> S = pd . Series ( ... [ - 2 , 1 , 3 , 4 , 5 , 11 , 15 , 20 , 22 ], ... [ \"a1\" , \"a2\" , \"a3\" , \"a4\" , \"a5\" , \"a6\" , \"a7\" , \"a8\" , \"a9\" ], ... name = \"ABC\" , ... ) >>> f ( S ) a1 ( - 2.001 , 3.0 ] a2 ( - 2.001 , 3.0 ] a3 ( - 2.001 , 3.0 ] a4 ( 3.0 , 5.0 ] a5 ( 3.0 , 5.0 ] a6 ( 5.0 , 15.0 ] a7 ( 5.0 , 15.0 ] a8 ( 15.0 , 22.0 ] a9 ( 15.0 , 22.0 ] Name : ABC , dtype : category Categories ( 4 , interval [ float64 , right ]): [( - 2.001 , 3.0 ] < ( 3.0 , 5.0 ] < ( 5.0 , 15.0 ] < ( 15.0 , 22.0 ]] pd.merge \u00b6 pandas. merge (left, right, how=\"inner\", on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=(\"_x\", \"_y\"), copy=True, indicator=False, validate=None, _bodo_na_equal=True) Supported Arguments argument datatypes other requirements left DataFrame right DataFrame how String Must be one of \"inner\" , \"outer\" , \"left\" , \"right\" Must be constant at Compile Time on Column Name, List of Column Names, or General Merge Condition String (see merge-notes ) Must be constant at Compile Time left_on Column Name or List of Column Names Must be constant at Compile Time right_on Column Name or List of Column Names Must be constant at Compile Time left_index Boolean Must be constant at Compile Time right_index Boolean Must be constant at Compile Time suffixes Tuple of Strings Must be constant at Compile Time indicator Boolean Must be constant at Compile Time _bodo_na_equal Boolean Must be constant at Compile Time This argument is unique to Bodo and not available in Pandas. If False, Bodo won't consider NA/nan keys as equal, which differs from Pandas. Important The argument _bodo_na_equal is unique to Bodo and not available in Pandas. If it is False , Bodo won't consider NA/nan keys as equal, which differs from Pandas. Merge Notes \u00b6 Output Ordering : The output dataframe is not sorted by default for better parallel performance (Pandas may preserve key order depending on how ). One can use explicit sort if needed. General Merge Conditions : Within Pandas, the merge criteria supported by pd.merge are limited to equality between 1 or more pairs of keys. For some use cases, this is not sufficient and more generalized support is necessary. For example, with these limitations, a left outer join where df1.A == df2.B & df2.C < df1.A cannot be efficiently computed. Bodo supports these use cases by allowing users to pass general merge conditions to pd.merge . We plan to contribute this feature to Pandas to ensure full compatibility of Bodo and Pandas code. General merge conditions are performed by providing the condition as a string via the on argument. Columns in the left table are referred to by left.{column name} and columns in the right table are referred to by right.{column name} . Here's an example demonstrating the above: >>> @bodo . jit ... def general_merge ( df1 , df2 ): ... return df1 . merge ( df2 , on = \"left.`A` == right.`B` & right.`C` < left.`A`\" , how = \"left\" ) >>> df1 = pd . DataFrame ({ \"col\" : [ 2 , 3 , 5 , 1 , 2 , 8 ], \"A\" : [ 4 , 6 , 3 , 9 , 9 , - 1 ]}) >>> df2 = pd . DataFrame ({ \"B\" : [ 1 , 2 , 9 , 3 , 2 ], \"C\" : [ 1 , 7 , 2 , 6 , 5 ]}) >>> general_merge ( df1 , df2 ) col A B C 0 2 4 < NA > < NA > 1 3 6 < NA > < NA > 2 5 3 < NA > < NA > 3 1 9 9 2 4 2 9 9 2 5 8 - 1 < NA > < NA > These calls have a few additional requirements: The condition must be constant string. The condition must be of the form cond_1 & ... & cond_N where at least one cond_i is a simple equality. This restriction will be removed in a future release. The columns specified in these conditions are limited to certain column types. We currently support boolean , integer , float , datetime64 , timedelta64 , datetime.date , and string columns. Example Usage >>> @bodo . jit ... def f ( df1 , df2 ): ... return pd . merge ( df1 , df2 , how = \"inner\" , on = \"key\" ) >>> df1 = pd . DataFrame ({ \"key\" : [ 2 , 3 , 5 , 1 , 2 , 8 ], \"A\" : np . array ([ 4 , 6 , 3 , 9 , 9 , - 1 ], float )}) >>> df2 = pd . DataFrame ({ \"key\" : [ 1 , 2 , 9 , 3 , 2 ], \"B\" : np . array ([ 1 , 7 , 2 , 6 , 5 ], float )}) >>> f ( df1 , df2 ) key A B 0 2 4.0 7.0 1 2 4.0 5.0 2 3 6.0 6.0 3 1 9.0 1.0 4 2 9.0 7.0 5 2 9.0 5.0 pd.merge_asof \u00b6 pandas. merge_asof (left, right, on=None, left_on=None, right_on=None, left_index=False, right_index=False, by=None, left_by=None, right_by=None, suffixes=(\"_x\", \"_y\"), tolerance=None, allow_exact_matches=True, direction=\"backward\") Supported Arguments argument datatypes other requirements left DataFrame right DataFrame on Column Name, List of Column Names Must be constant at Compile Time left_on Column Name or List of Column Names Must be constant at Compile Time right_on Column Name or List of Column Names Must be constant at Compile Time left_index Boolean Must be constant at Compile Time right_index Boolean Must be constant at Compile Time suffixes Tuple of Strings Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( df1 , df2 ): ... return pd . merge_asof ( df1 , df2 , on = \"time\" ) >>> df1 = pd . DataFrame ( ... { ... \"time\" : pd . DatetimeIndex ([ \"2017-01-03\" , \"2017-01-06\" , \"2017-02-21\" ]), ... \"B\" : [ 4 , 5 , 6 ], ... } ... ) >>> df2 = pd . DataFrame ( ... { ... \"time\" : pd . DatetimeIndex ( ... [ \"2017-01-01\" , \"2017-01-02\" , \"2017-01-04\" , \"2017-02-23\" , \"2017-02-25\" ] ... ), ... \"A\" : [ 2 , 3 , 7 , 8 , 9 ], ... } ... ) >>> f ( df1 , df2 ) time B A 0 2017 - 01 - 03 4 3 1 2017 - 01 - 06 5 7 2 2017 - 02 - 21 6 7 pd.concat \u00b6 pandas. concat (objs, axis=0, join=\"outer\", join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=None, copy=True) Supported Arguments argument datatypes other requirements objs List or Tuple of DataFrames/Series axis Integer with either 0 or 1 Must be constant at Compile Time ignore_index Boolean Must be constant at Compile Time Important Bodo currently concatenates local data chunks for distributed datasets, which does not preserve global order of concatenated objects in output. Example Usage >>> @bodo . jit ... def f ( df1 , df2 ): ... return pd . concat ([ df1 , df2 ], axis = 1 ) >>> df1 = pd . DataFrame ({ \"A\" : [ 3 , 2 , 1 , - 4 , 7 ]}) >>> df2 = pd . DataFrame ({ \"B\" : [ 3 , 25 , 1 , - 4 , - 24 ]}) >>> f ( df1 , df2 ) A B 0 3 3 1 2 25 2 1 1 3 - 4 - 4 4 7 - 24 pd.get_dummies \u00b6 pandas. get_dummies (data, prefix=None, prefix_sep=\"_\", dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None) Supported Arguments argument datatypes other requirements data Array or Series with Categorical dtypes Categories must be known at compile time. Example Usage >>> @bodo . jit ... def f ( S ): ... return pd . get_dummies ( S ) >>> S = pd . Series ([ \"CC\" , \"AA\" , \"B\" , \"D\" , \"AA\" , None , \"B\" , \"CC\" ]) . astype ( \"category\" ) >>> f ( S ) AA B CC D 0 0 0 1 0 1 1 0 0 0 2 0 1 0 0 3 0 0 0 1 4 1 0 0 0 5 0 0 0 0 6 0 1 0 0 7 0 0 1 0 Top-level missing data \u00b6 pd.isna \u00b6 pandas. isna (obj) Supported Arguments argument datatypes obj DataFrame, Series, Index, Array, or Scalar Example Usage >>> @bodo . jit ... def f ( df ): ... return pd . isna ( df ) >>> df = pd . DataFrame ( ... { \"A\" : [ \"AA\" , np . nan , \"\" , \"D\" , \"GG\" ], \"B\" : [ 1 , 8 , 4 , - 1 , 2 ]}, ... [ 1.1 , - 2.1 , 7.1 , 0.1 , 3.1 ], ... ) >>> f ( df ) A B 1.1 False False - 2.1 True False 7.1 False False 0.1 False False 3.1 False False pd.isnull \u00b6 pandas. isnull (obj) Supported Arguments argument datatypes obj DataFrame, Series, Index, Array, or Scalar Example Usage >>> @bodo . jit ... def f ( df ): ... return pd . isnull ( df ) >>> df = pd . DataFrame ( ... { \"A\" : [ \"AA\" , np . nan , \"\" , \"D\" , \"GG\" ], \"B\" : [ 1 , 8 , 4 , - 1 , 2 ]}, ... [ 1.1 , - 2.1 , 7.1 , 0.1 , 3.1 ], ... ) >>> f ( df ) A B 1.1 False False - 2.1 True False 7.1 False False 0.1 False False 3.1 False False pd.notna \u00b6 pandas. notna (obj) Supported Arguments argument datatypes obj DataFrame, Series, Index, Array, or Scalar Example Usage >>> @bodo . jit ... def f ( df ): ... return pd . notna ( df ) >>> df = pd . DataFrame ( ... { \"A\" : [ \"AA\" , np . nan , \"\" , \"D\" , \"GG\" ], \"B\" : [ 1 , 8 , 4 , - 1 , 2 ]}, ... [ 1.1 , - 2.1 , 7.1 , 0.1 , 3.1 ], ... ) >>> f ( df ) A B 1.1 True True - 2.1 False True 7.1 True True 0.1 True True 3.1 True True pd.notnull \u00b6 pandas. notnull (obj) Supported Arguments argument datatypes obj DataFrame, Series, Index, Array, or Scalar Example Usage >>> @bodo . jit ... def f ( df ): ... return pd . notnull ( df ) >>> df = pd . DataFrame ( ... { \"A\" : [ \"AA\" , np . nan , \"\" , \"D\" , \"GG\" ], \"B\" : [ 1 , 8 , 4 , - 1 , 2 ]}, ... [ 1.1 , - 2.1 , 7.1 , 0.1 , 3.1 ], ... ) >>> f ( df ) A B 1.1 True True - 2.1 False True 7.1 True True 0.1 True True 3.1 True True Top-level conversions \u00b6 pd.to_numeric \u00b6 pandas. to_numeric (arg, errors=\"raise\", downcast=None) Supported Arguments argument datatypes other requirements arg Series or Array downcast String and one of ( 'integer' , 'signed' , 'unsigned' , 'float' ) Must be constant at Compile Time Note Output type is float64 by default Unlike Pandas, Bodo does not dynamically determine output type, and does not downcast to the smallest numerical type. downcast parameter should be used for type annotation of output. Example Usage >>> @bodo . jit ... def f ( S ): ... return pd . to_numeric ( S , errors = \"coerce\" , downcast = \"integer\" ) >>> S = pd . Series ([ \"1\" , \"3\" , \"12\" , \"4\" , None , \"-555\" ]) >>> f ( S ) 0 1 1 3 2 12 3 4 4 < NA > 5 - 555 dtype : Int64 Top-level dealing with datetime and timedelta like \u00b6 pd.to_datetime \u00b6 pandas. to_datetime (arg, errors='raise', dayfirst=False, yearfirst=False, utc=None, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix', cache=True) Supported Arguments argument datatypes other requirements arg Series, Array or scalar of integers or strings errors String and one of ('ignore', 'raise', 'coerce') dayfirst Boolean yearfirst Boolean utc Boolean format String matching Pandas strftime /strptime exact Boolean unit String Must be a valid Pandas timedelta unit infer _datetime_format Boolean origin Scalar string or timestamp value cache Boolean Note The function is not optimized. Bodo doesn't support Timezone-Aware datetime values Example Usage >>> @bodo . jit ... def f ( val ): ... return pd . to_datetime ( val , format = \"%Y- %d -%m\" ) >>> val = \"2016-01-06\" >>> f ( val ) Timestamp ( '2016-06-01 00:00:00' ) pd.to_timedelta \u00b6 pandas. to_timedelta (arg, unit=None, errors='raise') Supported Arguments argument datatypes other requirements arg Series, Array or scalar of integers or strings unit String Must be a valid Pandas timedelta unit Note Passing string data as arg is not optimized. Example Usage >>> @bodo . jit ... def f ( S ): ... return pd . to_timedelta ( S , unit = \"D\" ) >>> S = pd . Series ([ 1.0 , 2.2 , np . nan , 4.2 ], [ 3 , 1 , 0 , - 2 ], name = \"AA\" ) >>> f ( val ) 3 1 days 00 : 00 : 00 1 2 days 04 : 48 : 00 0 NaT - 2 4 days 04 : 48 : 00 Name : AA , dtype : timedelta64 [ ns ] pd.date_range \u00b6 pandas. date_range (start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, **kwargs) Supported Arguments argument datatypes other requirements start String or Timestamp end String or Timestamp periods Integer freq String Must be a valid Pandas frequ ency name String closed String and one of ( 'left' , 'right' ) Note Exactly three of start , end , periods , and freq must be provided. Bodo Does Not support kwargs , even for compatibility. This function is not parallelized yet. Example Usage >>> @bodo . jit ... def f (): ... return pd . date_range ( start = \"2018-04-24\" , end = \"2018-04-27\" , periods = 3 ) >>> f () DatetimeIndex ([ '2018-04-24 00:00:00' , '2018-04-25 12:00:00' , '2018-04-27 00:00:00' ], dtype = 'datetime64[ns]' , freq = None ) pd.timedelta_range \u00b6 pandas. timedelta_range (start=None, end=None, periods=None, freq=None, name=None, closed=None) Supported Arguments argument datatypes other requirements start String or Timedelta end String or Timedelta periods Integer freq String Must be a valid Pandas frequ ency name String closed String and one of ('left', 'right') Note Exactly three of start , end , periods , and freq must be provided. This function is not parallelized yet. Example Usage >>> @bodo . jit ... def f (): ... return pd . timedelta_range ( start = \"1 day\" , end = \"11 days 1 hour\" , periods = 3 ) >>> f () TimedeltaIndex ([ '1 days 00:00:00' , '6 days 00:30:00' , '11 days 01:00:00' ], dtype = 'timedelta64[ns]' , freq = None )","title":"General Functions"},{"location":"api_docs/pandas/general/#general","text":"","title":"General functions"},{"location":"api_docs/pandas/general/#data-manipulations","text":"","title":"Data manipulations"},{"location":"api_docs/pandas/general/#pdpivot","text":"pandas. pivot (data, values=None, index=None, columns=None) Supported Arguments argument datatypes data DataFrame values Constant Column Label or list of labels index Constant Column Label or list of labels columns Constant Column Label Note The the number of columns and names of the output DataFrame won't be known at compile time. To update typing information on DataFrame you should pass it back to Python. Example Usage >>> @bodo . jit ... def f (): ... df = pd . DataFrame ({ \"A\" : [ \"X\" , \"X\" , \"X\" , \"X\" , \"Y\" , \"Y\" ], \"B\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], \"C\" : [ 10 , 11 , 12 , 20 , 21 , 22 ]}) ... pivoted_tbl = pd . pivot ( data , columns = \"A\" , index = \"B\" , values = \"C\" ) ... return pivoted_tbl >>> f () A X Y B 1 10.0 NaN 2 11.0 NaN 3 12.0 NaN 4 20.0 NaN 5 NaN 21.0 6 NaN 22.0","title":"pd.pivot"},{"location":"api_docs/pandas/general/#pdpivot_table","text":"pandas. pivot_table (data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True) Supported Arguments argument datatypes data DataFrame values Constant Column Label or list of labels index Constant Column Label or list of labels columns Constant Column Label aggfunc String Constant Note This code takes two different paths depending on if pivot values are annotated. When pivot values are annotated then output columns are set to the annotated values. For example, @bodo.jit(pivots={'pt': ['small', 'large']}) declares the output pivot table pt will have columns called small and large . If pivot values are not annotated, then the number of columns and names of the output DataFrame won't be known at compile time. To update typing information on DataFrame you should pass it back to Python. Example Usage >>> @bodo . jit ( pivots = { 'pivoted_tbl' : [ 'X' , 'Y' ]}) ... def f (): ... df = pd . DataFrame ({ \"A\" : [ \"X\" , \"X\" , \"X\" , \"X\" , \"Y\" , \"Y\" ], \"B\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], \"C\" : [ 10 , 11 , 12 , 20 , 21 , 22 ]}) ... pivoted_tbl = pd . pivot_table ( df , columns = \"A\" , index = \"B\" , values = \"C\" , aggfunc = \"mean\" ) ... return pivoted_tbl >>> f () X Y B 1 10.0 NaN 2 11.0 NaN 3 12.0 NaN 4 20.0 NaN 5 NaN 21.0 6 NaN 22.0","title":"pd.pivot_table"},{"location":"api_docs/pandas/general/#pdcrosstab","text":"pandas. crosstab (index, columns, values=None, rownames=None, colnames=None, aggfunc=None, margins=False, margins_name='All', dropna=True, normalize=False) Supported Arguments argument datatypes index SeriesType columns SeriesType Note Annotation of pivot values is required. For example, @bodo.jit(pivots={'pt': ['small', 'large']}) declares the output table pt will have columns called small and large . Example Usage >>> @bodo . jit ( pivots = { \"pt\" : [ \"small\" , \"large\" ]}) ... def f ( df ): ... pt = pd . crosstab ( df . A , df . C ) ... return pt >>> list_A = [ \"foo\" , \"foo\" , \"bar\" , \"bar\" , \"bar\" , \"bar\" ] >>> list_C = [ \"small\" , \"small\" , \"large\" , \"small\" , \"small\" , \"middle\" ] >>> df = pd . DataFrame ({ \"A\" : list_A , \"C\" : list_C }) >>> f ( df ) small large index foo 2 0 bar 2 1","title":"pd.crosstab"},{"location":"api_docs/pandas/general/#pdcut","text":"pandas. cut (x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates=\"raise\", ordered=True) Supported Arguments argument datatypes x Series or Array like bins Integer or Array like include_lowest Boolean Example Usage >>> @bodo . jit ... def f ( S ): ... bins = 4 ... include_lowest = True ... return pd . cut ( S , bins , include_lowest = include_lowest ) >>> S = pd . Series ( ... [ - 2 , 1 , 3 , 4 , 5 , 11 , 15 , 20 , 22 ], ... [ \"a1\" , \"a2\" , \"a3\" , \"a4\" , \"a5\" , \"a6\" , \"a7\" , \"a8\" , \"a9\" ], ... name = \"ABC\" , ... ) >>> f ( S ) a1 ( - 2.025 , 4.0 ] a2 ( - 2.025 , 4.0 ] a3 ( - 2.025 , 4.0 ] a4 ( - 2.025 , 4.0 ] a5 ( 4.0 , 10.0 ] a6 ( 10.0 , 16.0 ] a7 ( 10.0 , 16.0 ] a8 ( 16.0 , 22.0 ] a9 ( 16.0 , 22.0 ] Name : ABC , dtype : category Categories ( 4 , interval [ float64 , right ]): [( - 2.025 , 4.0 ] < ( 4.0 , 10.0 ] < ( 10.0 , 16.0 ] < ( 16.0 , 22.0 ]]","title":"pd.cut"},{"location":"api_docs/pandas/general/#pdqcut","text":"pandas. qcut (x, q, labels=None, retbins=False, precision=3, duplicates=\"raise\") Supported Arguments argument datatypes x Series or Array like q Integer or Array like of floats Example Usage >>> @bodo . jit ... def f ( S ): ... q = 4 ... return pd . qcut ( S , q ) >>> S = pd . Series ( ... [ - 2 , 1 , 3 , 4 , 5 , 11 , 15 , 20 , 22 ], ... [ \"a1\" , \"a2\" , \"a3\" , \"a4\" , \"a5\" , \"a6\" , \"a7\" , \"a8\" , \"a9\" ], ... name = \"ABC\" , ... ) >>> f ( S ) a1 ( - 2.001 , 3.0 ] a2 ( - 2.001 , 3.0 ] a3 ( - 2.001 , 3.0 ] a4 ( 3.0 , 5.0 ] a5 ( 3.0 , 5.0 ] a6 ( 5.0 , 15.0 ] a7 ( 5.0 , 15.0 ] a8 ( 15.0 , 22.0 ] a9 ( 15.0 , 22.0 ] Name : ABC , dtype : category Categories ( 4 , interval [ float64 , right ]): [( - 2.001 , 3.0 ] < ( 3.0 , 5.0 ] < ( 5.0 , 15.0 ] < ( 15.0 , 22.0 ]]","title":"pd.qcut"},{"location":"api_docs/pandas/general/#pdmerge","text":"pandas. merge (left, right, how=\"inner\", on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=(\"_x\", \"_y\"), copy=True, indicator=False, validate=None, _bodo_na_equal=True) Supported Arguments argument datatypes other requirements left DataFrame right DataFrame how String Must be one of \"inner\" , \"outer\" , \"left\" , \"right\" Must be constant at Compile Time on Column Name, List of Column Names, or General Merge Condition String (see merge-notes ) Must be constant at Compile Time left_on Column Name or List of Column Names Must be constant at Compile Time right_on Column Name or List of Column Names Must be constant at Compile Time left_index Boolean Must be constant at Compile Time right_index Boolean Must be constant at Compile Time suffixes Tuple of Strings Must be constant at Compile Time indicator Boolean Must be constant at Compile Time _bodo_na_equal Boolean Must be constant at Compile Time This argument is unique to Bodo and not available in Pandas. If False, Bodo won't consider NA/nan keys as equal, which differs from Pandas. Important The argument _bodo_na_equal is unique to Bodo and not available in Pandas. If it is False , Bodo won't consider NA/nan keys as equal, which differs from Pandas.","title":"pd.merge"},{"location":"api_docs/pandas/general/#merge-notes","text":"Output Ordering : The output dataframe is not sorted by default for better parallel performance (Pandas may preserve key order depending on how ). One can use explicit sort if needed. General Merge Conditions : Within Pandas, the merge criteria supported by pd.merge are limited to equality between 1 or more pairs of keys. For some use cases, this is not sufficient and more generalized support is necessary. For example, with these limitations, a left outer join where df1.A == df2.B & df2.C < df1.A cannot be efficiently computed. Bodo supports these use cases by allowing users to pass general merge conditions to pd.merge . We plan to contribute this feature to Pandas to ensure full compatibility of Bodo and Pandas code. General merge conditions are performed by providing the condition as a string via the on argument. Columns in the left table are referred to by left.{column name} and columns in the right table are referred to by right.{column name} . Here's an example demonstrating the above: >>> @bodo . jit ... def general_merge ( df1 , df2 ): ... return df1 . merge ( df2 , on = \"left.`A` == right.`B` & right.`C` < left.`A`\" , how = \"left\" ) >>> df1 = pd . DataFrame ({ \"col\" : [ 2 , 3 , 5 , 1 , 2 , 8 ], \"A\" : [ 4 , 6 , 3 , 9 , 9 , - 1 ]}) >>> df2 = pd . DataFrame ({ \"B\" : [ 1 , 2 , 9 , 3 , 2 ], \"C\" : [ 1 , 7 , 2 , 6 , 5 ]}) >>> general_merge ( df1 , df2 ) col A B C 0 2 4 < NA > < NA > 1 3 6 < NA > < NA > 2 5 3 < NA > < NA > 3 1 9 9 2 4 2 9 9 2 5 8 - 1 < NA > < NA > These calls have a few additional requirements: The condition must be constant string. The condition must be of the form cond_1 & ... & cond_N where at least one cond_i is a simple equality. This restriction will be removed in a future release. The columns specified in these conditions are limited to certain column types. We currently support boolean , integer , float , datetime64 , timedelta64 , datetime.date , and string columns. Example Usage >>> @bodo . jit ... def f ( df1 , df2 ): ... return pd . merge ( df1 , df2 , how = \"inner\" , on = \"key\" ) >>> df1 = pd . DataFrame ({ \"key\" : [ 2 , 3 , 5 , 1 , 2 , 8 ], \"A\" : np . array ([ 4 , 6 , 3 , 9 , 9 , - 1 ], float )}) >>> df2 = pd . DataFrame ({ \"key\" : [ 1 , 2 , 9 , 3 , 2 ], \"B\" : np . array ([ 1 , 7 , 2 , 6 , 5 ], float )}) >>> f ( df1 , df2 ) key A B 0 2 4.0 7.0 1 2 4.0 5.0 2 3 6.0 6.0 3 1 9.0 1.0 4 2 9.0 7.0 5 2 9.0 5.0","title":"Merge Notes"},{"location":"api_docs/pandas/general/#pdmerge_asof","text":"pandas. merge_asof (left, right, on=None, left_on=None, right_on=None, left_index=False, right_index=False, by=None, left_by=None, right_by=None, suffixes=(\"_x\", \"_y\"), tolerance=None, allow_exact_matches=True, direction=\"backward\") Supported Arguments argument datatypes other requirements left DataFrame right DataFrame on Column Name, List of Column Names Must be constant at Compile Time left_on Column Name or List of Column Names Must be constant at Compile Time right_on Column Name or List of Column Names Must be constant at Compile Time left_index Boolean Must be constant at Compile Time right_index Boolean Must be constant at Compile Time suffixes Tuple of Strings Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( df1 , df2 ): ... return pd . merge_asof ( df1 , df2 , on = \"time\" ) >>> df1 = pd . DataFrame ( ... { ... \"time\" : pd . DatetimeIndex ([ \"2017-01-03\" , \"2017-01-06\" , \"2017-02-21\" ]), ... \"B\" : [ 4 , 5 , 6 ], ... } ... ) >>> df2 = pd . DataFrame ( ... { ... \"time\" : pd . DatetimeIndex ( ... [ \"2017-01-01\" , \"2017-01-02\" , \"2017-01-04\" , \"2017-02-23\" , \"2017-02-25\" ] ... ), ... \"A\" : [ 2 , 3 , 7 , 8 , 9 ], ... } ... ) >>> f ( df1 , df2 ) time B A 0 2017 - 01 - 03 4 3 1 2017 - 01 - 06 5 7 2 2017 - 02 - 21 6 7","title":"pd.merge_asof"},{"location":"api_docs/pandas/general/#pdconcat","text":"pandas. concat (objs, axis=0, join=\"outer\", join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=None, copy=True) Supported Arguments argument datatypes other requirements objs List or Tuple of DataFrames/Series axis Integer with either 0 or 1 Must be constant at Compile Time ignore_index Boolean Must be constant at Compile Time Important Bodo currently concatenates local data chunks for distributed datasets, which does not preserve global order of concatenated objects in output. Example Usage >>> @bodo . jit ... def f ( df1 , df2 ): ... return pd . concat ([ df1 , df2 ], axis = 1 ) >>> df1 = pd . DataFrame ({ \"A\" : [ 3 , 2 , 1 , - 4 , 7 ]}) >>> df2 = pd . DataFrame ({ \"B\" : [ 3 , 25 , 1 , - 4 , - 24 ]}) >>> f ( df1 , df2 ) A B 0 3 3 1 2 25 2 1 1 3 - 4 - 4 4 7 - 24","title":"pd.concat"},{"location":"api_docs/pandas/general/#pdget_dummies","text":"pandas. get_dummies (data, prefix=None, prefix_sep=\"_\", dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None) Supported Arguments argument datatypes other requirements data Array or Series with Categorical dtypes Categories must be known at compile time. Example Usage >>> @bodo . jit ... def f ( S ): ... return pd . get_dummies ( S ) >>> S = pd . Series ([ \"CC\" , \"AA\" , \"B\" , \"D\" , \"AA\" , None , \"B\" , \"CC\" ]) . astype ( \"category\" ) >>> f ( S ) AA B CC D 0 0 0 1 0 1 1 0 0 0 2 0 1 0 0 3 0 0 0 1 4 1 0 0 0 5 0 0 0 0 6 0 1 0 0 7 0 0 1 0","title":"pd.get_dummies"},{"location":"api_docs/pandas/general/#top-level-missing-data","text":"","title":"Top-level missing data"},{"location":"api_docs/pandas/general/#pdisna","text":"pandas. isna (obj) Supported Arguments argument datatypes obj DataFrame, Series, Index, Array, or Scalar Example Usage >>> @bodo . jit ... def f ( df ): ... return pd . isna ( df ) >>> df = pd . DataFrame ( ... { \"A\" : [ \"AA\" , np . nan , \"\" , \"D\" , \"GG\" ], \"B\" : [ 1 , 8 , 4 , - 1 , 2 ]}, ... [ 1.1 , - 2.1 , 7.1 , 0.1 , 3.1 ], ... ) >>> f ( df ) A B 1.1 False False - 2.1 True False 7.1 False False 0.1 False False 3.1 False False","title":"pd.isna"},{"location":"api_docs/pandas/general/#pdisnull","text":"pandas. isnull (obj) Supported Arguments argument datatypes obj DataFrame, Series, Index, Array, or Scalar Example Usage >>> @bodo . jit ... def f ( df ): ... return pd . isnull ( df ) >>> df = pd . DataFrame ( ... { \"A\" : [ \"AA\" , np . nan , \"\" , \"D\" , \"GG\" ], \"B\" : [ 1 , 8 , 4 , - 1 , 2 ]}, ... [ 1.1 , - 2.1 , 7.1 , 0.1 , 3.1 ], ... ) >>> f ( df ) A B 1.1 False False - 2.1 True False 7.1 False False 0.1 False False 3.1 False False","title":"pd.isnull"},{"location":"api_docs/pandas/general/#pdnotna","text":"pandas. notna (obj) Supported Arguments argument datatypes obj DataFrame, Series, Index, Array, or Scalar Example Usage >>> @bodo . jit ... def f ( df ): ... return pd . notna ( df ) >>> df = pd . DataFrame ( ... { \"A\" : [ \"AA\" , np . nan , \"\" , \"D\" , \"GG\" ], \"B\" : [ 1 , 8 , 4 , - 1 , 2 ]}, ... [ 1.1 , - 2.1 , 7.1 , 0.1 , 3.1 ], ... ) >>> f ( df ) A B 1.1 True True - 2.1 False True 7.1 True True 0.1 True True 3.1 True True","title":"pd.notna"},{"location":"api_docs/pandas/general/#pdnotnull","text":"pandas. notnull (obj) Supported Arguments argument datatypes obj DataFrame, Series, Index, Array, or Scalar Example Usage >>> @bodo . jit ... def f ( df ): ... return pd . notnull ( df ) >>> df = pd . DataFrame ( ... { \"A\" : [ \"AA\" , np . nan , \"\" , \"D\" , \"GG\" ], \"B\" : [ 1 , 8 , 4 , - 1 , 2 ]}, ... [ 1.1 , - 2.1 , 7.1 , 0.1 , 3.1 ], ... ) >>> f ( df ) A B 1.1 True True - 2.1 False True 7.1 True True 0.1 True True 3.1 True True","title":"pd.notnull"},{"location":"api_docs/pandas/general/#top-level-conversions","text":"","title":"Top-level conversions"},{"location":"api_docs/pandas/general/#pdto_numeric","text":"pandas. to_numeric (arg, errors=\"raise\", downcast=None) Supported Arguments argument datatypes other requirements arg Series or Array downcast String and one of ( 'integer' , 'signed' , 'unsigned' , 'float' ) Must be constant at Compile Time Note Output type is float64 by default Unlike Pandas, Bodo does not dynamically determine output type, and does not downcast to the smallest numerical type. downcast parameter should be used for type annotation of output. Example Usage >>> @bodo . jit ... def f ( S ): ... return pd . to_numeric ( S , errors = \"coerce\" , downcast = \"integer\" ) >>> S = pd . Series ([ \"1\" , \"3\" , \"12\" , \"4\" , None , \"-555\" ]) >>> f ( S ) 0 1 1 3 2 12 3 4 4 < NA > 5 - 555 dtype : Int64","title":"pd.to_numeric"},{"location":"api_docs/pandas/general/#top-level-dealing-with-datetime-and-timedelta-like","text":"","title":"Top-level dealing with datetime and timedelta like"},{"location":"api_docs/pandas/general/#pdto_datetime","text":"pandas. to_datetime (arg, errors='raise', dayfirst=False, yearfirst=False, utc=None, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix', cache=True) Supported Arguments argument datatypes other requirements arg Series, Array or scalar of integers or strings errors String and one of ('ignore', 'raise', 'coerce') dayfirst Boolean yearfirst Boolean utc Boolean format String matching Pandas strftime /strptime exact Boolean unit String Must be a valid Pandas timedelta unit infer _datetime_format Boolean origin Scalar string or timestamp value cache Boolean Note The function is not optimized. Bodo doesn't support Timezone-Aware datetime values Example Usage >>> @bodo . jit ... def f ( val ): ... return pd . to_datetime ( val , format = \"%Y- %d -%m\" ) >>> val = \"2016-01-06\" >>> f ( val ) Timestamp ( '2016-06-01 00:00:00' )","title":"pd.to_datetime"},{"location":"api_docs/pandas/general/#pdto_timedelta","text":"pandas. to_timedelta (arg, unit=None, errors='raise') Supported Arguments argument datatypes other requirements arg Series, Array or scalar of integers or strings unit String Must be a valid Pandas timedelta unit Note Passing string data as arg is not optimized. Example Usage >>> @bodo . jit ... def f ( S ): ... return pd . to_timedelta ( S , unit = \"D\" ) >>> S = pd . Series ([ 1.0 , 2.2 , np . nan , 4.2 ], [ 3 , 1 , 0 , - 2 ], name = \"AA\" ) >>> f ( val ) 3 1 days 00 : 00 : 00 1 2 days 04 : 48 : 00 0 NaT - 2 4 days 04 : 48 : 00 Name : AA , dtype : timedelta64 [ ns ]","title":"pd.to_timedelta"},{"location":"api_docs/pandas/general/#pddate_range","text":"pandas. date_range (start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, **kwargs) Supported Arguments argument datatypes other requirements start String or Timestamp end String or Timestamp periods Integer freq String Must be a valid Pandas frequ ency name String closed String and one of ( 'left' , 'right' ) Note Exactly three of start , end , periods , and freq must be provided. Bodo Does Not support kwargs , even for compatibility. This function is not parallelized yet. Example Usage >>> @bodo . jit ... def f (): ... return pd . date_range ( start = \"2018-04-24\" , end = \"2018-04-27\" , periods = 3 ) >>> f () DatetimeIndex ([ '2018-04-24 00:00:00' , '2018-04-25 12:00:00' , '2018-04-27 00:00:00' ], dtype = 'datetime64[ns]' , freq = None )","title":"pd.date_range"},{"location":"api_docs/pandas/general/#pdtimedelta_range","text":"pandas. timedelta_range (start=None, end=None, periods=None, freq=None, name=None, closed=None) Supported Arguments argument datatypes other requirements start String or Timedelta end String or Timedelta periods Integer freq String Must be a valid Pandas frequ ency name String closed String and one of ('left', 'right') Note Exactly three of start , end , periods , and freq must be provided. This function is not parallelized yet. Example Usage >>> @bodo . jit ... def f (): ... return pd . timedelta_range ( start = \"1 day\" , end = \"11 days 1 hour\" , periods = 3 ) >>> f () TimedeltaIndex ([ '1 days 00:00:00' , '6 days 00:30:00' , '11 days 01:00:00' ], dtype = 'timedelta64[ns]' , freq = None )","title":"pd.timedelta_range"},{"location":"api_docs/pandas/groupby/","text":"GroupBy \u00b6 pd.DataFrame.groupby \u00b6 pandas.DataFrame. groupby (by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) Supported Arguments by : Column label or list of column labels Must be constant at Compile Time This argument is required as_index : Boolean Must be constant at Compile Time dropna : Boolean Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" , dropna = True , as_index = False ) . count () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) B A C 0 421 10 10 1 f31 5 10 pd.Series.groupby \u00b6 pandas.Series. groupby (by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) Supported Arguments by : Array-like or Series data. This is not supported with Decimal or Categorical data. Must be constant at Compile Time level : integer Must be constant at Compile Time Only level=0 is supported and not with MultiIndex. Important You must provide exactly one of by and level Example Usage >>> @bodo . jit ... def f ( S , by_series ): ... return S . groupby ( by_series ) . count () >>> S = pd . Series ([ 1 , 2 , 24 , None ] * 5 ) >>> by_series = pd . Series ([ \"421\" , \"f31\" ] * 10 ) >>> f ( S , by_series ) 421 10 f31 5 Name : , dtype : int64 Note Series.groupby doesn't currently keep the name of the original Series. pd.core.groupby.Groupby.apply \u00b6 pandas.core.groupby.Groupby. apply (func, *args, **kwargs) Supported Arguments func : JIT function, callable defined within a JIT function that returns a DataFrame or Series Additional arguments for func can be passed as additional arguments. Example Usage >>> @bodo . jit ... def f ( df , y ): ... return df . groupby ( \"B\" , dropna = True ) . apply ( lambda group , y : group . sum ( axis = 1 ) + y , y = y ) >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> y = 4 >>> f ( df , y ) B 421 0 6.510 2 8.421 4 233260.000 6 16.210 8 6.510 10 8.421 12 233260.000 14 16.210 16 6.510 18 8.421 f31 1 233260.000 3 16.210 5 6.510 7 8.421 9 233260.000 11 16.210 13 6.510 15 8.421 17 233260.000 19 16.210 dtype : float64 pd.core.groupby.Groupby.agg \u00b6 pandas.core.groupby.Groupby. agg (func, *args, **kwargs) Supported Arguments func : JIT function, callable defined within a JIT function, constant dictionary mapping column name to a function Additional arguments for func can be passed as additional arguments. Note Passing a list of functions is also supported if only one output column is selected. Output column names can be specified using keyword arguments and pd.NamedAgg() . Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" , dropna = True ) . agg ({ \"A\" : lambda x : max ( x )}) >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A B 421 24.0 f31 2.0 pd.core.groupby.DataFrameGroupby.aggregate \u00b6 pandas.core.groupby.DataFrameGroupby. aggregate (func, *args, **kwargs) Supported Arguments func : JIT function, callable defined within a JIT function, constant dictionary mapping column name to a function Additional arguments for func can be passed as additional arguments. Note Passing a list of functions is also supported if only one output column is selected. Output column names can be specified using keyword arguments and pd.NamedAgg() . Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" , dropna = True ) . agg ({ \"A\" : lambda x : max ( x )}) >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A B 421 24.0 f31 2.0 pd.core.groupby.DataFrameGroupby.transform \u00b6 pandas.core.groupby.DataFrameGroupby. transform (func, *args, engine=None, engine_kwargs=None, **kwargs) Supported Arguments func : Constant string, Python function from the builtins module that matches a supported operation Numpy functions cannot be provided. Note The supported builtin functions are 'count' , 'first' , 'last' , 'min' , 'max' , 'mean' , 'median' , 'nunique' , 'prod' , 'std' , 'sum' , and 'var' Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" , dropna = True ) . transform ( max ) >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C 0 24.0 233232.00 1 2.0 12.21 2 24.0 233232.00 3 2.0 12.21 4 24.0 233232.00 5 2.0 12.21 6 24.0 233232.00 7 2.0 12.21 8 24.0 233232.00 9 2.0 12.21 10 24.0 233232.00 11 2.0 12.21 12 24.0 233232.00 13 2.0 12.21 14 24.0 233232.00 15 2.0 12.21 16 24.0 233232.00 17 2.0 12.21 18 24.0 233232.00 19 2.0 12.21 pd.core.groupby.Groupby.pipe \u00b6 pandas.core.groupby.Groupby. pipe (func, *args, **kwargs) Supported Arguments func : JIT function, callable defined within a JIT function. Additional arguments for func can be passed as additional arguments. Note func cannot be a tuple Example Usage >>> @bodo . jit ... def f ( df , y ): ... return df . groupby ( \"B\" ) . pipe ( lambda grp , y : grp . sum () - y , y = y ) >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> y = 5 >>> f ( df , y ) A C B 421 120.0 1166162.550 f31 5.0 68.155 pd.core.groupby.Groupby.count \u00b6 pandas.core.groupby.Groupby. count () Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . count () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 10 10 f31 5 10 pd.core.groupby.Groupby.cumsum \u00b6 pandas.core.groupby.Groupby. cumsum (axis=0) Note cumsum is only supported on numeric columns and is not supported on boolean columns Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . cumsum () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C 0 1.0 1.510 1 2.0 2.421 2 25.0 233233.510 3 NaN 14.631 4 26.0 233235.020 5 4.0 17.052 6 50.0 466467.020 7 NaN 29.262 8 51.0 466468.530 9 6.0 31.683 10 75.0 699700.530 11 NaN 43.893 12 76.0 699702.040 13 8.0 46.314 14 100.0 932934.040 15 NaN 58.524 16 101.0 932935.550 17 10.0 60.945 18 125.0 1166167.550 19 NaN 73.155 pd.core.groupby.Groupby.first \u00b6 pandas.core.groupby.Groupby. first (numeric_only=False, min_count=-1) Note first is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . first () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 1.0 1.510 f31 2.0 2.421 pd.core.groupby.Groupby.head \u00b6 pandas.core.groupby.Groupby. head (n=5) Supported Arguments n : Non-negative integer Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . head () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A B C 0 1.0 421 1.510 1 2.0 f31 2.421 2 24.0 421 233232.000 3 NaN f31 12.210 4 1.0 421 1.510 5 2.0 f31 2.421 6 24.0 421 233232.000 7 NaN f31 12.210 8 1.0 421 1.510 9 2.0 f31 2.421 pd.core.groupby.Groupby.last \u00b6 pandas.core.groupby.Groupby. last (numeric_only=False, min_count=-1) Note last is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . last () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 24.0 233232.00 f31 2.0 12.21 pd.core.groupby.Groupby.max \u00b6 pandas.core.groupby.Groupby. max (numeric_only=False, min_count=-1) Note max is not supported on columns with nested array types. Categorical columns must be ordered. Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . max () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 24.0 233232.00 f31 2.0 12.21 pd.core.groupby.Groupby.mean \u00b6 pandas.core.groupby.Groupby. mean (numeric_only=NoDefault.no_default) Note mean is only supported on numeric columns and is not supported on boolean column Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . mean () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 12.5 116616.7550 f31 2.0 7.3155 pd.core.groupby.Groupby.median \u00b6 pandas.core.groupby.Groupby. median (numeric_only=NoDefault.no_default) Note median is only supported on numeric columns and is not supported on boolean column Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . median () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 12.5 116616.7550 f31 2.0 7.3155 pd.core.groupby.Groupby.min \u00b6 pandas.core.groupby.Groupby. min (numeric_only=False, min_count=-1) Note min is not supported on columns with nested array types Categorical columns must be ordered. Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . min () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 1.0 1.510 f31 2.0 2.421 pd.core.groupby.Groupby.prod \u00b6 pandas.core.groupby.Groupby. prod (numeric_only=NoDefault.no_default, min_count=0) Note prod is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . prod () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 7962624.0 5.417831e+27 f31 32.0 2.257108e+07 pd.core.groupby.Groupby.rolling \u00b6 pandas.core.groupby.Groupby. rolling (window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None, method='single') Supported Arguments window : Integer, String, Datetime, Timedelta min_periods : Integer center : Boolean on : Column label Must be constant at Compile Time Note This is equivalent to performing the DataFrame API on each groupby. All operations of the rolling API can be used with groupby. Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . rolling ( 2 ) . mean >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 0 NaN NaN 2 NaN NaN 4 12.5 116616.7550 6 NaN 7.3155 8 12.5 116616.7550 10 NaN 7.3155 12 12.5 116616.7550 14 NaN 7.3155 16 12.5 116616.7550 18 NaN 7.3155 f31 1 12.5 116616.7550 3 NaN 7.3155 5 12.5 116616.7550 7 NaN 7.3155 9 12.5 116616.7550 11 NaN 7.3155 13 12.5 116616.7550 15 NaN 7.3155 17 12.5 116616.7550 19 NaN 7.3155 pd.core.groupby.Groupby.size \u00b6 pandas.core.groupby.Groupby. size () Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . size () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) B 421 10 f31 10 dtype : int64 pd.core.groupby.Groupby.std \u00b6 pandas.core.groupby.Groupby. std (ddof=1) Note std is only supported on numeric columns and is not supported on boolean column Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . std () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 12.122064 122923.261366 f31 0.000000 5.159256 pd.core.groupby.Groupby.sum \u00b6 pandas.core.groupby.Groupby. sum (numeric_only=NoDefault.no_default, min_count=0) Note sum is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . sum () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 125.0 1166167.550 f31 10.0 73.155 pd.core.groupby.Groupby.var \u00b6 pandas.core.groupby.Groupby. var (ddof=1) Note var is only supported on numeric columns and is not supported on boolean column Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . var () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 146.944444 1.511013e+10 f31 0.000000 2.661792e+01 pd.core.groupby.DataFrameGroupby.idxmax \u00b6 pandas.core.groupby.DataFrameGroupby. idxmax (axis=0, skipna=True) Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . idxmax () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 2 2 f31 1 3 pd.core.groupby.DataFrameGroupby.idxmin \u00b6 pandas.core.groupby.DataFrameGroupby. idxmin (axis=0, skipna=True) Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . idxmin () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 0 0 f31 1 1 pd.core.groupby.DataFrameGroupby.nunique \u00b6 pandas.core.groupby.DataFrameGroupby. nunique (dropna=True) Supported Arguments dropna : boolean Note nunique is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . nunique () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 2 2 f31 1 2 pd.core.groupby.DataFrameGroupby.shift \u00b6 pandas.core.groupby.DataFrameGroupby. shift (periods=1, freq=None, axis=0, fill_value=None) Note shift is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . shift () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C 0 NaN NaN 1 NaN NaN 2 1.0 1.510 3 2.0 2.421 4 24.0 233232.000 5 NaN 12.210 6 1.0 1.510 7 2.0 2.421 8 24.0 233232.000 9 NaN 12.210 10 1.0 1.510 11 2.0 2.421 12 24.0 233232.000 13 NaN 12.210 14 1.0 1.510 15 2.0 2.421 16 24.0 233232.000 17 NaN 12.210 18 1.0 1.510 19 2.0 2.421 pd.core.groupby.SeriesGroupBy.value_counts \u00b6 pandas.core.groupby.SeriesGroupby. value_counts (normalize=False, sort=True, ascending=False, bins=None, dropna=True) Supported Arguments ascending : boolean Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( S ): ... return S . groupby ( level = 0 ) . value_counts () >>> S = pd . Series ([ 1 , 2 , 24 , None ] * 5 , index = [ \"421\" , \"f31\" ] * 10 ) >>> f ( S ) 421 1.0 5 24.0 5 f31 2.0 5 Name : , dtype : int64","title":"Groupby"},{"location":"api_docs/pandas/groupby/#pd_groupby_section","text":"","title":"GroupBy"},{"location":"api_docs/pandas/groupby/#pddataframegroupby","text":"pandas.DataFrame. groupby (by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) Supported Arguments by : Column label or list of column labels Must be constant at Compile Time This argument is required as_index : Boolean Must be constant at Compile Time dropna : Boolean Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" , dropna = True , as_index = False ) . count () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) B A C 0 421 10 10 1 f31 5 10","title":"pd.DataFrame.groupby"},{"location":"api_docs/pandas/groupby/#pdseriesgroupby","text":"pandas.Series. groupby (by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) Supported Arguments by : Array-like or Series data. This is not supported with Decimal or Categorical data. Must be constant at Compile Time level : integer Must be constant at Compile Time Only level=0 is supported and not with MultiIndex. Important You must provide exactly one of by and level Example Usage >>> @bodo . jit ... def f ( S , by_series ): ... return S . groupby ( by_series ) . count () >>> S = pd . Series ([ 1 , 2 , 24 , None ] * 5 ) >>> by_series = pd . Series ([ \"421\" , \"f31\" ] * 10 ) >>> f ( S , by_series ) 421 10 f31 5 Name : , dtype : int64 Note Series.groupby doesn't currently keep the name of the original Series.","title":"pd.Series.groupby"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbyapply","text":"pandas.core.groupby.Groupby. apply (func, *args, **kwargs) Supported Arguments func : JIT function, callable defined within a JIT function that returns a DataFrame or Series Additional arguments for func can be passed as additional arguments. Example Usage >>> @bodo . jit ... def f ( df , y ): ... return df . groupby ( \"B\" , dropna = True ) . apply ( lambda group , y : group . sum ( axis = 1 ) + y , y = y ) >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> y = 4 >>> f ( df , y ) B 421 0 6.510 2 8.421 4 233260.000 6 16.210 8 6.510 10 8.421 12 233260.000 14 16.210 16 6.510 18 8.421 f31 1 233260.000 3 16.210 5 6.510 7 8.421 9 233260.000 11 16.210 13 6.510 15 8.421 17 233260.000 19 16.210 dtype : float64","title":"pd.core.groupby.Groupby.apply"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbyagg","text":"pandas.core.groupby.Groupby. agg (func, *args, **kwargs) Supported Arguments func : JIT function, callable defined within a JIT function, constant dictionary mapping column name to a function Additional arguments for func can be passed as additional arguments. Note Passing a list of functions is also supported if only one output column is selected. Output column names can be specified using keyword arguments and pd.NamedAgg() . Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" , dropna = True ) . agg ({ \"A\" : lambda x : max ( x )}) >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A B 421 24.0 f31 2.0","title":"pd.core.groupby.Groupby.agg"},{"location":"api_docs/pandas/groupby/#pdcoregroupbydataframegroupbyaggregate","text":"pandas.core.groupby.DataFrameGroupby. aggregate (func, *args, **kwargs) Supported Arguments func : JIT function, callable defined within a JIT function, constant dictionary mapping column name to a function Additional arguments for func can be passed as additional arguments. Note Passing a list of functions is also supported if only one output column is selected. Output column names can be specified using keyword arguments and pd.NamedAgg() . Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" , dropna = True ) . agg ({ \"A\" : lambda x : max ( x )}) >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A B 421 24.0 f31 2.0","title":"pd.core.groupby.DataFrameGroupby.aggregate"},{"location":"api_docs/pandas/groupby/#pdcoregroupbydataframegroupbytransform","text":"pandas.core.groupby.DataFrameGroupby. transform (func, *args, engine=None, engine_kwargs=None, **kwargs) Supported Arguments func : Constant string, Python function from the builtins module that matches a supported operation Numpy functions cannot be provided. Note The supported builtin functions are 'count' , 'first' , 'last' , 'min' , 'max' , 'mean' , 'median' , 'nunique' , 'prod' , 'std' , 'sum' , and 'var' Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" , dropna = True ) . transform ( max ) >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C 0 24.0 233232.00 1 2.0 12.21 2 24.0 233232.00 3 2.0 12.21 4 24.0 233232.00 5 2.0 12.21 6 24.0 233232.00 7 2.0 12.21 8 24.0 233232.00 9 2.0 12.21 10 24.0 233232.00 11 2.0 12.21 12 24.0 233232.00 13 2.0 12.21 14 24.0 233232.00 15 2.0 12.21 16 24.0 233232.00 17 2.0 12.21 18 24.0 233232.00 19 2.0 12.21","title":"pd.core.groupby.DataFrameGroupby.transform"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbypipe","text":"pandas.core.groupby.Groupby. pipe (func, *args, **kwargs) Supported Arguments func : JIT function, callable defined within a JIT function. Additional arguments for func can be passed as additional arguments. Note func cannot be a tuple Example Usage >>> @bodo . jit ... def f ( df , y ): ... return df . groupby ( \"B\" ) . pipe ( lambda grp , y : grp . sum () - y , y = y ) >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> y = 5 >>> f ( df , y ) A C B 421 120.0 1166162.550 f31 5.0 68.155","title":"pd.core.groupby.Groupby.pipe"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbycount","text":"pandas.core.groupby.Groupby. count () Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . count () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 10 10 f31 5 10","title":"pd.core.groupby.Groupby.count"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbycumsum","text":"pandas.core.groupby.Groupby. cumsum (axis=0) Note cumsum is only supported on numeric columns and is not supported on boolean columns Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . cumsum () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C 0 1.0 1.510 1 2.0 2.421 2 25.0 233233.510 3 NaN 14.631 4 26.0 233235.020 5 4.0 17.052 6 50.0 466467.020 7 NaN 29.262 8 51.0 466468.530 9 6.0 31.683 10 75.0 699700.530 11 NaN 43.893 12 76.0 699702.040 13 8.0 46.314 14 100.0 932934.040 15 NaN 58.524 16 101.0 932935.550 17 10.0 60.945 18 125.0 1166167.550 19 NaN 73.155","title":"pd.core.groupby.Groupby.cumsum"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbyfirst","text":"pandas.core.groupby.Groupby. first (numeric_only=False, min_count=-1) Note first is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . first () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 1.0 1.510 f31 2.0 2.421","title":"pd.core.groupby.Groupby.first"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbyhead","text":"pandas.core.groupby.Groupby. head (n=5) Supported Arguments n : Non-negative integer Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . head () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A B C 0 1.0 421 1.510 1 2.0 f31 2.421 2 24.0 421 233232.000 3 NaN f31 12.210 4 1.0 421 1.510 5 2.0 f31 2.421 6 24.0 421 233232.000 7 NaN f31 12.210 8 1.0 421 1.510 9 2.0 f31 2.421","title":"pd.core.groupby.Groupby.head"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbylast","text":"pandas.core.groupby.Groupby. last (numeric_only=False, min_count=-1) Note last is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . last () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 24.0 233232.00 f31 2.0 12.21","title":"pd.core.groupby.Groupby.last"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbymax","text":"pandas.core.groupby.Groupby. max (numeric_only=False, min_count=-1) Note max is not supported on columns with nested array types. Categorical columns must be ordered. Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . max () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 24.0 233232.00 f31 2.0 12.21","title":"pd.core.groupby.Groupby.max"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbymean","text":"pandas.core.groupby.Groupby. mean (numeric_only=NoDefault.no_default) Note mean is only supported on numeric columns and is not supported on boolean column Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . mean () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 12.5 116616.7550 f31 2.0 7.3155","title":"pd.core.groupby.Groupby.mean"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbymedian","text":"pandas.core.groupby.Groupby. median (numeric_only=NoDefault.no_default) Note median is only supported on numeric columns and is not supported on boolean column Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . median () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 12.5 116616.7550 f31 2.0 7.3155","title":"pd.core.groupby.Groupby.median"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbymin","text":"pandas.core.groupby.Groupby. min (numeric_only=False, min_count=-1) Note min is not supported on columns with nested array types Categorical columns must be ordered. Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . min () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 1.0 1.510 f31 2.0 2.421","title":"pd.core.groupby.Groupby.min"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbyprod","text":"pandas.core.groupby.Groupby. prod (numeric_only=NoDefault.no_default, min_count=0) Note prod is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . prod () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 7962624.0 5.417831e+27 f31 32.0 2.257108e+07","title":"pd.core.groupby.Groupby.prod"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbyrolling","text":"pandas.core.groupby.Groupby. rolling (window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None, method='single') Supported Arguments window : Integer, String, Datetime, Timedelta min_periods : Integer center : Boolean on : Column label Must be constant at Compile Time Note This is equivalent to performing the DataFrame API on each groupby. All operations of the rolling API can be used with groupby. Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . rolling ( 2 ) . mean >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 0 NaN NaN 2 NaN NaN 4 12.5 116616.7550 6 NaN 7.3155 8 12.5 116616.7550 10 NaN 7.3155 12 12.5 116616.7550 14 NaN 7.3155 16 12.5 116616.7550 18 NaN 7.3155 f31 1 12.5 116616.7550 3 NaN 7.3155 5 12.5 116616.7550 7 NaN 7.3155 9 12.5 116616.7550 11 NaN 7.3155 13 12.5 116616.7550 15 NaN 7.3155 17 12.5 116616.7550 19 NaN 7.3155","title":"pd.core.groupby.Groupby.rolling"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbysize","text":"pandas.core.groupby.Groupby. size () Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . size () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) B 421 10 f31 10 dtype : int64","title":"pd.core.groupby.Groupby.size"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbystd","text":"pandas.core.groupby.Groupby. std (ddof=1) Note std is only supported on numeric columns and is not supported on boolean column Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . std () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 12.122064 122923.261366 f31 0.000000 5.159256","title":"pd.core.groupby.Groupby.std"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbysum","text":"pandas.core.groupby.Groupby. sum (numeric_only=NoDefault.no_default, min_count=0) Note sum is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . sum () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 125.0 1166167.550 f31 10.0 73.155","title":"pd.core.groupby.Groupby.sum"},{"location":"api_docs/pandas/groupby/#pdcoregroupbygroupbyvar","text":"pandas.core.groupby.Groupby. var (ddof=1) Note var is only supported on numeric columns and is not supported on boolean column Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . var () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 146.944444 1.511013e+10 f31 0.000000 2.661792e+01","title":"pd.core.groupby.Groupby.var"},{"location":"api_docs/pandas/groupby/#pdcoregroupbydataframegroupbyidxmax","text":"pandas.core.groupby.DataFrameGroupby. idxmax (axis=0, skipna=True) Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . idxmax () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 2 2 f31 1 3","title":"pd.core.groupby.DataFrameGroupby.idxmax"},{"location":"api_docs/pandas/groupby/#pdcoregroupbydataframegroupbyidxmin","text":"pandas.core.groupby.DataFrameGroupby. idxmin (axis=0, skipna=True) Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . idxmin () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 0 0 f31 1 1","title":"pd.core.groupby.DataFrameGroupby.idxmin"},{"location":"api_docs/pandas/groupby/#pdcoregroupbydataframegroupbynunique","text":"pandas.core.groupby.DataFrameGroupby. nunique (dropna=True) Supported Arguments dropna : boolean Note nunique is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . nunique () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C B 421 2 2 f31 1 2","title":"pd.core.groupby.DataFrameGroupby.nunique"},{"location":"api_docs/pandas/groupby/#pdcoregroupbydataframegroupbyshift","text":"pandas.core.groupby.DataFrameGroupby. shift (periods=1, freq=None, axis=0, fill_value=None) Note shift is not supported on columns with nested array types Example Usage >>> @bodo . jit ... def f ( df ): ... return df . groupby ( \"B\" ) . shift () >>> df = pd . DataFrame ( ... { ... \"A\" : [ 1 , 2 , 24 , None ] * 5 , ... \"B\" : [ \"421\" , \"f31\" ] * 10 , ... \"C\" : [ 1.51 , 2.421 , 233232 , 12.21 ] * 5 ... } ... ) >>> f ( df ) A C 0 NaN NaN 1 NaN NaN 2 1.0 1.510 3 2.0 2.421 4 24.0 233232.000 5 NaN 12.210 6 1.0 1.510 7 2.0 2.421 8 24.0 233232.000 9 NaN 12.210 10 1.0 1.510 11 2.0 2.421 12 24.0 233232.000 13 NaN 12.210 14 1.0 1.510 15 2.0 2.421 16 24.0 233232.000 17 NaN 12.210 18 1.0 1.510 19 2.0 2.421","title":"pd.core.groupby.DataFrameGroupby.shift"},{"location":"api_docs/pandas/groupby/#pdcoregroupbyseriesgroupbyvalue_counts","text":"pandas.core.groupby.SeriesGroupby. value_counts (normalize=False, sort=True, ascending=False, bins=None, dropna=True) Supported Arguments ascending : boolean Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( S ): ... return S . groupby ( level = 0 ) . value_counts () >>> S = pd . Series ([ 1 , 2 , 24 , None ] * 5 , index = [ \"421\" , \"f31\" ] * 10 ) >>> f ( S ) 421 1.0 5 24.0 5 f31 2.0 5 Name : , dtype : int64","title":"pd.core.groupby.SeriesGroupBy.value_counts"},{"location":"api_docs/pandas/indexapi/","text":"Index objects \u00b6 Index \u00b6 Properties \u00b6 pd.Index.name \u00b6 pandas.Index. name Example Usage >>> @bodo . jit ... def f ( I ): ... return I . name >>> I = pd . Index ([ 1 , 2 , 3 ], name = \"hello world\" ) >>> f ( I ) \"hello world\" pd.Index.shape \u00b6 pandas.Index. shape Unsupported Index Types MultiIndex IntervalIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . shape >>> I = pd . Index ([ 1 , 2 , 3 ]) >>> f ( I ) ( 3 ,) pd.Index.is_monotonic_increasing \u00b6 pandas.Index. is_monotonic_increasing and pandas. Index.is_monotonic Unsupported Index Types StringIndex BinaryIndex IntervalIndex CategoricalIndex PeriodIndex MultiIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_monotonic_increasing >>> I = pd . Index ([ 1 , 2 , 3 ]) >>> f ( I ) True >>> @bodo . jit ... def g ( I ): ... return I . is_monotonic >>> I = pd . Index ( 1 , 2 , 3 ]) >>> g ( I ) True pd.Index.is_monotonic_decreasing \u00b6 pandas.Index. is_monotonic_decreasing Unsupported Index Types StringIndex BinaryIndex IntervalIndex CategoricalIndex PeriodIndex MultiIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_monotonic_decreasing >>> I = pd . Index ([ 1 , 2 , 3 ]) >>> f ( I ) False pd.Index.values \u00b6 pandas.Index. values Unsupported Index Types MultiIndex IntervalIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . values >>> I = pd . Index ([ 1 , 2 , 3 ]) >>> f ( I ) [ 1 2 3 ] pd.Index.nbytes \u00b6 pandas.Index. nbytes Unsupported Index Types MultiIndex IntervalIndex Important Currently, Bodo upcasts all numeric index data types to 64 bitwidth. Example Usage >>> @bodo . jit ... def f ( I ): ... return I . nbytes >>> I1 = pd . Index ([ 1 , 2 , 3 , 4 , 5 , 6 ], dtype = np . int64 ) >>> f ( I1 ) 48 >>> I2 = pd . Index ([ 1 , 2 , 3 ], dtype = np . int64 ) >>> f ( I2 ) 24 >>> I3 = pd . Index ([ 1 , 2 , 3 ], dtype = np . int32 ) >>> f ( I3 ) 24 Modifications and computations \u00b6 pd.Index.copy \u00b6 pandas.Index. copy (name=None, deep=False, dtype=None, names=None) Unsupported Index Types MultiIndex IntervalIndex Supported arguments name Example Usage >>> @bodo . jit ... def f ( I ): ... return I . copy ( name = \"new_name\" ) >>> I = pd . Index ([ 1 , 2 , 3 ], name = \"origial_name\" ) >>> f ( I ) Int64Index ([ 1 , 2 , 3 ], dtype = 'int64' , name = 'new_name' ) pd.Index.get_loc \u00b6 pandas.Index. get_loc (key, method=None, tolerance=None) Note Should be about as fast as standard python, maybe slightly slower. Unsupported Index Types CategoricalIndex MultiIndex IntervalIndex Supported Arguments key : must be of same type as the index Important Only works for index with unique values (scalar return). Only works with replicated Index Example Usage >>> @bodo . jit ... def f ( I ): ... return I . get_loc ( 2 ) >>> I = pd . Index ([ 1 , 2 , 3 ]) >>> f ( I ) 1 pd.Index.take \u00b6 pandas.Index. take (indices, axis=0, allow_fill=True, fill_value=None, **kwargs) Supported Arguments indices : can be boolean Array like, integer Array like, or slice Unsupported Index Types MultiIndex IntervalIndex Important Bodo Does Not support kwargs , even for compatibility. pd.Index.min \u00b6 pandas.Index. min (axis=None, skipna=True, args, *kwargs) Supported Arguments : None Supported Index Types TimedeltaIndex DatetimeIndex Important Bodo Does Not support args and kwargs , even for compatibility. For DatetimeIndex, will throw an error if all values in the index are null. Example Usage >>> @bodo . jit ... def f ( I ): ... return I . min () >>> I = pd . Index ( pd . date_range ( start = \"2018-04-24\" , end = \"2018-04-25\" , periods = 5 )) >>> f ( I ) 2018 - 04 - 24 00 : 00 : 00 pd.Index.max \u00b6 pandas.Index. max (axis=None, skipna=True, args, *kwargs) Supported Arguments : None Supported Index Types TimedeltaIndex DatetimeIndex Important Bodo Does Not support args and kwargs , even for compatibility. For DatetimeIndex, will throw an error if all values in the index are null. Example Usage >>> @bodo . jit ... def f ( I ): ... return I . min () >>> I = pd . Index ( pd . date_range ( start = \"2018-04-24\" , end = \"2018-04-25\" , periods = 5 )) >>> f ( I ) 2018 - 04 - 25 00 : 00 : 00 pd.Index.rename \u00b6 pandas.Index. rename (name, inplace=False) Supported Arguments name : label or list of labels Unsupported Index Types MultiIndex Example Usage >>> @bodo . jit ... def f ( I , name ): ... return I . rename ( name ) >>> I = pd . Index ([ \"a\" , \"b\" , \"c\" ]) >>> f ( I , \"new_name\" ) Index ([ 'a' , 'b' , 'c' ], dtype = 'object' , name = 'new_name' ) pd.Index.duplicated \u00b6 pandas.Index. duplicated (keep='first') Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... return I . duplicated () >>> idx = pd . Index ([ 'a' , 'b' , None , 'a' , 'c' , None , 'd' , 'b' ]) >>> f ( idx ) array ([ False , False , False , True , False , True , False , True ]) pd.Index.drop_duplicates \u00b6 pandas.Index. drop_duplicates (keep='first') Supported Arguments : None Unsupported Index Types MultiIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . drop_duplicates () >>> I = pd . Index ([ \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"c\" ]) >>> f ( I ) Index ([ 'a' , 'b' , 'c' ], dtype = 'object' ) Missing values \u00b6 pd.Index.isna \u00b6 pandas.Index. isna () Unsupported Index Types MultiIndex IntervalIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . isna () >>> I = pd . Index ([ 1 , None , 3 ]) >>> f ( I ) [ False True False ] pd.Index.isnull \u00b6 pandas.Index. isnull () Unsupported Index Types MultiIndex IntervalIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . isnull () >>> I = pd . Index ([ 1 , None , 3 ]) >>> f ( I ) [ False True False ] Conversion \u00b6 pd.Index.map \u00b6 pandas.Index. map (mapper, na_action=None) Unsupported Index Types MultiIndex IntervalIndex Supported Arguments mapper : must be a function, function cannot return tuple type Example Usage >>> @bodo . jit ... def f ( I ): ... return I . map ( lambda x : x + 2 ) >>> I = pd . Index ([ 1 , None , 3 ]) >>> f ( I ) Float64Index ([ 3.0 , nan , 5.0 ], dtype = 'float64' ) Numeric Index \u00b6 Numeric index objects RangeIndex , Int64Index , UInt64Index and Float64Index are supported as index to dataframes and series. Constructing them in Bodo functions, passing them to Bodo functions (unboxing), and returning them from Bodo functions (boxing) are also supported. pd.RangeIndex \u00b6 pandas. RangeIndex (start=None, stop=None, step=None, dtype=None, copy=False, name=None) Supported Arguments start : integer stop : integer step : integer name : String Example Usage >>> @bodo . jit ... def f (): ... return pd . RangeIndex ( 0 , 10 , 2 ) >>> f ( I ) RangeIndex ( start = 0 , stop = 10 , step = 2 ) pd.Int64Index \u00b6 pandas. Int64Index (data=None, dtype=None, copy=False, name=None) Example Usage >>> @bodo . jit ... def f (): ... return pd . Int64Index ( np . arange ( 3 )) >>> f () Int64Index ([ 0 , 1 , 2 ], dtype = 'int64' ) pd.UInt64Index \u00b6 pandas. UInt64Index (data=None, dtype=None, copy=False, name=None) Example Usage >>> @bodo . jit ... def f (): ... return pd . UInt64Index ([ 1 , 2 , 3 ]) >>> f () UInt64Index ([ 0 , 1 , 2 ], dtype = 'uint64' ) pd.Float64Index \u00b6 pandas. Float64Index (data=None, dtype=None, copy=False, name=None) Supported Arguments data : list or array copy : Boolean name : String Example Usage >>> @bodo . jit ... def f (): ... return pd . Float64Index ( np . arange ( 3 )) >>> f () Float64Index ([ 0.0 , 1.0 , 2.0 ], dtype = 'float64' ) DatetimeIndex \u00b6 DatetimeIndex objects are supported. They can be constructed, boxed/unboxed, and set as index to dataframes and series. pd.DateTimeIndex \u00b6 pandas. DatetimeIndex Supported Arguments data : array-like of datetime64, Integer, or strings Date fields \u00b6 pd.DateTimeIndex.year \u00b6 pandas.DatetimeIndex. year Example Usage >>> @bodo . jit ... def f ( I ): ... return I . year >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 2019 , 2019 , 2019 , 2020 , 2020 ], dtype = 'int64' ) pd.DateTimeIndex.month \u00b6 pandas.DatetimeIndex. month Example Usage >>> @bodo . jit ... def f ( I ): ... return I . month >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 12 , 12 , 12 , 1 , 1 ], dtype = 'int64' ) pd.DateTimeIndex.day \u00b6 pandas.DatetimeIndex. day Example Usage >>> @bodo . jit ... def f ( I ): ... return I . day >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 31 , 31 , 31 , 1 , 1 ], dtype = 'int64' ) pd.DateTimeIndex.hour \u00b6 pandas.DatetimeIndex. hour Example Usage >>> @bodo . jit ... def f ( I ): ... return I . hour >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 2 , 12 , 22 , 9 , 19 ], dtype = 'int64' ) pd.DateTimeIndex.minute \u00b6 pandas.DatetimeIndex. minute Example Usage >>> @bodo . jit ... def f ( I ): ... return I . minute >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 32 , 42 , 52 , 2 , 12 ], dtype = 'int64' ) pd.DateTimeIndex.second \u00b6 pandas.DatetimeIndex. second Example Usage >>> @bodo . jit ... def f ( I ): ... return I . second >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 45 , 35 , 25 , 15 , 5 ], dtype = 'int64' ) pd.DateTimeIndex.microsecond \u00b6 pandas.DatetimeIndex. microsecond Example Usage >>> @bodo . jit ... def f ( I ): ... return I . microsecond >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 01:01:01\" , end = \"2019-12-31 01:01:02\" , periods = 5 )) >>> f ( I ) Int64Index ([ 0 , 250000 , 500000 , 750000 , 0 ], dtype = 'int64' ) pd.DateTimeIndex.nanosecond \u00b6 pandas.DatetimeIndex. nanosecond Example Usage >>> @bodo . jit ... def f ( I ): ... return I . nanosecond >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 01:01:01.0000001\" , end = \"2019-12-31 01:01:01.0000002\" , periods = 5 )) >>> f ( I ) Int64Index ([ 100 , 125 , 150 , 175 , 200 ], dtype = 'int64' ) pd.DateTimeIndex.date \u00b6 pandas.DatetimeIndex. date Example Usage >>> @bodo . jit ... def f ( I ): ... return I . date >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) [ datetime . date ( 2019 , 12 , 31 ) datetime . date ( 2019 , 12 , 31 ) datetime . date ( 2019 , 12 , 31 ) datetime . date ( 2020 , 1 , 1 ) datetime . date ( 2020 , 1 , 1 )] pd.DateTimeIndex.dayofyear \u00b6 pandas.DatetimeIndex. dayofyear Example Usage >>> @bodo . jit ... def f ( I ): ... return I . dayofyear >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 365 , 365 , 365 , 1 , 1 ], dtype = 'int64' ) pd.DateTimeIndex.day_of_year \u00b6 pandas.DatetimeIndex. day_of_year Example Usage >>> @bodo . jit ... def f ( I ): ... return I . day_of_year >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 365 , 365 , 365 , 1 , 1 ], dtype = 'int64' ) pd.DateTimeIndex.dayofweek \u00b6 pandas.DatetimeIndex. dayofweek Example Usage >>> @bodo . jit ... def f ( I ): ... return I . dayofweek >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 2 , 2 ], dtype = 'int64' ) pd.DateTimeIndex.day_of_week \u00b6 pandas.DatetimeIndex. day_of_week Example Usage >>> @bodo . jit ... def f ( I ): ... return I . day_of_week >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 2 , 2 ], dtype = 'int64' ) pd.DateTimeIndex.is_leap_year \u00b6 pandas.DatetimeIndex. is_leap_year Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_leap_year >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) [ Flase False False True True ] pd.DateTimeIndex.is_month_start \u00b6 pandas.DatetimeIndex. is_month_start Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_month_start >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 0 , 0 , 0 , 1 , 1 ], dtype = 'int64' ) pd.DateTimeIndex.is_month_end \u00b6 pandas.DatetimeIndex. is_month_end Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_month_end >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 0 , 0 ], dtype = 'int64' ) pd.DateTimeIndex.is_quarter_start \u00b6 pandas.DatetimeIndex. is_quarter_start Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_quarter_start >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 0 , 0 , 0 , 1 , 1 ], dtype = 'int64' ) pd.DateTimeIndex.is_quarter_end \u00b6 pandas.DatetimeIndex. is_quarter_end Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_quarter_end >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 0 , 0 ], dtype = 'int64' ) pd.DateTimeIndex.is_year_start \u00b6 pandas.DatetimeIndex. is_year_start Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_year_start >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 0 , 0 , 0 , 1 , 1 ], dtype = 'int64' ) pd.DateTimeIndex.is_year_end \u00b6 pandas.DatetimeIndex. is_year_end Example Usage >>> @bodo.jit ... def f(I): ... return I.is_year_end >>> I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5)) >>> f(I) Int64Index([1, 1, 1, 0, 0], dtype='int64') pd.DateTimeIndex.week \u00b6 pandas.DatetimeIndex. week Example Usage >>> @bodo . jit ... def f ( I ): ... return I . week >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 1 , 1 ], dtype = 'int64' ) pd.DateTimeIndex.weekday \u00b6 pandas.DatetimeIndex. weekday Example Usage >>> @bodo . jit ... def f ( I ): ... return I . weekday >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 2 , 2 ], dtype = 'int64' ) pd.DateTimeIndex.weekofyear \u00b6 pandas.DatetimeIndex. weekofyear Example Usage >>> @bodo . jit ... def f ( I ): ... return I . weekofyear >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 1 , 1 ], dtype = 'int64' ) pd.DateTimeIndex.quarter \u00b6 pandas.DatetimeIndex. quarter Example Usage >>> @bodo . jit ... def f ( I ): ... return I . quarter >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 4 , 4 , 4 , 1 , 1 ], dtype = 'int64' ) Subtraction of Timestamp from DatetimeIndex and vice versa is supported. Comparison operators == , != , >= , > , <= , < between DatetimeIndex and a string of datetime are supported. TimedeltaIndex \u00b6 TimedeltaIndex objects are supported. They can be constructed, boxed/unboxed, and set as index to dataframes and series. pd.TimedeltaIndex \u00b6 pandas. TimedeltaIndex (data=None, unit=None, freq=NoDefault.no_default, closed=None, dtype=dtype('<m8[ns]'), copy=False, name=None) Supported Arguments - data : must be array-like of timedelta64ns or Ingetger. pd.TimedeltaIndex.days \u00b6 pandas.TimedeltaIndex. days Example Usage >>> @bodo . jit ... def f ( I ): ... return I . days >>> I = pd . TimedeltaIndex ([ pd . Timedelta ( 3 , unit = \"D\" ))]) >>> f ( I ) Int64Index ([ 3 ], dtype = 'int64' ) pd.TimedeltaIndex.seconds \u00b6 pandas.TimedeltaIndex. seconds Example Usage >>> @bodo . jit ... def f ( I ): ... return I . seconds >>> I = pd . TimedeltaIndex ([ pd . Timedelta ( - 2 , unit = \"S\" ))]) >>> f ( I ) Int64Index ([ - 2 ], dtype = 'int64' ) pd.TimedeltaIndex.microseconds \u00b6 pandas.TimedeltaIndex. microseconds Example Usage >>> @bodo . jit ... def f ( I ): ... return I . microseconds >>> I = pd . TimedeltaIndex ([ pd . Timedelta ( 11 , unit = \"micros\" ))]) >>> f ( I ) Int64Index ([ 11 ], dtype = 'int64' ) pd.TimedeltaIndex.nanoseconds \u00b6 pandas.TimedeltaIndex. nanoseconds Example Usage >>> @bodo . jit ... def f ( I ): ... return I . nanoseconds >>> I = pd . TimedeltaIndex ([ pd . Timedelta ( 7 , unit = \"nanos\" ))]) >>> f ( I ) Int64Index ([ 7 ], dtype = 'int64' ) PeriodIndex \u00b6 PeriodIndex objects can be boxed/unboxed and set as index to dataframes and series. Operations on them will be supported in upcoming releases. BinaryIndex \u00b6 BinaryIndex objects can be boxed/unboxed and set as index to dataframes and series. Operations on them will be supported in upcoming releases. MultiIndex \u00b6 pd.MultiIndex.from_product \u00b6 pandas. MultiIndex.from_product ( iterables and names supported as tuples, no parallel support yet)","title":"Index Objects"},{"location":"api_docs/pandas/indexapi/#index-objects","text":"","title":"Index objects"},{"location":"api_docs/pandas/indexapi/#index","text":"","title":"Index"},{"location":"api_docs/pandas/indexapi/#properties","text":"","title":"Properties"},{"location":"api_docs/pandas/indexapi/#pdindexname","text":"pandas.Index. name Example Usage >>> @bodo . jit ... def f ( I ): ... return I . name >>> I = pd . Index ([ 1 , 2 , 3 ], name = \"hello world\" ) >>> f ( I ) \"hello world\"","title":"pd.Index.name"},{"location":"api_docs/pandas/indexapi/#pdindexshape","text":"pandas.Index. shape Unsupported Index Types MultiIndex IntervalIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . shape >>> I = pd . Index ([ 1 , 2 , 3 ]) >>> f ( I ) ( 3 ,)","title":"pd.Index.shape"},{"location":"api_docs/pandas/indexapi/#pdindexis_monotonic_increasing","text":"pandas.Index. is_monotonic_increasing and pandas. Index.is_monotonic Unsupported Index Types StringIndex BinaryIndex IntervalIndex CategoricalIndex PeriodIndex MultiIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_monotonic_increasing >>> I = pd . Index ([ 1 , 2 , 3 ]) >>> f ( I ) True >>> @bodo . jit ... def g ( I ): ... return I . is_monotonic >>> I = pd . Index ( 1 , 2 , 3 ]) >>> g ( I ) True","title":"pd.Index.is_monotonic_increasing"},{"location":"api_docs/pandas/indexapi/#pdindexis_monotonic_decreasing","text":"pandas.Index. is_monotonic_decreasing Unsupported Index Types StringIndex BinaryIndex IntervalIndex CategoricalIndex PeriodIndex MultiIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_monotonic_decreasing >>> I = pd . Index ([ 1 , 2 , 3 ]) >>> f ( I ) False","title":"pd.Index.is_monotonic_decreasing"},{"location":"api_docs/pandas/indexapi/#pdindexvalues","text":"pandas.Index. values Unsupported Index Types MultiIndex IntervalIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . values >>> I = pd . Index ([ 1 , 2 , 3 ]) >>> f ( I ) [ 1 2 3 ]","title":"pd.Index.values"},{"location":"api_docs/pandas/indexapi/#pdindexnbytes","text":"pandas.Index. nbytes Unsupported Index Types MultiIndex IntervalIndex Important Currently, Bodo upcasts all numeric index data types to 64 bitwidth. Example Usage >>> @bodo . jit ... def f ( I ): ... return I . nbytes >>> I1 = pd . Index ([ 1 , 2 , 3 , 4 , 5 , 6 ], dtype = np . int64 ) >>> f ( I1 ) 48 >>> I2 = pd . Index ([ 1 , 2 , 3 ], dtype = np . int64 ) >>> f ( I2 ) 24 >>> I3 = pd . Index ([ 1 , 2 , 3 ], dtype = np . int32 ) >>> f ( I3 ) 24","title":"pd.Index.nbytes"},{"location":"api_docs/pandas/indexapi/#modifications-and-computations","text":"","title":"Modifications and computations"},{"location":"api_docs/pandas/indexapi/#pdindexcopy","text":"pandas.Index. copy (name=None, deep=False, dtype=None, names=None) Unsupported Index Types MultiIndex IntervalIndex Supported arguments name Example Usage >>> @bodo . jit ... def f ( I ): ... return I . copy ( name = \"new_name\" ) >>> I = pd . Index ([ 1 , 2 , 3 ], name = \"origial_name\" ) >>> f ( I ) Int64Index ([ 1 , 2 , 3 ], dtype = 'int64' , name = 'new_name' )","title":"pd.Index.copy"},{"location":"api_docs/pandas/indexapi/#pdindexget_loc","text":"pandas.Index. get_loc (key, method=None, tolerance=None) Note Should be about as fast as standard python, maybe slightly slower. Unsupported Index Types CategoricalIndex MultiIndex IntervalIndex Supported Arguments key : must be of same type as the index Important Only works for index with unique values (scalar return). Only works with replicated Index Example Usage >>> @bodo . jit ... def f ( I ): ... return I . get_loc ( 2 ) >>> I = pd . Index ([ 1 , 2 , 3 ]) >>> f ( I ) 1","title":"pd.Index.get_loc"},{"location":"api_docs/pandas/indexapi/#pdindextake","text":"pandas.Index. take (indices, axis=0, allow_fill=True, fill_value=None, **kwargs) Supported Arguments indices : can be boolean Array like, integer Array like, or slice Unsupported Index Types MultiIndex IntervalIndex Important Bodo Does Not support kwargs , even for compatibility.","title":"pd.Index.take"},{"location":"api_docs/pandas/indexapi/#pdindexmin","text":"pandas.Index. min (axis=None, skipna=True, args, *kwargs) Supported Arguments : None Supported Index Types TimedeltaIndex DatetimeIndex Important Bodo Does Not support args and kwargs , even for compatibility. For DatetimeIndex, will throw an error if all values in the index are null. Example Usage >>> @bodo . jit ... def f ( I ): ... return I . min () >>> I = pd . Index ( pd . date_range ( start = \"2018-04-24\" , end = \"2018-04-25\" , periods = 5 )) >>> f ( I ) 2018 - 04 - 24 00 : 00 : 00","title":"pd.Index.min"},{"location":"api_docs/pandas/indexapi/#pdindexmax","text":"pandas.Index. max (axis=None, skipna=True, args, *kwargs) Supported Arguments : None Supported Index Types TimedeltaIndex DatetimeIndex Important Bodo Does Not support args and kwargs , even for compatibility. For DatetimeIndex, will throw an error if all values in the index are null. Example Usage >>> @bodo . jit ... def f ( I ): ... return I . min () >>> I = pd . Index ( pd . date_range ( start = \"2018-04-24\" , end = \"2018-04-25\" , periods = 5 )) >>> f ( I ) 2018 - 04 - 25 00 : 00 : 00","title":"pd.Index.max"},{"location":"api_docs/pandas/indexapi/#pdindexrename","text":"pandas.Index. rename (name, inplace=False) Supported Arguments name : label or list of labels Unsupported Index Types MultiIndex Example Usage >>> @bodo . jit ... def f ( I , name ): ... return I . rename ( name ) >>> I = pd . Index ([ \"a\" , \"b\" , \"c\" ]) >>> f ( I , \"new_name\" ) Index ([ 'a' , 'b' , 'c' ], dtype = 'object' , name = 'new_name' )","title":"pd.Index.rename"},{"location":"api_docs/pandas/indexapi/#pdindexduplicated","text":"pandas.Index. duplicated (keep='first') Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... return I . duplicated () >>> idx = pd . Index ([ 'a' , 'b' , None , 'a' , 'c' , None , 'd' , 'b' ]) >>> f ( idx ) array ([ False , False , False , True , False , True , False , True ])","title":"pd.Index.duplicated"},{"location":"api_docs/pandas/indexapi/#pdindexdrop_duplicates","text":"pandas.Index. drop_duplicates (keep='first') Supported Arguments : None Unsupported Index Types MultiIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . drop_duplicates () >>> I = pd . Index ([ \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"c\" ]) >>> f ( I ) Index ([ 'a' , 'b' , 'c' ], dtype = 'object' )","title":"pd.Index.drop_duplicates"},{"location":"api_docs/pandas/indexapi/#missing-values","text":"","title":"Missing values"},{"location":"api_docs/pandas/indexapi/#pdindexisna","text":"pandas.Index. isna () Unsupported Index Types MultiIndex IntervalIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . isna () >>> I = pd . Index ([ 1 , None , 3 ]) >>> f ( I ) [ False True False ]","title":"pd.Index.isna"},{"location":"api_docs/pandas/indexapi/#pdindexisnull","text":"pandas.Index. isnull () Unsupported Index Types MultiIndex IntervalIndex Example Usage >>> @bodo . jit ... def f ( I ): ... return I . isnull () >>> I = pd . Index ([ 1 , None , 3 ]) >>> f ( I ) [ False True False ]","title":"pd.Index.isnull"},{"location":"api_docs/pandas/indexapi/#conversion","text":"","title":"Conversion"},{"location":"api_docs/pandas/indexapi/#pdindexmap","text":"pandas.Index. map (mapper, na_action=None) Unsupported Index Types MultiIndex IntervalIndex Supported Arguments mapper : must be a function, function cannot return tuple type Example Usage >>> @bodo . jit ... def f ( I ): ... return I . map ( lambda x : x + 2 ) >>> I = pd . Index ([ 1 , None , 3 ]) >>> f ( I ) Float64Index ([ 3.0 , nan , 5.0 ], dtype = 'float64' )","title":"pd.Index.map"},{"location":"api_docs/pandas/indexapi/#numeric-index","text":"Numeric index objects RangeIndex , Int64Index , UInt64Index and Float64Index are supported as index to dataframes and series. Constructing them in Bodo functions, passing them to Bodo functions (unboxing), and returning them from Bodo functions (boxing) are also supported.","title":"Numeric Index"},{"location":"api_docs/pandas/indexapi/#pdrangeindex","text":"pandas. RangeIndex (start=None, stop=None, step=None, dtype=None, copy=False, name=None) Supported Arguments start : integer stop : integer step : integer name : String Example Usage >>> @bodo . jit ... def f (): ... return pd . RangeIndex ( 0 , 10 , 2 ) >>> f ( I ) RangeIndex ( start = 0 , stop = 10 , step = 2 )","title":"pd.RangeIndex"},{"location":"api_docs/pandas/indexapi/#pdint64index","text":"pandas. Int64Index (data=None, dtype=None, copy=False, name=None) Example Usage >>> @bodo . jit ... def f (): ... return pd . Int64Index ( np . arange ( 3 )) >>> f () Int64Index ([ 0 , 1 , 2 ], dtype = 'int64' )","title":"pd.Int64Index"},{"location":"api_docs/pandas/indexapi/#pduint64index","text":"pandas. UInt64Index (data=None, dtype=None, copy=False, name=None) Example Usage >>> @bodo . jit ... def f (): ... return pd . UInt64Index ([ 1 , 2 , 3 ]) >>> f () UInt64Index ([ 0 , 1 , 2 ], dtype = 'uint64' )","title":"pd.UInt64Index"},{"location":"api_docs/pandas/indexapi/#pdfloat64index","text":"pandas. Float64Index (data=None, dtype=None, copy=False, name=None) Supported Arguments data : list or array copy : Boolean name : String Example Usage >>> @bodo . jit ... def f (): ... return pd . Float64Index ( np . arange ( 3 )) >>> f () Float64Index ([ 0.0 , 1.0 , 2.0 ], dtype = 'float64' )","title":"pd.Float64Index"},{"location":"api_docs/pandas/indexapi/#datetimeindex","text":"DatetimeIndex objects are supported. They can be constructed, boxed/unboxed, and set as index to dataframes and series.","title":"DatetimeIndex"},{"location":"api_docs/pandas/indexapi/#pddatetimeindex","text":"pandas. DatetimeIndex Supported Arguments data : array-like of datetime64, Integer, or strings","title":"pd.DateTimeIndex"},{"location":"api_docs/pandas/indexapi/#date-fields","text":"","title":"Date fields"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexyear","text":"pandas.DatetimeIndex. year Example Usage >>> @bodo . jit ... def f ( I ): ... return I . year >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 2019 , 2019 , 2019 , 2020 , 2020 ], dtype = 'int64' )","title":"pd.DateTimeIndex.year"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexmonth","text":"pandas.DatetimeIndex. month Example Usage >>> @bodo . jit ... def f ( I ): ... return I . month >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 12 , 12 , 12 , 1 , 1 ], dtype = 'int64' )","title":"pd.DateTimeIndex.month"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexday","text":"pandas.DatetimeIndex. day Example Usage >>> @bodo . jit ... def f ( I ): ... return I . day >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 31 , 31 , 31 , 1 , 1 ], dtype = 'int64' )","title":"pd.DateTimeIndex.day"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexhour","text":"pandas.DatetimeIndex. hour Example Usage >>> @bodo . jit ... def f ( I ): ... return I . hour >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 2 , 12 , 22 , 9 , 19 ], dtype = 'int64' )","title":"pd.DateTimeIndex.hour"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexminute","text":"pandas.DatetimeIndex. minute Example Usage >>> @bodo . jit ... def f ( I ): ... return I . minute >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 32 , 42 , 52 , 2 , 12 ], dtype = 'int64' )","title":"pd.DateTimeIndex.minute"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexsecond","text":"pandas.DatetimeIndex. second Example Usage >>> @bodo . jit ... def f ( I ): ... return I . second >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 45 , 35 , 25 , 15 , 5 ], dtype = 'int64' )","title":"pd.DateTimeIndex.second"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexmicrosecond","text":"pandas.DatetimeIndex. microsecond Example Usage >>> @bodo . jit ... def f ( I ): ... return I . microsecond >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 01:01:01\" , end = \"2019-12-31 01:01:02\" , periods = 5 )) >>> f ( I ) Int64Index ([ 0 , 250000 , 500000 , 750000 , 0 ], dtype = 'int64' )","title":"pd.DateTimeIndex.microsecond"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexnanosecond","text":"pandas.DatetimeIndex. nanosecond Example Usage >>> @bodo . jit ... def f ( I ): ... return I . nanosecond >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 01:01:01.0000001\" , end = \"2019-12-31 01:01:01.0000002\" , periods = 5 )) >>> f ( I ) Int64Index ([ 100 , 125 , 150 , 175 , 200 ], dtype = 'int64' )","title":"pd.DateTimeIndex.nanosecond"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexdate","text":"pandas.DatetimeIndex. date Example Usage >>> @bodo . jit ... def f ( I ): ... return I . date >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) [ datetime . date ( 2019 , 12 , 31 ) datetime . date ( 2019 , 12 , 31 ) datetime . date ( 2019 , 12 , 31 ) datetime . date ( 2020 , 1 , 1 ) datetime . date ( 2020 , 1 , 1 )]","title":"pd.DateTimeIndex.date"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexdayofyear","text":"pandas.DatetimeIndex. dayofyear Example Usage >>> @bodo . jit ... def f ( I ): ... return I . dayofyear >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 365 , 365 , 365 , 1 , 1 ], dtype = 'int64' )","title":"pd.DateTimeIndex.dayofyear"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexday_of_year","text":"pandas.DatetimeIndex. day_of_year Example Usage >>> @bodo . jit ... def f ( I ): ... return I . day_of_year >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 365 , 365 , 365 , 1 , 1 ], dtype = 'int64' )","title":"pd.DateTimeIndex.day_of_year"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexdayofweek","text":"pandas.DatetimeIndex. dayofweek Example Usage >>> @bodo . jit ... def f ( I ): ... return I . dayofweek >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 2 , 2 ], dtype = 'int64' )","title":"pd.DateTimeIndex.dayofweek"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexday_of_week","text":"pandas.DatetimeIndex. day_of_week Example Usage >>> @bodo . jit ... def f ( I ): ... return I . day_of_week >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 2 , 2 ], dtype = 'int64' )","title":"pd.DateTimeIndex.day_of_week"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexis_leap_year","text":"pandas.DatetimeIndex. is_leap_year Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_leap_year >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) [ Flase False False True True ]","title":"pd.DateTimeIndex.is_leap_year"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexis_month_start","text":"pandas.DatetimeIndex. is_month_start Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_month_start >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 0 , 0 , 0 , 1 , 1 ], dtype = 'int64' )","title":"pd.DateTimeIndex.is_month_start"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexis_month_end","text":"pandas.DatetimeIndex. is_month_end Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_month_end >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 0 , 0 ], dtype = 'int64' )","title":"pd.DateTimeIndex.is_month_end"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexis_quarter_start","text":"pandas.DatetimeIndex. is_quarter_start Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_quarter_start >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 0 , 0 , 0 , 1 , 1 ], dtype = 'int64' )","title":"pd.DateTimeIndex.is_quarter_start"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexis_quarter_end","text":"pandas.DatetimeIndex. is_quarter_end Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_quarter_end >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 0 , 0 ], dtype = 'int64' )","title":"pd.DateTimeIndex.is_quarter_end"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexis_year_start","text":"pandas.DatetimeIndex. is_year_start Example Usage >>> @bodo . jit ... def f ( I ): ... return I . is_year_start >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 0 , 0 , 0 , 1 , 1 ], dtype = 'int64' )","title":"pd.DateTimeIndex.is_year_start"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexis_year_end","text":"pandas.DatetimeIndex. is_year_end Example Usage >>> @bodo.jit ... def f(I): ... return I.is_year_end >>> I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5)) >>> f(I) Int64Index([1, 1, 1, 0, 0], dtype='int64')","title":"pd.DateTimeIndex.is_year_end"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexweek","text":"pandas.DatetimeIndex. week Example Usage >>> @bodo . jit ... def f ( I ): ... return I . week >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 1 , 1 ], dtype = 'int64' )","title":"pd.DateTimeIndex.week"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexweekday","text":"pandas.DatetimeIndex. weekday Example Usage >>> @bodo . jit ... def f ( I ): ... return I . weekday >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 2 , 2 ], dtype = 'int64' )","title":"pd.DateTimeIndex.weekday"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexweekofyear","text":"pandas.DatetimeIndex. weekofyear Example Usage >>> @bodo . jit ... def f ( I ): ... return I . weekofyear >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 1 , 1 , 1 , 1 , 1 ], dtype = 'int64' )","title":"pd.DateTimeIndex.weekofyear"},{"location":"api_docs/pandas/indexapi/#pddatetimeindexquarter","text":"pandas.DatetimeIndex. quarter Example Usage >>> @bodo . jit ... def f ( I ): ... return I . quarter >>> I = pd . DatetimeIndex ( pd . date_range ( start = \"2019-12-31 02:32:45\" , end = \"2020-01-01 19:12:05\" , periods = 5 )) >>> f ( I ) Int64Index ([ 4 , 4 , 4 , 1 , 1 ], dtype = 'int64' ) Subtraction of Timestamp from DatetimeIndex and vice versa is supported. Comparison operators == , != , >= , > , <= , < between DatetimeIndex and a string of datetime are supported.","title":"pd.DateTimeIndex.quarter"},{"location":"api_docs/pandas/indexapi/#timedeltaindex","text":"TimedeltaIndex objects are supported. They can be constructed, boxed/unboxed, and set as index to dataframes and series.","title":"TimedeltaIndex"},{"location":"api_docs/pandas/indexapi/#pdtimedeltaindex","text":"pandas. TimedeltaIndex (data=None, unit=None, freq=NoDefault.no_default, closed=None, dtype=dtype('<m8[ns]'), copy=False, name=None) Supported Arguments - data : must be array-like of timedelta64ns or Ingetger.","title":"pd.TimedeltaIndex"},{"location":"api_docs/pandas/indexapi/#pdtimedeltaindexdays","text":"pandas.TimedeltaIndex. days Example Usage >>> @bodo . jit ... def f ( I ): ... return I . days >>> I = pd . TimedeltaIndex ([ pd . Timedelta ( 3 , unit = \"D\" ))]) >>> f ( I ) Int64Index ([ 3 ], dtype = 'int64' )","title":"pd.TimedeltaIndex.days"},{"location":"api_docs/pandas/indexapi/#pdtimedeltaindexseconds","text":"pandas.TimedeltaIndex. seconds Example Usage >>> @bodo . jit ... def f ( I ): ... return I . seconds >>> I = pd . TimedeltaIndex ([ pd . Timedelta ( - 2 , unit = \"S\" ))]) >>> f ( I ) Int64Index ([ - 2 ], dtype = 'int64' )","title":"pd.TimedeltaIndex.seconds"},{"location":"api_docs/pandas/indexapi/#pdtimedeltaindexmicroseconds","text":"pandas.TimedeltaIndex. microseconds Example Usage >>> @bodo . jit ... def f ( I ): ... return I . microseconds >>> I = pd . TimedeltaIndex ([ pd . Timedelta ( 11 , unit = \"micros\" ))]) >>> f ( I ) Int64Index ([ 11 ], dtype = 'int64' )","title":"pd.TimedeltaIndex.microseconds"},{"location":"api_docs/pandas/indexapi/#pdtimedeltaindexnanoseconds","text":"pandas.TimedeltaIndex. nanoseconds Example Usage >>> @bodo . jit ... def f ( I ): ... return I . nanoseconds >>> I = pd . TimedeltaIndex ([ pd . Timedelta ( 7 , unit = \"nanos\" ))]) >>> f ( I ) Int64Index ([ 7 ], dtype = 'int64' )","title":"pd.TimedeltaIndex.nanoseconds"},{"location":"api_docs/pandas/indexapi/#periodindex","text":"PeriodIndex objects can be boxed/unboxed and set as index to dataframes and series. Operations on them will be supported in upcoming releases.","title":"PeriodIndex"},{"location":"api_docs/pandas/indexapi/#binaryindex","text":"BinaryIndex objects can be boxed/unboxed and set as index to dataframes and series. Operations on them will be supported in upcoming releases.","title":"BinaryIndex"},{"location":"api_docs/pandas/indexapi/#multiindex","text":"","title":"MultiIndex"},{"location":"api_docs/pandas/indexapi/#pdmultiindexfrom_product","text":"pandas. MultiIndex.from_product ( iterables and names supported as tuples, no parallel support yet)","title":"pd.MultiIndex.from_product"},{"location":"api_docs/pandas/io/","text":"Input/Output \u00b6 See more in File IO , such as S3 and HDFS configuration requirements. pd.read_csv \u00b6 pandas. read_csv example usage and more system specific instructions filepath_or_buffer should be a string and is required. It could be pointing to a single CSV file, or a directory containing multiple partitioned CSV files (must have csv file extension inside directory). Arguments sep , delimiter , header , names , index_col , usecols , dtype , nrows , skiprows , chunksize , parse_dates , and low_memory are supported. Either names and dtype arguments should be provided to enable type inference, or filepath_or_buffer should be inferrable as a constant string. This is required so bodo can infer the types at compile time, see compile time constants names , usecols , parse_dates should be constant lists. dtype should be a constant dictionary of strings and types. skiprows must be an integer or list of integers and if it is not a constant, names must be provided to enable type inference. chunksize is supported for uncompressed files only. low_memory internally process file in chunks while parsing. In Bodo this is set to False by default. When set to True , Bodo parses file in chunks but like Pandas the entire file is read into a single DataFrame regardless. If you want to load data in chunks, use the chunksize argument. When a CSV file is read in parallel (distributed mode) and each process reads only a portion of the file, reading columns that contain line breaks is not supported. pd.read_excel \u00b6 pandas. read_excel output dataframe cannot be parallelized automatically yet. only arguments io , sheet_name , header , names , comment , dtype , skiprows , parse_dates are supported. io should be a string and is required. Either names and dtype arguments should be provided to enable type inference, or io should be inferrable as a constant string. This is required so bodo can infer the types at compile time, see compile time constants sheet_name , header , comment , and skiprows should be constant if provided. names and parse_dates should be constant lists if provided. dtype should be a constant dictionary of strings and types if provided. pd.read_sql \u00b6 pandas. read_sql example usage and more system specific instructions Argument sql is supported but only as a string form. SQLalchemy Selectable is not supported. There is no restriction on the form of the sql request. Argument con is supported but only as a string form. SQLalchemy connectable is not supported. Argument index_col is supported. Arguments chunksize , column , coerce_float , params are not supported. pd.read_parquet \u00b6 pandas. read_parquet example usage and more system specific instructions Arguments path and columns are supported. columns should be a constant list of strings if provided. Argument anon of storage_options is supported for S3 filepaths. If path can be inferred as a constant (e.g. it is a function argument), Bodo finds the schema from file at compilation time. Otherwise, schema should be provided using the numba syntax . For example: @bodo . jit ( locals = { 'df' :{ 'A' : bodo . float64 [:], 'B' : bodo . string_array_type }}) def impl ( f ): df = pd . read_parquet ( f ) return df pd.read_json \u00b6 pandas. read_json Example usage and more system specific instructions Only supports reading JSON Lines text file format ( pd.read_json(filepath_or_buffer, orient='records', lines=True) ) and regular multi-line JSON file( pd.read_json(filepath_or_buffer, orient='records', lines=False) ). Argument filepath_or_buffer is supported: it can point to a single JSON file, or a directory containing multiple partitioned JSON files. When reading a directory, the JSON files inside the directory must be JSON Lines text file format with json file extension. Argument orient = 'records' is used as default, instead of Pandas' default 'columns' for dataframes. 'records' is the only supported value for orient . Argument typ is supported. 'frame' is the only supported value for typ . filepath_or_buffer must be inferrable as a constant string. This is required so bodo can infer the types at compile time, see compile time constants . Arguments convert_dates , precise_float , lines are supported.","title":"Input/Output"},{"location":"api_docs/pandas/io/#pandas-f-in","text":"See more in File IO , such as S3 and HDFS configuration requirements.","title":"Input/Output"},{"location":"api_docs/pandas/io/#pdread_csv","text":"pandas. read_csv example usage and more system specific instructions filepath_or_buffer should be a string and is required. It could be pointing to a single CSV file, or a directory containing multiple partitioned CSV files (must have csv file extension inside directory). Arguments sep , delimiter , header , names , index_col , usecols , dtype , nrows , skiprows , chunksize , parse_dates , and low_memory are supported. Either names and dtype arguments should be provided to enable type inference, or filepath_or_buffer should be inferrable as a constant string. This is required so bodo can infer the types at compile time, see compile time constants names , usecols , parse_dates should be constant lists. dtype should be a constant dictionary of strings and types. skiprows must be an integer or list of integers and if it is not a constant, names must be provided to enable type inference. chunksize is supported for uncompressed files only. low_memory internally process file in chunks while parsing. In Bodo this is set to False by default. When set to True , Bodo parses file in chunks but like Pandas the entire file is read into a single DataFrame regardless. If you want to load data in chunks, use the chunksize argument. When a CSV file is read in parallel (distributed mode) and each process reads only a portion of the file, reading columns that contain line breaks is not supported.","title":"pd.read_csv"},{"location":"api_docs/pandas/io/#pdread_excel","text":"pandas. read_excel output dataframe cannot be parallelized automatically yet. only arguments io , sheet_name , header , names , comment , dtype , skiprows , parse_dates are supported. io should be a string and is required. Either names and dtype arguments should be provided to enable type inference, or io should be inferrable as a constant string. This is required so bodo can infer the types at compile time, see compile time constants sheet_name , header , comment , and skiprows should be constant if provided. names and parse_dates should be constant lists if provided. dtype should be a constant dictionary of strings and types if provided.","title":"pd.read_excel"},{"location":"api_docs/pandas/io/#pdread_sql","text":"pandas. read_sql example usage and more system specific instructions Argument sql is supported but only as a string form. SQLalchemy Selectable is not supported. There is no restriction on the form of the sql request. Argument con is supported but only as a string form. SQLalchemy connectable is not supported. Argument index_col is supported. Arguments chunksize , column , coerce_float , params are not supported.","title":"pd.read_sql"},{"location":"api_docs/pandas/io/#pdread_parquet","text":"pandas. read_parquet example usage and more system specific instructions Arguments path and columns are supported. columns should be a constant list of strings if provided. Argument anon of storage_options is supported for S3 filepaths. If path can be inferred as a constant (e.g. it is a function argument), Bodo finds the schema from file at compilation time. Otherwise, schema should be provided using the numba syntax . For example: @bodo . jit ( locals = { 'df' :{ 'A' : bodo . float64 [:], 'B' : bodo . string_array_type }}) def impl ( f ): df = pd . read_parquet ( f ) return df","title":"pd.read_parquet"},{"location":"api_docs/pandas/io/#pdread_json","text":"pandas. read_json Example usage and more system specific instructions Only supports reading JSON Lines text file format ( pd.read_json(filepath_or_buffer, orient='records', lines=True) ) and regular multi-line JSON file( pd.read_json(filepath_or_buffer, orient='records', lines=False) ). Argument filepath_or_buffer is supported: it can point to a single JSON file, or a directory containing multiple partitioned JSON files. When reading a directory, the JSON files inside the directory must be JSON Lines text file format with json file extension. Argument orient = 'records' is used as default, instead of Pandas' default 'columns' for dataframes. 'records' is the only supported value for orient . Argument typ is supported. 'frame' is the only supported value for typ . filepath_or_buffer must be inferrable as a constant string. This is required so bodo can infer the types at compile time, see compile time constants . Arguments convert_dates , precise_float , lines are supported.","title":"pd.read_json"},{"location":"api_docs/pandas/na_issues/","text":"Integer NA issue in Pandas \u00b6 DataFrame and Series objects with integer data need special care due to integer NA issues in Pandas . By default, Pandas dynamically converts integer columns to floating point when missing values (NAs) are needed (which can result in loss of precision). This is because Pandas uses the NaN floating point value as NA, and Numpy does not support NaN values for integers. Bodo does not perform this conversion unless enough information is available at compilation time. Pandas introduced a new nullable integer data type that can solve this issue, which is also supported by Bodo. For example, this code reads column A into a nullable integer array (the capital \"I\" denotes nullable integer type): @bodo . jit def example ( fname ): dtype = { 'A' : 'Int64' , 'B' : 'float64' } df = pd . read_csv ( fname , names = dtype . keys (), dtype = dtype , ) ...","title":"Integer NA Issue in Pandas"},{"location":"api_docs/pandas/na_issues/#integer-na-issue-pandas","text":"DataFrame and Series objects with integer data need special care due to integer NA issues in Pandas . By default, Pandas dynamically converts integer columns to floating point when missing values (NAs) are needed (which can result in loss of precision). This is because Pandas uses the NaN floating point value as NA, and Numpy does not support NaN values for integers. Bodo does not perform this conversion unless enough information is available at compilation time. Pandas introduced a new nullable integer data type that can solve this issue, which is also supported by Bodo. For example, this code reads column A into a nullable integer array (the capital \"I\" denotes nullable integer type): @bodo . jit def example ( fname ): dtype = { 'A' : 'Int64' , 'B' : 'float64' } df = pd . read_csv ( fname , names = dtype . keys (), dtype = dtype , ) ...","title":"Integer NA issue in Pandas"},{"location":"api_docs/pandas/series/","text":"Series \u00b6 Bodo provides extensive Series support. However, operations between Series (+, -, /, ,* ) do not implicitly align values based on their associated index values yet. pd.Series \u00b6 pandas. Series (data=None, index=None, dtype=None, name=None, copy=False, fastpath=False) Supported Arguments argument datatypes other requirements data Series type List type Array type Constant Dictionary None index SeriesType dtype Numpy or Pandas Type String name for Numpy/Pandas Type Must be constant at Compile Time String/Data Type must be one of the supported types (see Series.astype() ) name String Note If data is a Series and index is provided, implicit alignment is not performed yet. Example Usage >>> @bodo . jit ... def f (): ... return pd . Series ( np . arange ( 1000 ), dtype = np . float64 , name = \"my_series\" ) >>> f () 0 0.0 1 1.0 2 2.0 3 3.0 4 4.0 ... 995 995.0 996 996.0 997 997.0 998 998.0 999 999.0 Name : my_series , Length : 1000 , dtype : float64 Attributes \u00b6 pd.Series.index \u00b6 pandas.Series. index Example Usage >>> @bodo . jit ... def f ( S ): ... return S . index >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) RangeIndex ( start = 0 , stop = 1000 , step = 1 ) pd.Series.values \u00b6 pandas.Series. values Example Usage >>> @bodo . jit ... def f ( S ): ... return S . values >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) array ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 85 , 86 , 87 , 88 , 89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , 100 , 101 , 102 , 103 , 104 , 105 , 106 , 107 , 108 , 109 , 110 , 111 , 112 , 113 , 114 , 115 , 116 , 117 , 118 , 119 , 120 , 121 , 122 , 123 , 124 , 125 , 126 , 127 , 128 , 129 , 130 , 131 , 132 , 133 , 134 , 135 , 136 , 137 , 138 , 139 , 140 , 141 , 142 , 143 , 144 , 145 , 146 , 147 , 148 , 149 , 150 , 151 , 152 , 153 , 154 , 155 , 156 , 157 , 158 , 159 , 160 , 161 , 162 , 163 , 164 , 165 , 166 , 167 , 168 , 169 , 170 , 171 , 172 , 173 , 174 , 175 , 176 , 177 , 178 , 179 , 180 , 181 , 182 , 183 , 184 , 185 , 186 , 187 , 188 , 189 , 190 , 191 , 192 , 193 , 194 , 195 , 196 , 197 , 198 , 199 , 200 , 201 , 202 , 203 , 204 , 205 , 206 , 207 , 208 , 209 , 210 , 211 , 212 , 213 , 214 , 215 , 216 , 217 , 218 , 219 , 220 , 221 , 222 , 223 , 224 , 225 , 226 , 227 , 228 , 229 , 230 , 231 , 232 , 233 , 234 , 235 , 236 , 237 , 238 , 239 , 240 , 241 , 242 , 243 , 244 , 245 , 246 , 247 , 248 , 249 , 250 , 251 , 252 , 253 , 254 , 255 , 256 , 257 , 258 , 259 , 260 , 261 , 262 , 263 , 264 , 265 , 266 , 267 , 268 , 269 , 270 , 271 , 272 , 273 , 274 , 275 , 276 , 277 , 278 , 279 , 280 , 281 , 282 , 283 , 284 , 285 , 286 , 287 , 288 , 289 , 290 , 291 , 292 , 293 , 294 , 295 , 296 , 297 , 298 , 299 , 300 , 301 , 302 , 303 , 304 , 305 , 306 , 307 , 308 , 309 , 310 , 311 , 312 , 313 , 314 , 315 , 316 , 317 , 318 , 319 , 320 , 321 , 322 , 323 , 324 , 325 , 326 , 327 , 328 , 329 , 330 , 331 , 332 , 333 , 334 , 335 , 336 , 337 , 338 , 339 , 340 , 341 , 342 , 343 , 344 , 345 , 346 , 347 , 348 , 349 , 350 , 351 , 352 , 353 , 354 , 355 , 356 , 357 , 358 , 359 , 360 , 361 , 362 , 363 , 364 , 365 , 366 , 367 , 368 , 369 , 370 , 371 , 372 , 373 , 374 , 375 , 376 , 377 , 378 , 379 , 380 , 381 , 382 , 383 , 384 , 385 , 386 , 387 , 388 , 389 , 390 , 391 , 392 , 393 , 394 , 395 , 396 , 397 , 398 , 399 , 400 , 401 , 402 , 403 , 404 , 405 , 406 , 407 , 408 , 409 , 410 , 411 , 412 , 413 , 414 , 415 , 416 , 417 , 418 , 419 , 420 , 421 , 422 , 423 , 424 , 425 , 426 , 427 , 428 , 429 , 430 , 431 , 432 , 433 , 434 , 435 , 436 , 437 , 438 , 439 , 440 , 441 , 442 , 443 , 444 , 445 , 446 , 447 , 448 , 449 , 450 , 451 , 452 , 453 , 454 , 455 , 456 , 457 , 458 , 459 , 460 , 461 , 462 , 463 , 464 , 465 , 466 , 467 , 468 , 469 , 470 , 471 , 472 , 473 , 474 , 475 , 476 , 477 , 478 , 479 , 480 , 481 , 482 , 483 , 484 , 485 , 486 , 487 , 488 , 489 , 490 , 491 , 492 , 493 , 494 , 495 , 496 , 497 , 498 , 499 , 500 , 501 , 502 , 503 , 504 , 505 , 506 , 507 , 508 , 509 , 510 , 511 , 512 , 513 , 514 , 515 , 516 , 517 , 518 , 519 , 520 , 521 , 522 , 523 , 524 , 525 , 526 , 527 , 528 , 529 , 530 , 531 , 532 , 533 , 534 , 535 , 536 , 537 , 538 , 539 , 540 , 541 , 542 , 543 , 544 , 545 , 546 , 547 , 548 , 549 , 550 , 551 , 552 , 553 , 554 , 555 , 556 , 557 , 558 , 559 , 560 , 561 , 562 , 563 , 564 , 565 , 566 , 567 , 568 , 569 , 570 , 571 , 572 , 573 , 574 , 575 , 576 , 577 , 578 , 579 , 580 , 581 , 582 , 583 , 584 , 585 , 586 , 587 , 588 , 589 , 590 , 591 , 592 , 593 , 594 , 595 , 596 , 597 , 598 , 599 , 600 , 601 , 602 , 603 , 604 , 605 , 606 , 607 , 608 , 609 , 610 , 611 , 612 , 613 , 614 , 615 , 616 , 617 , 618 , 619 , 620 , 621 , 622 , 623 , 624 , 625 , 626 , 627 , 628 , 629 , 630 , 631 , 632 , 633 , 634 , 635 , 636 , 637 , 638 , 639 , 640 , 641 , 642 , 643 , 644 , 645 , 646 , 647 , 648 , 649 , 650 , 651 , 652 , 653 , 654 , 655 , 656 , 657 , 658 , 659 , 660 , 661 , 662 , 663 , 664 , 665 , 666 , 667 , 668 , 669 , 670 , 671 , 672 , 673 , 674 , 675 , 676 , 677 , 678 , 679 , 680 , 681 , 682 , 683 , 684 , 685 , 686 , 687 , 688 , 689 , 690 , 691 , 692 , 693 , 694 , 695 , 696 , 697 , 698 , 699 , 700 , 701 , 702 , 703 , 704 , 705 , 706 , 707 , 708 , 709 , 710 , 711 , 712 , 713 , 714 , 715 , 716 , 717 , 718 , 719 , 720 , 721 , 722 , 723 , 724 , 725 , 726 , 727 , 728 , 729 , 730 , 731 , 732 , 733 , 734 , 735 , 736 , 737 , 738 , 739 , 740 , 741 , 742 , 743 , 744 , 745 , 746 , 747 , 748 , 749 , 750 , 751 , 752 , 753 , 754 , 755 , 756 , 757 , 758 , 759 , 760 , 761 , 762 , 763 , 764 , 765 , 766 , 767 , 768 , 769 , 770 , 771 , 772 , 773 , 774 , 775 , 776 , 777 , 778 , 779 , 780 , 781 , 782 , 783 , 784 , 785 , 786 , 787 , 788 , 789 , 790 , 791 , 792 , 793 , 794 , 795 , 796 , 797 , 798 , 799 , 800 , 801 , 802 , 803 , 804 , 805 , 806 , 807 , 808 , 809 , 810 , 811 , 812 , 813 , 814 , 815 , 816 , 817 , 818 , 819 , 820 , 821 , 822 , 823 , 824 , 825 , 826 , 827 , 828 , 829 , 830 , 831 , 832 , 833 , 834 , 835 , 836 , 837 , 838 , 839 , 840 , 841 , 842 , 843 , 844 , 845 , 846 , 847 , 848 , 849 , 850 , 851 , 852 , 853 , 854 , 855 , 856 , 857 , 858 , 859 , 860 , 861 , 862 , 863 , 864 , 865 , 866 , 867 , 868 , 869 , 870 , 871 , 872 , 873 , 874 , 875 , 876 , 877 , 878 , 879 , 880 , 881 , 882 , 883 , 884 , 885 , 886 , 887 , 888 , 889 , 890 , 891 , 892 , 893 , 894 , 895 , 896 , 897 , 898 , 899 , 900 , 901 , 902 , 903 , 904 , 905 , 906 , 907 , 908 , 909 , 910 , 911 , 912 , 913 , 914 , 915 , 916 , 917 , 918 , 919 , 920 , 921 , 922 , 923 , 924 , 925 , 926 , 927 , 928 , 929 , 930 , 931 , 932 , 933 , 934 , 935 , 936 , 937 , 938 , 939 , 940 , 941 , 942 , 943 , 944 , 945 , 946 , 947 , 948 , 949 , 950 , 951 , 952 , 953 , 954 , 955 , 956 , 957 , 958 , 959 , 960 , 961 , 962 , 963 , 964 , 965 , 966 , 967 , 968 , 969 , 970 , 971 , 972 , 973 , 974 , 975 , 976 , 977 , 978 , 979 , 980 , 981 , 982 , 983 , 984 , 985 , 986 , 987 , 988 , 989 , 990 , 991 , 992 , 993 , 994 , 995 , 996 , 997 , 998 , 999 ]) pd.Series.dtype \u00b6 pandas.Series. dtype (object data types such as dtype of string series not supported yet) Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dtype >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) dtype ( 'int64' ) pd.Series.shape \u00b6 pandas.Series. shape Example Usage >>> @bodo . jit ... def f ( S ): ... return S . shape >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) ( 1000 ,) pd.Series.nbytes \u00b6 pandas.Series. nbytes Note This tracks the number of bytes used by Bodo which may differ from the Pandas values. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . nbytes >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 8000 pd.Series.ndim \u00b6 pandas.Series. ndim Example Usage >>> @bodo . jit ... def f ( S ): ... return S . ndim >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 1 pd.Series.size \u00b6 pandas.Series. size Example Usage >>> @bodo . jit ... def f ( S ): ... return S . size >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 1000 pd.Series.T \u00b6 pandas.Series. T Example Usage >>> @bodo . jit ... def f ( S ): ... return S . T >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 ... 995 995 996 996 997 997 998 998 999 999 Length : 1000 , dtype : int64 pd.Series.memory_usage \u00b6 pandas.Series. memory_usage (index=True, deep=False) Supported Arguments argument datatypes other requirements index Boolean Must be constant at Compile Time Note This tracks the number of bytes used by Bodo which may differ from the Pandas values. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . memory_usage () >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 8024 pd.Series.hasnans \u00b6 pandas.Series. hasnans Example Usage >>> @bodo . jit ... def f ( S ): ... return S . hasnans >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) False pd.Series.empty \u00b6 pandas.Series. empty Example Usage >>> @bodo . jit ... def f ( S ): ... return S . empty >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) False pd.Series.dtypes \u00b6 pandas.Series. dtypes Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dtypes >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) dtype ( 'int64' ) pd.Series.name \u00b6 pandas.Series. name Example Usage >>> @bodo . jit ... def f ( S ): ... return S . name >>> S = pd . Series ( np . arange ( 1000 ), name = \"my_series\" ) >>> f ( S ) 'my_series' Conversion: \u00b6 pd.Series.astype \u00b6 pandas.Series. astype (dtype, copy=True, errors=\"raise\", _bodo_nan_to_str=True) Supported Arguments argument datatypes other requirements dtype String (string must be parsable by np.dtype ) Valid type (see types) The following functions: float, int, bool, str Must be constant at Compile Time copy Boolean Must be constant at Compile Time _bodo_nan_to_str Boolean Must be constant at Compile Time Argument unique to Bodo. When True NA values in when converting to string are represented as NA instead of a string representation of the NA value (i.e. 'nan'), the default Pandas behavior. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . astype ( np . float32 ) >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 0 0.0 1 1.0 2 2.0 3 3.0 4 4.0 ... 995 995.0 996 996.0 997 997.0 998 998.0 999 999.0 Length : 1000 , dtype : float32 pd.Series.copy \u00b6 pandas.Series. copy (deep=True) Supported Arguments argument datatypes deep Boolean Example Usage >>> @bodo . jit ... def f ( S ): ... return S . copy () >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 ... 995 995 996 996 997 997 998 998 999 999 Length : 1000 , dtype : int64 pd.Series.to_numpy \u00b6 pandas.Series. to_numpy (dtype=None, copy=False, na_value=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . to_numpy () >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) array ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 85 , 86 , 87 , 88 , 89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , 100 , 101 , 102 , 103 , 104 , 105 , 106 , 107 , 108 , 109 , 110 , 111 , 112 , 113 , 114 , 115 , 116 , 117 , 118 , 119 , 120 , 121 , 122 , 123 , 124 , 125 , 126 , 127 , 128 , 129 , 130 , 131 , 132 , 133 , 134 , 135 , 136 , 137 , 138 , 139 , 140 , 141 , 142 , 143 , 144 , 145 , 146 , 147 , 148 , 149 , 150 , 151 , 152 , 153 , 154 , 155 , 156 , 157 , 158 , 159 , 160 , 161 , 162 , 163 , 164 , 165 , 166 , 167 , 168 , 169 , 170 , 171 , 172 , 173 , 174 , 175 , 176 , 177 , 178 , 179 , 180 , 181 , 182 , 183 , 184 , 185 , 186 , 187 , 188 , 189 , 190 , 191 , 192 , 193 , 194 , 195 , 196 , 197 , 198 , 199 , 200 , 201 , 202 , 203 , 204 , 205 , 206 , 207 , 208 , 209 , 210 , 211 , 212 , 213 , 214 , 215 , 216 , 217 , 218 , 219 , 220 , 221 , 222 , 223 , 224 , 225 , 226 , 227 , 228 , 229 , 230 , 231 , 232 , 233 , 234 , 235 , 236 , 237 , 238 , 239 , 240 , 241 , 242 , 243 , 244 , 245 , 246 , 247 , 248 , 249 , 250 , 251 , 252 , 253 , 254 , 255 , 256 , 257 , 258 , 259 , 260 , 261 , 262 , 263 , 264 , 265 , 266 , 267 , 268 , 269 , 270 , 271 , 272 , 273 , 274 , 275 , 276 , 277 , 278 , 279 , 280 , 281 , 282 , 283 , 284 , 285 , 286 , 287 , 288 , 289 , 290 , 291 , 292 , 293 , 294 , 295 , 296 , 297 , 298 , 299 , 300 , 301 , 302 , 303 , 304 , 305 , 306 , 307 , 308 , 309 , 310 , 311 , 312 , 313 , 314 , 315 , 316 , 317 , 318 , 319 , 320 , 321 , 322 , 323 , 324 , 325 , 326 , 327 , 328 , 329 , 330 , 331 , 332 , 333 , 334 , 335 , 336 , 337 , 338 , 339 , 340 , 341 , 342 , 343 , 344 , 345 , 346 , 347 , 348 , 349 , 350 , 351 , 352 , 353 , 354 , 355 , 356 , 357 , 358 , 359 , 360 , 361 , 362 , 363 , 364 , 365 , 366 , 367 , 368 , 369 , 370 , 371 , 372 , 373 , 374 , 375 , 376 , 377 , 378 , 379 , 380 , 381 , 382 , 383 , 384 , 385 , 386 , 387 , 388 , 389 , 390 , 391 , 392 , 393 , 394 , 395 , 396 , 397 , 398 , 399 , 400 , 401 , 402 , 403 , 404 , 405 , 406 , 407 , 408 , 409 , 410 , 411 , 412 , 413 , 414 , 415 , 416 , 417 , 418 , 419 , 420 , 421 , 422 , 423 , 424 , 425 , 426 , 427 , 428 , 429 , 430 , 431 , 432 , 433 , 434 , 435 , 436 , 437 , 438 , 439 , 440 , 441 , 442 , 443 , 444 , 445 , 446 , 447 , 448 , 449 , 450 , 451 , 452 , 453 , 454 , 455 , 456 , 457 , 458 , 459 , 460 , 461 , 462 , 463 , 464 , 465 , 466 , 467 , 468 , 469 , 470 , 471 , 472 , 473 , 474 , 475 , 476 , 477 , 478 , 479 , 480 , 481 , 482 , 483 , 484 , 485 , 486 , 487 , 488 , 489 , 490 , 491 , 492 , 493 , 494 , 495 , 496 , 497 , 498 , 499 , 500 , 501 , 502 , 503 , 504 , 505 , 506 , 507 , 508 , 509 , 510 , 511 , 512 , 513 , 514 , 515 , 516 , 517 , 518 , 519 , 520 , 521 , 522 , 523 , 524 , 525 , 526 , 527 , 528 , 529 , 530 , 531 , 532 , 533 , 534 , 535 , 536 , 537 , 538 , 539 , 540 , 541 , 542 , 543 , 544 , 545 , 546 , 547 , 548 , 549 , 550 , 551 , 552 , 553 , 554 , 555 , 556 , 557 , 558 , 559 , 560 , 561 , 562 , 563 , 564 , 565 , 566 , 567 , 568 , 569 , 570 , 571 , 572 , 573 , 574 , 575 , 576 , 577 , 578 , 579 , 580 , 581 , 582 , 583 , 584 , 585 , 586 , 587 , 588 , 589 , 590 , 591 , 592 , 593 , 594 , 595 , 596 , 597 , 598 , 599 , 600 , 601 , 602 , 603 , 604 , 605 , 606 , 607 , 608 , 609 , 610 , 611 , 612 , 613 , 614 , 615 , 616 , 617 , 618 , 619 , 620 , 621 , 622 , 623 , 624 , 625 , 626 , 627 , 628 , 629 , 630 , 631 , 632 , 633 , 634 , 635 , 636 , 637 , 638 , 639 , 640 , 641 , 642 , 643 , 644 , 645 , 646 , 647 , 648 , 649 , 650 , 651 , 652 , 653 , 654 , 655 , 656 , 657 , 658 , 659 , 660 , 661 , 662 , 663 , 664 , 665 , 666 , 667 , 668 , 669 , 670 , 671 , 672 , 673 , 674 , 675 , 676 , 677 , 678 , 679 , 680 , 681 , 682 , 683 , 684 , 685 , 686 , 687 , 688 , 689 , 690 , 691 , 692 , 693 , 694 , 695 , 696 , 697 , 698 , 699 , 700 , 701 , 702 , 703 , 704 , 705 , 706 , 707 , 708 , 709 , 710 , 711 , 712 , 713 , 714 , 715 , 716 , 717 , 718 , 719 , 720 , 721 , 722 , 723 , 724 , 725 , 726 , 727 , 728 , 729 , 730 , 731 , 732 , 733 , 734 , 735 , 736 , 737 , 738 , 739 , 740 , 741 , 742 , 743 , 744 , 745 , 746 , 747 , 748 , 749 , 750 , 751 , 752 , 753 , 754 , 755 , 756 , 757 , 758 , 759 , 760 , 761 , 762 , 763 , 764 , 765 , 766 , 767 , 768 , 769 , 770 , 771 , 772 , 773 , 774 , 775 , 776 , 777 , 778 , 779 , 780 , 781 , 782 , 783 , 784 , 785 , 786 , 787 , 788 , 789 , 790 , 791 , 792 , 793 , 794 , 795 , 796 , 797 , 798 , 799 , 800 , 801 , 802 , 803 , 804 , 805 , 806 , 807 , 808 , 809 , 810 , 811 , 812 , 813 , 814 , 815 , 816 , 817 , 818 , 819 , 820 , 821 , 822 , 823 , 824 , 825 , 826 , 827 , 828 , 829 , 830 , 831 , 832 , 833 , 834 , 835 , 836 , 837 , 838 , 839 , 840 , 841 , 842 , 843 , 844 , 845 , 846 , 847 , 848 , 849 , 850 , 851 , 852 , 853 , 854 , 855 , 856 , 857 , 858 , 859 , 860 , 861 , 862 , 863 , 864 , 865 , 866 , 867 , 868 , 869 , 870 , 871 , 872 , 873 , 874 , 875 , 876 , 877 , 878 , 879 , 880 , 881 , 882 , 883 , 884 , 885 , 886 , 887 , 888 , 889 , 890 , 891 , 892 , 893 , 894 , 895 , 896 , 897 , 898 , 899 , 900 , 901 , 902 , 903 , 904 , 905 , 906 , 907 , 908 , 909 , 910 , 911 , 912 , 913 , 914 , 915 , 916 , 917 , 918 , 919 , 920 , 921 , 922 , 923 , 924 , 925 , 926 , 927 , 928 , 929 , 930 , 931 , 932 , 933 , 934 , 935 , 936 , 937 , 938 , 939 , 940 , 941 , 942 , 943 , 944 , 945 , 946 , 947 , 948 , 949 , 950 , 951 , 952 , 953 , 954 , 955 , 956 , 957 , 958 , 959 , 960 , 961 , 962 , 963 , 964 , 965 , 966 , 967 , 968 , 969 , 970 , 971 , 972 , 973 , 974 , 975 , 976 , 977 , 978 , 979 , 980 , 981 , 982 , 983 , 984 , 985 , 986 , 987 , 988 , 989 , 990 , 991 , 992 , 993 , 994 , 995 , 996 , 997 , 998 , 999 ]) pd.Series.tolist \u00b6 pandas.Series. tolist () Note Calling tolist on a non-float array with NA values with cause a runtime exception. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . tolist () >>> S = pd . Series ( np . arange ( 50 )) >>> f ( S ) [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 ] Indexing, iteration: \u00b6 Location based indexing using [] , iat , and iloc is supported. Changing values of existing string Series using these operators is not supported yet. pd.Series.iat \u00b6 pandas.Series. iat We only support indexing using iat using a pair of integers Example Usage >>> @bodo . jit ... def f ( S , i ): ... return S . iat [ i ] >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S , 27 ) 27 pd.Series.iloc \u00b6 pandas.Series. iloc getitem : Series.iloc supports single integer indexing (returns a scalar) S.iloc[0] Series.iloc supports list/array/series of integers/bool (returns a Series) S.iloc[[0,1,2]] Series.iloc supports integer slice (returns a Series) S.iloc[[0:2]] setitem : Supports the same cases as getitem but the array type must be mutable (i.e. numeric array) Example Usage >>> @bodo . jit ... def f ( S , idx ): ... return S . iloc [ idx ] >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S , [ 1 , 4 , 29 ]) 1 1 4 4 29 29 dtype : int64 pd.Series.loc \u00b6 pandas.Series. loc getitem : Series.loc supports list/array of booleans Series.loc supports integer with RangeIndex setitem : Series.loc supports list/array of booleans Example Usage >>> @bodo . jit ... def f ( S , idx ): ... return S . loc [ idx ] >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S , S < 10 ) 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 dtype : int64 Binary operator functions: \u00b6 pd.Series.add \u00b6 pandas.Series. add (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.add is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . add ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1001 1 1001 2 1001 3 1001 4 1001 ... 995 1001 996 1001 997 1001 998 1001 999 1001 Length : 1000 , dtype : int64 pd.Series.sub \u00b6 pandas.Series. sub (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.sub is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . sub ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 - 999 1 - 997 2 - 995 3 - 993 4 - 991 ... 995 991 996 993 997 995 998 997 999 999 Length : 1000 , dtype : int64 pd.Series.mul \u00b6 pandas.Series. mul (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.mul is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . mul ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000 1 1998 2 2994 3 3988 4 4980 ... 995 4980 996 3988 997 2994 998 1998 999 1000 Length : 1000 , dtype : int64 pd.Series.div \u00b6 pandas.Series. div (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.div is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . div ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 0.001000 1 0.002002 2 0.003006 3 0.004012 4 0.005020 ... 995 199.200000 996 249.250000 997 332.666667 998 499.500000 999 1000.000000 Length : 1000 , dtype : float64 pd.Series.truediv \u00b6 pandas.Series. truediv (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.truediv is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . truediv ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 0.001000 1 0.002002 2 0.003006 3 0.004012 4 0.005020 ... 995 199.200000 996 249.250000 997 332.666667 998 499.500000 999 1000.000000 Length : 1000 , dtype : float64 pd.Series.floordiv \u00b6 pandas.Series. floordiv (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.floordiv is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . floordiv ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 0 1 0 2 0 3 0 4 0 ... 995 199 996 249 997 332 998 499 999 1000 Length : 1000 , dtype : int64 pd.Series.mod \u00b6 pandas.Series. mod (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.mod is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . mod ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1 1 2 2 3 3 4 4 5 .. 995 1 996 1 997 2 998 1 999 0 Length : 1000 , dtype : int64 pd.Series.pow \u00b6 pandas.Series. pow (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.pow is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . pow ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1 1 0 2 - 5459658280481875879 3 0 4 3767675092665006833 ... 995 980159361278976 996 988053892081 997 994011992 998 998001 999 1000 Length : 1000 , dtype : int64 pd.Series.radd \u00b6 pandas.Series. radd (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.radd is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . radd ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1001 1 1001 2 1001 3 1001 4 1001 ... 995 1001 996 1001 997 1001 998 1001 999 1001 Length : 1000 , dtype : int64 pd.Series.rsub \u00b6 pandas.Series. rsub (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rsub is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rsub ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 999 1 997 2 995 3 993 4 991 ... 995 - 991 996 - 993 997 - 995 998 - 997 999 - 999 Length : 1000 , dtype : int64 pd.Series.rmul \u00b6 pandas.Series. rmul (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rmul is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rmul ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000 1 1998 2 2994 3 3988 4 4980 ... 995 4980 996 3988 997 2994 998 1998 999 1000 Length : 1000 , dtype : int64 pd.Series.rdiv \u00b6 pandas.Series. rdiv (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rdiv is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rdiv ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000.000000 1 499.500000 2 332.666667 3 249.250000 4 199.200000 ... 995 0.005020 996 0.004012 997 0.003006 998 0.002002 999 0.001000 Length : 1000 , dtype : float64 pd.Series.rtruediv \u00b6 pandas.Series. rtruediv (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rtruediv is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rtruediv ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000.000000 1 499.500000 2 332.666667 3 249.250000 4 199.200000 ... 995 0.005020 996 0.004012 997 0.003006 998 0.002002 999 0.001000 Length : 1000 , dtype : float64 pd.Series.rfloordiv \u00b6 pandas.Series. rfloordiv (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rfloordiv is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rfloordiv ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000 1 499 2 332 3 249 4 199 ... 995 0 996 0 997 0 998 0 999 0 Length : 1000 , dtype : int64 pd.Series.rmod \u00b6 pandas.Series. rmod (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rmod is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rmod ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 0 1 1 2 2 3 1 4 1 .. 995 5 996 4 997 3 998 2 999 1 Length : 1000 , dtype : int64 pd.Series.rpow \u00b6 pandas.Series. rpow (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rpow is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rpow ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000 1 998001 2 994011992 3 988053892081 4 980159361278976 ... 995 3767675092665006833 996 0 997 - 5459658280481875879 998 0 999 1 Length : 1000 , dtype : int64 pd.Series.combine \u00b6 pandas.Series. combine (other, func, fill_value=None) Supported Arguments argument datatypes other requirements other Array Series func Function that takes two scalar arguments and returns a scalar value. fill_value scalar Must be provided if the Series lengths aren't equal and the dtypes aren't floats. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . combine ( other , lambda a , b : 2 * a + b ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1002 1 1003 2 1004 3 1005 4 1006 ... 995 1997 996 1998 997 1999 998 2000 999 2001 Length : 1000 , dtype : int64 pd.Series.round \u00b6 pandas.Series. round (decimals=0) Supported Arguments argument datatypes other Series with numeric data Note Series.round is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . round ( 2 ) >>> S = pd . Series ( np . linspace ( 100 , 1000 )) >>> f ( S ) 0 100.00 1 118.37 2 136.73 3 155.10 4 173.47 5 191.84 6 210.20 7 228.57 8 246.94 9 265.31 10 283.67 11 302.04 12 320.41 13 338.78 14 357.14 15 375.51 16 393.88 17 412.24 18 430.61 19 448.98 20 467.35 21 485.71 22 504.08 23 522.45 24 540.82 25 559.18 26 577.55 27 595.92 28 614.29 29 632.65 30 651.02 31 669.39 32 687.76 33 706.12 34 724.49 35 742.86 36 761.22 37 779.59 38 797.96 39 816.33 40 834.69 41 853.06 42 871.43 43 889.80 44 908.16 45 926.53 46 944.90 47 963.27 48 981.63 49 1000.00 dtype : float64 pd.Series.lt \u00b6 pandas.Series. lt (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.lt is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . lt ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 True 1 True 2 True 3 True 4 True ... 995 False 996 False 997 False 998 False 999 False Length : 1000 , dtype : bool pd.Series.gt \u00b6 pandas.Series. gt (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.gt is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . gt ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 False 1 False 2 False 3 False 4 False ... 995 True 996 True 997 True 998 True 999 True Length : 1000 , dtype : bool pd.Series.le \u00b6 pandas.Series. le (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.le is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . le ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 True 1 True 2 True 3 True 4 True ... 995 False 996 False 997 False 998 False 999 False Length : 1000 , dtype : bool pd.Series.ge \u00b6 pandas.Series. ge (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.ge is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . ge ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 False 1 False 2 False 3 False 4 False ... 995 True 996 True 997 True 998 True 999 True Length : 1000 , dtype : bool pd.Series.ne \u00b6 pandas.Series. ne (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.ne is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . ne ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 True 1 True 2 True 3 True 4 True ... 995 True 996 True 997 True 998 True 999 True Length : 1000 , dtype : bool pd.Series.eq \u00b6 pandas.Series. eq (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.eq is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . eq ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 False 1 False 2 False 3 False 4 False ... 995 False 996 False 997 False 998 False 999 False Length : 1000 , dtype : bool pd.Series.dot \u00b6 pandas.Series. dot (other) Supported Arguments argument datatypes other Series with numeric data Note Series.dot is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . dot ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 167167000 Function application, GroupBy & Window \u00b6 pd.Series.apply \u00b6 pandas.Series. apply f(func, convert_dtype=True, args=(), **kwargs) Supported Arguments argument datatypes other requirements func JIT function or callable defined within a JIT function Numpy ufunc Constant String which is the name of a supported Series method or Numpy ufunc Additional arguments for func can be passed as additional arguments. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . apply ( lambda x : x ** 0.75 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0.000000 1 1.000000 2 1.681793 3 2.279507 4 2.828427 ... 95 30.429352 96 30.669269 97 30.908562 98 31.147239 99 31.385308 Length : 100 , dtype : float64 pd.Series.map \u00b6 pandas.Series. map (arg, na_action=None) Supported Arguments argument datatypes arg Dictionary JIT function or callable defined within a JIT function Constant String which refers to a supported Series method or Numpy ufunc Numpy ufunc Example Usage >>> @bodo . jit ... def f ( S ): ... return S . map ( lambda x : x ** 0.75 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0.000000 1 1.000000 2 1.681793 3 2.279507 4 2.828427 ... 95 30.429352 96 30.669269 97 30.908562 98 31.147239 99 31.385308 Length : 100 , dtype : float64 pd.Series.groupby \u00b6 pandas.Series. groupby (by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) Supported Arguments argument datatypes other requirements by Array-like or Series data. This is not supported with Decimal or Categorical data. Must be constant at Compile Time level integer Must be constant at Compile Time Only level=0 is supported and not with MultiIndex. You must provide exactly one of by and level Example Usage >>> @bodo . jit ... def f ( S , by_series ): ... return S . groupby ( by_series ) . count () >>> S = pd . Series ([ 1 , 2 , 24 , None ] * 5 ) >>> by_series = pd . Series ([ \"421\" , \"f31\" ] * 10 ) >>> f ( S , by_series ) > 421 10 f31 5 Name : , dtype : int64 Note Series.groupby doesn't currently keep the name of the original Series. pd.Series.rolling \u00b6 pandas.Series. rolling (window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None, method='single') Supported Arguments argument datatypes window Integer String representing a Time Offset Timedelta min_periods Integer center Boolean Example Usage >>> @bodo . jit ... def f ( S ): ... return S . rolling ( 2 ) . mean () >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 NaN 1 0.5 2 1.5 3 2.5 4 3.5 ... 95 94.5 96 95.5 97 96.5 98 97.5 99 98.5 Length : 100 , dtype : float64 pd.Series.pipe \u00b6 pandas.Series. pipe (func, *args, **kwargs) Supported Arguments argument datatypes other requirements func JIT function or callable defined within a JIT function. Additional arguments for func can be passed as additional arguments. Note func cannot be a tuple Example Usage >>> @bodo . jit ... def f ( S ): ... def g ( row , y ): ... return row + y ... ... def f ( row ): ... return row * 2 ... ... return S . pipe ( h ) . pipe ( g , y = 32 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 32 1 34 2 36 3 38 4 40 ... 95 222 96 224 97 226 98 228 99 230 Length : 100 , dtype : int64 Computations / Descriptive Stats \u00b6 Statistical functions below are supported without optional arguments unless support is explicitly mentioned. pd.Series.abs \u00b6 pandas.Series. abs () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . abs () >>> S = ( pd . Series ( np . arange ( 100 )) % 7 ) - 2 >>> f ( S ) 0 2 1 1 2 0 3 1 4 2 .. 95 2 96 3 97 4 98 2 99 1 Length : 100 , dtype : int64 pd.Series.all \u00b6 pandas.Series. all (axis=0, bool_only=None, skipna=True, level=None) Supported Arguments None Note Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . all () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) False pd.Series.any \u00b6 pandas.Series. any (axis=0, bool_only=None, skipna=True, level=None) Supported Arguments None Note Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . any () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) True pd.Series.autocorr \u00b6 pandas.Series. autocorr (lag=1) Supported Arguments argument datatypes lag Integer Example Usage >>> @bodo . jit ... def f ( S ): ... return S . autocorr ( 3 ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) - 0.49872171657407155 pd.Series.between \u00b6 pandas.Series. between (left, right, inclusive='both') Supported Arguments argument datatypes other requirements left Scalar matching the Series type right Scalar matching the Series type inclusive One of (\"both\", \"neither\") Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( S ): ... return S . between ( 3 , 5 , \"both\" ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 False 1 False 2 False 3 True 4 True ... 95 True 96 True 97 False 98 False 99 False Length : 100 , dtype : bool pd.Series.corr \u00b6 pandas.Series. corr (other, method='pearson', min_periods=None) Supported Arguments argument datatypes other Numeric Series or Array Note Series type must be numeric Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . cov ( other ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> other = pd . Series ( np . arange ( 100 )) % 10 >>> f ( S , other ) 0.004326329627279103 pd.Series.count \u00b6 pandas.Series. count (level=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . count () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 100 pd.Series.cov \u00b6 pandas.Series. cov (other, min_periods=None, ddof=1) Supported Arguments argument datatypes other Numeric Series or Array ddof Integer Note Series type must be numeric Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . cov ( other ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> other = pd . Series ( np . arange ( 100 )) % 10 >>> f ( S , other ) 0.025252525252525252 pd.Series.cummin \u00b6 pandas.Series. cummin (axis=None, skipna=True) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . cummin () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 0 1 0 2 0 3 0 4 0 .. 95 0 96 0 97 0 98 0 99 0 Length : 100 , dtype : int64 pd.Series.cummax \u00b6 pandas.Series. cummax (axis=None, skipna=True) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . cummax () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 .. 95 6 96 6 97 6 98 6 99 6 Length : 100 , dtype : int64 pd.Series.cumprod \u00b6 pandas.Series. cumprod (axis=None, skipna=True) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . cumprod () >>> S = ( pd . Series ( np . arange ( 10 )) % 7 ) + 1 >>> f ( S ) 0 1 1 2 2 6 3 24 4 120 5 720 6 5040 7 5040 8 10080 9 30240 dtype : int64 pd.Series.cumsum \u00b6 pandas.Series. cumsum (axis=None, skipna=True) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . cumsum () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 0 1 1 2 3 3 6 4 10 ... 95 283 96 288 97 294 98 294 99 295 Length : 100 , dtype : int64 pd.Series.describe \u00b6 pandas.Series. describe (percentiles=None, include=None, exclude=None, datetime_is_numeric=False) Supported Arguments None Note Bodo only supports numeric and datetime64 types and assumes datetime_is_numeric=True Example Usage >>> @bodo . jit ... def f ( S ): ... return S . describe () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) count 100.000000 mean 2.950000 std 2.021975 min 0.000000 25 % 1.000000 50 % 3.000000 75 % 5.000000 max 6.000000 dtype : float64 pd.Series.diff \u00b6 pandas.Series. diff (periods=1) Supported Arguments argument datatypes periods Integer Note Bodo only supports numeric and datetime64 types Example Usage >>> @bodo . jit ... def f ( S ): ... return S . diff ( 3 ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 NaN 1 NaN 2 NaN 3 3.0 4 3.0 ... 95 3.0 96 3.0 97 3.0 98 - 4.0 99 - 4.0 Length : 100 , dtype : float64 pd.Series.kurt \u00b6 pandas.Series. kurt (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . kurt () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) - 1.269562153611973 pd.Series.mad \u00b6 pandas.Series. mad (axis=None, skipna=None, level=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Example Usage >>> @bodo . jit ... def f ( S ): ... return S . mad () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 1.736 pd.Series.max \u00b6 pandas.Series. max (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . max () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 6 pd.Series.mean \u00b6 pandas.Series. mean (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . mean () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 2.95 pd.Series.median \u00b6 pandas.Series. median (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . median () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 3.0 pd.Series.min \u00b6 pandas.Series. min (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . min () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 pd.Series.nlargest \u00b6 pandas.Series. nlargest (n=5, keep='first') Supported Arguments argument datatypes n Integer Note Series type must be numeric Example Usage >>> @bodo . jit ... def f ( S ): ... return S . nlargest ( 20 ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 20 6 27 6 41 6 34 6 55 6 13 6 83 6 90 6 6 6 69 6 48 6 76 6 62 6 97 6 19 5 5 5 26 5 61 5 12 5 68 5 dtype : int64 pd.Series.nsmallest \u00b6 pandas.Series. nsmallest (n=5, keep='first') Supported Arguments argument datatypes n Integer Note Series type must be numeric Example Usage >>> @bodo . jit ... def f ( S ): ... return S . nsmallest ( 20 ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 63 0 7 0 56 0 98 0 77 0 91 0 49 0 42 0 35 0 84 0 28 0 21 0 70 0 0 0 14 0 43 1 1 1 57 1 15 1 36 1 dtype : int64 pd.Series.pct_change \u00b6 pandas.Series. pct_change (periods=1, fill_method='pad', limit=None, freq=None) Supported Arguments argument datatypes periods Integer Note Series type must be numeric Bodo does not accept any additional arguments to pass to shift Example Usage >>> @bodo . jit ... def f ( S ): ... return S . pct_change ( 3 ) >>> S = ( pd . Series ( np . arange ( 100 )) % 7 ) + 1 >>> f ( S ) 0 NaN 1 NaN 2 NaN 3 3.000000 4 1.500000 ... 95 1.500000 96 1.000000 97 0.750000 98 - 0.800000 99 - 0.666667 Length : 100 , dtype : float64 pd.Series.prod \u00b6 pandas.Series. prod (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . prod () >>> S = ( pd . Series ( np . arange ( 20 )) % 3 ) + 1 >>> f ( S ) 93312 pd.Series.product \u00b6 pandas.Series. product (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . product () >>> S = ( pd . Series ( np . arange ( 20 )) % 3 ) + 1 >>> f ( S ) 93312 pd.Series.quantile \u00b6 pandas.Series. quantile (q=0.5, interpolation='linear') Supported Arguments argument datatypes q Float in [0.0, 1.0] Iterable of floats in [0.0, 1.0] Example Usage >>> @bodo . jit ... def f ( S ): ... return S . quantile ([ 0.25 , 0.5 , 0.75 ]) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0.25 1.0 0.50 3.0 0.75 5.0 dtype : float64 pd.Series.sem \u00b6 pandas.Series. sem (axis=None, skipna=None, level=None, ddof=1, numeric_only=None) Supported Arguments argument datatypes skipna Boolean ddof Integer Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . sem () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0.20219752318917852 pd.Series.skew \u00b6 pandas.Series. skew (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . skew () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0.032074996591991714 pd.Series.std \u00b6 pandas.Series. std (axis=None, skipna=None, level=None, ddof=1, numeric_only=None) Supported Arguments argument datatypes skipna Boolean ddof Integer Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . std () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 2.021975231891785 pd.Series.sum \u00b6 pandas.Series. sum (axis=None, skipna=None, level=None, numeric_only=None, min_count=0) Supported Arguments argument datatypes skipna Boolean min_count Integer Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . sum () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 295 pd.Series.var \u00b6 pandas.Series. var (axis=None, skipna=None, level=None, ddof=1, numeric_only=None) Supported Arguments argument datatypes skipna Boolean ddof Integer Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . var () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 4.088383838383838 pd.Series.kurtosis \u00b6 pandas.Series. kurtosis (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . kurtosis () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) - 1.269562153611973 pd.Series.unique \u00b6 pandas.Series. unique () Note The output is assumed to be \"small\" relative to input and is replicated. Use Series.drop_duplicates() if the output should remain distributed. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . unique () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) [ 0 1 2 3 4 5 6 ] pd.Series.nunique \u00b6 pandas.Series. nunique (dropna=True) Supported Arguments argument datatypes dropna Boolean Example Usage >>> @bodo . jit ... def f ( S ): ... return S . nunique () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 7 pd.Series.is_monotonic \u00b6 ++pandas.Series. is_monotonic Example Usage >>> @bodo . jit ... def f ( S ): ... return S . is_monotonic >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) True pd.Series.is_monotonic_increasing \u00b6 ++pandas.Series. is_monotonic_increasing Example Usage >>> @bodo . jit ... def f ( S ): ... return S . is_monotonic_increasing >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) True pd.Series.is_monotonic_decreasing \u00b6 ++pandas.Series. is_monotonic_decreasing Example Usage >>> @bodo . jit ... def f ( S ): ... return S . is_monotonic_decreasing >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) False pd.Series.value_counts \u00b6 pandas.Series. value_counts (normalize=False, sort=True, ascending=False, bins=None, dropna=True) Supported Arguments argument datatypes other requirements normalize Boolean Must be constant at Compile Time sort Boolean Must be constant at Compile Time ascending Boolean bins Integer Array-like of integers Example Usage >>> @bodo . jit ... def f ( S ): ... return S . value_counts () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 15 1 15 2 14 3 14 4 14 5 14 6 14 dtype : int64 Reindexing / Selection / Label manipulation \u00b6 pd.Series.drop_duplicates \u00b6 pandas.Series. drop_duplicates (keep='first', inplace=False) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . drop_duplicates () >>> S = pd . Series ( np . arange ( 100 )) % 10 >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 dtype : int64 pd.Series.duplicated \u00b6 pandas.Series. duplicated (keep='first') Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . duplicated () > >>> S = pd . Series ([ 1 , 2 , 1 , np . nan , 3 , 2 , np . nan , 4 ]) 0 False 1 False 2 True 3 False 4 False 5 True 6 True 7 False dtype : bool pd.Series.equals \u00b6 pandas.Series. equals (other) Supported Arguments argument datatypes other Series Note Series and other must contain scalar values in each row Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . equals ( other ) >>> S = pd . Series ( np . arange ( 100 )) % 10 >>> other = pd . Series ( np . arange ( 100 )) % 5 >>> f ( S , other ) False pd.Series.first \u00b6 pandas.Series. first (offset) Supported Arguments argument datatypes other requirements offset String or Offset type String argument be a valid frequency alias Note Series must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted. Example Usage >>> @bodo . jit ... def f ( S , offset ): ... return S . first ( offset ) >>> S = pd . Series ( np . arange ( 100 ), index = pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 100 )) >>> f ( S , \"2M\" ) 2022 - 01 - 01 00 : 00 : 00.000000000 0 2022 - 01 - 12 01 : 27 : 16.363636363 1 2022 - 01 - 23 02 : 54 : 32.727272727 2 2022 - 02 - 03 04 : 21 : 49.090909091 3 2022 - 02 - 14 05 : 49 : 05.454545454 4 2022 - 02 - 25 07 : 16 : 21.818181818 5 dtype : int64 pd.Series.head \u00b6 pandas.Series. head (n=5) Supported Arguments argument datatypes n Integer Example Usage >>> @bodo . jit ... def f ( S ): ... return S . head ( 10 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 dtype : int64 pd.Series.idxmax \u00b6 pandas.Series. idxmax (axis=0, skipna=True) Supported Arguments None Note Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . idxmax () >>> S = pd . Series ( np . arange ( 100 )) >>> S [( S % 3 == 0 )] = 100 >>> f ( S ) 0 pd.Series.idxmin \u00b6 pandas.Series. idxmin (axis=0, skipna=True) Supported Arguments None Note Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . idxmin () >>> S = pd . Series ( np . arange ( 100 )) >>> S [( S % 3 == 0 )] = 100 >>> f ( S ) 1 pd.Series.isin \u00b6 pandas.Series. isin (values) Supported Arguments argument datatypes values Series Array List Note values argument supports both distributed array/Series and replicated list/array/Series Example Usage >>> @bodo . jit ... def f ( S ): ... return S . isin ([ 3 , 11 , 98 ]) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 False 1 False 2 False 3 True 4 False ... 95 False 96 False 97 False 98 True 99 False Length : 100 , dtype : bool pd.Series.last \u00b6 pandas.Series. last (offset) Supported Arguments argument datatypes other requirements offset String or Offset type String argument be a valid frequency alias Note Series must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted. Example Usage >>> @bodo . jit ... def f ( S , offset ): ... return S . last ( offset ) >>> S = pd . Series ( np . arange ( 100 ), index = pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 100 )) >>> f ( S , \"2M\" ) 2024 - 11 - 05 16 : 43 : 38.181818176 94 2024 - 11 - 16 18 : 10 : 54.545454544 95 2024 - 11 - 27 19 : 38 : 10.909090912 96 2024 - 12 - 08 21 : 05 : 27.272727264 97 2024 - 12 - 19 22 : 32 : 43.636363632 98 2024 - 12 - 31 00 : 00 : 00.000000000 99 dtype : int64 pd.Series.rename \u00b6 pandas.Series. rename (index=None, , axis=None, copy=True, inplace=False, level=None, errors='ignore') ***Supported Arguments** argument datatypes index String axis Any value. Bodo ignores this argument entirely, which is consistent with Pandas. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . rename ( \"a\" ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 .. 95 95 96 96 97 97 98 98 99 99 Name : a , Length : 100 , dtype : int64 pd.Series.reset_index \u00b6 pandas.Series. reset_index (level=None, drop=False, name=None, inplace=False) Supported Arguments argument datatypes other requirements level Integer Boolean Must be constant at Compile Time drop Boolean Must be constant at Compile Time If False , Index name must be known at compilation time Note For MultiIndex case, only dropping all levels is supported. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . reset_index () >>> S = pd . Series ( np . arange ( 100 ), index = pd . RangeIndex ( 100 , 200 , 1 , name = \"b\" )) >>> f ( S ) b 0 0 100 0 1 101 1 2 102 2 3 103 3 4 104 4 .. ... .. 95 195 95 96 196 96 97 197 97 98 198 98 99 199 99 > [ 100 rows x 2 columns ] pd.Series.take \u00b6 pandas.Series. take (indices, axis=0, is_copy=None) Supported Arguments argument datatypes other requirements indices Array like with integer data To have distributed data indices must be an array with the same distribution as S. Note Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . take ([ 2 , 7 , 4 , 19 ]) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 2 2 7 7 4 4 19 19 dtype : int64 pd.Series.tail \u00b6 pandas.Series. tail (n=5) Supported Arguments argument datatypes n Integer Example Usage >>> @bodo . jit ... def f ( S ): ... return S . tail ( 10 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 90 90 91 91 92 92 93 93 94 94 95 95 96 96 97 97 98 98 99 99 dtype : int64 pd.Series.where \u00b6 pandas.Series. where (cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=NoDefault.no_default) Supported Arguments argument datatypes cond boolean array 1d bool numpy array other 1d numpy array scalar Note Series can contain categorical data if other is a scalar Example Usage >>> @bodo . jit ... def f ( S ): ... return S . where (( S % 3 ) != 0 , 0 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0 1 1 2 2 3 0 4 4 .. 95 95 96 0 97 97 98 98 99 0 Length : 100 , dtype : int64 pd.Series.mask \u00b6 pandas.Series. mask (cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=NoDefault.no_default) Supported Arguments argument datatypes cond boolean array 1d bool numpy array other 1d numpy array scalar Note Series can contain categorical data if other is a scalar Example Usage >>> @bodo . jit ... def f ( S ): ... return S . mask (( S % 3 ) != 0 , 0 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0 1 0 2 0 3 3 4 0 .. 95 0 96 96 97 0 98 0 99 99 Length : 100 , dtype : int64 Missing data handling \u00b6 pd.Series.backfill \u00b6 pandas.Series. backfill (axis=None, inplace=False, limit=None, downcast=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . backfill () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 1 1 1 2 - 2 3 - 2 4 5 5 5 6 < NA > dtype : Int64 pd.Series.bfill \u00b6 pandas.Series. bfill (axis=None, inplace=False, limit=None, downcast=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . bfill () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 1 1 1 2 - 2 3 - 2 4 5 5 5 6 < NA > dtype : Int64 pd.Series.dropna \u00b6 pandas.Series. dropna (axis=0, inplace=False, how=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dropna () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 1 1 3 - 2 5 5 dtype : Int64 pd.Series.ffill \u00b6 pandas.Series. ffill (axis=None, inplace=False, limit=None, downcast=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . ffill () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 < NA > 1 1 2 1 3 - 2 4 - 2 5 5 6 5 dtype : Int64 pd.Series.fillna \u00b6 pandas.Series. fillna (value=None, method=None, axis=None, inplace=False, limit=None, downcast=None) Supported Arguments argument datatypes other requirements value Scalar method One of (\"bfill\", \"backfill\", \"ffill\", and \"pad\") Must be constant at Compile Time inplace Boolean Must be constant at Compile Time If value is provided then method must be None and vice-versa If method is provided then inplace must be False Example Usage >>> @bodo . jit ... def f ( S ): ... return S . fillna ( - 1 ) >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 - 1 1 1 2 - 1 3 - 2 4 - 1 5 5 6 - 1 dtype : Int64 pd.Series.isna \u00b6 pandas.Series. isna () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . isna () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 True 1 False 2 True 3 False 4 True 5 False 6 True dtype : bool pd.Series.isnull \u00b6 pandas.Series. isnull () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . isnull () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 True 1 False 2 True 3 False 4 True 5 False 6 True dtype : bool pd.Series.notna \u00b6 pandas.Series. notna () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . notna () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 False 1 True 2 False 3 True 4 False 5 True 6 False dtype : bool pd.Series.notnull \u00b6 pandas.Series. notnull () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . notnull () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 False 1 True 2 False 3 True 4 False 5 True 6 False dtype : bool pd.Series.pad \u00b6 pandas.Series. pad (axis=None, inplace=False, limit=None, downcast=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . pad () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 < NA > 1 1 2 1 3 - 2 4 - 2 5 5 6 5 dtype : Int64 pd.Series.replace \u00b6 pandas.Series. replace (to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad') Supported Arguments argument datatypes other requirements to_replace Scalar List of Scalars Dictionary mapping scalars of the same type value Scalar If to_replace is not a scalar, value must be None Example Usage >>> @bodo . jit ... def f ( S , replace_dict ): ... return S . replace ( replace_dict ) >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S , { 1 : - 2 , - 2 : 5 , 5 : 27 }) 0 < NA > 1 - 2 2 < NA > 3 5 4 < NA > 5 27 6 < NA > dtype : Int64 Reshaping, sorting \u00b6 pd.Series.argsort \u00b6 pandas.Series. argsort (axis=0, kind='quicksort', order=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . sort_values () >>> S = pd . Series ( np . arange ( 99 , - 1 , - 1 ), index = np . arange ( 100 )) >>> f ( S ) 0 99 1 98 2 97 3 96 4 95 .. 95 4 96 3 97 2 98 1 99 0 Length : 100 , dtype : int64 pd.Series.sort_values \u00b6 pandas.Series. sort_values (axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None) Supported Arguments argument datatypes other requirements ascending Boolean na_position One of (\"first\", \"last\") Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( S ): ... return S . sort_values () >>> S = pd . Series ( np . arange ( 99 , - 1 , - 1 ), index = np . arange ( 100 )) >>> f ( S ) 99 0 98 1 97 2 96 3 95 4 .. 4 95 3 96 2 97 1 98 0 99 Length : 100 , dtype : int64 pd.Series.sort_index \u00b6 pandas.Series. sort_index (axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None) Supported Arguments argument datatypes other requirements ascending Boolean na_position One of (\"first\", \"last\") Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( S ): ... return S . sort_index () >>> S = pd . Series ( np . arange ( 100 ), index = np . arange ( 99 , - 1 , - 1 )) >>> f ( S ) 0 99 1 98 2 97 3 96 4 95 .. 95 4 96 3 97 2 98 1 99 0 Length : 100 , dtype : int64 pd.Series.explode \u00b6 pandas.Series. explode (ignore_index=False) Supported Arguments None Note Bodo's output type may differ from Pandas because Bodo must convert to a nullable type at compile time. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . explode () >>> S = pd . Series ([ np . arange ( i ) for i in range ( 10 )]) >>> f ( S ) 0 < NA > 1 0 2 0 2 1 3 0 3 1 3 2 4 0 4 1 4 2 4 3 5 0 5 1 5 2 5 3 5 4 6 0 6 1 6 2 6 3 6 4 6 5 7 0 7 1 7 2 7 3 7 4 7 5 7 6 8 0 8 1 8 2 8 3 8 4 8 5 8 6 8 7 9 0 9 1 9 2 9 3 9 4 9 5 9 6 9 7 9 8 dtype : Int64 pd.Series.repeat \u00b6 pandas.Series. repeat (repeats, axis=None) Supported Arguments argument datatypes repeats Integer Array-like of integers the same length as the Series Example Usage >>> @bodo . jit ... def f ( S ): ... return S . repeat ( 3 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0 0 0 0 0 1 1 1 1 .. 98 98 98 98 99 99 99 99 99 99 Length : 300 , dtype : int64 Combining / comparing / joining / merging \u00b6 pd.Series.append \u00b6 pandas.Series. append (to_append, ignore_index=False, verify_integrity=False) Supported Arguments argument datatypes other requirements to_append Series List of Series Tuple of Series ignore_index Boolean Must be constant at Compile Time Note Setting a name for the output Series is not supported yet Important Bodo currently concatenates local data chunks for distributed datasets, which does not preserve global order of concatenated objects in output. Example Usage >>> @bodo . jit ... def f ( S1 , S2 ): ... return S1 . append ( S2 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S , S ) 0 0 1 1 2 2 3 3 4 4 .. 95 95 96 96 97 97 98 98 99 99 Length : 200 , dtype : int64 Time series-related \u00b6 pd.Series.shift \u00b6 pandas.Series. shift (periods=1, freq=None, axis=0, fill_value=None) Supported Arguments argument datatypes periods Integer Note This data type for the series must be one of: - Integer - Float - Boolean - datetime.data - datetime64 - timedelta64 - string Example Usage >>> @bodo . jit ... def f ( S ): ... return S . shift ( 1 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 NaN 1 0.0 2 1.0 3 2.0 4 3.0 ... 95 94.0 96 95.0 97 96.0 98 97.0 99 98.0 Length : 100 , dtype : float64 Datetime properties \u00b6 `pd.Series.dt.date \u00b6 ++pandas.Series.dt. date Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . date >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 2022 - 01 - 01 1 2022 - 01 - 01 2 2022 - 01 - 01 3 2022 - 01 - 01 4 2022 - 01 - 02 5 2022 - 01 - 02 6 2022 - 01 - 02 7 2022 - 01 - 03 8 2022 - 01 - 03 9 2022 - 01 - 03 10 2022 - 01 - 04 11 2022 - 01 - 04 12 2022 - 01 - 04 13 2022 - 01 - 05 14 2022 - 01 - 05 15 2022 - 01 - 05 16 2022 - 01 - 05 17 2022 - 01 - 06 18 2022 - 01 - 06 19 2022 - 01 - 06 20 2022 - 01 - 07 21 2022 - 01 - 07 22 2022 - 01 - 07 23 2022 - 01 - 08 24 2022 - 01 - 08 25 2022 - 01 - 08 26 2022 - 01 - 09 27 2022 - 01 - 09 28 2022 - 01 - 09 29 2022 - 01 - 10 dtype : object pd.Series.dt.year \u00b6 pandas.Series.dt. year Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . year >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 2022 1 2022 2 2022 3 2022 4 2022 5 2022 6 2022 7 2022 8 2022 9 2022 10 2023 11 2023 12 2023 13 2023 14 2023 15 2023 16 2023 17 2023 18 2023 19 2023 20 2024 21 2024 22 2024 23 2024 24 2024 25 2024 26 2024 27 2024 28 2024 29 2025 dtype : Int64 pd.Series.dt.month \u00b6 pandas.Series.dt. month Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . month >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 1 1 2 2 3 3 4 4 6 5 7 6 8 7 9 8 11 9 12 10 1 11 2 12 4 13 5 14 6 15 7 16 9 17 10 18 11 19 12 20 2 21 3 22 4 23 5 24 7 25 8 26 9 27 10 28 12 29 1 dtype : Int64 pd.Series.dt.day \u00b6 pandas.Series.dt. day Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . day >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 1 1 8 2 18 3 25 4 2 5 10 6 17 7 24 8 1 9 9 10 17 11 24 12 3 13 11 14 18 15 26 16 2 17 10 18 17 19 25 20 2 21 11 22 18 23 26 24 3 25 10 26 17 27 25 28 2 29 10 dtype : Int64 pd.Series.dt.hour \u00b6 pandas.Series.dt. hour Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . hour >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 0 1 2 2 4 3 7 4 9 5 12 6 14 7 17 8 19 9 22 10 0 11 3 12 5 13 8 14 10 15 13 16 15 17 18 18 20 19 23 20 1 21 4 22 6 23 9 24 11 25 14 26 16 27 19 28 21 29 0 dtype : Int64 pd.Series.dt.minute \u00b6 pandas.Series.dt. minute Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . minute >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 0 1 28 2 57 3 26 4 55 5 24 6 53 7 22 8 51 9 20 10 49 11 18 12 47 13 16 14 45 15 14 16 43 17 12 18 41 19 10 20 39 21 8 22 37 23 6 24 35 25 4 26 33 27 2 28 31 29 0 dtype : Int64 pd.Series.dt.second \u00b6 pandas.Series.dt. second Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . second >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 0 1 57 2 55 3 53 4 51 5 49 6 47 7 45 8 43 9 41 10 39 11 37 12 35 13 33 14 31 15 28 16 26 17 24 18 22 19 20 20 18 21 16 22 14 23 12 24 10 25 8 26 6 27 4 28 2 29 0 dtype : Int64 pd.Series.dt.microsecond \u00b6 pandas.Series.dt. microsecond Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . microsecond >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 0 1 931034 2 862068 3 793103 4 724137 5 655172 6 586206 7 517241 8 448275 9 379310 10 310344 11 241379 12 172413 13 103448 14 34482 15 965517 16 896551 17 827586 18 758620 19 689655 20 620689 21 551724 22 482758 23 413793 24 344827 25 275862 26 206896 27 137931 28 68965 29 0 dtype : Int64 pd.Series.dt.nanosecond \u00b6 pandas.Series.dt. nanosecond Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . nanosecond >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 0 1 483 2 966 3 448 4 932 5 416 6 896 7 380 8 864 9 348 10 832 11 312 12 792 13 280 14 760 15 248 16 728 17 208 18 696 19 176 20 664 21 144 22 624 23 104 24 584 25 80 26 560 27 40 28 520 29 0 dtype : Int64 pd.Series.dt.week \u00b6 pandas.Series.dt. week Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . week >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 52 1 6 2 11 3 17 4 22 5 27 6 33 7 38 8 44 9 49 10 3 11 8 12 14 13 19 14 24 15 30 16 35 17 41 18 46 19 52 20 5 21 11 22 16 23 21 24 27 25 32 26 38 27 43 28 49 29 2 dtype : Int64 pd.Series.dt.weekofyear \u00b6 pandas.Series.dt. weekofyear Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . weekofyear >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 52 1 6 2 11 3 17 4 22 5 27 6 33 7 38 8 44 9 49 10 3 11 8 12 14 13 19 14 24 15 30 16 35 17 41 18 46 19 52 20 5 21 11 22 16 23 21 24 27 25 32 26 38 27 43 28 49 29 2 dtype : Int64 pd.Series.dt.day_of_week \u00b6 pandas.Series.dt. day_of_week Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . day_of_week >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 5 1 1 2 4 3 0 4 3 5 6 6 2 7 5 8 1 9 4 10 1 11 4 12 0 13 3 14 6 15 2 16 5 17 1 18 4 19 0 20 4 21 0 22 3 23 6 24 2 25 5 26 1 27 4 28 0 29 4 dtype : Int64 pd.Series.dt.weekday \u00b6 pandas.Series.dt. weekday Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . weekday >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 5 1 1 2 4 3 0 4 3 5 6 6 2 7 5 8 1 9 4 10 1 11 4 12 0 13 3 14 6 15 2 16 5 17 1 18 4 19 0 20 4 21 0 22 3 23 6 24 2 25 5 26 1 27 4 28 0 29 4 dtype : Int64 pd.Series.dt.dayofyear \u00b6 pandas.Series.dt. dayofyear Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . dayofyear >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 1 1 39 2 77 3 115 4 153 5 191 6 229 7 267 8 305 9 343 10 17 11 55 12 93 13 131 14 169 15 207 16 245 17 283 18 321 19 359 20 33 21 71 22 109 23 147 24 185 25 223 26 261 27 299 28 337 29 10 dtype : Int64 pd.Series.dt.day_of_year \u00b6 pandas.Series.dt. day_of_year Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . day_of_year >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 1 1 39 2 77 3 115 4 153 5 191 6 229 7 267 8 305 9 343 10 17 11 55 12 93 13 131 14 169 15 207 16 245 17 283 18 321 19 359 20 33 21 71 22 109 23 147 24 185 25 223 26 261 27 299 28 337 29 10 dtype : Int64 pd.Series.dt.quarter \u00b6 pandas.Series.dt. quarter Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . quarter >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 1 1 1 2 1 3 2 4 2 5 3 6 3 7 3 8 4 9 4 10 1 11 1 12 2 13 2 14 2 15 3 16 3 17 4 18 4 19 4 20 1 21 1 22 2 23 2 24 3 25 3 26 3 27 4 28 4 29 1 dtype : Int64 pd.Series.dt.is_month_start \u00b6 pandas.Series.dt. is_month_start Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_month_start >>> SS = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 True 1 False 2 False 3 False 4 True 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 True 26 False 27 False 28 False 29 False dtype : bool pd.Series.dt.is_month_end \u00b6 pandas.Series.dt. is_month_end Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_month_end >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 False 1 False 2 False 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 True dtype : bool pd.Series.dt.is_quarter_start \u00b6 pandas.Series.dt. is_quarter_start Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_quarter_start >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 True 1 False 2 False 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 False dtype : bool pd.Series.dt.is_quarter_end \u00b6 pandas.Series.dt. is_quarter_end Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_quarter_end >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 False 1 False 2 False 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 True dtype : bool pd.Series.dt.is_year_start \u00b6 pandas.Series.dt. is_year_start Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_year_start >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 True 1 False 2 False 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 False dtype : bool pd.Series.dt.is_year_end \u00b6 pandas.Series.dt. is_year_end Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_year_end >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 False 1 False 2 False 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 True dtype : bool pd.Series.dt.daysinmonth \u00b6 pandas.Series.dt. daysinmonth Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . daysinmonth >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 31 1 28 2 31 3 30 4 30 5 31 6 31 7 30 8 31 9 31 10 31 11 28 12 31 13 31 14 30 15 31 16 31 17 31 18 30 19 31 20 31 21 31 22 30 23 31 24 30 25 31 26 30 27 31 28 30 29 31 dtype : Int64 pd.Series.dt.days_in_month \u00b6 pandas.Series.dt. days_in_month Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . days_in_month >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 31 1 28 2 31 3 30 4 30 5 31 6 31 7 30 8 31 9 31 10 31 11 28 12 31 13 31 14 30 15 31 16 31 17 31 18 30 19 31 20 31 21 31 22 30 23 31 24 30 25 31 26 30 27 31 28 30 29 31 dtype : Int64 Datetime methods \u00b6 pd.Series.dt.normalize \u00b6 pandas.Series.dt. normalize () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . normalize () >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 2022 - 01 - 01 1 2022 - 01 - 01 2 2022 - 01 - 01 3 2022 - 01 - 01 4 2022 - 01 - 02 5 2022 - 01 - 02 6 2022 - 01 - 02 7 2022 - 01 - 03 8 2022 - 01 - 03 9 2022 - 01 - 03 10 2022 - 01 - 04 11 2022 - 01 - 04 12 2022 - 01 - 04 13 2022 - 01 - 05 14 2022 - 01 - 05 15 2022 - 01 - 05 16 2022 - 01 - 05 17 2022 - 01 - 06 18 2022 - 01 - 06 19 2022 - 01 - 06 20 2022 - 01 - 07 21 2022 - 01 - 07 22 2022 - 01 - 07 23 2022 - 01 - 08 24 2022 - 01 - 08 25 2022 - 01 - 08 26 2022 - 01 - 09 27 2022 - 01 - 09 28 2022 - 01 - 09 29 2022 - 01 - 10 dtype : datetime64 [ ns ] pd.Series.dt.strftime \u00b6 pandas.Series.dt. strftime (date_format) Supported Arguments argument datatypes other requirements date_format String Must be a valid datetime format string Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . strftime ( \"%B %d , %Y, %r \" ) >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 January 01 , 2022 , 12 : 00 : 00 AM 1 January 01 , 2022 , 07 : 26 : 53 AM 2 January 01 , 2022 , 02 : 53 : 47 PM 3 January 01 , 2022 , 10 : 20 : 41 PM 4 January 02 , 2022 , 05 : 47 : 35 AM 5 January 02 , 2022 , 01 : 14 : 28 PM 6 January 02 , 2022 , 08 : 41 : 22 PM 7 January 03 , 2022 , 04 : 08 : 16 AM 8 January 03 , 2022 , 11 : 35 : 10 AM 9 January 03 , 2022 , 07 : 02 : 04 PM 10 January 04 , 2022 , 02 : 28 : 57 AM 11 January 04 , 2022 , 09 : 55 : 51 AM 12 January 04 , 2022 , 05 : 22 : 45 PM 13 January 05 , 2022 , 12 : 49 : 39 AM 14 January 05 , 2022 , 08 : 16 : 33 AM 15 January 05 , 2022 , 03 : 43 : 26 PM 16 January 05 , 2022 , 11 : 10 : 20 PM 17 January 06 , 2022 , 06 : 37 : 14 AM 18 January 06 , 2022 , 02 : 04 : 08 PM 19 January 06 , 2022 , 09 : 31 : 02 PM 20 January 07 , 2022 , 04 : 57 : 55 AM 21 January 07 , 2022 , 12 : 24 : 49 PM 22 January 07 , 2022 , 07 : 51 : 43 PM 23 January 08 , 2022 , 03 : 18 : 37 AM 24 January 08 , 2022 , 10 : 45 : 31 AM 25 January 08 , 2022 , 06 : 12 : 24 PM 26 January 09 , 2022 , 01 : 39 : 18 AM 27 January 09 , 2022 , 09 : 06 : 12 AM 28 January 09 , 2022 , 04 : 33 : 06 PM 29 January 10 , 2022 , 12 : 00 : 00 AM dtype : object pd.Series.dt.round \u00b6 pandas.Series.dt. round (freq, ambiguous='raise', nonexistent='raise') Supported Arguments argument datatypes other requirements freq String Must be a valid fixed frequency alias Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . round ( \"H\" ) >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 2022 - 01 - 01 00 : 00 : 00 1 2022 - 01 - 01 07 : 00 : 00 2 2022 - 01 - 01 15 : 00 : 00 3 2022 - 01 - 01 22 : 00 : 00 4 2022 - 01 - 02 06 : 00 : 00 5 2022 - 01 - 02 13 : 00 : 00 6 2022 - 01 - 02 21 : 00 : 00 7 2022 - 01 - 03 04 : 00 : 00 8 2022 - 01 - 03 12 : 00 : 00 9 2022 - 01 - 03 19 : 00 : 00 10 2022 - 01 - 04 02 : 00 : 00 11 2022 - 01 - 04 10 : 00 : 00 12 2022 - 01 - 04 17 : 00 : 00 13 2022 - 01 - 05 01 : 00 : 00 14 2022 - 01 - 05 08 : 00 : 00 15 2022 - 01 - 05 16 : 00 : 00 16 2022 - 01 - 05 23 : 00 : 00 17 2022 - 01 - 06 07 : 00 : 00 18 2022 - 01 - 06 14 : 00 : 00 19 2022 - 01 - 06 22 : 00 : 00 20 2022 - 01 - 07 05 : 00 : 00 21 2022 - 01 - 07 12 : 00 : 00 22 2022 - 01 - 07 20 : 00 : 00 23 2022 - 01 - 08 03 : 00 : 00 24 2022 - 01 - 08 11 : 00 : 00 25 2022 - 01 - 08 18 : 00 : 00 26 2022 - 01 - 09 02 : 00 : 00 27 2022 - 01 - 09 09 : 00 : 00 28 2022 - 01 - 09 17 : 00 : 00 29 2022 - 01 - 10 00 : 00 : 00 dtype : datetime64 [ ns ] pd.Series.dt.floor \u00b6 pandas.Series.dt. floor (freq, ambiguous='raise', nonexistent='raise') Supported Arguments argument datatypes other requirements freq String Must be a valid fixed frequency alias Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . floor ( \"H\" ) >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 2022 - 01 - 01 00 : 00 : 00 1 2022 - 01 - 01 07 : 00 : 00 2 2022 - 01 - 01 14 : 00 : 00 3 2022 - 01 - 01 22 : 00 : 00 4 2022 - 01 - 02 05 : 00 : 00 5 2022 - 01 - 02 13 : 00 : 00 6 2022 - 01 - 02 20 : 00 : 00 7 2022 - 01 - 03 04 : 00 : 00 8 2022 - 01 - 03 11 : 00 : 00 9 2022 - 01 - 03 19 : 00 : 00 10 2022 - 01 - 04 02 : 00 : 00 11 2022 - 01 - 04 09 : 00 : 00 12 2022 - 01 - 04 17 : 00 : 00 13 2022 - 01 - 05 00 : 00 : 00 14 2022 - 01 - 05 08 : 00 : 00 15 2022 - 01 - 05 15 : 00 : 00 16 2022 - 01 - 05 23 : 00 : 00 17 2022 - 01 - 06 06 : 00 : 00 18 2022 - 01 - 06 14 : 00 : 00 19 2022 - 01 - 06 21 : 00 : 00 20 2022 - 01 - 07 04 : 00 : 00 21 2022 - 01 - 07 12 : 00 : 00 22 2022 - 01 - 07 19 : 00 : 00 23 2022 - 01 - 08 03 : 00 : 00 24 2022 - 01 - 08 10 : 00 : 00 25 2022 - 01 - 08 18 : 00 : 00 26 2022 - 01 - 09 01 : 00 : 00 27 2022 - 01 - 09 09 : 00 : 00 28 2022 - 01 - 09 16 : 00 : 00 29 2022 - 01 - 10 00 : 00 : 00 dtype : datetime64 [ ns ] pd.Series.dt.ceil \u00b6 pandas.Series.dt. ceil (freq, ambiguous='raise', nonexistent='raise') Supported Arguments argument datatypes other requirements freq String Must be a valid fixed frequency alias Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . ceil ( \"H\" ) >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 2022 - 01 - 01 00 : 00 : 00 1 2022 - 01 - 01 08 : 00 : 00 2 2022 - 01 - 01 15 : 00 : 00 3 2022 - 01 - 01 23 : 00 : 00 4 2022 - 01 - 02 06 : 00 : 00 5 2022 - 01 - 02 14 : 00 : 00 6 2022 - 01 - 02 21 : 00 : 00 7 2022 - 01 - 03 05 : 00 : 00 8 2022 - 01 - 03 12 : 00 : 00 9 2022 - 01 - 03 20 : 00 : 00 10 2022 - 01 - 04 03 : 00 : 00 11 2022 - 01 - 04 10 : 00 : 00 12 2022 - 01 - 04 18 : 00 : 00 13 2022 - 01 - 05 01 : 00 : 00 14 2022 - 01 - 05 09 : 00 : 00 15 2022 - 01 - 05 16 : 00 : 00 16 2022 - 01 - 06 00 : 00 : 00 17 2022 - 01 - 06 07 : 00 : 00 18 2022 - 01 - 06 15 : 00 : 00 19 2022 - 01 - 06 22 : 00 : 00 20 2022 - 01 - 07 05 : 00 : 00 21 2022 - 01 - 07 13 : 00 : 00 22 2022 - 01 - 07 20 : 00 : 00 23 2022 - 01 - 08 04 : 00 : 00 24 2022 - 01 - 08 11 : 00 : 00 25 2022 - 01 - 08 19 : 00 : 00 26 2022 - 01 - 09 02 : 00 : 00 27 2022 - 01 - 09 10 : 00 : 00 28 2022 - 01 - 09 17 : 00 : 00 29 2022 - 01 - 10 00 : 00 : 00 dtype : datetime64 [ ns ] pd.Series.dt.month_name \u00b6 pandas.Series.dt. month_name (locale=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . month_name () >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 January 1 February 2 March 3 April 4 June 5 July 6 August 7 September 8 November 9 December 10 January 11 February 12 April 13 May 14 June 15 July 16 September 17 October 18 November 19 December 20 February 21 March 22 April 23 May 24 July 25 August 26 September 27 October 28 December 29 January dtype : object pd.Series.dt.day_name \u00b6 pandas.Series.dt. day_name (locale=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . day_name () >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 Saturday 1 Saturday 2 Saturday 3 Saturday 4 Sunday 5 Sunday 6 Sunday 7 Monday 8 Monday 9 Monday 10 Tuesday 11 Tuesday 12 Tuesday 13 Wednesday 14 Wednesday 15 Wednesday 16 Wednesday 17 Thursday 18 Thursday 19 Thursday 20 Friday 21 Friday 22 Friday 23 Saturday 24 Saturday 25 Saturday 26 Sunday 27 Sunday 28 Sunday 29 Monday dtype : object String handling \u00b6 pd.Series.str.capitalize \u00b6 pandas.Series.str. capitalize () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . str . capitalize () >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 Ce 2 Erw 3 A3 4 @ 5 A n 6 ^ Ef dtype : object pd.Series.str.center \u00b6 pandas.Series.str. center (width, fillchar=' ') Supported Arguments argument datatypes width Integer fillchar String with a single character Example Usage >>> @bodo . jit ... def f ( S ): ... return S . str . center ( 4 ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 a 1 ce 2 Erw 3 a3 4 @ 5 a n 6 ^ Ef dtype : object pd.Series.str.contains \u00b6 pandas.Series.str. contains (pat, case=True, flags=0, na=None, regex=True) Supported Arguments argument datatypes other requirements pat String case Boolean Must be constant at Compile Time flags Integer regex Boolean Must be constant at Compile Time >>> @bodo . jit ... def f ( S ): ... return S . str . contains ( \"a.+\" ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 False 2 False 3 True 4 False 5 True 6 False dtype : boolean pd.Series.str.count \u00b6 pandas.Series.str. count (pat, flags=0) Supported Arguments argument datatypes pat String flags Integer >>> @bodo . jit ... def f ( S ): ... return S . str . count ( \"w\" ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 1 1 2 2 3 3 2 4 0 5 2 6 2 dtype : Int64 pd.Series.str.endswith \u00b6 pandas.Series.str. endswith (pat, na=None) Supported Arguments argument datatypes pat String >>> @bodo . jit ... def f ( S ): ... return S . str . endswith ( \"e\" ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 True 2 False 3 False 4 False 5 False 6 False dtype : boolean pd.Series.str.extract \u00b6 pandas.Series.str. extract (pat, flags=0, expand=True) Supported Arguments argument datatypes other requirements pat String Must be constant at Compile Time flags Integer Must be constant at Compile Time expand Boolean Must be constant at Compile Time >>> @bodo . jit ... def f ( S ): ... return S . str . extract ( \"(a|e)\" ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 0 a 1 e 2 NaN 3 a 4 NaN 5 a 6 NaN pd.Series.str.extractall \u00b6 pandas.Series.str. extractall (pat, flags=0) Supported Arguments argument datatypes other requirements pat String Must be constant at Compile Time flags Integer Must be constant at Compile Time >>> @bodo . jit ... def f ( S ): ... return S . str . extractall ( \"(a|n)\" ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 match 0 0 a 3 0 a 5 0 a 1 n pd.Series.str.find \u00b6 pandas.Series.str. find (sub, start=0, end=None) Supported Arguments argument datatypes sub String start Integer end Integer >>> @bodo . jit ... def f ( S ): ... return S . str . find ( \"a3\" , start = 1 ) >>> S = pd . Series ([ \"Aa3\" , \"cea3\" , \"14a3\" , \" a3\" , \"a3@\" , \"a n3\" , \"^ Ea3f\" ]) >>> f ( S ) 0 1 1 2 2 2 3 1 4 - 1 5 - 1 6 3 dtype : Int64 pd.Series.str.get \u00b6 pandas.Series.str. get (i) Supported Arguments argument datatypes i Integer >>> @bodo . jit ... def f ( S ): ... return S . str . get ( 1 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 NaN 1 e 2 4 3 NaN 4 NaN 5 6 dtype : object pd.Series.str.join \u00b6 pandas.Series.str. join (sep) Supported Arguments argument datatypes sep String >>> @bodo . jit ... def f ( S ): ... return S . str . join ( \",\" ) >>> S = pd . Series ([[ \"a\" , \"fe\" , \"@23\" ], [ \"a\" , \"b\" ], [], [ \"c\" ]]) >>> f ( S ) 0 a , fe , @ 23 1 a , b 2 3 c dtype : object pd.Series.str.len \u00b6 pandas.Series.str. len () >>> @bodo . jit ... def f ( S ): ... return S . str . len () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 1 1 2 2 2 3 1 4 1 5 3 6 4 dtype : Int64 pd.Series.str.ljust \u00b6 pandas.Series.str. ljust (width, fillchar=' ') Supported Arguments argument datatypes width Integer fillchar String with a single character >>> @bodo . jit ... def f ( S ): ... return S . str . ljust ( 5 , fillchar = \",\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A ,,,, 1 ce ,,, 2 14 ,,, 3 ,,,, 4 @ ,,,, 5 a n ,, 6 ^ Ef , dtype : object pd.Series.str.lower \u00b6 pandas.Series.str. lower () >>> @bodo . jit ... def f ( S ): ... return S . str . lower () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 a 1 ce 2 14 3 4 @ 5 a n 6 ^ Ef dtype : object pd.Series.str.lstrip \u00b6 pandas.Series.str. lstrip (to_strip=None) Supported Arguments argument datatypes to_strip String >>> @bodo . jit ... def f ( S ): ... return S . str . lstrip ( \"c\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 e 2 14 3 4 @ 5 a n 6 ^ Ef dtype : object pd.Series.str.pad \u00b6 pandas.Series.str. pad (width, side='left', fillchar=' ') Supported Arguments argument datatypes other requirements width Integer width One of (\"left\", \"right\", \"both\") Must be constant at Compile Time fillchar String with a single character >>> @bodo . jit ... def f ( S ): ... return S . str . pad ( 5 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 ce 2 14 3 4 @ 5 a n 6 ^ Ef dtype : object pd.Series.str.repeat \u00b6 pandas.Series.str. repeat (repeats) Supported Arguments argument datatypes other requirements repeats Integer Array Like containing integers If repeats is array like, then it must be the same length as the Series. >>> @bodo . jit ... def f ( S ): ... return S . str . repeat ( 2 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 AA 1 cece 2 1414 3 4 @@ 5 a na n 6 ^ Ef ^ Ef dtype : object pd.Series.str.replace \u00b6 pandas.Series.str. replace (pat, repl, n=- 1, case=None, flags=0, regex=None) Supported Arguments regex >>> @bodo . jit ... def f ( S ): ... return S . str . replace ( \"(a|e)\" , \"yellow\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 cyellow 2 14 3 4 @ 5 yellow n 6 ^ Ef dtype : object pd.Series.str.rfind \u00b6 pandas.Series.str. rfind (sub, start=0, end=None) Supported Arguments argument datatypes sub String start Integer end Integer >>> @bodo . jit ... def f ( S ): ... return S . str . rfind ( \"a3\" , start = 1 ) >>> S = pd . Series ([ \"Aa3\" , \"cea3\" , \"14a3\" , \" a3\" , \"a3@\" , \"a n3\" , \"^ Ea3f\" ]) >>> f ( S ) 0 1 1 2 2 2 3 1 4 - 1 5 - 1 6 3 dtype : Int64 pd.Series.str.rjist \u00b6 pandas.Series.str. rjust (width, fillchar=' ') Supported arguments`: argument datatypes width Integer fillchar String with a single character >>> @bodo . jit ... def f ( S ): ... return S . str . rjust ( 10 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 ce 2 14 3 4 @ 5 a n 6 ^ Ef dtype : object pd.Series.str.restrip \u00b6 pandas.Series.str. rstrip (to_strip=None) Supported Arguments argument datatypes to_strip String >>> @bodo . jit ... def f ( S ): ... return S . str . rstrip ( \"n\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 ce 2 14 3 4 @ 5 a 6 ^ Ef dtype : object pd.Series.str.slice \u00b6 pandas.Series.str. slice (start=None, stop=None, step=None) Supported Arguments argument datatypes start Integer stop Integer step Integer >>> @bodo . jit ... def f ( S ): ... return S . str . slice ( 1 , 4 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 c 2 1 3 4 @ 5 a 6 # dtype : object pd.Series.str.slice_replace \u00b6 pandas.Series.str. slice_replace (start=None, stop=None, repl=None) Supported Arguments argument datatypes start Integer stop Integer repl String >>> @bodo . jit ... def f ( S ): ... return S . str . slice_replace ( 1 , 4 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 c 2 1 3 4 @ 5 a 6 # dtype : object pd.Series.str.split \u00b6 pandas.Series.str. split (pat=None, n=-1, expand=False) Supported Arguments argument datatypes pat String n Integer >>> @bodo . jit ... def f ( S ): ... return S . str . split ( \" \" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 [ A ] 1 [ ce ] 2 [ 14 ] 3 [, ] 4 [ @ ] 5 [ a , n ] 6 [ #, Ef] dtype : object pd.Series.str.startswith \u00b6 pandas.Series.str. startswith (pat, na=None) Supported Arguments argument datatypes pat String >>> @bodo . jit ... def f ( S ): ... return S . str . startswith ( \"A\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 True 1 False 2 False 3 False 4 False 5 False 6 False dtype : boolean pd.Series.str.strip \u00b6 pandas.Series.str. strip (to_strip=None) Supported Arguments argument datatypes to_strip String >>> @bodo . jit ... def f ( S ): ... return S . str . strip ( \"n\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 ce 2 14 3 4 @ 5 a 6 ^ Ef dtype : object pd.Series.str.swapcase \u00b6 pandas.Series.str. swapcase () >>> @bodo . jit ... def f ( S ): ... return S . str . swapcase () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 a 1 CE 2 14 3 4 @ 5 A N 6 ^ Ef dtype : object pd.Series.str.title \u00b6 pandas.Series.str. title () >>> @bodo . jit ... def f ( S ): ... return S . str . title () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 Ce 2 14 3 4 @ 5 A N 6 ^ Ef dtype : object pd.Series.str.upper \u00b6 pandas.Series.str. upper () >>> @bodo . jit ... def f ( S ): ... return S . str . upper () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 CE 2 14 3 4 @ 5 A N 6 ^ Ef dtype : object pd.Series.str.zfill \u00b6 pandas.Series.str. zfill (width) Supported Arguments argument datatypes width Integer >>> @bodo . jit ... def f ( S ): ... return S . str . zfill ( 5 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 0000 A 1 000 ce 2 00014 3 0000 4 0000 @ 5 00 a n 6 0 ^ Ef dtype : object pd.Series.str.isalnum \u00b6 pandas.Series.str. isalnum () >>> @bodo . jit ... def f ( S ): ... return S . str . isalnum () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 True 1 True 2 True 3 False 4 False 5 False 6 False dtype : boolean pd.Series.str.isalpha \u00b6 pandas.Series.str. isalpha () >>> @bodo . jit ... def f ( S ): ... return S . str . isalpha () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 True 1 True 2 False 3 False 4 False 5 False 6 False dtype : boolean pd.Series.str.isdigit \u00b6 pandas.Series.str. isdigit () >>> @bodo . jit ... def f ( S ): ... return S . str . isdigit () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 False 2 True 3 False 4 False 5 False 6 False dtype : boolean pd.Series.str.isspace \u00b6 pandas.Series.str. isspace () >>> @bodo . jit ... def f ( S ): ... return S . str . isspace () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 False 2 False 3 True 4 False 5 False 6 False dtype : boolean pd.Series.str.islower \u00b6 pandas.Series.str. islower () >>> @bodo . jit ... def f ( S ): ... return S . str . islower () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 True 2 False 3 True 4 False 5 True 6 False dtype : boolean pd.Series.str.isupper \u00b6 pandas.Series.str. isupper () >>> @bodo . jit ... def f ( S ): ... return S . str . isupper () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 True 1 False 2 False 3 False 4 False 5 False 6 False dtype : boolean pd.Series.str.istitle \u00b6 pandas.Series.str. istitle () >>> @bodo . jit ... def f ( S ): ... return S . str . istitle () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 True 1 False 2 False 3 False 4 False 5 False 6 True dtype : boolean pd.Series.str.isnumeric \u00b6 pandas.Series.str. isnumeric () >>> @bodo . jit ... def f ( S ): ... return S . str . isnumeric () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 False 2 True 3 False 4 False 5 False 6 False dtype : boolean pd.Series.str.isdecimal \u00b6 pandas.Series.str. isdecimal () >>> @bodo . jit ... def f ( S ): ... return S . str . isdecimal () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 False 2 True 3 False 4 False 5 False 6 False dtype : boolean Categorical accessor \u00b6 pd.Series.cat.codes \u00b6 pandas.Series. cat.codes Note If categories cannot be determined at compile time, then Bodo defaults to creating codes with an int64 , which may differ from Pandas. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . cat . codes >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" ] * 10 ) . astype ( \"category\" ) >>> f ( S ) 0 2 1 4 2 1 3 3 4 0 5 2 6 4 7 1 8 3 9 0 10 2 11 4 12 1 13 3 14 0 15 2 16 4 17 1 18 3 19 0 20 2 21 4 22 1 23 3 24 0 25 2 26 4 27 1 28 3 29 0 30 2 31 4 32 1 33 3 34 0 35 2 36 4 37 1 38 3 39 0 40 2 41 4 42 1 43 3 44 0 45 2 46 4 47 1 48 3 49 0 dtype : int8 Serialization / IO / Conversion \u00b6 pd.Series.to_csv \u00b6 pandas.Series. to_csv (path_or_buf=None, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict', storage_options=None) pd.Series.to_dict \u00b6 pandas.Series. to_dict (into= ) Supported Arguments None Note This method is not parallelized since dictionaries are not parallelized. This method returns a typedDict, which maintains typing information if passing the dictionary between JIT code and regular Python. This can be converted to a regular Python dictionary by using the dict constructor. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . to_dict () >>> S = pd . Series ( np . arange ( 10 )) >>> dict ( f ( S )) { 0 : 0 , 1 : 1 , 2 : 2 , 3 : 3 , 4 : 4 , 5 : 5 , 6 : 6 , 7 : 7 , 8 : 8 , 9 : 9 } pd.Series.to_frame \u00b6 pandas.Series. to_frame (name=None) Supported Arguments argument datatypes other requirements name String Must be constant at Compile Time Note If name is not provided Series name must be a known constant Example Usage >>> @bodo . jit ... def f ( S ): ... return S . to_frame ( \"my_column\" ) >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) my_column 0 0 1 1 2 2 3 3 4 4 .. ... 995 995 996 996 997 997 998 998 999 999 [1000 rows x 1 columns] Heterogeneous Series \u00b6 Bodo's Series implementation requires all elements to share a common data type. However, in situations where the size and types of the elements are constant at compile time, Bodo has some mixed type handling with its Heterogeneous Series type. Warning This type's primary purpose is for iterating through the rows of a DataFrame with different column types. You should not attempt to directly create Series with mixed types. Heterogeneous Series operations are a subset of those supported for Series and the supported operations are listed below. Please refer to series for detailed usage. Attributes \u00b6 pd.Series.index \u00b6 pandas.Series. index Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : len ( row . index ), axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 2 1 2 2 2 3 2 4 2 .. 95 2 96 2 97 2 98 2 99 2 Length : 100 , dtype : int64 pd.Series.values \u00b6 pandas.Series. values Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . values , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 ( 0 , A ) 1 ( 1 , b ) 2 ( 2 , A ) 3 ( 3 , b ) 4 ( 4 , A ) ... 95 ( 95 , b ) 96 ( 96 , A ) 97 ( 97 , b ) 98 ( 98 , A ) 99 ( 99 , b ) Length : 100 , dtype : object pd.Series.shape \u00b6 pandas.Series. shape Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . shape , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 ( 2 ,) 1 ( 2 ,) 2 ( 2 ,) 3 ( 2 ,) 4 ( 2 ,) ... 95 ( 2 ,) 96 ( 2 ,) 97 ( 2 ,) 98 ( 2 ,) 99 ( 2 ,) Length : 100 , dtype : object pd.Series.ndim \u00b6 pandas.Series. ndim Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . ndim , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 1 1 1 2 1 3 1 4 1 .. 95 1 96 1 97 1 98 1 99 1 Length : 100 , dtype : int64 pd.Series.size \u00b6 pandas.Series. size Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . size , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 2 1 2 2 2 3 2 4 2 .. 95 2 96 2 97 2 98 2 99 2 Length : 100 , dtype : int64 pd.Series.T \u00b6 pandas.Series. T Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . T . size , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 2 1 2 2 2 3 2 4 2 .. 95 2 96 2 97 2 98 2 99 2 Length : 100 , dtype : int64 pd.Series.empty \u00b6 pandas.Series. empty Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . empty , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 False 1 False 2 False 3 False 4 False ... 95 False 96 False 97 False 98 False 99 False Length : 100 , dtype : boolean pd.Series.name \u00b6 pandas.Series. name Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . name , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 0 1 1 2 2 3 3 4 4 .. 95 95 96 96 97 97 98 98 99 99 Length : 100 , dtype : int64","title":"Series"},{"location":"api_docs/pandas/series/#series","text":"Bodo provides extensive Series support. However, operations between Series (+, -, /, ,* ) do not implicitly align values based on their associated index values yet.","title":"Series"},{"location":"api_docs/pandas/series/#pdseries","text":"pandas. Series (data=None, index=None, dtype=None, name=None, copy=False, fastpath=False) Supported Arguments argument datatypes other requirements data Series type List type Array type Constant Dictionary None index SeriesType dtype Numpy or Pandas Type String name for Numpy/Pandas Type Must be constant at Compile Time String/Data Type must be one of the supported types (see Series.astype() ) name String Note If data is a Series and index is provided, implicit alignment is not performed yet. Example Usage >>> @bodo . jit ... def f (): ... return pd . Series ( np . arange ( 1000 ), dtype = np . float64 , name = \"my_series\" ) >>> f () 0 0.0 1 1.0 2 2.0 3 3.0 4 4.0 ... 995 995.0 996 996.0 997 997.0 998 998.0 999 999.0 Name : my_series , Length : 1000 , dtype : float64","title":"pd.Series"},{"location":"api_docs/pandas/series/#attributes","text":"","title":"Attributes"},{"location":"api_docs/pandas/series/#pdseriesindex","text":"pandas.Series. index Example Usage >>> @bodo . jit ... def f ( S ): ... return S . index >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) RangeIndex ( start = 0 , stop = 1000 , step = 1 )","title":"pd.Series.index"},{"location":"api_docs/pandas/series/#pdseriesvalues","text":"pandas.Series. values Example Usage >>> @bodo . jit ... def f ( S ): ... return S . values >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) array ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 85 , 86 , 87 , 88 , 89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , 100 , 101 , 102 , 103 , 104 , 105 , 106 , 107 , 108 , 109 , 110 , 111 , 112 , 113 , 114 , 115 , 116 , 117 , 118 , 119 , 120 , 121 , 122 , 123 , 124 , 125 , 126 , 127 , 128 , 129 , 130 , 131 , 132 , 133 , 134 , 135 , 136 , 137 , 138 , 139 , 140 , 141 , 142 , 143 , 144 , 145 , 146 , 147 , 148 , 149 , 150 , 151 , 152 , 153 , 154 , 155 , 156 , 157 , 158 , 159 , 160 , 161 , 162 , 163 , 164 , 165 , 166 , 167 , 168 , 169 , 170 , 171 , 172 , 173 , 174 , 175 , 176 , 177 , 178 , 179 , 180 , 181 , 182 , 183 , 184 , 185 , 186 , 187 , 188 , 189 , 190 , 191 , 192 , 193 , 194 , 195 , 196 , 197 , 198 , 199 , 200 , 201 , 202 , 203 , 204 , 205 , 206 , 207 , 208 , 209 , 210 , 211 , 212 , 213 , 214 , 215 , 216 , 217 , 218 , 219 , 220 , 221 , 222 , 223 , 224 , 225 , 226 , 227 , 228 , 229 , 230 , 231 , 232 , 233 , 234 , 235 , 236 , 237 , 238 , 239 , 240 , 241 , 242 , 243 , 244 , 245 , 246 , 247 , 248 , 249 , 250 , 251 , 252 , 253 , 254 , 255 , 256 , 257 , 258 , 259 , 260 , 261 , 262 , 263 , 264 , 265 , 266 , 267 , 268 , 269 , 270 , 271 , 272 , 273 , 274 , 275 , 276 , 277 , 278 , 279 , 280 , 281 , 282 , 283 , 284 , 285 , 286 , 287 , 288 , 289 , 290 , 291 , 292 , 293 , 294 , 295 , 296 , 297 , 298 , 299 , 300 , 301 , 302 , 303 , 304 , 305 , 306 , 307 , 308 , 309 , 310 , 311 , 312 , 313 , 314 , 315 , 316 , 317 , 318 , 319 , 320 , 321 , 322 , 323 , 324 , 325 , 326 , 327 , 328 , 329 , 330 , 331 , 332 , 333 , 334 , 335 , 336 , 337 , 338 , 339 , 340 , 341 , 342 , 343 , 344 , 345 , 346 , 347 , 348 , 349 , 350 , 351 , 352 , 353 , 354 , 355 , 356 , 357 , 358 , 359 , 360 , 361 , 362 , 363 , 364 , 365 , 366 , 367 , 368 , 369 , 370 , 371 , 372 , 373 , 374 , 375 , 376 , 377 , 378 , 379 , 380 , 381 , 382 , 383 , 384 , 385 , 386 , 387 , 388 , 389 , 390 , 391 , 392 , 393 , 394 , 395 , 396 , 397 , 398 , 399 , 400 , 401 , 402 , 403 , 404 , 405 , 406 , 407 , 408 , 409 , 410 , 411 , 412 , 413 , 414 , 415 , 416 , 417 , 418 , 419 , 420 , 421 , 422 , 423 , 424 , 425 , 426 , 427 , 428 , 429 , 430 , 431 , 432 , 433 , 434 , 435 , 436 , 437 , 438 , 439 , 440 , 441 , 442 , 443 , 444 , 445 , 446 , 447 , 448 , 449 , 450 , 451 , 452 , 453 , 454 , 455 , 456 , 457 , 458 , 459 , 460 , 461 , 462 , 463 , 464 , 465 , 466 , 467 , 468 , 469 , 470 , 471 , 472 , 473 , 474 , 475 , 476 , 477 , 478 , 479 , 480 , 481 , 482 , 483 , 484 , 485 , 486 , 487 , 488 , 489 , 490 , 491 , 492 , 493 , 494 , 495 , 496 , 497 , 498 , 499 , 500 , 501 , 502 , 503 , 504 , 505 , 506 , 507 , 508 , 509 , 510 , 511 , 512 , 513 , 514 , 515 , 516 , 517 , 518 , 519 , 520 , 521 , 522 , 523 , 524 , 525 , 526 , 527 , 528 , 529 , 530 , 531 , 532 , 533 , 534 , 535 , 536 , 537 , 538 , 539 , 540 , 541 , 542 , 543 , 544 , 545 , 546 , 547 , 548 , 549 , 550 , 551 , 552 , 553 , 554 , 555 , 556 , 557 , 558 , 559 , 560 , 561 , 562 , 563 , 564 , 565 , 566 , 567 , 568 , 569 , 570 , 571 , 572 , 573 , 574 , 575 , 576 , 577 , 578 , 579 , 580 , 581 , 582 , 583 , 584 , 585 , 586 , 587 , 588 , 589 , 590 , 591 , 592 , 593 , 594 , 595 , 596 , 597 , 598 , 599 , 600 , 601 , 602 , 603 , 604 , 605 , 606 , 607 , 608 , 609 , 610 , 611 , 612 , 613 , 614 , 615 , 616 , 617 , 618 , 619 , 620 , 621 , 622 , 623 , 624 , 625 , 626 , 627 , 628 , 629 , 630 , 631 , 632 , 633 , 634 , 635 , 636 , 637 , 638 , 639 , 640 , 641 , 642 , 643 , 644 , 645 , 646 , 647 , 648 , 649 , 650 , 651 , 652 , 653 , 654 , 655 , 656 , 657 , 658 , 659 , 660 , 661 , 662 , 663 , 664 , 665 , 666 , 667 , 668 , 669 , 670 , 671 , 672 , 673 , 674 , 675 , 676 , 677 , 678 , 679 , 680 , 681 , 682 , 683 , 684 , 685 , 686 , 687 , 688 , 689 , 690 , 691 , 692 , 693 , 694 , 695 , 696 , 697 , 698 , 699 , 700 , 701 , 702 , 703 , 704 , 705 , 706 , 707 , 708 , 709 , 710 , 711 , 712 , 713 , 714 , 715 , 716 , 717 , 718 , 719 , 720 , 721 , 722 , 723 , 724 , 725 , 726 , 727 , 728 , 729 , 730 , 731 , 732 , 733 , 734 , 735 , 736 , 737 , 738 , 739 , 740 , 741 , 742 , 743 , 744 , 745 , 746 , 747 , 748 , 749 , 750 , 751 , 752 , 753 , 754 , 755 , 756 , 757 , 758 , 759 , 760 , 761 , 762 , 763 , 764 , 765 , 766 , 767 , 768 , 769 , 770 , 771 , 772 , 773 , 774 , 775 , 776 , 777 , 778 , 779 , 780 , 781 , 782 , 783 , 784 , 785 , 786 , 787 , 788 , 789 , 790 , 791 , 792 , 793 , 794 , 795 , 796 , 797 , 798 , 799 , 800 , 801 , 802 , 803 , 804 , 805 , 806 , 807 , 808 , 809 , 810 , 811 , 812 , 813 , 814 , 815 , 816 , 817 , 818 , 819 , 820 , 821 , 822 , 823 , 824 , 825 , 826 , 827 , 828 , 829 , 830 , 831 , 832 , 833 , 834 , 835 , 836 , 837 , 838 , 839 , 840 , 841 , 842 , 843 , 844 , 845 , 846 , 847 , 848 , 849 , 850 , 851 , 852 , 853 , 854 , 855 , 856 , 857 , 858 , 859 , 860 , 861 , 862 , 863 , 864 , 865 , 866 , 867 , 868 , 869 , 870 , 871 , 872 , 873 , 874 , 875 , 876 , 877 , 878 , 879 , 880 , 881 , 882 , 883 , 884 , 885 , 886 , 887 , 888 , 889 , 890 , 891 , 892 , 893 , 894 , 895 , 896 , 897 , 898 , 899 , 900 , 901 , 902 , 903 , 904 , 905 , 906 , 907 , 908 , 909 , 910 , 911 , 912 , 913 , 914 , 915 , 916 , 917 , 918 , 919 , 920 , 921 , 922 , 923 , 924 , 925 , 926 , 927 , 928 , 929 , 930 , 931 , 932 , 933 , 934 , 935 , 936 , 937 , 938 , 939 , 940 , 941 , 942 , 943 , 944 , 945 , 946 , 947 , 948 , 949 , 950 , 951 , 952 , 953 , 954 , 955 , 956 , 957 , 958 , 959 , 960 , 961 , 962 , 963 , 964 , 965 , 966 , 967 , 968 , 969 , 970 , 971 , 972 , 973 , 974 , 975 , 976 , 977 , 978 , 979 , 980 , 981 , 982 , 983 , 984 , 985 , 986 , 987 , 988 , 989 , 990 , 991 , 992 , 993 , 994 , 995 , 996 , 997 , 998 , 999 ])","title":"pd.Series.values"},{"location":"api_docs/pandas/series/#pdseriesdtype","text":"pandas.Series. dtype (object data types such as dtype of string series not supported yet) Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dtype >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) dtype ( 'int64' )","title":"pd.Series.dtype"},{"location":"api_docs/pandas/series/#pdseriesshape","text":"pandas.Series. shape Example Usage >>> @bodo . jit ... def f ( S ): ... return S . shape >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) ( 1000 ,)","title":"pd.Series.shape"},{"location":"api_docs/pandas/series/#pdseriesnbytes","text":"pandas.Series. nbytes Note This tracks the number of bytes used by Bodo which may differ from the Pandas values. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . nbytes >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 8000","title":"pd.Series.nbytes"},{"location":"api_docs/pandas/series/#pdseriesndim","text":"pandas.Series. ndim Example Usage >>> @bodo . jit ... def f ( S ): ... return S . ndim >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 1","title":"pd.Series.ndim"},{"location":"api_docs/pandas/series/#pdseriessize","text":"pandas.Series. size Example Usage >>> @bodo . jit ... def f ( S ): ... return S . size >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 1000","title":"pd.Series.size"},{"location":"api_docs/pandas/series/#pdseriest","text":"pandas.Series. T Example Usage >>> @bodo . jit ... def f ( S ): ... return S . T >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 ... 995 995 996 996 997 997 998 998 999 999 Length : 1000 , dtype : int64","title":"pd.Series.T"},{"location":"api_docs/pandas/series/#pdseriesmemory_usage","text":"pandas.Series. memory_usage (index=True, deep=False) Supported Arguments argument datatypes other requirements index Boolean Must be constant at Compile Time Note This tracks the number of bytes used by Bodo which may differ from the Pandas values. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . memory_usage () >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 8024","title":"pd.Series.memory_usage"},{"location":"api_docs/pandas/series/#pdserieshasnans","text":"pandas.Series. hasnans Example Usage >>> @bodo . jit ... def f ( S ): ... return S . hasnans >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) False","title":"pd.Series.hasnans"},{"location":"api_docs/pandas/series/#pdseriesempty","text":"pandas.Series. empty Example Usage >>> @bodo . jit ... def f ( S ): ... return S . empty >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) False","title":"pd.Series.empty"},{"location":"api_docs/pandas/series/#pdseriesdtypes","text":"pandas.Series. dtypes Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dtypes >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) dtype ( 'int64' )","title":"pd.Series.dtypes"},{"location":"api_docs/pandas/series/#pdseriesname","text":"pandas.Series. name Example Usage >>> @bodo . jit ... def f ( S ): ... return S . name >>> S = pd . Series ( np . arange ( 1000 ), name = \"my_series\" ) >>> f ( S ) 'my_series'","title":"pd.Series.name"},{"location":"api_docs/pandas/series/#conversion","text":"","title":"Conversion:"},{"location":"api_docs/pandas/series/#pdseriesastype","text":"pandas.Series. astype (dtype, copy=True, errors=\"raise\", _bodo_nan_to_str=True) Supported Arguments argument datatypes other requirements dtype String (string must be parsable by np.dtype ) Valid type (see types) The following functions: float, int, bool, str Must be constant at Compile Time copy Boolean Must be constant at Compile Time _bodo_nan_to_str Boolean Must be constant at Compile Time Argument unique to Bodo. When True NA values in when converting to string are represented as NA instead of a string representation of the NA value (i.e. 'nan'), the default Pandas behavior. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . astype ( np . float32 ) >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 0 0.0 1 1.0 2 2.0 3 3.0 4 4.0 ... 995 995.0 996 996.0 997 997.0 998 998.0 999 999.0 Length : 1000 , dtype : float32","title":"pd.Series.astype"},{"location":"api_docs/pandas/series/#pdseriescopy","text":"pandas.Series. copy (deep=True) Supported Arguments argument datatypes deep Boolean Example Usage >>> @bodo . jit ... def f ( S ): ... return S . copy () >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 ... 995 995 996 996 997 997 998 998 999 999 Length : 1000 , dtype : int64","title":"pd.Series.copy"},{"location":"api_docs/pandas/series/#pdseriesto_numpy","text":"pandas.Series. to_numpy (dtype=None, copy=False, na_value=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . to_numpy () >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) array ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 85 , 86 , 87 , 88 , 89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , 100 , 101 , 102 , 103 , 104 , 105 , 106 , 107 , 108 , 109 , 110 , 111 , 112 , 113 , 114 , 115 , 116 , 117 , 118 , 119 , 120 , 121 , 122 , 123 , 124 , 125 , 126 , 127 , 128 , 129 , 130 , 131 , 132 , 133 , 134 , 135 , 136 , 137 , 138 , 139 , 140 , 141 , 142 , 143 , 144 , 145 , 146 , 147 , 148 , 149 , 150 , 151 , 152 , 153 , 154 , 155 , 156 , 157 , 158 , 159 , 160 , 161 , 162 , 163 , 164 , 165 , 166 , 167 , 168 , 169 , 170 , 171 , 172 , 173 , 174 , 175 , 176 , 177 , 178 , 179 , 180 , 181 , 182 , 183 , 184 , 185 , 186 , 187 , 188 , 189 , 190 , 191 , 192 , 193 , 194 , 195 , 196 , 197 , 198 , 199 , 200 , 201 , 202 , 203 , 204 , 205 , 206 , 207 , 208 , 209 , 210 , 211 , 212 , 213 , 214 , 215 , 216 , 217 , 218 , 219 , 220 , 221 , 222 , 223 , 224 , 225 , 226 , 227 , 228 , 229 , 230 , 231 , 232 , 233 , 234 , 235 , 236 , 237 , 238 , 239 , 240 , 241 , 242 , 243 , 244 , 245 , 246 , 247 , 248 , 249 , 250 , 251 , 252 , 253 , 254 , 255 , 256 , 257 , 258 , 259 , 260 , 261 , 262 , 263 , 264 , 265 , 266 , 267 , 268 , 269 , 270 , 271 , 272 , 273 , 274 , 275 , 276 , 277 , 278 , 279 , 280 , 281 , 282 , 283 , 284 , 285 , 286 , 287 , 288 , 289 , 290 , 291 , 292 , 293 , 294 , 295 , 296 , 297 , 298 , 299 , 300 , 301 , 302 , 303 , 304 , 305 , 306 , 307 , 308 , 309 , 310 , 311 , 312 , 313 , 314 , 315 , 316 , 317 , 318 , 319 , 320 , 321 , 322 , 323 , 324 , 325 , 326 , 327 , 328 , 329 , 330 , 331 , 332 , 333 , 334 , 335 , 336 , 337 , 338 , 339 , 340 , 341 , 342 , 343 , 344 , 345 , 346 , 347 , 348 , 349 , 350 , 351 , 352 , 353 , 354 , 355 , 356 , 357 , 358 , 359 , 360 , 361 , 362 , 363 , 364 , 365 , 366 , 367 , 368 , 369 , 370 , 371 , 372 , 373 , 374 , 375 , 376 , 377 , 378 , 379 , 380 , 381 , 382 , 383 , 384 , 385 , 386 , 387 , 388 , 389 , 390 , 391 , 392 , 393 , 394 , 395 , 396 , 397 , 398 , 399 , 400 , 401 , 402 , 403 , 404 , 405 , 406 , 407 , 408 , 409 , 410 , 411 , 412 , 413 , 414 , 415 , 416 , 417 , 418 , 419 , 420 , 421 , 422 , 423 , 424 , 425 , 426 , 427 , 428 , 429 , 430 , 431 , 432 , 433 , 434 , 435 , 436 , 437 , 438 , 439 , 440 , 441 , 442 , 443 , 444 , 445 , 446 , 447 , 448 , 449 , 450 , 451 , 452 , 453 , 454 , 455 , 456 , 457 , 458 , 459 , 460 , 461 , 462 , 463 , 464 , 465 , 466 , 467 , 468 , 469 , 470 , 471 , 472 , 473 , 474 , 475 , 476 , 477 , 478 , 479 , 480 , 481 , 482 , 483 , 484 , 485 , 486 , 487 , 488 , 489 , 490 , 491 , 492 , 493 , 494 , 495 , 496 , 497 , 498 , 499 , 500 , 501 , 502 , 503 , 504 , 505 , 506 , 507 , 508 , 509 , 510 , 511 , 512 , 513 , 514 , 515 , 516 , 517 , 518 , 519 , 520 , 521 , 522 , 523 , 524 , 525 , 526 , 527 , 528 , 529 , 530 , 531 , 532 , 533 , 534 , 535 , 536 , 537 , 538 , 539 , 540 , 541 , 542 , 543 , 544 , 545 , 546 , 547 , 548 , 549 , 550 , 551 , 552 , 553 , 554 , 555 , 556 , 557 , 558 , 559 , 560 , 561 , 562 , 563 , 564 , 565 , 566 , 567 , 568 , 569 , 570 , 571 , 572 , 573 , 574 , 575 , 576 , 577 , 578 , 579 , 580 , 581 , 582 , 583 , 584 , 585 , 586 , 587 , 588 , 589 , 590 , 591 , 592 , 593 , 594 , 595 , 596 , 597 , 598 , 599 , 600 , 601 , 602 , 603 , 604 , 605 , 606 , 607 , 608 , 609 , 610 , 611 , 612 , 613 , 614 , 615 , 616 , 617 , 618 , 619 , 620 , 621 , 622 , 623 , 624 , 625 , 626 , 627 , 628 , 629 , 630 , 631 , 632 , 633 , 634 , 635 , 636 , 637 , 638 , 639 , 640 , 641 , 642 , 643 , 644 , 645 , 646 , 647 , 648 , 649 , 650 , 651 , 652 , 653 , 654 , 655 , 656 , 657 , 658 , 659 , 660 , 661 , 662 , 663 , 664 , 665 , 666 , 667 , 668 , 669 , 670 , 671 , 672 , 673 , 674 , 675 , 676 , 677 , 678 , 679 , 680 , 681 , 682 , 683 , 684 , 685 , 686 , 687 , 688 , 689 , 690 , 691 , 692 , 693 , 694 , 695 , 696 , 697 , 698 , 699 , 700 , 701 , 702 , 703 , 704 , 705 , 706 , 707 , 708 , 709 , 710 , 711 , 712 , 713 , 714 , 715 , 716 , 717 , 718 , 719 , 720 , 721 , 722 , 723 , 724 , 725 , 726 , 727 , 728 , 729 , 730 , 731 , 732 , 733 , 734 , 735 , 736 , 737 , 738 , 739 , 740 , 741 , 742 , 743 , 744 , 745 , 746 , 747 , 748 , 749 , 750 , 751 , 752 , 753 , 754 , 755 , 756 , 757 , 758 , 759 , 760 , 761 , 762 , 763 , 764 , 765 , 766 , 767 , 768 , 769 , 770 , 771 , 772 , 773 , 774 , 775 , 776 , 777 , 778 , 779 , 780 , 781 , 782 , 783 , 784 , 785 , 786 , 787 , 788 , 789 , 790 , 791 , 792 , 793 , 794 , 795 , 796 , 797 , 798 , 799 , 800 , 801 , 802 , 803 , 804 , 805 , 806 , 807 , 808 , 809 , 810 , 811 , 812 , 813 , 814 , 815 , 816 , 817 , 818 , 819 , 820 , 821 , 822 , 823 , 824 , 825 , 826 , 827 , 828 , 829 , 830 , 831 , 832 , 833 , 834 , 835 , 836 , 837 , 838 , 839 , 840 , 841 , 842 , 843 , 844 , 845 , 846 , 847 , 848 , 849 , 850 , 851 , 852 , 853 , 854 , 855 , 856 , 857 , 858 , 859 , 860 , 861 , 862 , 863 , 864 , 865 , 866 , 867 , 868 , 869 , 870 , 871 , 872 , 873 , 874 , 875 , 876 , 877 , 878 , 879 , 880 , 881 , 882 , 883 , 884 , 885 , 886 , 887 , 888 , 889 , 890 , 891 , 892 , 893 , 894 , 895 , 896 , 897 , 898 , 899 , 900 , 901 , 902 , 903 , 904 , 905 , 906 , 907 , 908 , 909 , 910 , 911 , 912 , 913 , 914 , 915 , 916 , 917 , 918 , 919 , 920 , 921 , 922 , 923 , 924 , 925 , 926 , 927 , 928 , 929 , 930 , 931 , 932 , 933 , 934 , 935 , 936 , 937 , 938 , 939 , 940 , 941 , 942 , 943 , 944 , 945 , 946 , 947 , 948 , 949 , 950 , 951 , 952 , 953 , 954 , 955 , 956 , 957 , 958 , 959 , 960 , 961 , 962 , 963 , 964 , 965 , 966 , 967 , 968 , 969 , 970 , 971 , 972 , 973 , 974 , 975 , 976 , 977 , 978 , 979 , 980 , 981 , 982 , 983 , 984 , 985 , 986 , 987 , 988 , 989 , 990 , 991 , 992 , 993 , 994 , 995 , 996 , 997 , 998 , 999 ])","title":"pd.Series.to_numpy"},{"location":"api_docs/pandas/series/#pdseriestolist","text":"pandas.Series. tolist () Note Calling tolist on a non-float array with NA values with cause a runtime exception. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . tolist () >>> S = pd . Series ( np . arange ( 50 )) >>> f ( S ) [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 ]","title":"pd.Series.tolist"},{"location":"api_docs/pandas/series/#indexing-iteration","text":"Location based indexing using [] , iat , and iloc is supported. Changing values of existing string Series using these operators is not supported yet.","title":"Indexing, iteration:"},{"location":"api_docs/pandas/series/#pdseriesiat","text":"pandas.Series. iat We only support indexing using iat using a pair of integers Example Usage >>> @bodo . jit ... def f ( S , i ): ... return S . iat [ i ] >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S , 27 ) 27","title":"pd.Series.iat"},{"location":"api_docs/pandas/series/#pdseriesiloc","text":"pandas.Series. iloc getitem : Series.iloc supports single integer indexing (returns a scalar) S.iloc[0] Series.iloc supports list/array/series of integers/bool (returns a Series) S.iloc[[0,1,2]] Series.iloc supports integer slice (returns a Series) S.iloc[[0:2]] setitem : Supports the same cases as getitem but the array type must be mutable (i.e. numeric array) Example Usage >>> @bodo . jit ... def f ( S , idx ): ... return S . iloc [ idx ] >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S , [ 1 , 4 , 29 ]) 1 1 4 4 29 29 dtype : int64","title":"pd.Series.iloc"},{"location":"api_docs/pandas/series/#pdseriesloc","text":"pandas.Series. loc getitem : Series.loc supports list/array of booleans Series.loc supports integer with RangeIndex setitem : Series.loc supports list/array of booleans Example Usage >>> @bodo . jit ... def f ( S , idx ): ... return S . loc [ idx ] >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S , S < 10 ) 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 dtype : int64","title":"pd.Series.loc"},{"location":"api_docs/pandas/series/#binary-operator-functions","text":"","title":"Binary operator functions:"},{"location":"api_docs/pandas/series/#pdseriesadd","text":"pandas.Series. add (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.add is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . add ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1001 1 1001 2 1001 3 1001 4 1001 ... 995 1001 996 1001 997 1001 998 1001 999 1001 Length : 1000 , dtype : int64","title":"pd.Series.add"},{"location":"api_docs/pandas/series/#pdseriessub","text":"pandas.Series. sub (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.sub is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . sub ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 - 999 1 - 997 2 - 995 3 - 993 4 - 991 ... 995 991 996 993 997 995 998 997 999 999 Length : 1000 , dtype : int64","title":"pd.Series.sub"},{"location":"api_docs/pandas/series/#pdseriesmul","text":"pandas.Series. mul (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.mul is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . mul ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000 1 1998 2 2994 3 3988 4 4980 ... 995 4980 996 3988 997 2994 998 1998 999 1000 Length : 1000 , dtype : int64","title":"pd.Series.mul"},{"location":"api_docs/pandas/series/#pdseriesdiv","text":"pandas.Series. div (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.div is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . div ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 0.001000 1 0.002002 2 0.003006 3 0.004012 4 0.005020 ... 995 199.200000 996 249.250000 997 332.666667 998 499.500000 999 1000.000000 Length : 1000 , dtype : float64","title":"pd.Series.div"},{"location":"api_docs/pandas/series/#pdseriestruediv","text":"pandas.Series. truediv (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.truediv is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . truediv ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 0.001000 1 0.002002 2 0.003006 3 0.004012 4 0.005020 ... 995 199.200000 996 249.250000 997 332.666667 998 499.500000 999 1000.000000 Length : 1000 , dtype : float64","title":"pd.Series.truediv"},{"location":"api_docs/pandas/series/#pdseriesfloordiv","text":"pandas.Series. floordiv (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.floordiv is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . floordiv ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 0 1 0 2 0 3 0 4 0 ... 995 199 996 249 997 332 998 499 999 1000 Length : 1000 , dtype : int64","title":"pd.Series.floordiv"},{"location":"api_docs/pandas/series/#pdseriesmod","text":"pandas.Series. mod (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.mod is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . mod ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1 1 2 2 3 3 4 4 5 .. 995 1 996 1 997 2 998 1 999 0 Length : 1000 , dtype : int64","title":"pd.Series.mod"},{"location":"api_docs/pandas/series/#pdseriespow","text":"pandas.Series. pow (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.pow is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . pow ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1 1 0 2 - 5459658280481875879 3 0 4 3767675092665006833 ... 995 980159361278976 996 988053892081 997 994011992 998 998001 999 1000 Length : 1000 , dtype : int64","title":"pd.Series.pow"},{"location":"api_docs/pandas/series/#pdseriesradd","text":"pandas.Series. radd (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.radd is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . radd ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1001 1 1001 2 1001 3 1001 4 1001 ... 995 1001 996 1001 997 1001 998 1001 999 1001 Length : 1000 , dtype : int64","title":"pd.Series.radd"},{"location":"api_docs/pandas/series/#pdseriesrsub","text":"pandas.Series. rsub (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rsub is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rsub ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 999 1 997 2 995 3 993 4 991 ... 995 - 991 996 - 993 997 - 995 998 - 997 999 - 999 Length : 1000 , dtype : int64","title":"pd.Series.rsub"},{"location":"api_docs/pandas/series/#pdseriesrmul","text":"pandas.Series. rmul (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rmul is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rmul ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000 1 1998 2 2994 3 3988 4 4980 ... 995 4980 996 3988 997 2994 998 1998 999 1000 Length : 1000 , dtype : int64","title":"pd.Series.rmul"},{"location":"api_docs/pandas/series/#pdseriesrdiv","text":"pandas.Series. rdiv (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rdiv is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rdiv ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000.000000 1 499.500000 2 332.666667 3 249.250000 4 199.200000 ... 995 0.005020 996 0.004012 997 0.003006 998 0.002002 999 0.001000 Length : 1000 , dtype : float64","title":"pd.Series.rdiv"},{"location":"api_docs/pandas/series/#pdseriesrtruediv","text":"pandas.Series. rtruediv (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rtruediv is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rtruediv ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000.000000 1 499.500000 2 332.666667 3 249.250000 4 199.200000 ... 995 0.005020 996 0.004012 997 0.003006 998 0.002002 999 0.001000 Length : 1000 , dtype : float64","title":"pd.Series.rtruediv"},{"location":"api_docs/pandas/series/#pdseriesrfloordiv","text":"pandas.Series. rfloordiv (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rfloordiv is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rfloordiv ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000 1 499 2 332 3 249 4 199 ... 995 0 996 0 997 0 998 0 999 0 Length : 1000 , dtype : int64","title":"pd.Series.rfloordiv"},{"location":"api_docs/pandas/series/#pdseriesrmod","text":"pandas.Series. rmod (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rmod is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rmod ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 0 1 1 2 2 3 1 4 1 .. 995 5 996 4 997 3 998 2 999 1 Length : 1000 , dtype : int64","title":"pd.Series.rmod"},{"location":"api_docs/pandas/series/#pdseriesrpow","text":"pandas.Series. rpow (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.rpow is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . rpow ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1000 1 998001 2 994011992 3 988053892081 4 980159361278976 ... 995 3767675092665006833 996 0 997 - 5459658280481875879 998 0 999 1 Length : 1000 , dtype : int64","title":"pd.Series.rpow"},{"location":"api_docs/pandas/series/#pdseriescombine","text":"pandas.Series. combine (other, func, fill_value=None) Supported Arguments argument datatypes other requirements other Array Series func Function that takes two scalar arguments and returns a scalar value. fill_value scalar Must be provided if the Series lengths aren't equal and the dtypes aren't floats. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . combine ( other , lambda a , b : 2 * a + b ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 1002 1 1003 2 1004 3 1005 4 1006 ... 995 1997 996 1998 997 1999 998 2000 999 2001 Length : 1000 , dtype : int64","title":"pd.Series.combine"},{"location":"api_docs/pandas/series/#pdseriesround","text":"pandas.Series. round (decimals=0) Supported Arguments argument datatypes other Series with numeric data Note Series.round is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . round ( 2 ) >>> S = pd . Series ( np . linspace ( 100 , 1000 )) >>> f ( S ) 0 100.00 1 118.37 2 136.73 3 155.10 4 173.47 5 191.84 6 210.20 7 228.57 8 246.94 9 265.31 10 283.67 11 302.04 12 320.41 13 338.78 14 357.14 15 375.51 16 393.88 17 412.24 18 430.61 19 448.98 20 467.35 21 485.71 22 504.08 23 522.45 24 540.82 25 559.18 26 577.55 27 595.92 28 614.29 29 632.65 30 651.02 31 669.39 32 687.76 33 706.12 34 724.49 35 742.86 36 761.22 37 779.59 38 797.96 39 816.33 40 834.69 41 853.06 42 871.43 43 889.80 44 908.16 45 926.53 46 944.90 47 963.27 48 981.63 49 1000.00 dtype : float64","title":"pd.Series.round"},{"location":"api_docs/pandas/series/#pdserieslt","text":"pandas.Series. lt (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.lt is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . lt ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 True 1 True 2 True 3 True 4 True ... 995 False 996 False 997 False 998 False 999 False Length : 1000 , dtype : bool","title":"pd.Series.lt"},{"location":"api_docs/pandas/series/#pdseriesgt","text":"pandas.Series. gt (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.gt is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . gt ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 False 1 False 2 False 3 False 4 False ... 995 True 996 True 997 True 998 True 999 True Length : 1000 , dtype : bool","title":"pd.Series.gt"},{"location":"api_docs/pandas/series/#pdseriesle","text":"pandas.Series. le (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.le is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . le ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 True 1 True 2 True 3 True 4 True ... 995 False 996 False 997 False 998 False 999 False Length : 1000 , dtype : bool","title":"pd.Series.le"},{"location":"api_docs/pandas/series/#pdseriesge","text":"pandas.Series. ge (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.ge is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . ge ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 False 1 False 2 False 3 False 4 False ... 995 True 996 True 997 True 998 True 999 True Length : 1000 , dtype : bool","title":"pd.Series.ge"},{"location":"api_docs/pandas/series/#pdseriesne","text":"pandas.Series. ne (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.ne is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . ne ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 True 1 True 2 True 3 True 4 True ... 995 True 996 True 997 True 998 True 999 True Length : 1000 , dtype : bool","title":"pd.Series.ne"},{"location":"api_docs/pandas/series/#pdserieseq","text":"pandas.Series. eq (other, level=None, fill_value=None, axis=0) Supported Arguments argument datatypes other numeric scalar array with numeric data Series with numeric data fill_value numeric scalar Note Series.eq is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . eq ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 0 False 1 False 2 False 3 False 4 False ... 995 False 996 False 997 False 998 False 999 False Length : 1000 , dtype : bool","title":"pd.Series.eq"},{"location":"api_docs/pandas/series/#pdseriesdot","text":"pandas.Series. dot (other) Supported Arguments argument datatypes other Series with numeric data Note Series.dot is only supported on Series of numeric data. Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . dot ( other ) >>> S = pd . Series ( np . arange ( 1 , 1001 )) >>> other = pd . Series ( reversed ( np . arange ( 1 , 1001 ))) >>> f ( S , other ) 167167000","title":"pd.Series.dot"},{"location":"api_docs/pandas/series/#function-application-groupby-window","text":"","title":"Function application, GroupBy &amp; Window"},{"location":"api_docs/pandas/series/#pdseriesapply","text":"pandas.Series. apply f(func, convert_dtype=True, args=(), **kwargs) Supported Arguments argument datatypes other requirements func JIT function or callable defined within a JIT function Numpy ufunc Constant String which is the name of a supported Series method or Numpy ufunc Additional arguments for func can be passed as additional arguments. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . apply ( lambda x : x ** 0.75 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0.000000 1 1.000000 2 1.681793 3 2.279507 4 2.828427 ... 95 30.429352 96 30.669269 97 30.908562 98 31.147239 99 31.385308 Length : 100 , dtype : float64","title":"pd.Series.apply"},{"location":"api_docs/pandas/series/#pdseriesmap","text":"pandas.Series. map (arg, na_action=None) Supported Arguments argument datatypes arg Dictionary JIT function or callable defined within a JIT function Constant String which refers to a supported Series method or Numpy ufunc Numpy ufunc Example Usage >>> @bodo . jit ... def f ( S ): ... return S . map ( lambda x : x ** 0.75 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0.000000 1 1.000000 2 1.681793 3 2.279507 4 2.828427 ... 95 30.429352 96 30.669269 97 30.908562 98 31.147239 99 31.385308 Length : 100 , dtype : float64","title":"pd.Series.map"},{"location":"api_docs/pandas/series/#pdseriesgroupby","text":"pandas.Series. groupby (by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) Supported Arguments argument datatypes other requirements by Array-like or Series data. This is not supported with Decimal or Categorical data. Must be constant at Compile Time level integer Must be constant at Compile Time Only level=0 is supported and not with MultiIndex. You must provide exactly one of by and level Example Usage >>> @bodo . jit ... def f ( S , by_series ): ... return S . groupby ( by_series ) . count () >>> S = pd . Series ([ 1 , 2 , 24 , None ] * 5 ) >>> by_series = pd . Series ([ \"421\" , \"f31\" ] * 10 ) >>> f ( S , by_series ) > 421 10 f31 5 Name : , dtype : int64 Note Series.groupby doesn't currently keep the name of the original Series.","title":"pd.Series.groupby"},{"location":"api_docs/pandas/series/#pdseriesrolling","text":"pandas.Series. rolling (window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None, method='single') Supported Arguments argument datatypes window Integer String representing a Time Offset Timedelta min_periods Integer center Boolean Example Usage >>> @bodo . jit ... def f ( S ): ... return S . rolling ( 2 ) . mean () >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 NaN 1 0.5 2 1.5 3 2.5 4 3.5 ... 95 94.5 96 95.5 97 96.5 98 97.5 99 98.5 Length : 100 , dtype : float64","title":"pd.Series.rolling"},{"location":"api_docs/pandas/series/#pdseriespipe","text":"pandas.Series. pipe (func, *args, **kwargs) Supported Arguments argument datatypes other requirements func JIT function or callable defined within a JIT function. Additional arguments for func can be passed as additional arguments. Note func cannot be a tuple Example Usage >>> @bodo . jit ... def f ( S ): ... def g ( row , y ): ... return row + y ... ... def f ( row ): ... return row * 2 ... ... return S . pipe ( h ) . pipe ( g , y = 32 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 32 1 34 2 36 3 38 4 40 ... 95 222 96 224 97 226 98 228 99 230 Length : 100 , dtype : int64","title":"pd.Series.pipe"},{"location":"api_docs/pandas/series/#computations-descriptive-stats","text":"Statistical functions below are supported without optional arguments unless support is explicitly mentioned.","title":"Computations / Descriptive Stats"},{"location":"api_docs/pandas/series/#pdseriesabs","text":"pandas.Series. abs () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . abs () >>> S = ( pd . Series ( np . arange ( 100 )) % 7 ) - 2 >>> f ( S ) 0 2 1 1 2 0 3 1 4 2 .. 95 2 96 3 97 4 98 2 99 1 Length : 100 , dtype : int64","title":"pd.Series.abs"},{"location":"api_docs/pandas/series/#pdseriesall","text":"pandas.Series. all (axis=0, bool_only=None, skipna=True, level=None) Supported Arguments None Note Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . all () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) False","title":"pd.Series.all"},{"location":"api_docs/pandas/series/#pdseriesany","text":"pandas.Series. any (axis=0, bool_only=None, skipna=True, level=None) Supported Arguments None Note Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . any () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) True","title":"pd.Series.any"},{"location":"api_docs/pandas/series/#pdseriesautocorr","text":"pandas.Series. autocorr (lag=1) Supported Arguments argument datatypes lag Integer Example Usage >>> @bodo . jit ... def f ( S ): ... return S . autocorr ( 3 ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) - 0.49872171657407155","title":"pd.Series.autocorr"},{"location":"api_docs/pandas/series/#pdseriesbetween","text":"pandas.Series. between (left, right, inclusive='both') Supported Arguments argument datatypes other requirements left Scalar matching the Series type right Scalar matching the Series type inclusive One of (\"both\", \"neither\") Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( S ): ... return S . between ( 3 , 5 , \"both\" ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 False 1 False 2 False 3 True 4 True ... 95 True 96 True 97 False 98 False 99 False Length : 100 , dtype : bool","title":"pd.Series.between"},{"location":"api_docs/pandas/series/#pdseriescorr","text":"pandas.Series. corr (other, method='pearson', min_periods=None) Supported Arguments argument datatypes other Numeric Series or Array Note Series type must be numeric Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . cov ( other ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> other = pd . Series ( np . arange ( 100 )) % 10 >>> f ( S , other ) 0.004326329627279103","title":"pd.Series.corr"},{"location":"api_docs/pandas/series/#pdseriescount","text":"pandas.Series. count (level=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . count () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 100","title":"pd.Series.count"},{"location":"api_docs/pandas/series/#pdseriescov","text":"pandas.Series. cov (other, min_periods=None, ddof=1) Supported Arguments argument datatypes other Numeric Series or Array ddof Integer Note Series type must be numeric Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . cov ( other ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> other = pd . Series ( np . arange ( 100 )) % 10 >>> f ( S , other ) 0.025252525252525252","title":"pd.Series.cov"},{"location":"api_docs/pandas/series/#pdseriescummin","text":"pandas.Series. cummin (axis=None, skipna=True) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . cummin () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 0 1 0 2 0 3 0 4 0 .. 95 0 96 0 97 0 98 0 99 0 Length : 100 , dtype : int64","title":"pd.Series.cummin"},{"location":"api_docs/pandas/series/#pdseriescummax","text":"pandas.Series. cummax (axis=None, skipna=True) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . cummax () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 .. 95 6 96 6 97 6 98 6 99 6 Length : 100 , dtype : int64","title":"pd.Series.cummax"},{"location":"api_docs/pandas/series/#pdseriescumprod","text":"pandas.Series. cumprod (axis=None, skipna=True) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . cumprod () >>> S = ( pd . Series ( np . arange ( 10 )) % 7 ) + 1 >>> f ( S ) 0 1 1 2 2 6 3 24 4 120 5 720 6 5040 7 5040 8 10080 9 30240 dtype : int64","title":"pd.Series.cumprod"},{"location":"api_docs/pandas/series/#pdseriescumsum","text":"pandas.Series. cumsum (axis=None, skipna=True) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . cumsum () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 0 1 1 2 3 3 6 4 10 ... 95 283 96 288 97 294 98 294 99 295 Length : 100 , dtype : int64","title":"pd.Series.cumsum"},{"location":"api_docs/pandas/series/#pdseriesdescribe","text":"pandas.Series. describe (percentiles=None, include=None, exclude=None, datetime_is_numeric=False) Supported Arguments None Note Bodo only supports numeric and datetime64 types and assumes datetime_is_numeric=True Example Usage >>> @bodo . jit ... def f ( S ): ... return S . describe () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) count 100.000000 mean 2.950000 std 2.021975 min 0.000000 25 % 1.000000 50 % 3.000000 75 % 5.000000 max 6.000000 dtype : float64","title":"pd.Series.describe"},{"location":"api_docs/pandas/series/#pdseriesdiff","text":"pandas.Series. diff (periods=1) Supported Arguments argument datatypes periods Integer Note Bodo only supports numeric and datetime64 types Example Usage >>> @bodo . jit ... def f ( S ): ... return S . diff ( 3 ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 NaN 1 NaN 2 NaN 3 3.0 4 3.0 ... 95 3.0 96 3.0 97 3.0 98 - 4.0 99 - 4.0 Length : 100 , dtype : float64","title":"pd.Series.diff"},{"location":"api_docs/pandas/series/#pdserieskurt","text":"pandas.Series. kurt (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . kurt () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) - 1.269562153611973","title":"pd.Series.kurt"},{"location":"api_docs/pandas/series/#pdseriesmad","text":"pandas.Series. mad (axis=None, skipna=None, level=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Example Usage >>> @bodo . jit ... def f ( S ): ... return S . mad () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 1.736","title":"pd.Series.mad"},{"location":"api_docs/pandas/series/#pdseriesmax","text":"pandas.Series. max (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . max () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 6","title":"pd.Series.max"},{"location":"api_docs/pandas/series/#pdseriesmean","text":"pandas.Series. mean (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . mean () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 2.95","title":"pd.Series.mean"},{"location":"api_docs/pandas/series/#pdseriesmedian","text":"pandas.Series. median (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . median () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 3.0","title":"pd.Series.median"},{"location":"api_docs/pandas/series/#pdseriesmin","text":"pandas.Series. min (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments None Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . min () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0","title":"pd.Series.min"},{"location":"api_docs/pandas/series/#pdseriesnlargest","text":"pandas.Series. nlargest (n=5, keep='first') Supported Arguments argument datatypes n Integer Note Series type must be numeric Example Usage >>> @bodo . jit ... def f ( S ): ... return S . nlargest ( 20 ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 20 6 27 6 41 6 34 6 55 6 13 6 83 6 90 6 6 6 69 6 48 6 76 6 62 6 97 6 19 5 5 5 26 5 61 5 12 5 68 5 dtype : int64","title":"pd.Series.nlargest"},{"location":"api_docs/pandas/series/#pdseriesnsmallest","text":"pandas.Series. nsmallest (n=5, keep='first') Supported Arguments argument datatypes n Integer Note Series type must be numeric Example Usage >>> @bodo . jit ... def f ( S ): ... return S . nsmallest ( 20 ) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 63 0 7 0 56 0 98 0 77 0 91 0 49 0 42 0 35 0 84 0 28 0 21 0 70 0 0 0 14 0 43 1 1 1 57 1 15 1 36 1 dtype : int64","title":"pd.Series.nsmallest"},{"location":"api_docs/pandas/series/#pdseriespct_change","text":"pandas.Series. pct_change (periods=1, fill_method='pad', limit=None, freq=None) Supported Arguments argument datatypes periods Integer Note Series type must be numeric Bodo does not accept any additional arguments to pass to shift Example Usage >>> @bodo . jit ... def f ( S ): ... return S . pct_change ( 3 ) >>> S = ( pd . Series ( np . arange ( 100 )) % 7 ) + 1 >>> f ( S ) 0 NaN 1 NaN 2 NaN 3 3.000000 4 1.500000 ... 95 1.500000 96 1.000000 97 0.750000 98 - 0.800000 99 - 0.666667 Length : 100 , dtype : float64","title":"pd.Series.pct_change"},{"location":"api_docs/pandas/series/#pdseriesprod","text":"pandas.Series. prod (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . prod () >>> S = ( pd . Series ( np . arange ( 20 )) % 3 ) + 1 >>> f ( S ) 93312","title":"pd.Series.prod"},{"location":"api_docs/pandas/series/#pdseriesproduct","text":"pandas.Series. product (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . product () >>> S = ( pd . Series ( np . arange ( 20 )) % 3 ) + 1 >>> f ( S ) 93312","title":"pd.Series.product"},{"location":"api_docs/pandas/series/#pdseriesquantile","text":"pandas.Series. quantile (q=0.5, interpolation='linear') Supported Arguments argument datatypes q Float in [0.0, 1.0] Iterable of floats in [0.0, 1.0] Example Usage >>> @bodo . jit ... def f ( S ): ... return S . quantile ([ 0.25 , 0.5 , 0.75 ]) >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0.25 1.0 0.50 3.0 0.75 5.0 dtype : float64","title":"pd.Series.quantile"},{"location":"api_docs/pandas/series/#pdseriessem","text":"pandas.Series. sem (axis=None, skipna=None, level=None, ddof=1, numeric_only=None) Supported Arguments argument datatypes skipna Boolean ddof Integer Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . sem () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0.20219752318917852","title":"pd.Series.sem"},{"location":"api_docs/pandas/series/#pdseriesskew","text":"pandas.Series. skew (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . skew () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0.032074996591991714","title":"pd.Series.skew"},{"location":"api_docs/pandas/series/#pdseriesstd","text":"pandas.Series. std (axis=None, skipna=None, level=None, ddof=1, numeric_only=None) Supported Arguments argument datatypes skipna Boolean ddof Integer Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . std () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 2.021975231891785","title":"pd.Series.std"},{"location":"api_docs/pandas/series/#pdseriessum","text":"pandas.Series. sum (axis=None, skipna=None, level=None, numeric_only=None, min_count=0) Supported Arguments argument datatypes skipna Boolean min_count Integer Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . sum () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 295","title":"pd.Series.sum"},{"location":"api_docs/pandas/series/#pdseriesvar","text":"pandas.Series. var (axis=None, skipna=None, level=None, ddof=1, numeric_only=None) Supported Arguments argument datatypes skipna Boolean ddof Integer Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . var () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 4.088383838383838","title":"pd.Series.var"},{"location":"api_docs/pandas/series/#pdserieskurtosis","text":"pandas.Series. kurtosis (axis=None, skipna=None, level=None, numeric_only=None) Supported Arguments argument datatypes skipna Boolean Note Series type must be numeric Bodo does not accept any additional arguments to pass to the function Example Usage >>> @bodo . jit ... def f ( S ): ... return S . kurtosis () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) - 1.269562153611973","title":"pd.Series.kurtosis"},{"location":"api_docs/pandas/series/#pdseriesunique","text":"pandas.Series. unique () Note The output is assumed to be \"small\" relative to input and is replicated. Use Series.drop_duplicates() if the output should remain distributed. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . unique () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) [ 0 1 2 3 4 5 6 ]","title":"pd.Series.unique"},{"location":"api_docs/pandas/series/#pdseriesnunique","text":"pandas.Series. nunique (dropna=True) Supported Arguments argument datatypes dropna Boolean Example Usage >>> @bodo . jit ... def f ( S ): ... return S . nunique () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 7","title":"pd.Series.nunique"},{"location":"api_docs/pandas/series/#pdseriesis_monotonic","text":"++pandas.Series. is_monotonic Example Usage >>> @bodo . jit ... def f ( S ): ... return S . is_monotonic >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) True","title":"pd.Series.is_monotonic"},{"location":"api_docs/pandas/series/#pdseriesis_monotonic_increasing","text":"++pandas.Series. is_monotonic_increasing Example Usage >>> @bodo . jit ... def f ( S ): ... return S . is_monotonic_increasing >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) True","title":"pd.Series.is_monotonic_increasing"},{"location":"api_docs/pandas/series/#pdseriesis_monotonic_decreasing","text":"++pandas.Series. is_monotonic_decreasing Example Usage >>> @bodo . jit ... def f ( S ): ... return S . is_monotonic_decreasing >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) False","title":"pd.Series.is_monotonic_decreasing"},{"location":"api_docs/pandas/series/#pdseriesvalue_counts","text":"pandas.Series. value_counts (normalize=False, sort=True, ascending=False, bins=None, dropna=True) Supported Arguments argument datatypes other requirements normalize Boolean Must be constant at Compile Time sort Boolean Must be constant at Compile Time ascending Boolean bins Integer Array-like of integers Example Usage >>> @bodo . jit ... def f ( S ): ... return S . value_counts () >>> S = pd . Series ( np . arange ( 100 )) % 7 >>> f ( S ) 0 15 1 15 2 14 3 14 4 14 5 14 6 14 dtype : int64","title":"pd.Series.value_counts"},{"location":"api_docs/pandas/series/#reindexing-selection-label-manipulation","text":"","title":"Reindexing / Selection / Label manipulation"},{"location":"api_docs/pandas/series/#pdseriesdrop_duplicates","text":"pandas.Series. drop_duplicates (keep='first', inplace=False) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . drop_duplicates () >>> S = pd . Series ( np . arange ( 100 )) % 10 >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 dtype : int64","title":"pd.Series.drop_duplicates"},{"location":"api_docs/pandas/series/#pdseriesduplicated","text":"pandas.Series. duplicated (keep='first') Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . duplicated () > >>> S = pd . Series ([ 1 , 2 , 1 , np . nan , 3 , 2 , np . nan , 4 ]) 0 False 1 False 2 True 3 False 4 False 5 True 6 True 7 False dtype : bool","title":"pd.Series.duplicated"},{"location":"api_docs/pandas/series/#pdseriesequals","text":"pandas.Series. equals (other) Supported Arguments argument datatypes other Series Note Series and other must contain scalar values in each row Example Usage >>> @bodo . jit ... def f ( S , other ): ... return S . equals ( other ) >>> S = pd . Series ( np . arange ( 100 )) % 10 >>> other = pd . Series ( np . arange ( 100 )) % 5 >>> f ( S , other ) False","title":"pd.Series.equals"},{"location":"api_docs/pandas/series/#pdseriesfirst","text":"pandas.Series. first (offset) Supported Arguments argument datatypes other requirements offset String or Offset type String argument be a valid frequency alias Note Series must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted. Example Usage >>> @bodo . jit ... def f ( S , offset ): ... return S . first ( offset ) >>> S = pd . Series ( np . arange ( 100 ), index = pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 100 )) >>> f ( S , \"2M\" ) 2022 - 01 - 01 00 : 00 : 00.000000000 0 2022 - 01 - 12 01 : 27 : 16.363636363 1 2022 - 01 - 23 02 : 54 : 32.727272727 2 2022 - 02 - 03 04 : 21 : 49.090909091 3 2022 - 02 - 14 05 : 49 : 05.454545454 4 2022 - 02 - 25 07 : 16 : 21.818181818 5 dtype : int64","title":"pd.Series.first"},{"location":"api_docs/pandas/series/#pdserieshead","text":"pandas.Series. head (n=5) Supported Arguments argument datatypes n Integer Example Usage >>> @bodo . jit ... def f ( S ): ... return S . head ( 10 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 dtype : int64","title":"pd.Series.head"},{"location":"api_docs/pandas/series/#pdseriesidxmax","text":"pandas.Series. idxmax (axis=0, skipna=True) Supported Arguments None Note Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . idxmax () >>> S = pd . Series ( np . arange ( 100 )) >>> S [( S % 3 == 0 )] = 100 >>> f ( S ) 0","title":"pd.Series.idxmax"},{"location":"api_docs/pandas/series/#pdseriesidxmin","text":"pandas.Series. idxmin (axis=0, skipna=True) Supported Arguments None Note Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . idxmin () >>> S = pd . Series ( np . arange ( 100 )) >>> S [( S % 3 == 0 )] = 100 >>> f ( S ) 1","title":"pd.Series.idxmin"},{"location":"api_docs/pandas/series/#pdseriesisin","text":"pandas.Series. isin (values) Supported Arguments argument datatypes values Series Array List Note values argument supports both distributed array/Series and replicated list/array/Series Example Usage >>> @bodo . jit ... def f ( S ): ... return S . isin ([ 3 , 11 , 98 ]) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 False 1 False 2 False 3 True 4 False ... 95 False 96 False 97 False 98 True 99 False Length : 100 , dtype : bool","title":"pd.Series.isin"},{"location":"api_docs/pandas/series/#pdserieslast","text":"pandas.Series. last (offset) Supported Arguments argument datatypes other requirements offset String or Offset type String argument be a valid frequency alias Note Series must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted. Example Usage >>> @bodo . jit ... def f ( S , offset ): ... return S . last ( offset ) >>> S = pd . Series ( np . arange ( 100 ), index = pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 100 )) >>> f ( S , \"2M\" ) 2024 - 11 - 05 16 : 43 : 38.181818176 94 2024 - 11 - 16 18 : 10 : 54.545454544 95 2024 - 11 - 27 19 : 38 : 10.909090912 96 2024 - 12 - 08 21 : 05 : 27.272727264 97 2024 - 12 - 19 22 : 32 : 43.636363632 98 2024 - 12 - 31 00 : 00 : 00.000000000 99 dtype : int64","title":"pd.Series.last"},{"location":"api_docs/pandas/series/#pdseriesrename","text":"pandas.Series. rename (index=None, , axis=None, copy=True, inplace=False, level=None, errors='ignore') ***Supported Arguments** argument datatypes index String axis Any value. Bodo ignores this argument entirely, which is consistent with Pandas. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . rename ( \"a\" ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0 1 1 2 2 3 3 4 4 .. 95 95 96 96 97 97 98 98 99 99 Name : a , Length : 100 , dtype : int64","title":"pd.Series.rename"},{"location":"api_docs/pandas/series/#pdseriesreset_index","text":"pandas.Series. reset_index (level=None, drop=False, name=None, inplace=False) Supported Arguments argument datatypes other requirements level Integer Boolean Must be constant at Compile Time drop Boolean Must be constant at Compile Time If False , Index name must be known at compilation time Note For MultiIndex case, only dropping all levels is supported. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . reset_index () >>> S = pd . Series ( np . arange ( 100 ), index = pd . RangeIndex ( 100 , 200 , 1 , name = \"b\" )) >>> f ( S ) b 0 0 100 0 1 101 1 2 102 2 3 103 3 4 104 4 .. ... .. 95 195 95 96 196 96 97 197 97 98 198 98 99 199 99 > [ 100 rows x 2 columns ]","title":"pd.Series.reset_index"},{"location":"api_docs/pandas/series/#pdseriestake","text":"pandas.Series. take (indices, axis=0, is_copy=None) Supported Arguments argument datatypes other requirements indices Array like with integer data To have distributed data indices must be an array with the same distribution as S. Note Bodo does not accept any additional arguments for Numpy compatibility Example Usage >>> @bodo . jit ... def f ( S ): ... return S . take ([ 2 , 7 , 4 , 19 ]) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 2 2 7 7 4 4 19 19 dtype : int64","title":"pd.Series.take"},{"location":"api_docs/pandas/series/#pdseriestail","text":"pandas.Series. tail (n=5) Supported Arguments argument datatypes n Integer Example Usage >>> @bodo . jit ... def f ( S ): ... return S . tail ( 10 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 90 90 91 91 92 92 93 93 94 94 95 95 96 96 97 97 98 98 99 99 dtype : int64","title":"pd.Series.tail"},{"location":"api_docs/pandas/series/#pdserieswhere","text":"pandas.Series. where (cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=NoDefault.no_default) Supported Arguments argument datatypes cond boolean array 1d bool numpy array other 1d numpy array scalar Note Series can contain categorical data if other is a scalar Example Usage >>> @bodo . jit ... def f ( S ): ... return S . where (( S % 3 ) != 0 , 0 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0 1 1 2 2 3 0 4 4 .. 95 95 96 0 97 97 98 98 99 0 Length : 100 , dtype : int64","title":"pd.Series.where"},{"location":"api_docs/pandas/series/#pdseriesmask","text":"pandas.Series. mask (cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=NoDefault.no_default) Supported Arguments argument datatypes cond boolean array 1d bool numpy array other 1d numpy array scalar Note Series can contain categorical data if other is a scalar Example Usage >>> @bodo . jit ... def f ( S ): ... return S . mask (( S % 3 ) != 0 , 0 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0 1 0 2 0 3 3 4 0 .. 95 0 96 96 97 0 98 0 99 99 Length : 100 , dtype : int64","title":"pd.Series.mask"},{"location":"api_docs/pandas/series/#missing-data-handling","text":"","title":"Missing data handling"},{"location":"api_docs/pandas/series/#pdseriesbackfill","text":"pandas.Series. backfill (axis=None, inplace=False, limit=None, downcast=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . backfill () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 1 1 1 2 - 2 3 - 2 4 5 5 5 6 < NA > dtype : Int64","title":"pd.Series.backfill"},{"location":"api_docs/pandas/series/#pdseriesbfill","text":"pandas.Series. bfill (axis=None, inplace=False, limit=None, downcast=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . bfill () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 1 1 1 2 - 2 3 - 2 4 5 5 5 6 < NA > dtype : Int64","title":"pd.Series.bfill"},{"location":"api_docs/pandas/series/#pdseriesdropna","text":"pandas.Series. dropna (axis=0, inplace=False, how=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dropna () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 1 1 3 - 2 5 5 dtype : Int64","title":"pd.Series.dropna"},{"location":"api_docs/pandas/series/#pdseriesffill","text":"pandas.Series. ffill (axis=None, inplace=False, limit=None, downcast=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . ffill () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 < NA > 1 1 2 1 3 - 2 4 - 2 5 5 6 5 dtype : Int64","title":"pd.Series.ffill"},{"location":"api_docs/pandas/series/#pdseriesfillna","text":"pandas.Series. fillna (value=None, method=None, axis=None, inplace=False, limit=None, downcast=None) Supported Arguments argument datatypes other requirements value Scalar method One of (\"bfill\", \"backfill\", \"ffill\", and \"pad\") Must be constant at Compile Time inplace Boolean Must be constant at Compile Time If value is provided then method must be None and vice-versa If method is provided then inplace must be False Example Usage >>> @bodo . jit ... def f ( S ): ... return S . fillna ( - 1 ) >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 - 1 1 1 2 - 1 3 - 2 4 - 1 5 5 6 - 1 dtype : Int64","title":"pd.Series.fillna"},{"location":"api_docs/pandas/series/#pdseriesisna","text":"pandas.Series. isna () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . isna () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 True 1 False 2 True 3 False 4 True 5 False 6 True dtype : bool","title":"pd.Series.isna"},{"location":"api_docs/pandas/series/#pdseriesisnull","text":"pandas.Series. isnull () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . isnull () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 True 1 False 2 True 3 False 4 True 5 False 6 True dtype : bool","title":"pd.Series.isnull"},{"location":"api_docs/pandas/series/#pdseriesnotna","text":"pandas.Series. notna () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . notna () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 False 1 True 2 False 3 True 4 False 5 True 6 False dtype : bool","title":"pd.Series.notna"},{"location":"api_docs/pandas/series/#pdseriesnotnull","text":"pandas.Series. notnull () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . notnull () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 False 1 True 2 False 3 True 4 False 5 True 6 False dtype : bool","title":"pd.Series.notnull"},{"location":"api_docs/pandas/series/#pdseriespad","text":"pandas.Series. pad (axis=None, inplace=False, limit=None, downcast=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . pad () >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S ) 0 < NA > 1 1 2 1 3 - 2 4 - 2 5 5 6 5 dtype : Int64","title":"pd.Series.pad"},{"location":"api_docs/pandas/series/#pdseriesreplace","text":"pandas.Series. replace (to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad') Supported Arguments argument datatypes other requirements to_replace Scalar List of Scalars Dictionary mapping scalars of the same type value Scalar If to_replace is not a scalar, value must be None Example Usage >>> @bodo . jit ... def f ( S , replace_dict ): ... return S . replace ( replace_dict ) >>> S = pd . Series ( pd . array ([ None , 1 , None , - 2 , None , 5 , None ])) >>> f ( S , { 1 : - 2 , - 2 : 5 , 5 : 27 }) 0 < NA > 1 - 2 2 < NA > 3 5 4 < NA > 5 27 6 < NA > dtype : Int64","title":"pd.Series.replace"},{"location":"api_docs/pandas/series/#reshaping-sorting","text":"","title":"Reshaping, sorting"},{"location":"api_docs/pandas/series/#pdseriesargsort","text":"pandas.Series. argsort (axis=0, kind='quicksort', order=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . sort_values () >>> S = pd . Series ( np . arange ( 99 , - 1 , - 1 ), index = np . arange ( 100 )) >>> f ( S ) 0 99 1 98 2 97 3 96 4 95 .. 95 4 96 3 97 2 98 1 99 0 Length : 100 , dtype : int64","title":"pd.Series.argsort"},{"location":"api_docs/pandas/series/#pdseriessort_values","text":"pandas.Series. sort_values (axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None) Supported Arguments argument datatypes other requirements ascending Boolean na_position One of (\"first\", \"last\") Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( S ): ... return S . sort_values () >>> S = pd . Series ( np . arange ( 99 , - 1 , - 1 ), index = np . arange ( 100 )) >>> f ( S ) 99 0 98 1 97 2 96 3 95 4 .. 4 95 3 96 2 97 1 98 0 99 Length : 100 , dtype : int64","title":"pd.Series.sort_values"},{"location":"api_docs/pandas/series/#pdseriessort_index","text":"pandas.Series. sort_index (axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None) Supported Arguments argument datatypes other requirements ascending Boolean na_position One of (\"first\", \"last\") Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( S ): ... return S . sort_index () >>> S = pd . Series ( np . arange ( 100 ), index = np . arange ( 99 , - 1 , - 1 )) >>> f ( S ) 0 99 1 98 2 97 3 96 4 95 .. 95 4 96 3 97 2 98 1 99 0 Length : 100 , dtype : int64","title":"pd.Series.sort_index"},{"location":"api_docs/pandas/series/#pdseriesexplode","text":"pandas.Series. explode (ignore_index=False) Supported Arguments None Note Bodo's output type may differ from Pandas because Bodo must convert to a nullable type at compile time. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . explode () >>> S = pd . Series ([ np . arange ( i ) for i in range ( 10 )]) >>> f ( S ) 0 < NA > 1 0 2 0 2 1 3 0 3 1 3 2 4 0 4 1 4 2 4 3 5 0 5 1 5 2 5 3 5 4 6 0 6 1 6 2 6 3 6 4 6 5 7 0 7 1 7 2 7 3 7 4 7 5 7 6 8 0 8 1 8 2 8 3 8 4 8 5 8 6 8 7 9 0 9 1 9 2 9 3 9 4 9 5 9 6 9 7 9 8 dtype : Int64","title":"pd.Series.explode"},{"location":"api_docs/pandas/series/#pdseriesrepeat","text":"pandas.Series. repeat (repeats, axis=None) Supported Arguments argument datatypes repeats Integer Array-like of integers the same length as the Series Example Usage >>> @bodo . jit ... def f ( S ): ... return S . repeat ( 3 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 0 0 0 0 0 1 1 1 1 .. 98 98 98 98 99 99 99 99 99 99 Length : 300 , dtype : int64","title":"pd.Series.repeat"},{"location":"api_docs/pandas/series/#combining-comparing-joining-merging","text":"","title":"Combining / comparing / joining / merging"},{"location":"api_docs/pandas/series/#pdseriesappend","text":"pandas.Series. append (to_append, ignore_index=False, verify_integrity=False) Supported Arguments argument datatypes other requirements to_append Series List of Series Tuple of Series ignore_index Boolean Must be constant at Compile Time Note Setting a name for the output Series is not supported yet Important Bodo currently concatenates local data chunks for distributed datasets, which does not preserve global order of concatenated objects in output. Example Usage >>> @bodo . jit ... def f ( S1 , S2 ): ... return S1 . append ( S2 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S , S ) 0 0 1 1 2 2 3 3 4 4 .. 95 95 96 96 97 97 98 98 99 99 Length : 200 , dtype : int64","title":"pd.Series.append"},{"location":"api_docs/pandas/series/#time-series-related","text":"","title":"Time series-related"},{"location":"api_docs/pandas/series/#pdseriesshift","text":"pandas.Series. shift (periods=1, freq=None, axis=0, fill_value=None) Supported Arguments argument datatypes periods Integer Note This data type for the series must be one of: - Integer - Float - Boolean - datetime.data - datetime64 - timedelta64 - string Example Usage >>> @bodo . jit ... def f ( S ): ... return S . shift ( 1 ) >>> S = pd . Series ( np . arange ( 100 )) >>> f ( S ) 0 NaN 1 0.0 2 1.0 3 2.0 4 3.0 ... 95 94.0 96 95.0 97 96.0 98 97.0 99 98.0 Length : 100 , dtype : float64","title":"pd.Series.shift"},{"location":"api_docs/pandas/series/#datetime-properties","text":"","title":"Datetime properties"},{"location":"api_docs/pandas/series/#pdseriesdtdate","text":"++pandas.Series.dt. date Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . date >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 2022 - 01 - 01 1 2022 - 01 - 01 2 2022 - 01 - 01 3 2022 - 01 - 01 4 2022 - 01 - 02 5 2022 - 01 - 02 6 2022 - 01 - 02 7 2022 - 01 - 03 8 2022 - 01 - 03 9 2022 - 01 - 03 10 2022 - 01 - 04 11 2022 - 01 - 04 12 2022 - 01 - 04 13 2022 - 01 - 05 14 2022 - 01 - 05 15 2022 - 01 - 05 16 2022 - 01 - 05 17 2022 - 01 - 06 18 2022 - 01 - 06 19 2022 - 01 - 06 20 2022 - 01 - 07 21 2022 - 01 - 07 22 2022 - 01 - 07 23 2022 - 01 - 08 24 2022 - 01 - 08 25 2022 - 01 - 08 26 2022 - 01 - 09 27 2022 - 01 - 09 28 2022 - 01 - 09 29 2022 - 01 - 10 dtype : object","title":"`pd.Series.dt.date"},{"location":"api_docs/pandas/series/#pdseriesdtyear","text":"pandas.Series.dt. year Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . year >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 2022 1 2022 2 2022 3 2022 4 2022 5 2022 6 2022 7 2022 8 2022 9 2022 10 2023 11 2023 12 2023 13 2023 14 2023 15 2023 16 2023 17 2023 18 2023 19 2023 20 2024 21 2024 22 2024 23 2024 24 2024 25 2024 26 2024 27 2024 28 2024 29 2025 dtype : Int64","title":"pd.Series.dt.year"},{"location":"api_docs/pandas/series/#pdseriesdtmonth","text":"pandas.Series.dt. month Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . month >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 1 1 2 2 3 3 4 4 6 5 7 6 8 7 9 8 11 9 12 10 1 11 2 12 4 13 5 14 6 15 7 16 9 17 10 18 11 19 12 20 2 21 3 22 4 23 5 24 7 25 8 26 9 27 10 28 12 29 1 dtype : Int64","title":"pd.Series.dt.month"},{"location":"api_docs/pandas/series/#pdseriesdtday","text":"pandas.Series.dt. day Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . day >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 1 1 8 2 18 3 25 4 2 5 10 6 17 7 24 8 1 9 9 10 17 11 24 12 3 13 11 14 18 15 26 16 2 17 10 18 17 19 25 20 2 21 11 22 18 23 26 24 3 25 10 26 17 27 25 28 2 29 10 dtype : Int64","title":"pd.Series.dt.day"},{"location":"api_docs/pandas/series/#pdseriesdthour","text":"pandas.Series.dt. hour Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . hour >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 0 1 2 2 4 3 7 4 9 5 12 6 14 7 17 8 19 9 22 10 0 11 3 12 5 13 8 14 10 15 13 16 15 17 18 18 20 19 23 20 1 21 4 22 6 23 9 24 11 25 14 26 16 27 19 28 21 29 0 dtype : Int64","title":"pd.Series.dt.hour"},{"location":"api_docs/pandas/series/#pdseriesdtminute","text":"pandas.Series.dt. minute Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . minute >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 0 1 28 2 57 3 26 4 55 5 24 6 53 7 22 8 51 9 20 10 49 11 18 12 47 13 16 14 45 15 14 16 43 17 12 18 41 19 10 20 39 21 8 22 37 23 6 24 35 25 4 26 33 27 2 28 31 29 0 dtype : Int64","title":"pd.Series.dt.minute"},{"location":"api_docs/pandas/series/#pdseriesdtsecond","text":"pandas.Series.dt. second Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . second >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 0 1 57 2 55 3 53 4 51 5 49 6 47 7 45 8 43 9 41 10 39 11 37 12 35 13 33 14 31 15 28 16 26 17 24 18 22 19 20 20 18 21 16 22 14 23 12 24 10 25 8 26 6 27 4 28 2 29 0 dtype : Int64","title":"pd.Series.dt.second"},{"location":"api_docs/pandas/series/#pdseriesdtmicrosecond","text":"pandas.Series.dt. microsecond Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . microsecond >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 0 1 931034 2 862068 3 793103 4 724137 5 655172 6 586206 7 517241 8 448275 9 379310 10 310344 11 241379 12 172413 13 103448 14 34482 15 965517 16 896551 17 827586 18 758620 19 689655 20 620689 21 551724 22 482758 23 413793 24 344827 25 275862 26 206896 27 137931 28 68965 29 0 dtype : Int64","title":"pd.Series.dt.microsecond"},{"location":"api_docs/pandas/series/#pdseriesdtnanosecond","text":"pandas.Series.dt. nanosecond Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . nanosecond >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 0 1 483 2 966 3 448 4 932 5 416 6 896 7 380 8 864 9 348 10 832 11 312 12 792 13 280 14 760 15 248 16 728 17 208 18 696 19 176 20 664 21 144 22 624 23 104 24 584 25 80 26 560 27 40 28 520 29 0 dtype : Int64","title":"pd.Series.dt.nanosecond"},{"location":"api_docs/pandas/series/#pdseriesdtweek","text":"pandas.Series.dt. week Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . week >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 52 1 6 2 11 3 17 4 22 5 27 6 33 7 38 8 44 9 49 10 3 11 8 12 14 13 19 14 24 15 30 16 35 17 41 18 46 19 52 20 5 21 11 22 16 23 21 24 27 25 32 26 38 27 43 28 49 29 2 dtype : Int64","title":"pd.Series.dt.week"},{"location":"api_docs/pandas/series/#pdseriesdtweekofyear","text":"pandas.Series.dt. weekofyear Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . weekofyear >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 52 1 6 2 11 3 17 4 22 5 27 6 33 7 38 8 44 9 49 10 3 11 8 12 14 13 19 14 24 15 30 16 35 17 41 18 46 19 52 20 5 21 11 22 16 23 21 24 27 25 32 26 38 27 43 28 49 29 2 dtype : Int64","title":"pd.Series.dt.weekofyear"},{"location":"api_docs/pandas/series/#pdseriesdtday_of_week","text":"pandas.Series.dt. day_of_week Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . day_of_week >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 5 1 1 2 4 3 0 4 3 5 6 6 2 7 5 8 1 9 4 10 1 11 4 12 0 13 3 14 6 15 2 16 5 17 1 18 4 19 0 20 4 21 0 22 3 23 6 24 2 25 5 26 1 27 4 28 0 29 4 dtype : Int64","title":"pd.Series.dt.day_of_week"},{"location":"api_docs/pandas/series/#pdseriesdtweekday","text":"pandas.Series.dt. weekday Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . weekday >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 5 1 1 2 4 3 0 4 3 5 6 6 2 7 5 8 1 9 4 10 1 11 4 12 0 13 3 14 6 15 2 16 5 17 1 18 4 19 0 20 4 21 0 22 3 23 6 24 2 25 5 26 1 27 4 28 0 29 4 dtype : Int64","title":"pd.Series.dt.weekday"},{"location":"api_docs/pandas/series/#pdseriesdtdayofyear","text":"pandas.Series.dt. dayofyear Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . dayofyear >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 1 1 39 2 77 3 115 4 153 5 191 6 229 7 267 8 305 9 343 10 17 11 55 12 93 13 131 14 169 15 207 16 245 17 283 18 321 19 359 20 33 21 71 22 109 23 147 24 185 25 223 26 261 27 299 28 337 29 10 dtype : Int64","title":"pd.Series.dt.dayofyear"},{"location":"api_docs/pandas/series/#pdseriesdtday_of_year","text":"pandas.Series.dt. day_of_year Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . day_of_year >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 1 1 39 2 77 3 115 4 153 5 191 6 229 7 267 8 305 9 343 10 17 11 55 12 93 13 131 14 169 15 207 16 245 17 283 18 321 19 359 20 33 21 71 22 109 23 147 24 185 25 223 26 261 27 299 28 337 29 10 dtype : Int64","title":"pd.Series.dt.day_of_year"},{"location":"api_docs/pandas/series/#pdseriesdtquarter","text":"pandas.Series.dt. quarter Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . quarter >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 1 1 1 2 1 3 2 4 2 5 3 6 3 7 3 8 4 9 4 10 1 11 1 12 2 13 2 14 2 15 3 16 3 17 4 18 4 19 4 20 1 21 1 22 2 23 2 24 3 25 3 26 3 27 4 28 4 29 1 dtype : Int64","title":"pd.Series.dt.quarter"},{"location":"api_docs/pandas/series/#pdseriesdtis_month_start","text":"pandas.Series.dt. is_month_start Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_month_start >>> SS = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 True 1 False 2 False 3 False 4 True 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 True 26 False 27 False 28 False 29 False dtype : bool","title":"pd.Series.dt.is_month_start"},{"location":"api_docs/pandas/series/#pdseriesdtis_month_end","text":"pandas.Series.dt. is_month_end Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_month_end >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 False 1 False 2 False 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 True dtype : bool","title":"pd.Series.dt.is_month_end"},{"location":"api_docs/pandas/series/#pdseriesdtis_quarter_start","text":"pandas.Series.dt. is_quarter_start Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_quarter_start >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 True 1 False 2 False 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 False dtype : bool","title":"pd.Series.dt.is_quarter_start"},{"location":"api_docs/pandas/series/#pdseriesdtis_quarter_end","text":"pandas.Series.dt. is_quarter_end Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_quarter_end >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 False 1 False 2 False 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 True dtype : bool","title":"pd.Series.dt.is_quarter_end"},{"location":"api_docs/pandas/series/#pdseriesdtis_year_start","text":"pandas.Series.dt. is_year_start Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_year_start >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 True 1 False 2 False 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 False dtype : bool","title":"pd.Series.dt.is_year_start"},{"location":"api_docs/pandas/series/#pdseriesdtis_year_end","text":"pandas.Series.dt. is_year_end Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . is_year_end >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 False 1 False 2 False 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 True dtype : bool","title":"pd.Series.dt.is_year_end"},{"location":"api_docs/pandas/series/#pdseriesdtdaysinmonth","text":"pandas.Series.dt. daysinmonth Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . daysinmonth >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 31 1 28 2 31 3 30 4 30 5 31 6 31 7 30 8 31 9 31 10 31 11 28 12 31 13 31 14 30 15 31 16 31 17 31 18 30 19 31 20 31 21 31 22 30 23 31 24 30 25 31 26 30 27 31 28 30 29 31 dtype : Int64","title":"pd.Series.dt.daysinmonth"},{"location":"api_docs/pandas/series/#pdseriesdtdays_in_month","text":"pandas.Series.dt. days_in_month Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . days_in_month >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '12/31/2024' , periods = 30 )) >>> f ( S ) 0 31 1 28 2 31 3 30 4 30 5 31 6 31 7 30 8 31 9 31 10 31 11 28 12 31 13 31 14 30 15 31 16 31 17 31 18 30 19 31 20 31 21 31 22 30 23 31 24 30 25 31 26 30 27 31 28 30 29 31 dtype : Int64","title":"pd.Series.dt.days_in_month"},{"location":"api_docs/pandas/series/#datetime-methods","text":"","title":"Datetime methods"},{"location":"api_docs/pandas/series/#pdseriesdtnormalize","text":"pandas.Series.dt. normalize () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . normalize () >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 2022 - 01 - 01 1 2022 - 01 - 01 2 2022 - 01 - 01 3 2022 - 01 - 01 4 2022 - 01 - 02 5 2022 - 01 - 02 6 2022 - 01 - 02 7 2022 - 01 - 03 8 2022 - 01 - 03 9 2022 - 01 - 03 10 2022 - 01 - 04 11 2022 - 01 - 04 12 2022 - 01 - 04 13 2022 - 01 - 05 14 2022 - 01 - 05 15 2022 - 01 - 05 16 2022 - 01 - 05 17 2022 - 01 - 06 18 2022 - 01 - 06 19 2022 - 01 - 06 20 2022 - 01 - 07 21 2022 - 01 - 07 22 2022 - 01 - 07 23 2022 - 01 - 08 24 2022 - 01 - 08 25 2022 - 01 - 08 26 2022 - 01 - 09 27 2022 - 01 - 09 28 2022 - 01 - 09 29 2022 - 01 - 10 dtype : datetime64 [ ns ]","title":"pd.Series.dt.normalize"},{"location":"api_docs/pandas/series/#pdseriesdtstrftime","text":"pandas.Series.dt. strftime (date_format) Supported Arguments argument datatypes other requirements date_format String Must be a valid datetime format string Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . strftime ( \"%B %d , %Y, %r \" ) >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 January 01 , 2022 , 12 : 00 : 00 AM 1 January 01 , 2022 , 07 : 26 : 53 AM 2 January 01 , 2022 , 02 : 53 : 47 PM 3 January 01 , 2022 , 10 : 20 : 41 PM 4 January 02 , 2022 , 05 : 47 : 35 AM 5 January 02 , 2022 , 01 : 14 : 28 PM 6 January 02 , 2022 , 08 : 41 : 22 PM 7 January 03 , 2022 , 04 : 08 : 16 AM 8 January 03 , 2022 , 11 : 35 : 10 AM 9 January 03 , 2022 , 07 : 02 : 04 PM 10 January 04 , 2022 , 02 : 28 : 57 AM 11 January 04 , 2022 , 09 : 55 : 51 AM 12 January 04 , 2022 , 05 : 22 : 45 PM 13 January 05 , 2022 , 12 : 49 : 39 AM 14 January 05 , 2022 , 08 : 16 : 33 AM 15 January 05 , 2022 , 03 : 43 : 26 PM 16 January 05 , 2022 , 11 : 10 : 20 PM 17 January 06 , 2022 , 06 : 37 : 14 AM 18 January 06 , 2022 , 02 : 04 : 08 PM 19 January 06 , 2022 , 09 : 31 : 02 PM 20 January 07 , 2022 , 04 : 57 : 55 AM 21 January 07 , 2022 , 12 : 24 : 49 PM 22 January 07 , 2022 , 07 : 51 : 43 PM 23 January 08 , 2022 , 03 : 18 : 37 AM 24 January 08 , 2022 , 10 : 45 : 31 AM 25 January 08 , 2022 , 06 : 12 : 24 PM 26 January 09 , 2022 , 01 : 39 : 18 AM 27 January 09 , 2022 , 09 : 06 : 12 AM 28 January 09 , 2022 , 04 : 33 : 06 PM 29 January 10 , 2022 , 12 : 00 : 00 AM dtype : object","title":"pd.Series.dt.strftime"},{"location":"api_docs/pandas/series/#pdseriesdtround","text":"pandas.Series.dt. round (freq, ambiguous='raise', nonexistent='raise') Supported Arguments argument datatypes other requirements freq String Must be a valid fixed frequency alias Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . round ( \"H\" ) >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 2022 - 01 - 01 00 : 00 : 00 1 2022 - 01 - 01 07 : 00 : 00 2 2022 - 01 - 01 15 : 00 : 00 3 2022 - 01 - 01 22 : 00 : 00 4 2022 - 01 - 02 06 : 00 : 00 5 2022 - 01 - 02 13 : 00 : 00 6 2022 - 01 - 02 21 : 00 : 00 7 2022 - 01 - 03 04 : 00 : 00 8 2022 - 01 - 03 12 : 00 : 00 9 2022 - 01 - 03 19 : 00 : 00 10 2022 - 01 - 04 02 : 00 : 00 11 2022 - 01 - 04 10 : 00 : 00 12 2022 - 01 - 04 17 : 00 : 00 13 2022 - 01 - 05 01 : 00 : 00 14 2022 - 01 - 05 08 : 00 : 00 15 2022 - 01 - 05 16 : 00 : 00 16 2022 - 01 - 05 23 : 00 : 00 17 2022 - 01 - 06 07 : 00 : 00 18 2022 - 01 - 06 14 : 00 : 00 19 2022 - 01 - 06 22 : 00 : 00 20 2022 - 01 - 07 05 : 00 : 00 21 2022 - 01 - 07 12 : 00 : 00 22 2022 - 01 - 07 20 : 00 : 00 23 2022 - 01 - 08 03 : 00 : 00 24 2022 - 01 - 08 11 : 00 : 00 25 2022 - 01 - 08 18 : 00 : 00 26 2022 - 01 - 09 02 : 00 : 00 27 2022 - 01 - 09 09 : 00 : 00 28 2022 - 01 - 09 17 : 00 : 00 29 2022 - 01 - 10 00 : 00 : 00 dtype : datetime64 [ ns ]","title":"pd.Series.dt.round"},{"location":"api_docs/pandas/series/#pdseriesdtfloor","text":"pandas.Series.dt. floor (freq, ambiguous='raise', nonexistent='raise') Supported Arguments argument datatypes other requirements freq String Must be a valid fixed frequency alias Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . floor ( \"H\" ) >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 2022 - 01 - 01 00 : 00 : 00 1 2022 - 01 - 01 07 : 00 : 00 2 2022 - 01 - 01 14 : 00 : 00 3 2022 - 01 - 01 22 : 00 : 00 4 2022 - 01 - 02 05 : 00 : 00 5 2022 - 01 - 02 13 : 00 : 00 6 2022 - 01 - 02 20 : 00 : 00 7 2022 - 01 - 03 04 : 00 : 00 8 2022 - 01 - 03 11 : 00 : 00 9 2022 - 01 - 03 19 : 00 : 00 10 2022 - 01 - 04 02 : 00 : 00 11 2022 - 01 - 04 09 : 00 : 00 12 2022 - 01 - 04 17 : 00 : 00 13 2022 - 01 - 05 00 : 00 : 00 14 2022 - 01 - 05 08 : 00 : 00 15 2022 - 01 - 05 15 : 00 : 00 16 2022 - 01 - 05 23 : 00 : 00 17 2022 - 01 - 06 06 : 00 : 00 18 2022 - 01 - 06 14 : 00 : 00 19 2022 - 01 - 06 21 : 00 : 00 20 2022 - 01 - 07 04 : 00 : 00 21 2022 - 01 - 07 12 : 00 : 00 22 2022 - 01 - 07 19 : 00 : 00 23 2022 - 01 - 08 03 : 00 : 00 24 2022 - 01 - 08 10 : 00 : 00 25 2022 - 01 - 08 18 : 00 : 00 26 2022 - 01 - 09 01 : 00 : 00 27 2022 - 01 - 09 09 : 00 : 00 28 2022 - 01 - 09 16 : 00 : 00 29 2022 - 01 - 10 00 : 00 : 00 dtype : datetime64 [ ns ]","title":"pd.Series.dt.floor"},{"location":"api_docs/pandas/series/#pdseriesdtceil","text":"pandas.Series.dt. ceil (freq, ambiguous='raise', nonexistent='raise') Supported Arguments argument datatypes other requirements freq String Must be a valid fixed frequency alias Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . ceil ( \"H\" ) >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 2022 - 01 - 01 00 : 00 : 00 1 2022 - 01 - 01 08 : 00 : 00 2 2022 - 01 - 01 15 : 00 : 00 3 2022 - 01 - 01 23 : 00 : 00 4 2022 - 01 - 02 06 : 00 : 00 5 2022 - 01 - 02 14 : 00 : 00 6 2022 - 01 - 02 21 : 00 : 00 7 2022 - 01 - 03 05 : 00 : 00 8 2022 - 01 - 03 12 : 00 : 00 9 2022 - 01 - 03 20 : 00 : 00 10 2022 - 01 - 04 03 : 00 : 00 11 2022 - 01 - 04 10 : 00 : 00 12 2022 - 01 - 04 18 : 00 : 00 13 2022 - 01 - 05 01 : 00 : 00 14 2022 - 01 - 05 09 : 00 : 00 15 2022 - 01 - 05 16 : 00 : 00 16 2022 - 01 - 06 00 : 00 : 00 17 2022 - 01 - 06 07 : 00 : 00 18 2022 - 01 - 06 15 : 00 : 00 19 2022 - 01 - 06 22 : 00 : 00 20 2022 - 01 - 07 05 : 00 : 00 21 2022 - 01 - 07 13 : 00 : 00 22 2022 - 01 - 07 20 : 00 : 00 23 2022 - 01 - 08 04 : 00 : 00 24 2022 - 01 - 08 11 : 00 : 00 25 2022 - 01 - 08 19 : 00 : 00 26 2022 - 01 - 09 02 : 00 : 00 27 2022 - 01 - 09 10 : 00 : 00 28 2022 - 01 - 09 17 : 00 : 00 29 2022 - 01 - 10 00 : 00 : 00 dtype : datetime64 [ ns ]","title":"pd.Series.dt.ceil"},{"location":"api_docs/pandas/series/#pdseriesdtmonth_name","text":"pandas.Series.dt. month_name (locale=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . month_name () >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2025' , periods = 30 )) >>> f ( S ) 0 January 1 February 2 March 3 April 4 June 5 July 6 August 7 September 8 November 9 December 10 January 11 February 12 April 13 May 14 June 15 July 16 September 17 October 18 November 19 December 20 February 21 March 22 April 23 May 24 July 25 August 26 September 27 October 28 December 29 January dtype : object","title":"pd.Series.dt.month_name"},{"location":"api_docs/pandas/series/#pdseriesdtday_name","text":"pandas.Series.dt. day_name (locale=None) Supported Arguments None Example Usage >>> @bodo . jit ... def f ( S ): ... return S . dt . day_name () >>> S = pd . Series ( pd . date_range ( start = '1/1/2022' , end = '1/10/2022' , periods = 30 )) >>> f ( S ) 0 Saturday 1 Saturday 2 Saturday 3 Saturday 4 Sunday 5 Sunday 6 Sunday 7 Monday 8 Monday 9 Monday 10 Tuesday 11 Tuesday 12 Tuesday 13 Wednesday 14 Wednesday 15 Wednesday 16 Wednesday 17 Thursday 18 Thursday 19 Thursday 20 Friday 21 Friday 22 Friday 23 Saturday 24 Saturday 25 Saturday 26 Sunday 27 Sunday 28 Sunday 29 Monday dtype : object","title":"pd.Series.dt.day_name"},{"location":"api_docs/pandas/series/#string-handling","text":"","title":"String handling"},{"location":"api_docs/pandas/series/#pdseriesstrcapitalize","text":"pandas.Series.str. capitalize () Example Usage >>> @bodo . jit ... def f ( S ): ... return S . str . capitalize () >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 Ce 2 Erw 3 A3 4 @ 5 A n 6 ^ Ef dtype : object","title":"pd.Series.str.capitalize"},{"location":"api_docs/pandas/series/#pdseriesstrcenter","text":"pandas.Series.str. center (width, fillchar=' ') Supported Arguments argument datatypes width Integer fillchar String with a single character Example Usage >>> @bodo . jit ... def f ( S ): ... return S . str . center ( 4 ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 a 1 ce 2 Erw 3 a3 4 @ 5 a n 6 ^ Ef dtype : object","title":"pd.Series.str.center"},{"location":"api_docs/pandas/series/#pdseriesstrcontains","text":"pandas.Series.str. contains (pat, case=True, flags=0, na=None, regex=True) Supported Arguments argument datatypes other requirements pat String case Boolean Must be constant at Compile Time flags Integer regex Boolean Must be constant at Compile Time >>> @bodo . jit ... def f ( S ): ... return S . str . contains ( \"a.+\" ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 False 2 False 3 True 4 False 5 True 6 False dtype : boolean","title":"pd.Series.str.contains"},{"location":"api_docs/pandas/series/#pdseriesstrcount","text":"pandas.Series.str. count (pat, flags=0) Supported Arguments argument datatypes pat String flags Integer >>> @bodo . jit ... def f ( S ): ... return S . str . count ( \"w\" ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 1 1 2 2 3 3 2 4 0 5 2 6 2 dtype : Int64","title":"pd.Series.str.count"},{"location":"api_docs/pandas/series/#pdseriesstrendswith","text":"pandas.Series.str. endswith (pat, na=None) Supported Arguments argument datatypes pat String >>> @bodo . jit ... def f ( S ): ... return S . str . endswith ( \"e\" ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 True 2 False 3 False 4 False 5 False 6 False dtype : boolean","title":"pd.Series.str.endswith"},{"location":"api_docs/pandas/series/#pdseriesstrextract","text":"pandas.Series.str. extract (pat, flags=0, expand=True) Supported Arguments argument datatypes other requirements pat String Must be constant at Compile Time flags Integer Must be constant at Compile Time expand Boolean Must be constant at Compile Time >>> @bodo . jit ... def f ( S ): ... return S . str . extract ( \"(a|e)\" ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 0 a 1 e 2 NaN 3 a 4 NaN 5 a 6 NaN","title":"pd.Series.str.extract"},{"location":"api_docs/pandas/series/#pdseriesstrextractall","text":"pandas.Series.str. extractall (pat, flags=0) Supported Arguments argument datatypes other requirements pat String Must be constant at Compile Time flags Integer Must be constant at Compile Time >>> @bodo . jit ... def f ( S ): ... return S . str . extractall ( \"(a|n)\" ) >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 match 0 0 a 3 0 a 5 0 a 1 n","title":"pd.Series.str.extractall"},{"location":"api_docs/pandas/series/#pdseriesstrfind","text":"pandas.Series.str. find (sub, start=0, end=None) Supported Arguments argument datatypes sub String start Integer end Integer >>> @bodo . jit ... def f ( S ): ... return S . str . find ( \"a3\" , start = 1 ) >>> S = pd . Series ([ \"Aa3\" , \"cea3\" , \"14a3\" , \" a3\" , \"a3@\" , \"a n3\" , \"^ Ea3f\" ]) >>> f ( S ) 0 1 1 2 2 2 3 1 4 - 1 5 - 1 6 3 dtype : Int64","title":"pd.Series.str.find"},{"location":"api_docs/pandas/series/#pdseriesstrget","text":"pandas.Series.str. get (i) Supported Arguments argument datatypes i Integer >>> @bodo . jit ... def f ( S ): ... return S . str . get ( 1 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 NaN 1 e 2 4 3 NaN 4 NaN 5 6 dtype : object","title":"pd.Series.str.get"},{"location":"api_docs/pandas/series/#pdseriesstrjoin","text":"pandas.Series.str. join (sep) Supported Arguments argument datatypes sep String >>> @bodo . jit ... def f ( S ): ... return S . str . join ( \",\" ) >>> S = pd . Series ([[ \"a\" , \"fe\" , \"@23\" ], [ \"a\" , \"b\" ], [], [ \"c\" ]]) >>> f ( S ) 0 a , fe , @ 23 1 a , b 2 3 c dtype : object","title":"pd.Series.str.join"},{"location":"api_docs/pandas/series/#pdseriesstrlen","text":"pandas.Series.str. len () >>> @bodo . jit ... def f ( S ): ... return S . str . len () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 1 1 2 2 2 3 1 4 1 5 3 6 4 dtype : Int64","title":"pd.Series.str.len"},{"location":"api_docs/pandas/series/#pdseriesstrljust","text":"pandas.Series.str. ljust (width, fillchar=' ') Supported Arguments argument datatypes width Integer fillchar String with a single character >>> @bodo . jit ... def f ( S ): ... return S . str . ljust ( 5 , fillchar = \",\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A ,,,, 1 ce ,,, 2 14 ,,, 3 ,,,, 4 @ ,,,, 5 a n ,, 6 ^ Ef , dtype : object","title":"pd.Series.str.ljust"},{"location":"api_docs/pandas/series/#pdseriesstrlower","text":"pandas.Series.str. lower () >>> @bodo . jit ... def f ( S ): ... return S . str . lower () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 a 1 ce 2 14 3 4 @ 5 a n 6 ^ Ef dtype : object","title":"pd.Series.str.lower"},{"location":"api_docs/pandas/series/#pdseriesstrlstrip","text":"pandas.Series.str. lstrip (to_strip=None) Supported Arguments argument datatypes to_strip String >>> @bodo . jit ... def f ( S ): ... return S . str . lstrip ( \"c\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 e 2 14 3 4 @ 5 a n 6 ^ Ef dtype : object","title":"pd.Series.str.lstrip"},{"location":"api_docs/pandas/series/#pdseriesstrpad","text":"pandas.Series.str. pad (width, side='left', fillchar=' ') Supported Arguments argument datatypes other requirements width Integer width One of (\"left\", \"right\", \"both\") Must be constant at Compile Time fillchar String with a single character >>> @bodo . jit ... def f ( S ): ... return S . str . pad ( 5 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 ce 2 14 3 4 @ 5 a n 6 ^ Ef dtype : object","title":"pd.Series.str.pad"},{"location":"api_docs/pandas/series/#pdseriesstrrepeat","text":"pandas.Series.str. repeat (repeats) Supported Arguments argument datatypes other requirements repeats Integer Array Like containing integers If repeats is array like, then it must be the same length as the Series. >>> @bodo . jit ... def f ( S ): ... return S . str . repeat ( 2 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 AA 1 cece 2 1414 3 4 @@ 5 a na n 6 ^ Ef ^ Ef dtype : object","title":"pd.Series.str.repeat"},{"location":"api_docs/pandas/series/#pdseriesstrreplace","text":"pandas.Series.str. replace (pat, repl, n=- 1, case=None, flags=0, regex=None) Supported Arguments regex >>> @bodo . jit ... def f ( S ): ... return S . str . replace ( \"(a|e)\" , \"yellow\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 cyellow 2 14 3 4 @ 5 yellow n 6 ^ Ef dtype : object","title":"pd.Series.str.replace"},{"location":"api_docs/pandas/series/#pdseriesstrrfind","text":"pandas.Series.str. rfind (sub, start=0, end=None) Supported Arguments argument datatypes sub String start Integer end Integer >>> @bodo . jit ... def f ( S ): ... return S . str . rfind ( \"a3\" , start = 1 ) >>> S = pd . Series ([ \"Aa3\" , \"cea3\" , \"14a3\" , \" a3\" , \"a3@\" , \"a n3\" , \"^ Ea3f\" ]) >>> f ( S ) 0 1 1 2 2 2 3 1 4 - 1 5 - 1 6 3 dtype : Int64","title":"pd.Series.str.rfind"},{"location":"api_docs/pandas/series/#pdseriesstrrjist","text":"pandas.Series.str. rjust (width, fillchar=' ') Supported arguments`: argument datatypes width Integer fillchar String with a single character >>> @bodo . jit ... def f ( S ): ... return S . str . rjust ( 10 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 ce 2 14 3 4 @ 5 a n 6 ^ Ef dtype : object","title":"pd.Series.str.rjist"},{"location":"api_docs/pandas/series/#pdseriesstrrestrip","text":"pandas.Series.str. rstrip (to_strip=None) Supported Arguments argument datatypes to_strip String >>> @bodo . jit ... def f ( S ): ... return S . str . rstrip ( \"n\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 ce 2 14 3 4 @ 5 a 6 ^ Ef dtype : object","title":"pd.Series.str.restrip"},{"location":"api_docs/pandas/series/#pdseriesstrslice","text":"pandas.Series.str. slice (start=None, stop=None, step=None) Supported Arguments argument datatypes start Integer stop Integer step Integer >>> @bodo . jit ... def f ( S ): ... return S . str . slice ( 1 , 4 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 c 2 1 3 4 @ 5 a 6 # dtype : object","title":"pd.Series.str.slice"},{"location":"api_docs/pandas/series/#pdseriesstrslice_replace","text":"pandas.Series.str. slice_replace (start=None, stop=None, repl=None) Supported Arguments argument datatypes start Integer stop Integer repl String >>> @bodo . jit ... def f ( S ): ... return S . str . slice_replace ( 1 , 4 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 c 2 1 3 4 @ 5 a 6 # dtype : object","title":"pd.Series.str.slice_replace"},{"location":"api_docs/pandas/series/#pdseriesstrsplit","text":"pandas.Series.str. split (pat=None, n=-1, expand=False) Supported Arguments argument datatypes pat String n Integer >>> @bodo . jit ... def f ( S ): ... return S . str . split ( \" \" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 [ A ] 1 [ ce ] 2 [ 14 ] 3 [, ] 4 [ @ ] 5 [ a , n ] 6 [ #, Ef] dtype : object","title":"pd.Series.str.split"},{"location":"api_docs/pandas/series/#pdseriesstrstartswith","text":"pandas.Series.str. startswith (pat, na=None) Supported Arguments argument datatypes pat String >>> @bodo . jit ... def f ( S ): ... return S . str . startswith ( \"A\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 True 1 False 2 False 3 False 4 False 5 False 6 False dtype : boolean","title":"pd.Series.str.startswith"},{"location":"api_docs/pandas/series/#pdseriesstrstrip","text":"pandas.Series.str. strip (to_strip=None) Supported Arguments argument datatypes to_strip String >>> @bodo . jit ... def f ( S ): ... return S . str . strip ( \"n\" ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 ce 2 14 3 4 @ 5 a 6 ^ Ef dtype : object","title":"pd.Series.str.strip"},{"location":"api_docs/pandas/series/#pdseriesstrswapcase","text":"pandas.Series.str. swapcase () >>> @bodo . jit ... def f ( S ): ... return S . str . swapcase () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 a 1 CE 2 14 3 4 @ 5 A N 6 ^ Ef dtype : object","title":"pd.Series.str.swapcase"},{"location":"api_docs/pandas/series/#pdseriesstrtitle","text":"pandas.Series.str. title () >>> @bodo . jit ... def f ( S ): ... return S . str . title () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 Ce 2 14 3 4 @ 5 A N 6 ^ Ef dtype : object","title":"pd.Series.str.title"},{"location":"api_docs/pandas/series/#pdseriesstrupper","text":"pandas.Series.str. upper () >>> @bodo . jit ... def f ( S ): ... return S . str . upper () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 A 1 CE 2 14 3 4 @ 5 A N 6 ^ Ef dtype : object","title":"pd.Series.str.upper"},{"location":"api_docs/pandas/series/#pdseriesstrzfill","text":"pandas.Series.str. zfill (width) Supported Arguments argument datatypes width Integer >>> @bodo . jit ... def f ( S ): ... return S . str . zfill ( 5 ) >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 0000 A 1 000 ce 2 00014 3 0000 4 0000 @ 5 00 a n 6 0 ^ Ef dtype : object","title":"pd.Series.str.zfill"},{"location":"api_docs/pandas/series/#pdseriesstrisalnum","text":"pandas.Series.str. isalnum () >>> @bodo . jit ... def f ( S ): ... return S . str . isalnum () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 True 1 True 2 True 3 False 4 False 5 False 6 False dtype : boolean","title":"pd.Series.str.isalnum"},{"location":"api_docs/pandas/series/#pdseriesstrisalpha","text":"pandas.Series.str. isalpha () >>> @bodo . jit ... def f ( S ): ... return S . str . isalpha () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 True 1 True 2 False 3 False 4 False 5 False 6 False dtype : boolean","title":"pd.Series.str.isalpha"},{"location":"api_docs/pandas/series/#pdseriesstrisdigit","text":"pandas.Series.str. isdigit () >>> @bodo . jit ... def f ( S ): ... return S . str . isdigit () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 False 2 True 3 False 4 False 5 False 6 False dtype : boolean","title":"pd.Series.str.isdigit"},{"location":"api_docs/pandas/series/#pdseriesstrisspace","text":"pandas.Series.str. isspace () >>> @bodo . jit ... def f ( S ): ... return S . str . isspace () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \" \" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 False 2 False 3 True 4 False 5 False 6 False dtype : boolean","title":"pd.Series.str.isspace"},{"location":"api_docs/pandas/series/#pdseriesstrislower","text":"pandas.Series.str. islower () >>> @bodo . jit ... def f ( S ): ... return S . str . islower () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 True 2 False 3 True 4 False 5 True 6 False dtype : boolean","title":"pd.Series.str.islower"},{"location":"api_docs/pandas/series/#pdseriesstrisupper","text":"pandas.Series.str. isupper () >>> @bodo . jit ... def f ( S ): ... return S . str . isupper () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 True 1 False 2 False 3 False 4 False 5 False 6 False dtype : boolean","title":"pd.Series.str.isupper"},{"location":"api_docs/pandas/series/#pdseriesstristitle","text":"pandas.Series.str. istitle () >>> @bodo . jit ... def f ( S ): ... return S . str . istitle () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 True 1 False 2 False 3 False 4 False 5 False 6 True dtype : boolean","title":"pd.Series.str.istitle"},{"location":"api_docs/pandas/series/#pdseriesstrisnumeric","text":"pandas.Series.str. isnumeric () >>> @bodo . jit ... def f ( S ): ... return S . str . isnumeric () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 False 2 True 3 False 4 False 5 False 6 False dtype : boolean","title":"pd.Series.str.isnumeric"},{"location":"api_docs/pandas/series/#pdseriesstrisdecimal","text":"pandas.Series.str. isdecimal () >>> @bodo . jit ... def f ( S ): ... return S . str . isdecimal () >>> S = pd . Series ([ \"A\" , \"ce\" , \"14\" , \"a3\" , \"@\" , \"a n\" , \"^ Ef\" ]) >>> f ( S ) 0 False 1 False 2 True 3 False 4 False 5 False 6 False dtype : boolean","title":"pd.Series.str.isdecimal"},{"location":"api_docs/pandas/series/#categorical-accessor","text":"","title":"Categorical accessor"},{"location":"api_docs/pandas/series/#pdseriescatcodes","text":"pandas.Series. cat.codes Note If categories cannot be determined at compile time, then Bodo defaults to creating codes with an int64 , which may differ from Pandas. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . cat . codes >>> S = pd . Series ([ \"a\" , \"ce\" , \"Erw\" , \"a3\" , \"@\" ] * 10 ) . astype ( \"category\" ) >>> f ( S ) 0 2 1 4 2 1 3 3 4 0 5 2 6 4 7 1 8 3 9 0 10 2 11 4 12 1 13 3 14 0 15 2 16 4 17 1 18 3 19 0 20 2 21 4 22 1 23 3 24 0 25 2 26 4 27 1 28 3 29 0 30 2 31 4 32 1 33 3 34 0 35 2 36 4 37 1 38 3 39 0 40 2 41 4 42 1 43 3 44 0 45 2 46 4 47 1 48 3 49 0 dtype : int8","title":"pd.Series.cat.codes"},{"location":"api_docs/pandas/series/#serialization-io-conversion","text":"","title":"Serialization / IO / Conversion"},{"location":"api_docs/pandas/series/#pdseriesto_csv","text":"pandas.Series. to_csv (path_or_buf=None, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict', storage_options=None)","title":"pd.Series.to_csv"},{"location":"api_docs/pandas/series/#pdseriesto_dict","text":"pandas.Series. to_dict (into= ) Supported Arguments None Note This method is not parallelized since dictionaries are not parallelized. This method returns a typedDict, which maintains typing information if passing the dictionary between JIT code and regular Python. This can be converted to a regular Python dictionary by using the dict constructor. Example Usage >>> @bodo . jit ... def f ( S ): ... return S . to_dict () >>> S = pd . Series ( np . arange ( 10 )) >>> dict ( f ( S )) { 0 : 0 , 1 : 1 , 2 : 2 , 3 : 3 , 4 : 4 , 5 : 5 , 6 : 6 , 7 : 7 , 8 : 8 , 9 : 9 }","title":"pd.Series.to_dict"},{"location":"api_docs/pandas/series/#pdseriesto_frame","text":"pandas.Series. to_frame (name=None) Supported Arguments argument datatypes other requirements name String Must be constant at Compile Time Note If name is not provided Series name must be a known constant Example Usage >>> @bodo . jit ... def f ( S ): ... return S . to_frame ( \"my_column\" ) >>> S = pd . Series ( np . arange ( 1000 )) >>> f ( S ) my_column 0 0 1 1 2 2 3 3 4 4 .. ... 995 995 996 996 997 997 998 998 999 999 [1000 rows x 1 columns]","title":"pd.Series.to_frame"},{"location":"api_docs/pandas/series/#heterogeneous_series","text":"Bodo's Series implementation requires all elements to share a common data type. However, in situations where the size and types of the elements are constant at compile time, Bodo has some mixed type handling with its Heterogeneous Series type. Warning This type's primary purpose is for iterating through the rows of a DataFrame with different column types. You should not attempt to directly create Series with mixed types. Heterogeneous Series operations are a subset of those supported for Series and the supported operations are listed below. Please refer to series for detailed usage.","title":"Heterogeneous Series"},{"location":"api_docs/pandas/series/#attributes_1","text":"","title":"Attributes"},{"location":"api_docs/pandas/series/#pdseriesindex_1","text":"pandas.Series. index Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : len ( row . index ), axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 2 1 2 2 2 3 2 4 2 .. 95 2 96 2 97 2 98 2 99 2 Length : 100 , dtype : int64","title":"pd.Series.index"},{"location":"api_docs/pandas/series/#pdseriesvalues_1","text":"pandas.Series. values Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . values , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 ( 0 , A ) 1 ( 1 , b ) 2 ( 2 , A ) 3 ( 3 , b ) 4 ( 4 , A ) ... 95 ( 95 , b ) 96 ( 96 , A ) 97 ( 97 , b ) 98 ( 98 , A ) 99 ( 99 , b ) Length : 100 , dtype : object","title":"pd.Series.values"},{"location":"api_docs/pandas/series/#pdseriesshape_1","text":"pandas.Series. shape Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . shape , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 ( 2 ,) 1 ( 2 ,) 2 ( 2 ,) 3 ( 2 ,) 4 ( 2 ,) ... 95 ( 2 ,) 96 ( 2 ,) 97 ( 2 ,) 98 ( 2 ,) 99 ( 2 ,) Length : 100 , dtype : object","title":"pd.Series.shape"},{"location":"api_docs/pandas/series/#pdseriesndim_1","text":"pandas.Series. ndim Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . ndim , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 1 1 1 2 1 3 1 4 1 .. 95 1 96 1 97 1 98 1 99 1 Length : 100 , dtype : int64","title":"pd.Series.ndim"},{"location":"api_docs/pandas/series/#pdseriessize_1","text":"pandas.Series. size Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . size , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 2 1 2 2 2 3 2 4 2 .. 95 2 96 2 97 2 98 2 99 2 Length : 100 , dtype : int64","title":"pd.Series.size"},{"location":"api_docs/pandas/series/#pdseriest_1","text":"pandas.Series. T Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . T . size , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 2 1 2 2 2 3 2 4 2 .. 95 2 96 2 97 2 98 2 99 2 Length : 100 , dtype : int64","title":"pd.Series.T"},{"location":"api_docs/pandas/series/#pdseriesempty_1","text":"pandas.Series. empty Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . empty , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 False 1 False 2 False 3 False 4 False ... 95 False 96 False 97 False 98 False 99 False Length : 100 , dtype : boolean","title":"pd.Series.empty"},{"location":"api_docs/pandas/series/#pdseriesname_1","text":"pandas.Series. name Example Usage >>> @bodo . jit ... def f ( df ): ... return df . apply ( lambda row : row . name , axis = 1 ) >>> df = pd . DataFrame ({ \"A\" : np . arange ( 100 ), \"B\" : [ \"A\" , \"b\" ] * 50 }) >>> f ( df ) 0 0 1 1 2 2 3 3 4 4 .. 95 95 96 96 97 97 98 98 99 99 Length : 100 , dtype : int64","title":"pd.Series.name"},{"location":"api_docs/pandas/timedelta/","text":"Timedelta \u00b6 Timedelta functionality is documented in pandas.Timedelta . pd.Timedelta \u00b6 pandas. Timedelta (value=<object object>, unit=\"ns\", days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0) Supported Arguments value : Integer (with constant string unit argument), String, Pandas Timedelta, datetime Timedelta unit : Constant String. Only has an effect when passing an integer value , see here for allowed values. days : Integer seconds : Integer microseconds : Integer milliseconds : Integer minutes : Integer hours : Integer weeks : Integer Example Usage >>> @bodo . jit ... def f (): ... td1 = pd . Timedelta ( \"10 Seconds\" ) ... td2 = pd . Timedelta ( 10 , unit = \"W\" ) ... td3 = pd . Timedelta ( days = 10 , hours = 2 , microseconds = 23 ) ... return ( td1 , td2 , td3 ) >>> f () ( Timedelta ( '0 days 00:00:10' ), Timedelta ( '70 days 00:00:00' ), Timedelta ( '10 days 02:00:00.000023' )) pd.Timedelta.components \u00b6 pandas.Timedelta. components Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . components >>> f () Components ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 , nanoseconds = 0 ) pd.Timedelta.days \u00b6 pandas.Timedelta. days Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . days >>> f () 10 pd.Timedelta.delta \u00b6 pandas.Timedelta. delta Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( microseconds = 23 ) . delta >>> f () 23000 pd.Timedelta.microseconds \u00b6 pandas.Timedelta. microseconds Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . microseconds >>> f () 23 pd.Timedelta.nanoseconds \u00b6 pandas.Timedelta. nanoseconds Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . nanoseconds >>> f () 0 pd.Timedelta.seconds \u00b6 pandas.Timedelta. seconds Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( \"10 nanoseconds\" ) . nanoseconds >>> f () 10 pd.Timedelta.value \u00b6 pandas.Timedelta. value Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( \"13 nanoseconds\" ) . value >>> f () 13 pd.Timedelta.ceil \u00b6 pandas.Timedelta. ceil(freq) Supported Arguments freq : String Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . ceil ( \"D\" ) >>> f () 11 days 00 : 00 : 00 pd.Timedelta.floor \u00b6 pandas.Timedelta. floor Supported Arguments freq : String Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . floor ( \"D\" ) >>> f () 10 days 00 : 00 : 00 pd.Timedelta.round \u00b6 pandas.Timedelta. round Supported Arguments freq : String Example Usage >>> @bodo . jit ... def f (): ... return ( pd . Timedelta ( days = 10 , hours = 12 ) . round ( \"D\" ), pd . Timedelta ( days = 10 , hours = 13 ) . round ( \"D\" )) >>> f () ( Timedelta ( '10 days 00:00:00' ), Timedelta ( '11 days 00:00:00' )) pd.Timedelta.to_numpy \u00b6 pandas.Timedelta. to_numpy () Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . to_numpy () >>> f () 871623013023000 nanoseconds pd.Timedelta.to_pytimedelta \u00b6 pandas.Timedelta. to_pytimedelta () Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . to_pytimedelta () >>> f () 10 days , 2 : 07 : 03.013023 pd.Timedelta.to_timedelta64 \u00b6 pandas.Timedelta. to_timedelta64 () Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . to_timedelta64 () >>> f () 871623013023000 nanoseconds pd.Timedelta.total_seconds \u00b6 pandas.Timedelta. total_seconds () Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . total_seconds () >>> f () 871623.013023","title":"TimeDelta"},{"location":"api_docs/pandas/timedelta/#timedelta","text":"Timedelta functionality is documented in pandas.Timedelta .","title":"Timedelta"},{"location":"api_docs/pandas/timedelta/#pdtimedelta","text":"pandas. Timedelta (value=<object object>, unit=\"ns\", days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0) Supported Arguments value : Integer (with constant string unit argument), String, Pandas Timedelta, datetime Timedelta unit : Constant String. Only has an effect when passing an integer value , see here for allowed values. days : Integer seconds : Integer microseconds : Integer milliseconds : Integer minutes : Integer hours : Integer weeks : Integer Example Usage >>> @bodo . jit ... def f (): ... td1 = pd . Timedelta ( \"10 Seconds\" ) ... td2 = pd . Timedelta ( 10 , unit = \"W\" ) ... td3 = pd . Timedelta ( days = 10 , hours = 2 , microseconds = 23 ) ... return ( td1 , td2 , td3 ) >>> f () ( Timedelta ( '0 days 00:00:10' ), Timedelta ( '70 days 00:00:00' ), Timedelta ( '10 days 02:00:00.000023' ))","title":"pd.Timedelta"},{"location":"api_docs/pandas/timedelta/#pdtimedeltacomponents","text":"pandas.Timedelta. components Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . components >>> f () Components ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 , nanoseconds = 0 )","title":"pd.Timedelta.components"},{"location":"api_docs/pandas/timedelta/#pdtimedeltadays","text":"pandas.Timedelta. days Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . days >>> f () 10","title":"pd.Timedelta.days"},{"location":"api_docs/pandas/timedelta/#pdtimedeltadelta","text":"pandas.Timedelta. delta Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( microseconds = 23 ) . delta >>> f () 23000","title":"pd.Timedelta.delta"},{"location":"api_docs/pandas/timedelta/#pdtimedeltamicroseconds","text":"pandas.Timedelta. microseconds Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . microseconds >>> f () 23","title":"pd.Timedelta.microseconds"},{"location":"api_docs/pandas/timedelta/#pdtimedeltananoseconds","text":"pandas.Timedelta. nanoseconds Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . nanoseconds >>> f () 0","title":"pd.Timedelta.nanoseconds"},{"location":"api_docs/pandas/timedelta/#pdtimedeltaseconds","text":"pandas.Timedelta. seconds Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( \"10 nanoseconds\" ) . nanoseconds >>> f () 10","title":"pd.Timedelta.seconds"},{"location":"api_docs/pandas/timedelta/#pdtimedeltavalue","text":"pandas.Timedelta. value Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( \"13 nanoseconds\" ) . value >>> f () 13","title":"pd.Timedelta.value"},{"location":"api_docs/pandas/timedelta/#pdtimedeltaceil","text":"pandas.Timedelta. ceil(freq) Supported Arguments freq : String Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . ceil ( \"D\" ) >>> f () 11 days 00 : 00 : 00","title":"pd.Timedelta.ceil"},{"location":"api_docs/pandas/timedelta/#pdtimedeltafloor","text":"pandas.Timedelta. floor Supported Arguments freq : String Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . floor ( \"D\" ) >>> f () 10 days 00 : 00 : 00","title":"pd.Timedelta.floor"},{"location":"api_docs/pandas/timedelta/#pdtimedeltaround","text":"pandas.Timedelta. round Supported Arguments freq : String Example Usage >>> @bodo . jit ... def f (): ... return ( pd . Timedelta ( days = 10 , hours = 12 ) . round ( \"D\" ), pd . Timedelta ( days = 10 , hours = 13 ) . round ( \"D\" )) >>> f () ( Timedelta ( '10 days 00:00:00' ), Timedelta ( '11 days 00:00:00' ))","title":"pd.Timedelta.round"},{"location":"api_docs/pandas/timedelta/#pdtimedeltato_numpy","text":"pandas.Timedelta. to_numpy () Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . to_numpy () >>> f () 871623013023000 nanoseconds","title":"pd.Timedelta.to_numpy"},{"location":"api_docs/pandas/timedelta/#pdtimedeltato_pytimedelta","text":"pandas.Timedelta. to_pytimedelta () Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . to_pytimedelta () >>> f () 10 days , 2 : 07 : 03.013023","title":"pd.Timedelta.to_pytimedelta"},{"location":"api_docs/pandas/timedelta/#pdtimedeltato_timedelta64","text":"pandas.Timedelta. to_timedelta64 () Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . to_timedelta64 () >>> f () 871623013023000 nanoseconds","title":"pd.Timedelta.to_timedelta64"},{"location":"api_docs/pandas/timedelta/#pdtimedeltatotal_seconds","text":"pandas.Timedelta. total_seconds () Example Usage >>> @bodo . jit ... def f (): ... return pd . Timedelta ( days = 10 , hours = 2 , minutes = 7 , seconds = 3 , milliseconds = 13 , microseconds = 23 ) . total_seconds () >>> f () 871623.013023","title":"pd.Timedelta.total_seconds"},{"location":"api_docs/pandas/timestamp/","text":"Timestamp \u00b6 Timestamp functionality is documented in pandas.Timestamp . pd.Timestamp \u00b6 pandas. Timestamp (ts_input=<object object>, freq=None, tz=None, unit=None, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None, tzinfo=None, *, fold=None) Supported Arguments ts_input : string, integer, timestamp, datetimedate unit : constant string year : integer month : integer day : integer hour : integer minute : integer second : integer microsecond : integer nanosecond : integer Example Usage >>> @bodo . jit ... def f (): ... return I . copy ( name = \"new_name\" ) ... ts1 = pd . Timestamp ( '2021-12-09 09:57:44.114123' ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... ts3 = pd . Timestamp ( 100 , unit = \"days\" ) ... ts4 = pd . Timestamp ( datetime . date ( 2021 , 12 , 9 ), hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ( ts1 , ts2 , ts3 , ts4 ) >>> f () ( Timestamp ( '2021-12-09 09:57:44.114123' ), Timestamp ( '2021-12-09 09:57:44.114123' ), Timestamp ( '1970-04-11 00:00:00' ), Timestamp ( '2021-12-09 09:57:44.114123' )) pd.Timestamp.day \u00b6 pandas.Timestamp. day Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . day >>> f () 9 pd.Timestamp.hour \u00b6 pandas.Timestamp. hour Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . hour >>> f () 9 pd.Timestamp.microsecond \u00b6 pandas.Timestamp. microsecond Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . microsecond >>> f () 114123 pd.Timestamp.month \u00b6 pandas.Timestamp. month Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . month >>> f () month pd.Timestamp.nanosecond \u00b6 pandas.Timestamp. nanosecond Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( 12 , unit = \"ns\" ) ... return ts2 . nanosecond >>> f () 12 pd.Timestamp.second \u00b6 pandas.Timestamp. second Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . second >>> f () 44 pd.Timestamp.year \u00b6 pandas.Timestamp. year Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . year >>> f () 2021 pd.Timestamp.dayofyear \u00b6 pandas.Timestamp. dayofyear Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . dayofyear >>> f () 343 pd.Timestamp.day_of_year \u00b6 pandas.Timestamp. day_of_year Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . day_of_year >>> f () 343 pd.Timestamp.dayofweek \u00b6 pandas.Timestamp. dayofweek Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . day_of_year >>> f () 343 pd.Timestamp.day_of_week \u00b6 pandas.Timestamp. day_of_week Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . day_of_week >>> f () 3 pd.Timestamp.days_in_month \u00b6 pandas.Timestamp. days_in_month Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . days_in_month >>> f () 31 pd.Timestamp.daysinmonth \u00b6 pandas.Timestamp. daysinmonth Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . daysinmonth >>> f () 31 pd.Timestamp.is_leap_year \u00b6 pandas.Timestamp. is_leap_year Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2020 , month = 2 , day = 2 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ( ts1 . is_leap_year , ts2 . is_leap_year ) >>> f () ( True , False ) pd.Timestamp.is_month_start \u00b6 pandas.Timestamp. is_month_start Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 1 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 2 ) ... return ( ts1 . is_month_start , ts2 . is_month_start ) >>> f () ( True , False ) pd.Timestamp.is_month_end \u00b6 pandas.Timestamp. is_month_end Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 31 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 30 ) ... return ( ts1 . is_month_end , ts2 . is_month_end ) >>> f () ( True , False ) pd.Timestamp.is_quarter_start \u00b6 pandas.Timestamp. is_quarter_start Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 9 , day = 30 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 10 , day = 1 ) ... return ( ts1 . is_quarter_start , ts2 . is_quarter_start ) >>> f () ( False , True ) pd.Timestamp.is_quarter_end \u00b6 pandas.Timestamp. is_quarter_end Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 9 , day = 30 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 10 , day = 1 ) ... return ( ts1 . is_quarter_start , ts2 . is_quarter_start ) >>> f () ( True , False ) pd.Timestamp.is_year_start \u00b6 pandas.Timestamp. is_year_start Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 31 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 1 , day = 1 ) ... return ( ts1 . is_year_start , ts2 . is_year_start ) >>> f () ( False , True ) pd.Timestamp.is_year_end \u00b6 pandas.Timestamp. is_year_end Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 31 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 1 , day = 1 ) ... return ( ts1 . is_year_end , ts2 . is_year_end ) >>> f () ( True , False ) pd.Timestamp.quarter \u00b6 pandas.Timestamp. quarter Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 1 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 9 , day = 1 ) ... return ( ts1 . quarter , ts2 . quarter ) >>> f () ( 4 , 3 ) pd.Timestamp.week \u00b6 pandas.Timestamp. week Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 9 , day = 1 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 9 , day = 20 ) ... return ( ts1 . week , ts2 . week ) >>> f () ( 35 , 38 ) pd.Timestamp.weekofyear \u00b6 pandas.Timestamp. weekofyear Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 9 , day = 1 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 9 , day = 20 ) ... return ( ts1 . weekofyear , ts2 . weekofyear ) >>> f () ( 35 , 38 ) pd.Timestamp.value \u00b6 pandas.Timestamp. value Example Usage >>> @bodo . jit ... def f (): ... return pd . Timestamp ( 12345 , unit = \"ns\" ) . value >>> f () 12345 pd.Timestamp.ceil \u00b6 pandas.Timestamp. ceil (freq, ambiguous='raise', nonexistent='raise') Supported Arguments freq : string Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . ceil ( \"D\" ) ... return ( ts1 , ts2 ) >>> f () ( Timestamp ( '2021-12-09 09:57:44.114123' ), Timestamp ( '2021-12-10 00:00:00' )) pd.Timestamp.date \u00b6 pandas.Timestamp. date () Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . date () ... return ( ts1 , ts2 ) >>> f () ( Timestamp ( '2021-12-09 09:57:44.114123' ), datetime . date ( 2021 , 12 , 9 )) pd.Timestamp.day_name \u00b6 pandas.Timestamp. day_name ( args, *kwargs) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... day_1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 ) . day_name () ... day_2 = pd . Timestamp ( year = 2021 , month = 12 , day = 10 ) . day_name () ... day_3 = pd . Timestamp ( year = 2021 , month = 12 , day = 11 ) . day_name () ... return ( day_1 , day_2 , day_3 ) >>> f () ( 'Thursday' , 'Friday' , 'Saturday' ) pd.Timestamp.floor \u00b6 pandas.Timestamp. floor (freq, ambiguous='raise', nonexistent='raise') Supported Arguments freq : string Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . ceil ( \"D\" ) ... return ( ts1 , ts2 ) >>> f () ( Timestamp ( '2021-12-09 09:57:44.114123' ), Timestamp ( '2021-12-09 00:00:00' )) pd.Timestamp.isocalendar \u00b6 pandas.Timestamp. isocalendar () Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . isocalendar () ... return ( ts1 , ts2 ) >>> f () ( 2021 , 49 , 4 ) pd.Timestamp.isoformat \u00b6 pandas.Timestamp. isoformat () Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . isocalendar () ... return ( ts1 , ts2 ) >>> f () '2021-12-09T09:57:44' pd.Timestamp.month_name \u00b6 pandas.Timestamp. month_name (locale=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... return pd . Timestamp ( year = 2021 , month = 12 , day = 9 ) . month_name () >>> f () 'December' pd.Timestamp.normalize \u00b6 pandas.Timestamp. normalize () Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . normalize () ... return ( ts1 , ts2 ) >>> f () Timestamp ( '2021-12-09 00:00:00' ) pd.Timestamp.round \u00b6 pandas.Timestamp. round (freq, ambiguous='raise', nonexistent='raise') Supported Arguments freq : string Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 12 ) . round () ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 13 ) . round () ... return ( ts1 , ts2 ) >>> f () ( Timestamp ( '2021-12-09 00:00:00' ), Timestamp ( '2021-12-10 00:00:00' )) pd.Timestamp.strftime \u00b6 pandas.Timestamp. strftime (format) Supported Arguments format : string Example Usage >>> @bodo . jit ... def f (): ... return pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 12 ) . strftime ( '%Y-%m- %d %X ' ) >>> f () '2021-12-09 12:00:00' pd.Timestamp.toordinal \u00b6 pandas.Timestamp. toordinal () Example Usage >>> @bodo . jit ... def f (): ... return pd . Timestamp ( year = 2021 , month = 12 , day = 9 ) . toordinal () >>> f () 738133 pd.Timestamp.weekday \u00b6 pandas.Timestamp. weekday () Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 10 ) ... return ( ts1 . weekday (), ts2 . weekday ()) >>> f () ( 3 , 4 ) pd.Timestamp.now \u00b6 pandas.Timestamp. now (tz=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... return pd . Timestamp . now () >>> f () Timestamp ( '2021-12-10 10:54:06.457168' )","title":"Timestamp"},{"location":"api_docs/pandas/timestamp/#timestamp","text":"Timestamp functionality is documented in pandas.Timestamp .","title":"Timestamp"},{"location":"api_docs/pandas/timestamp/#pdtimestamp","text":"pandas. Timestamp (ts_input=<object object>, freq=None, tz=None, unit=None, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None, tzinfo=None, *, fold=None) Supported Arguments ts_input : string, integer, timestamp, datetimedate unit : constant string year : integer month : integer day : integer hour : integer minute : integer second : integer microsecond : integer nanosecond : integer Example Usage >>> @bodo . jit ... def f (): ... return I . copy ( name = \"new_name\" ) ... ts1 = pd . Timestamp ( '2021-12-09 09:57:44.114123' ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... ts3 = pd . Timestamp ( 100 , unit = \"days\" ) ... ts4 = pd . Timestamp ( datetime . date ( 2021 , 12 , 9 ), hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ( ts1 , ts2 , ts3 , ts4 ) >>> f () ( Timestamp ( '2021-12-09 09:57:44.114123' ), Timestamp ( '2021-12-09 09:57:44.114123' ), Timestamp ( '1970-04-11 00:00:00' ), Timestamp ( '2021-12-09 09:57:44.114123' ))","title":"pd.Timestamp"},{"location":"api_docs/pandas/timestamp/#pdtimestampday","text":"pandas.Timestamp. day Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . day >>> f () 9","title":"pd.Timestamp.day"},{"location":"api_docs/pandas/timestamp/#pdtimestamphour","text":"pandas.Timestamp. hour Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . hour >>> f () 9","title":"pd.Timestamp.hour"},{"location":"api_docs/pandas/timestamp/#pdtimestampmicrosecond","text":"pandas.Timestamp. microsecond Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . microsecond >>> f () 114123","title":"pd.Timestamp.microsecond"},{"location":"api_docs/pandas/timestamp/#pdtimestampmonth","text":"pandas.Timestamp. month Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . month >>> f () month","title":"pd.Timestamp.month"},{"location":"api_docs/pandas/timestamp/#pdtimestampnanosecond","text":"pandas.Timestamp. nanosecond Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( 12 , unit = \"ns\" ) ... return ts2 . nanosecond >>> f () 12","title":"pd.Timestamp.nanosecond"},{"location":"api_docs/pandas/timestamp/#pdtimestampsecond","text":"pandas.Timestamp. second Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . second >>> f () 44","title":"pd.Timestamp.second"},{"location":"api_docs/pandas/timestamp/#pdtimestampyear","text":"pandas.Timestamp. year Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . year >>> f () 2021","title":"pd.Timestamp.year"},{"location":"api_docs/pandas/timestamp/#pdtimestampdayofyear","text":"pandas.Timestamp. dayofyear Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . dayofyear >>> f () 343","title":"pd.Timestamp.dayofyear"},{"location":"api_docs/pandas/timestamp/#pdtimestampday_of_year","text":"pandas.Timestamp. day_of_year Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . day_of_year >>> f () 343","title":"pd.Timestamp.day_of_year"},{"location":"api_docs/pandas/timestamp/#pdtimestampdayofweek","text":"pandas.Timestamp. dayofweek Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . day_of_year >>> f () 343","title":"pd.Timestamp.dayofweek"},{"location":"api_docs/pandas/timestamp/#pdtimestampday_of_week","text":"pandas.Timestamp. day_of_week Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . day_of_week >>> f () 3","title":"pd.Timestamp.day_of_week"},{"location":"api_docs/pandas/timestamp/#pdtimestampdays_in_month","text":"pandas.Timestamp. days_in_month Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . days_in_month >>> f () 31","title":"pd.Timestamp.days_in_month"},{"location":"api_docs/pandas/timestamp/#pdtimestampdaysinmonth","text":"pandas.Timestamp. daysinmonth Example Usage >>> @bodo . jit ... def f (): ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ts2 . daysinmonth >>> f () 31","title":"pd.Timestamp.daysinmonth"},{"location":"api_docs/pandas/timestamp/#pdtimestampis_leap_year","text":"pandas.Timestamp. is_leap_year Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2020 , month = 2 , day = 2 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... return ( ts1 . is_leap_year , ts2 . is_leap_year ) >>> f () ( True , False )","title":"pd.Timestamp.is_leap_year"},{"location":"api_docs/pandas/timestamp/#pdtimestampis_month_start","text":"pandas.Timestamp. is_month_start Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 1 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 2 ) ... return ( ts1 . is_month_start , ts2 . is_month_start ) >>> f () ( True , False )","title":"pd.Timestamp.is_month_start"},{"location":"api_docs/pandas/timestamp/#pdtimestampis_month_end","text":"pandas.Timestamp. is_month_end Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 31 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 30 ) ... return ( ts1 . is_month_end , ts2 . is_month_end ) >>> f () ( True , False )","title":"pd.Timestamp.is_month_end"},{"location":"api_docs/pandas/timestamp/#pdtimestampis_quarter_start","text":"pandas.Timestamp. is_quarter_start Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 9 , day = 30 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 10 , day = 1 ) ... return ( ts1 . is_quarter_start , ts2 . is_quarter_start ) >>> f () ( False , True )","title":"pd.Timestamp.is_quarter_start"},{"location":"api_docs/pandas/timestamp/#pdtimestampis_quarter_end","text":"pandas.Timestamp. is_quarter_end Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 9 , day = 30 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 10 , day = 1 ) ... return ( ts1 . is_quarter_start , ts2 . is_quarter_start ) >>> f () ( True , False )","title":"pd.Timestamp.is_quarter_end"},{"location":"api_docs/pandas/timestamp/#pdtimestampis_year_start","text":"pandas.Timestamp. is_year_start Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 31 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 1 , day = 1 ) ... return ( ts1 . is_year_start , ts2 . is_year_start ) >>> f () ( False , True )","title":"pd.Timestamp.is_year_start"},{"location":"api_docs/pandas/timestamp/#pdtimestampis_year_end","text":"pandas.Timestamp. is_year_end Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 31 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 1 , day = 1 ) ... return ( ts1 . is_year_end , ts2 . is_year_end ) >>> f () ( True , False )","title":"pd.Timestamp.is_year_end"},{"location":"api_docs/pandas/timestamp/#pdtimestampquarter","text":"pandas.Timestamp. quarter Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 1 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 9 , day = 1 ) ... return ( ts1 . quarter , ts2 . quarter ) >>> f () ( 4 , 3 )","title":"pd.Timestamp.quarter"},{"location":"api_docs/pandas/timestamp/#pdtimestampweek","text":"pandas.Timestamp. week Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 9 , day = 1 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 9 , day = 20 ) ... return ( ts1 . week , ts2 . week ) >>> f () ( 35 , 38 )","title":"pd.Timestamp.week"},{"location":"api_docs/pandas/timestamp/#pdtimestampweekofyear","text":"pandas.Timestamp. weekofyear Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 9 , day = 1 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 9 , day = 20 ) ... return ( ts1 . weekofyear , ts2 . weekofyear ) >>> f () ( 35 , 38 )","title":"pd.Timestamp.weekofyear"},{"location":"api_docs/pandas/timestamp/#pdtimestampvalue","text":"pandas.Timestamp. value Example Usage >>> @bodo . jit ... def f (): ... return pd . Timestamp ( 12345 , unit = \"ns\" ) . value >>> f () 12345","title":"pd.Timestamp.value"},{"location":"api_docs/pandas/timestamp/#pdtimestampceil","text":"pandas.Timestamp. ceil (freq, ambiguous='raise', nonexistent='raise') Supported Arguments freq : string Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . ceil ( \"D\" ) ... return ( ts1 , ts2 ) >>> f () ( Timestamp ( '2021-12-09 09:57:44.114123' ), Timestamp ( '2021-12-10 00:00:00' ))","title":"pd.Timestamp.ceil"},{"location":"api_docs/pandas/timestamp/#pdtimestampdate","text":"pandas.Timestamp. date () Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . date () ... return ( ts1 , ts2 ) >>> f () ( Timestamp ( '2021-12-09 09:57:44.114123' ), datetime . date ( 2021 , 12 , 9 ))","title":"pd.Timestamp.date"},{"location":"api_docs/pandas/timestamp/#pdtimestampday_name","text":"pandas.Timestamp. day_name ( args, *kwargs) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... day_1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 ) . day_name () ... day_2 = pd . Timestamp ( year = 2021 , month = 12 , day = 10 ) . day_name () ... day_3 = pd . Timestamp ( year = 2021 , month = 12 , day = 11 ) . day_name () ... return ( day_1 , day_2 , day_3 ) >>> f () ( 'Thursday' , 'Friday' , 'Saturday' )","title":"pd.Timestamp.day_name"},{"location":"api_docs/pandas/timestamp/#pdtimestampfloor","text":"pandas.Timestamp. floor (freq, ambiguous='raise', nonexistent='raise') Supported Arguments freq : string Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . ceil ( \"D\" ) ... return ( ts1 , ts2 ) >>> f () ( Timestamp ( '2021-12-09 09:57:44.114123' ), Timestamp ( '2021-12-09 00:00:00' ))","title":"pd.Timestamp.floor"},{"location":"api_docs/pandas/timestamp/#pdtimestampisocalendar","text":"pandas.Timestamp. isocalendar () Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . isocalendar () ... return ( ts1 , ts2 ) >>> f () ( 2021 , 49 , 4 )","title":"pd.Timestamp.isocalendar"},{"location":"api_docs/pandas/timestamp/#pdtimestampisoformat","text":"pandas.Timestamp. isoformat () Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . isocalendar () ... return ( ts1 , ts2 ) >>> f () '2021-12-09T09:57:44'","title":"pd.Timestamp.isoformat"},{"location":"api_docs/pandas/timestamp/#pdtimestampmonth_name","text":"pandas.Timestamp. month_name (locale=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... return pd . Timestamp ( year = 2021 , month = 12 , day = 9 ) . month_name () >>> f () 'December'","title":"pd.Timestamp.month_name"},{"location":"api_docs/pandas/timestamp/#pdtimestampnormalize","text":"pandas.Timestamp. normalize () Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 9 , minute = 57 , second = 44 , microsecond = 114123 ) . normalize () ... return ( ts1 , ts2 ) >>> f () Timestamp ( '2021-12-09 00:00:00' )","title":"pd.Timestamp.normalize"},{"location":"api_docs/pandas/timestamp/#pdtimestampround","text":"pandas.Timestamp. round (freq, ambiguous='raise', nonexistent='raise') Supported Arguments freq : string Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 12 ) . round () ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 13 ) . round () ... return ( ts1 , ts2 ) >>> f () ( Timestamp ( '2021-12-09 00:00:00' ), Timestamp ( '2021-12-10 00:00:00' ))","title":"pd.Timestamp.round"},{"location":"api_docs/pandas/timestamp/#pdtimestampstrftime","text":"pandas.Timestamp. strftime (format) Supported Arguments format : string Example Usage >>> @bodo . jit ... def f (): ... return pd . Timestamp ( year = 2021 , month = 12 , day = 9 , hour = 12 ) . strftime ( '%Y-%m- %d %X ' ) >>> f () '2021-12-09 12:00:00'","title":"pd.Timestamp.strftime"},{"location":"api_docs/pandas/timestamp/#pdtimestamptoordinal","text":"pandas.Timestamp. toordinal () Example Usage >>> @bodo . jit ... def f (): ... return pd . Timestamp ( year = 2021 , month = 12 , day = 9 ) . toordinal () >>> f () 738133","title":"pd.Timestamp.toordinal"},{"location":"api_docs/pandas/timestamp/#pdtimestampweekday","text":"pandas.Timestamp. weekday () Example Usage >>> @bodo . jit ... def f (): ... ts1 = pd . Timestamp ( year = 2021 , month = 12 , day = 9 ) ... ts2 = pd . Timestamp ( year = 2021 , month = 12 , day = 10 ) ... return ( ts1 . weekday (), ts2 . weekday ()) >>> f () ( 3 , 4 )","title":"pd.Timestamp.weekday"},{"location":"api_docs/pandas/timestamp/#pdtimestampnow","text":"pandas.Timestamp. now (tz=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f (): ... return pd . Timestamp . now () >>> f () Timestamp ( '2021-12-10 10:54:06.457168' )","title":"pd.Timestamp.now"},{"location":"api_docs/pandas/type_inf_obj_data/","text":"Type Inference for Object Data \u00b6 Pandas stores some data types (e.g. strings) as object arrays which are untyped. Therefore, Bodo needs to infer the actual data type of object arrays when dataframes or series values are passed to JIT functions from regular Python. Bodo uses the first non-null value of the array to determine the type, and throws a warning if the array is empty or all nulls: BodoWarning: Empty object array passed to Bodo, which causes ambiguity in typing. This can cause errors in parallel execution. In this case, Bodo assumes the array is a string array which is the most common. However, this can cause errors if a distributed dataset is passed to Bodo, and some other processor has non-string data. This corner case can usually be avoided by load balancing the data across processors to avoid empty arrays.","title":"Type Inference for Object Data"},{"location":"api_docs/pandas/type_inf_obj_data/#type-inference-for-object-data","text":"Pandas stores some data types (e.g. strings) as object arrays which are untyped. Therefore, Bodo needs to infer the actual data type of object arrays when dataframes or series values are passed to JIT functions from regular Python. Bodo uses the first non-null value of the array to determine the type, and throws a warning if the array is empty or all nulls: BodoWarning: Empty object array passed to Bodo, which causes ambiguity in typing. This can cause errors in parallel execution. In this case, Bodo assumes the array is a string array which is the most common. However, this can cause errors if a distributed dataset is passed to Bodo, and some other processor has non-string data. This corner case can usually be avoided by load balancing the data across processors to avoid empty arrays.","title":"Type Inference for Object Data"},{"location":"api_docs/pandas/window/","text":"Window \u00b6 Rolling functionality is documented in pandas.DataFrame.rolling . pd.core.window.rolling.Rolling.count \u00b6 pandas.core.window.rolling.Rolling. count () Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 ], \"B\" : [ 6 , 7 , None , 9 , 10 ]}) ... return df . rolling ( 3 ) . count () A B 0 1.0 1.0 1 2.0 2.0 2 3.0 3.0 3 3.0 2.0 4 3.0 2.0 5 3.0 2.0 6 3.0 3.0 pd.core.window.rolling.Rolling.sum \u00b6 pandas.core.window.rolling.Rolling. sum (engine=None, engine_kwargs=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . sum () A B 0 NaN NaN 1 NaN NaN 2 6.0 27.0 3 9.0 NaN 4 12.0 NaN 5 15.0 NaN 6 18.0 36.0 pd.core.window.rolling.Rolling.mean \u00b6 pandas.core.window.rolling.Rolling. mean (engine=None, engine_kwargs=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . mean () A B 0 NaN NaN 1 NaN NaN 2 2.0 9.0 3 3.0 NaN 4 4.0 NaN 5 5.0 NaN 6 6.0 12.0 pd.core.window.rolling.Rolling.median \u00b6 pandas.core.window.rolling.Rolling. median (engine=None, engine_kwargs=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . median () A B 0 NaN NaN 1 NaN NaN 2 2.0 9.0 3 3.0 NaN 4 4.0 NaN 5 5.0 NaN 6 6.0 12.0 pd.core.window.rolling.Rolling.var \u00b6 pandas.core.window.rolling.Rolling. var (ddof=1) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . var () A B 0 NaN NaN 1 NaN NaN 2 1.0 1.0 3 1.0 NaN 4 1.0 NaN 5 1.0 NaN 6 1.0 1.0 pd.core.window.rolling.Rolling.std \u00b6 pandas.core.window.rolling.Rolling. std (ddof=1) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . std () A B 0 NaN NaN 1 NaN NaN 2 1.0 1.0 3 1.0 NaN 4 1.0 NaN 5 1.0 NaN 6 1.0 1.0 pd.core.window.rolling.Rolling.min \u00b6 pandas.core.window.rolling.Rolling. min (engine=None, engine_kwargs=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . min () A B 0 NaN NaN 1 NaN NaN 2 1.0 8.0 3 2.0 NaN 4 3.0 NaN 5 4.0 NaN 6 5.0 11.0 pd.core.window.rolling.Rolling.max \u00b6 pandas.core.window.rolling.Rolling. max (engine=None, engine_kwargs=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . max () A B 0 NaN NaN 1 NaN NaN 2 3.0 10.0 3 4.0 NaN 4 5.0 NaN 5 6.0 NaN 6 7.0 13.0 pd.core.window.rolling.Rolling.corr \u00b6 pandas.core.window.rolling.Rolling. corr (other=None, pairwise=None, ddof=1) Supported Arguments other : DataFrame or Series (cannot contain nullable Integer Types) Required If called with a DataFrame, other must be a DataFrame. If called with a Series, other must be a Series. Example Usage >>> @bodo . jit ... def f ( I ): ... df1 = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ]}) ... df2 = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , - 5 , - 6 , - 7 ]}) ... return df1 . rolling ( 3 ) . corr ( df2 ) A 0 NaN 1 NaN 2 1.000000 3 1.000000 4 - 0.810885 5 - 0.907841 6 - 1.000000 pd.core.window.rolling.Rolling.cov \u00b6 pandas.core.window.rolling.Rolling. cov (other=None, pairwise=None, ddof=1) Supported Arguments other : DataFrame or Series (cannot contain nullable Integer Types) Required If called with a DataFrame, other must be a DataFrame. If called with a Series, other must be a Series. Example Usage >>> @bodo . jit ... def f ( I ): ... df1 = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ]}) ... df2 = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , - 5 , - 6 , - 7 ]}) ... return df1 . rolling ( 3 ) . cov ( df2 ) A 0 NaN 1 NaN 2 1.0 3 1.0 4 - 4.0 5 - 5.0 6 - 1.0 pd.core.window.rolling.Rolling.%%apply \u00b6 pandas.core.window.rolling. apply (func, raw=False, engine=None, engine_kwargs=None, args=None, kwargs=None) Supported Arguments func : JIT function or callable defined within a JIT function Must be constant at Compile Time raw : boolean Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , - 5 , - 6 , - 7 ]}) ... return df . rolling ( 3 ) . apply ( lambda x : True if x . sum () > 0 else False ) A 0 NaN 1 NaN 2 1.0 3 1.0 4 1.0 5 0.0 6 0.0","title":"Window"},{"location":"api_docs/pandas/window/#pd_window_section","text":"Rolling functionality is documented in pandas.DataFrame.rolling .","title":"Window"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingcount","text":"pandas.core.window.rolling.Rolling. count () Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 ], \"B\" : [ 6 , 7 , None , 9 , 10 ]}) ... return df . rolling ( 3 ) . count () A B 0 1.0 1.0 1 2.0 2.0 2 3.0 3.0 3 3.0 2.0 4 3.0 2.0 5 3.0 2.0 6 3.0 3.0","title":"pd.core.window.rolling.Rolling.count"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingsum","text":"pandas.core.window.rolling.Rolling. sum (engine=None, engine_kwargs=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . sum () A B 0 NaN NaN 1 NaN NaN 2 6.0 27.0 3 9.0 NaN 4 12.0 NaN 5 15.0 NaN 6 18.0 36.0","title":"pd.core.window.rolling.Rolling.sum"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingmean","text":"pandas.core.window.rolling.Rolling. mean (engine=None, engine_kwargs=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . mean () A B 0 NaN NaN 1 NaN NaN 2 2.0 9.0 3 3.0 NaN 4 4.0 NaN 5 5.0 NaN 6 6.0 12.0","title":"pd.core.window.rolling.Rolling.mean"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingmedian","text":"pandas.core.window.rolling.Rolling. median (engine=None, engine_kwargs=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . median () A B 0 NaN NaN 1 NaN NaN 2 2.0 9.0 3 3.0 NaN 4 4.0 NaN 5 5.0 NaN 6 6.0 12.0","title":"pd.core.window.rolling.Rolling.median"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingvar","text":"pandas.core.window.rolling.Rolling. var (ddof=1) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . var () A B 0 NaN NaN 1 NaN NaN 2 1.0 1.0 3 1.0 NaN 4 1.0 NaN 5 1.0 NaN 6 1.0 1.0","title":"pd.core.window.rolling.Rolling.var"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingstd","text":"pandas.core.window.rolling.Rolling. std (ddof=1) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . std () A B 0 NaN NaN 1 NaN NaN 2 1.0 1.0 3 1.0 NaN 4 1.0 NaN 5 1.0 NaN 6 1.0 1.0","title":"pd.core.window.rolling.Rolling.std"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingmin","text":"pandas.core.window.rolling.Rolling. min (engine=None, engine_kwargs=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . min () A B 0 NaN NaN 1 NaN NaN 2 1.0 8.0 3 2.0 NaN 4 3.0 NaN 5 4.0 NaN 6 5.0 11.0","title":"pd.core.window.rolling.Rolling.min"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingmax","text":"pandas.core.window.rolling.Rolling. max (engine=None, engine_kwargs=None) Supported Arguments : None Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ], \"B\" : [ 8 , 9 , 10 , None , 11 , 12 , 13 ]}) ... return df . rolling ( 3 ) . max () A B 0 NaN NaN 1 NaN NaN 2 3.0 10.0 3 4.0 NaN 4 5.0 NaN 5 6.0 NaN 6 7.0 13.0","title":"pd.core.window.rolling.Rolling.max"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingcorr","text":"pandas.core.window.rolling.Rolling. corr (other=None, pairwise=None, ddof=1) Supported Arguments other : DataFrame or Series (cannot contain nullable Integer Types) Required If called with a DataFrame, other must be a DataFrame. If called with a Series, other must be a Series. Example Usage >>> @bodo . jit ... def f ( I ): ... df1 = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ]}) ... df2 = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , - 5 , - 6 , - 7 ]}) ... return df1 . rolling ( 3 ) . corr ( df2 ) A 0 NaN 1 NaN 2 1.000000 3 1.000000 4 - 0.810885 5 - 0.907841 6 - 1.000000","title":"pd.core.window.rolling.Rolling.corr"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingcov","text":"pandas.core.window.rolling.Rolling. cov (other=None, pairwise=None, ddof=1) Supported Arguments other : DataFrame or Series (cannot contain nullable Integer Types) Required If called with a DataFrame, other must be a DataFrame. If called with a Series, other must be a Series. Example Usage >>> @bodo . jit ... def f ( I ): ... df1 = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ]}) ... df2 = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , - 5 , - 6 , - 7 ]}) ... return df1 . rolling ( 3 ) . cov ( df2 ) A 0 NaN 1 NaN 2 1.0 3 1.0 4 - 4.0 5 - 5.0 6 - 1.0","title":"pd.core.window.rolling.Rolling.cov"},{"location":"api_docs/pandas/window/#pdcorewindowrollingrollingapply","text":"pandas.core.window.rolling. apply (func, raw=False, engine=None, engine_kwargs=None, args=None, kwargs=None) Supported Arguments func : JIT function or callable defined within a JIT function Must be constant at Compile Time raw : boolean Must be constant at Compile Time Example Usage >>> @bodo . jit ... def f ( I ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 , - 5 , - 6 , - 7 ]}) ... return df . rolling ( 3 ) . apply ( lambda x : True if x . sum () > 0 else False ) A 0 NaN 1 NaN 2 1.0 3 1.0 4 1.0 5 0.0 6 0.0","title":"pd.core.window.rolling.Rolling.%%apply"},{"location":"bodo_parallelism/advanced/","text":"Advanced Parallelism Topics \u00b6 This section discusses parallelism topics that may be useful for performance tuning and advanced use cases. Getting/Setting Distributed Data Directly \u00b6 Distributed data is usually accessed and modified through high-level Pandas and Numpy APIs. However, in many cases, Bodo allows direct access to distributed data without code modification. Here are such cases that Bodo currently supports: Getting values using boolean array indexing, e.g. B = A[A > 3] . The output can be distributed, but may be imbalanced ( bodo.rebalance() can be used if necessary). Getting values using a slice, e.g. B = A[::2] . The output can be distributed, but may be imbalanced ( bodo.rebalance() can be used if necessary). Getting a value using a scalar index, e.g. a = A[m] . The output can be replicated. Setting values using boolean array indexing, e.g. A[A > 3] = a . Only supports setting a scalar or lower-dimension value currently. Setting values using a slice, e.g. A[::2] = a . Only supports setting a scalar or lower-dimension value currently. Setting a value using a scalar index, e.g. A[m] = a . Concatenation Reduction \u00b6 Some algorithms require generating variable-length output data per input data element. Bodo supports parallelizing this pattern, which we refer to as concatenation reduction . For example: @bodo . jit def impl ( n ): df = pd . DataFrame () for i in bodo . prange ( n ): df = df . append ( pd . DataFrame ({ \"A\" : np . arange ( i )})) return df A common use case is simulation applications that generate possible outcomes based on parameters. For example: @bodo . jit def impl (): params = np . array ([ 0.1 , 0.2 , 0.5 , 1.0 , 1.2 , 1.5 , ... , 100 ]) params = bodo . scatterv ( params ) df = pd . DataFrame () for i in bodo . prange ( len ( params )): df = df . append ( get_result ( params [ i ])) return df In this example, we chose to manually parallelize the parameter array for simplicity, since the workload is compute-heavy and the parameter data is relatively small. Load Balancing Distributed Data \u00b6 Some computations such as filter , join or groupby can result in imbalanced data chunks across cores for distributed data. This may result in some cores operating on nearly empty dataframes, and others on relatively large ones. Bodo provides bodo.rebalance to allow manual load balance if necessary. For example: @bodo.jit(distributed={\"df\"}) def rebalance_example(df): df = df[df[\"A\"] > 3] df = bodo.rebalance(df) return df.sum() In this case, we use bodo.rebalance to make sure the filtered dataframe has near-equal data chunk sizes across cores, which would accelerate later computations ( sum in this case). We can also use the dests keyword to specify a subset of ranks to which bodo should distribute the data from all ranks. Example usage: @bodo . jit ( distributed = { \"df\" }) def rebalance_example ( df ): df = df [ df [ \"A\" ] > 3 ] df = bodo . rebalance ( df , dests = [ 0 , 1 ]) return df . sum () Explicit Parallel Loops \u00b6 Sometimes explicit parallel loops are required since a program cannot be written in terms of data-parallel operators easily. In this case, one can use Bodo's prange in place of range to specify that a loop can be parallelized. The user is required to make sure the loop does not have cross iteration dependencies except for supported reductions. The example below demonstrates a parallel loop with a reduction: import bodo from bodo import prange import numpy as np @bodo . jit def prange_test ( n ): A = np . random . ranf ( n ) s = 0 B = np . empty ( n ) for i in prange ( len ( A )): bodo . parallel_print ( \"rank\" , bodo . get_rank ()) # A[i]: distributed data access with loop index # s: a supported sum reduction s += A [ i ] # write array with loop index B [ i ] = 2 * A [ i ] return s + B . sum () res = prange_test ( 10 ) print ( res ) Output: [stdout:0] rank 0 rank 0 rank 0 13.077183553245497 [stdout:1] rank 1 rank 1 rank 1 13.077183553245497 [stdout:2] rank 2 rank 2 13.077183553245497 [stdout:3] rank 3 rank 3 13.077183553245497 Currently, reductions using += , *= , min , and max operators are supported. Iterations are simply divided between processes and executed in parallel, but reductions are handled using data exchange. Integration with non-Bodo APIs \u00b6 There are multiple methods for integration with APIs that Bodo does not support natively: 1. Switch to python object mode inside jit functions 2. Pass data in and out of jit functions Passing Distributed Data \u00b6 Bodo can receive or return chunks of distributed data to allow flexible integration with any non-Bodo Python code. The following example passes chunks of data to interpolate with Scipy, and returns interpolation results back to jit function. import scipy.interpolate @bodo . jit ( distributed = [ \"X\" , \"Y\" , \"X2\" ]) def dist_pass_test ( n ): X = np . arange ( n ) Y = np . exp ( - X / 3.0 ) X2 = np . arange ( 0 , n , 0.5 ) return X , Y , X2 X , Y , X2 = dist_pass_test ( 100 ) # clip potential out-of-range values X2 = np . minimum ( np . maximum ( X2 , X [ 0 ]), X [ - 1 ]) f = scipy . interpolate . interp1d ( X , Y ) Y2 = f ( X2 ) @bodo . jit ( distributed = { \"Y2\" }) def dist_pass_res ( Y2 ): return Y2 . sum () res = dist_pass_res ( Y2 ) print ( res ) [stdout:0] 6.555500504321469 [stdout:1] 6.555500504321469 [stdout:2] 6.555500504321469 [stdout:3] 6.555500504321469 Collections of Distributed Data \u00b6 List and dictionary collections can be used to hold distributed data structures: @bodo . jit ( distributed = [ \"df\" ]) def f (): to_concat = [] for i in range ( 10 ): to_concat . append ( pd . DataFrame ({ 'A' : np . arange ( 100 ), 'B' : np . random . random ( 100 )})) df = pd . concat ( to_concat ) return df f () Run code on a single rank \u00b6 In cases where some code needs to be run on a single MPI rank, you can do so in a python script as follows: if bodo . get_rank () == 0 : # Remove directory import os , shutil if os . path . exists ( \"data/data.pq\" ): shutil . rmtree ( \"data/data.pq\" ) # To synchronize all ranks before proceeding bodo . barrier () When running code on an IPyParallel cluster using the %%px magic, you can do this instead: %% px -- targets 0 # Install package ! conda install pandas - datareader An alias can be defined for convenience: % alias_magic p0 px - p \"--targets 0\" This can be used as any other magic: %% p0 # Install package ! conda install pandas - datareader Run code once on each node \u00b6 In cases where some code needs to be run once on each node in a multi-node cluster, such as a file system operation, installing packages, etc., it can be done as follows: if bodo . get_rank () in bodo . get_nodes_first_ranks (): # Remove directory on all nodes import os , shutil if os . path . exists ( \"data/data.pq\" ): shutil . rmtree ( \"data/data.pq\" ) # To synchronize all ranks before proceeding bodo . barrier () The same can be done when running on an IPyParallel cluster using the %%px magic: %% px if bodo . get_rank () in bodo . get_nodes_first_ranks (): # Install package on all nodes ! conda install pandas - datareader Warning Running code on a single rank or a subset of ranks can lead to deadlocks. Ensure that your code doesn't include any MPI or Bodo functions.","title":"Advanced Parallelism Topics"},{"location":"bodo_parallelism/advanced/#advanced","text":"This section discusses parallelism topics that may be useful for performance tuning and advanced use cases.","title":"Advanced Parallelism Topics"},{"location":"bodo_parallelism/advanced/#gettingsetting-distributed-data-directly","text":"Distributed data is usually accessed and modified through high-level Pandas and Numpy APIs. However, in many cases, Bodo allows direct access to distributed data without code modification. Here are such cases that Bodo currently supports: Getting values using boolean array indexing, e.g. B = A[A > 3] . The output can be distributed, but may be imbalanced ( bodo.rebalance() can be used if necessary). Getting values using a slice, e.g. B = A[::2] . The output can be distributed, but may be imbalanced ( bodo.rebalance() can be used if necessary). Getting a value using a scalar index, e.g. a = A[m] . The output can be replicated. Setting values using boolean array indexing, e.g. A[A > 3] = a . Only supports setting a scalar or lower-dimension value currently. Setting values using a slice, e.g. A[::2] = a . Only supports setting a scalar or lower-dimension value currently. Setting a value using a scalar index, e.g. A[m] = a .","title":"Getting/Setting Distributed Data Directly"},{"location":"bodo_parallelism/advanced/#concatenation-reduction","text":"Some algorithms require generating variable-length output data per input data element. Bodo supports parallelizing this pattern, which we refer to as concatenation reduction . For example: @bodo . jit def impl ( n ): df = pd . DataFrame () for i in bodo . prange ( n ): df = df . append ( pd . DataFrame ({ \"A\" : np . arange ( i )})) return df A common use case is simulation applications that generate possible outcomes based on parameters. For example: @bodo . jit def impl (): params = np . array ([ 0.1 , 0.2 , 0.5 , 1.0 , 1.2 , 1.5 , ... , 100 ]) params = bodo . scatterv ( params ) df = pd . DataFrame () for i in bodo . prange ( len ( params )): df = df . append ( get_result ( params [ i ])) return df In this example, we chose to manually parallelize the parameter array for simplicity, since the workload is compute-heavy and the parameter data is relatively small.","title":"Concatenation Reduction"},{"location":"bodo_parallelism/advanced/#load-balancing-distributed-data","text":"Some computations such as filter , join or groupby can result in imbalanced data chunks across cores for distributed data. This may result in some cores operating on nearly empty dataframes, and others on relatively large ones. Bodo provides bodo.rebalance to allow manual load balance if necessary. For example: @bodo.jit(distributed={\"df\"}) def rebalance_example(df): df = df[df[\"A\"] > 3] df = bodo.rebalance(df) return df.sum() In this case, we use bodo.rebalance to make sure the filtered dataframe has near-equal data chunk sizes across cores, which would accelerate later computations ( sum in this case). We can also use the dests keyword to specify a subset of ranks to which bodo should distribute the data from all ranks. Example usage: @bodo . jit ( distributed = { \"df\" }) def rebalance_example ( df ): df = df [ df [ \"A\" ] > 3 ] df = bodo . rebalance ( df , dests = [ 0 , 1 ]) return df . sum ()","title":"Load Balancing Distributed Data"},{"location":"bodo_parallelism/advanced/#explicit-parallel-loops","text":"Sometimes explicit parallel loops are required since a program cannot be written in terms of data-parallel operators easily. In this case, one can use Bodo's prange in place of range to specify that a loop can be parallelized. The user is required to make sure the loop does not have cross iteration dependencies except for supported reductions. The example below demonstrates a parallel loop with a reduction: import bodo from bodo import prange import numpy as np @bodo . jit def prange_test ( n ): A = np . random . ranf ( n ) s = 0 B = np . empty ( n ) for i in prange ( len ( A )): bodo . parallel_print ( \"rank\" , bodo . get_rank ()) # A[i]: distributed data access with loop index # s: a supported sum reduction s += A [ i ] # write array with loop index B [ i ] = 2 * A [ i ] return s + B . sum () res = prange_test ( 10 ) print ( res ) Output: [stdout:0] rank 0 rank 0 rank 0 13.077183553245497 [stdout:1] rank 1 rank 1 rank 1 13.077183553245497 [stdout:2] rank 2 rank 2 13.077183553245497 [stdout:3] rank 3 rank 3 13.077183553245497 Currently, reductions using += , *= , min , and max operators are supported. Iterations are simply divided between processes and executed in parallel, but reductions are handled using data exchange.","title":"Explicit Parallel Loops"},{"location":"bodo_parallelism/advanced/#integration-with-non-bodo-apis","text":"There are multiple methods for integration with APIs that Bodo does not support natively: 1. Switch to python object mode inside jit functions 2. Pass data in and out of jit functions","title":"Integration with non-Bodo APIs"},{"location":"bodo_parallelism/advanced/#passing-distributed-data","text":"Bodo can receive or return chunks of distributed data to allow flexible integration with any non-Bodo Python code. The following example passes chunks of data to interpolate with Scipy, and returns interpolation results back to jit function. import scipy.interpolate @bodo . jit ( distributed = [ \"X\" , \"Y\" , \"X2\" ]) def dist_pass_test ( n ): X = np . arange ( n ) Y = np . exp ( - X / 3.0 ) X2 = np . arange ( 0 , n , 0.5 ) return X , Y , X2 X , Y , X2 = dist_pass_test ( 100 ) # clip potential out-of-range values X2 = np . minimum ( np . maximum ( X2 , X [ 0 ]), X [ - 1 ]) f = scipy . interpolate . interp1d ( X , Y ) Y2 = f ( X2 ) @bodo . jit ( distributed = { \"Y2\" }) def dist_pass_res ( Y2 ): return Y2 . sum () res = dist_pass_res ( Y2 ) print ( res ) [stdout:0] 6.555500504321469 [stdout:1] 6.555500504321469 [stdout:2] 6.555500504321469 [stdout:3] 6.555500504321469","title":"Passing Distributed Data"},{"location":"bodo_parallelism/advanced/#collections-of-distributed-data","text":"List and dictionary collections can be used to hold distributed data structures: @bodo . jit ( distributed = [ \"df\" ]) def f (): to_concat = [] for i in range ( 10 ): to_concat . append ( pd . DataFrame ({ 'A' : np . arange ( 100 ), 'B' : np . random . random ( 100 )})) df = pd . concat ( to_concat ) return df f ()","title":"Collections of Distributed Data"},{"location":"bodo_parallelism/advanced/#run_on_single_rank","text":"In cases where some code needs to be run on a single MPI rank, you can do so in a python script as follows: if bodo . get_rank () == 0 : # Remove directory import os , shutil if os . path . exists ( \"data/data.pq\" ): shutil . rmtree ( \"data/data.pq\" ) # To synchronize all ranks before proceeding bodo . barrier () When running code on an IPyParallel cluster using the %%px magic, you can do this instead: %% px -- targets 0 # Install package ! conda install pandas - datareader An alias can be defined for convenience: % alias_magic p0 px - p \"--targets 0\" This can be used as any other magic: %% p0 # Install package ! conda install pandas - datareader","title":"Run code on a single rank"},{"location":"bodo_parallelism/advanced/#run_on_each_node","text":"In cases where some code needs to be run once on each node in a multi-node cluster, such as a file system operation, installing packages, etc., it can be done as follows: if bodo . get_rank () in bodo . get_nodes_first_ranks (): # Remove directory on all nodes import os , shutil if os . path . exists ( \"data/data.pq\" ): shutil . rmtree ( \"data/data.pq\" ) # To synchronize all ranks before proceeding bodo . barrier () The same can be done when running on an IPyParallel cluster using the %%px magic: %% px if bodo . get_rank () in bodo . get_nodes_first_ranks (): # Install package on all nodes ! conda install pandas - datareader Warning Running code on a single rank or a subset of ranks can lead to deadlocks. Ensure that your code doesn't include any MPI or Bodo functions.","title":"Run code once on each node"},{"location":"bodo_parallelism/bodo_parallelism_basics/","text":"Bodo Parallelism Basics \u00b6 In this section, we will discuss Bodo's JIT compilation workflow and the parallelism model and APIs provided by Bodo. JIT (Just-in-time) Compilation Workflow \u00b6 Bodo provides a just-in-time (JIT) compilation workflow using the @bodo.jit decorator, which replaces a Python function with a so-called Dispatcher object. Bodo compiles the function the first time a Dispatcher object is called and reuses the compiled version afterwards. The function is recompiled only if the same function is called with different argument types (not often in practice). All of this is completely transparent to the caller, and does not affect any Python code calling the function. >>> import numpy as np >>> import pandas as pd >>> import bodo >>> @bodo . jit ... def f ( n , a ): ... df = pd . DataFrame ({ \"A\" : np . arange ( n ) + a }) ... return df . head ( 3 ) ... >>> print ( f ) CPUDispatcher ( < function f at 0x100bec310 > ) >>> print ( f ( 8 , 1 )) # compiles for (int, int) input types A 0 1 1 2 2 3 >>> print ( f ( 8 , 2 )) # same input types, no need to compile A 0 2 1 3 2 4 >>> print ( f ( 8 , 2.2 )) # compiles for (int, float) input types A 0 2.2 1 3.2 2 4.2 Note In many cases, the binary that Bodo generates when compiling a function can be saved to disk and reused across program executions. See caching for more information. Parallel Execution Model \u00b6 As we saw in the \"Getting Started\" tutorial, Bodo transforms functions for parallel execution. Bodo uses Message Passing Interface ( MPI ) that follows Single Program Multiple Data ( SPMD ) paradigm. In this model, the dispatcher does not launch processes or threads on the fly. Instead, all processes are launched at the beginning and run the same file using mpiexec command. Bodo parallelizes functions with the bodo.jit decorator by distributing the data across the processes. Each rank runs the same code on a chunk of the data, and Bodo automatically communicates the data between the ranks (as needed). For example, save the following code in a test_bodo.py and use mpiexec to launch 4 processes as follows: import numpy as np import pandas as pd import bodo @bodo . jit def f ( n , a ): df = pd . DataFrame ({ \"A\" : np . arange ( n ) + a }) return df print ( f ( 8 , 1 )) mpiexec -n 4 python test_bodo.py Output: A 2 3 3 4 A 6 7 7 8 A 4 5 5 6 A 0 1 1 2 In this example, mpiexec launches 4 Python processes, each executing the same test_bodo.py file. Since the function f is decorated with bodo.jit and Bodo can parallelize it, each process generates a chunk of the data in np.arange . Note how the prints, which are regular Python code executed outside of Bodo, run for each process. Warning Python codes outside of Bodo functions execute sequentially on every process. Bodo functions run in parallel assuming that Bodo is able to parallelize them. Otherwise, Bodo prints the following warning and runs sequentially on every process. BodoWarning: No parallelism found for function On Jupyter notebook, parallel execution happens in very much the same way. We start a set of MPI engines through ipyparallel and activate a client. See how to use bodo with jupyter notebooks for more information and examples. See Also Parallel APIs Data Distribution \u00b6 Bodo parallelizes computation by dividing data into separate chunks across processes. However, some data handled by a Bodo function may not be divided into chunks. There are are two main data distribution schemes: Replicated ( REP ): the data associated with the variable is the same on every process. One-dimensional ( 1D ): the data is divided into chunks, split along one dimension (rows of a dataframe or first dimension of an array). Bodo determines distribution of variables automatically, using the nature of the computation that produces them. Let's see an example: import bodo import pandas as pd @bodo . jit def mean_power_speed (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) m = df [[ \"power\" , \"speed\" ]] . mean () return m res = mean_power_speed () print ( res ) Save code in mean_power_speed.py and run it with mpiexec as follows: mpiexec -n 4 python mean_power_speed.py [stdout:0] power 102.078421 speed 5.656851 dtype: float64 [stdout:1] power 102.078421 speed 5.656851 dtype: float64 [stdout:2] power 102.078421 speed 5.656851 dtype: float64 [stdout:3] power 102.078421 speed 5.656851 dtype: float64 In this example, df is parallelized (each process reads a different chunk) but m is replicated, even though it is a Series. Semantically, it makes sense for the output of mean operation to be replicated on all processors, since it is a reduction and produces \"small\" data. Distributed Diagnostics \u00b6 The distributions found by Bodo can be printed either by setting the environment variable BODO_DISTRIBUTED_DIAGNOSTICS=1 or calling distributed_diagnostics() on the compiled function. Let's examine the previous example's distributions by adding following line to mean_power_speed script: mean_power_speed.distributed_diagnostics () python mean_power_speed.py Distributed analysis replicated return variable $30return_value.12. Set distributed flag for the original variable if distributed partitions should be returned. [stdout:0] python mean_power_speed.py power 102.078421 speed 5.656851 dtype: float64 Distributed diagnostics for function mean_power_speed, /Users/mean_power_speed.py (3) Data distributions: pq_table.0 1D_Block pq_index.1 1D_Block data_74 REP Parfor distributions: 0 1D_Block 1 1D_Block Distributed listing for function mean_power_speed, /Users/hadia/Bodo/testing/mean_power_speed.py (3) ---------------------------------------------------------------------| parfor_id/variable: distribution @bodo.jit | def mean_power_speed(): | df = pd.read_parquet(\"Bodo-tutorial/data/cycling_dataset.pq\")----| pq_table.0: 1D_Block, pq_index.1: 1D_Block m = df[[\"power\", \"speed\"]].mean()--------------------------------| #0: 1D_Block, #1: 1D_Block, data_74: REP return m | Setting distribution of variable 'impl_v48_data_74' to REP: output of np.asarray() call on non-array is REP Bodo compiler optimizations rename the variables. The output shows that power and speed columns of df are distributed ( 1D_Block ), but m is replicated ( REP ). This is because df is the output from read_parquet and input to mean , both of which can be distributed by Bodo. m is the output from mean , which is replicated (available on every process).","title":"Basics of Bodo Parallelism"},{"location":"bodo_parallelism/bodo_parallelism_basics/#basics","text":"In this section, we will discuss Bodo's JIT compilation workflow and the parallelism model and APIs provided by Bodo.","title":"Bodo Parallelism Basics"},{"location":"bodo_parallelism/bodo_parallelism_basics/#jit","text":"Bodo provides a just-in-time (JIT) compilation workflow using the @bodo.jit decorator, which replaces a Python function with a so-called Dispatcher object. Bodo compiles the function the first time a Dispatcher object is called and reuses the compiled version afterwards. The function is recompiled only if the same function is called with different argument types (not often in practice). All of this is completely transparent to the caller, and does not affect any Python code calling the function. >>> import numpy as np >>> import pandas as pd >>> import bodo >>> @bodo . jit ... def f ( n , a ): ... df = pd . DataFrame ({ \"A\" : np . arange ( n ) + a }) ... return df . head ( 3 ) ... >>> print ( f ) CPUDispatcher ( < function f at 0x100bec310 > ) >>> print ( f ( 8 , 1 )) # compiles for (int, int) input types A 0 1 1 2 2 3 >>> print ( f ( 8 , 2 )) # same input types, no need to compile A 0 2 1 3 2 4 >>> print ( f ( 8 , 2.2 )) # compiles for (int, float) input types A 0 2.2 1 3.2 2 4.2 Note In many cases, the binary that Bodo generates when compiling a function can be saved to disk and reused across program executions. See caching for more information.","title":"JIT (Just-in-time) Compilation Workflow"},{"location":"bodo_parallelism/bodo_parallelism_basics/#parallel-execution-model","text":"As we saw in the \"Getting Started\" tutorial, Bodo transforms functions for parallel execution. Bodo uses Message Passing Interface ( MPI ) that follows Single Program Multiple Data ( SPMD ) paradigm. In this model, the dispatcher does not launch processes or threads on the fly. Instead, all processes are launched at the beginning and run the same file using mpiexec command. Bodo parallelizes functions with the bodo.jit decorator by distributing the data across the processes. Each rank runs the same code on a chunk of the data, and Bodo automatically communicates the data between the ranks (as needed). For example, save the following code in a test_bodo.py and use mpiexec to launch 4 processes as follows: import numpy as np import pandas as pd import bodo @bodo . jit def f ( n , a ): df = pd . DataFrame ({ \"A\" : np . arange ( n ) + a }) return df print ( f ( 8 , 1 )) mpiexec -n 4 python test_bodo.py Output: A 2 3 3 4 A 6 7 7 8 A 4 5 5 6 A 0 1 1 2 In this example, mpiexec launches 4 Python processes, each executing the same test_bodo.py file. Since the function f is decorated with bodo.jit and Bodo can parallelize it, each process generates a chunk of the data in np.arange . Note how the prints, which are regular Python code executed outside of Bodo, run for each process. Warning Python codes outside of Bodo functions execute sequentially on every process. Bodo functions run in parallel assuming that Bodo is able to parallelize them. Otherwise, Bodo prints the following warning and runs sequentially on every process. BodoWarning: No parallelism found for function On Jupyter notebook, parallel execution happens in very much the same way. We start a set of MPI engines through ipyparallel and activate a client. See how to use bodo with jupyter notebooks for more information and examples. See Also Parallel APIs","title":"Parallel Execution Model"},{"location":"bodo_parallelism/bodo_parallelism_basics/#data-distribution","text":"Bodo parallelizes computation by dividing data into separate chunks across processes. However, some data handled by a Bodo function may not be divided into chunks. There are are two main data distribution schemes: Replicated ( REP ): the data associated with the variable is the same on every process. One-dimensional ( 1D ): the data is divided into chunks, split along one dimension (rows of a dataframe or first dimension of an array). Bodo determines distribution of variables automatically, using the nature of the computation that produces them. Let's see an example: import bodo import pandas as pd @bodo . jit def mean_power_speed (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) m = df [[ \"power\" , \"speed\" ]] . mean () return m res = mean_power_speed () print ( res ) Save code in mean_power_speed.py and run it with mpiexec as follows: mpiexec -n 4 python mean_power_speed.py [stdout:0] power 102.078421 speed 5.656851 dtype: float64 [stdout:1] power 102.078421 speed 5.656851 dtype: float64 [stdout:2] power 102.078421 speed 5.656851 dtype: float64 [stdout:3] power 102.078421 speed 5.656851 dtype: float64 In this example, df is parallelized (each process reads a different chunk) but m is replicated, even though it is a Series. Semantically, it makes sense for the output of mean operation to be replicated on all processors, since it is a reduction and produces \"small\" data.","title":"Data Distribution"},{"location":"bodo_parallelism/bodo_parallelism_basics/#distributed-diagnostics","text":"The distributions found by Bodo can be printed either by setting the environment variable BODO_DISTRIBUTED_DIAGNOSTICS=1 or calling distributed_diagnostics() on the compiled function. Let's examine the previous example's distributions by adding following line to mean_power_speed script: mean_power_speed.distributed_diagnostics () python mean_power_speed.py Distributed analysis replicated return variable $30return_value.12. Set distributed flag for the original variable if distributed partitions should be returned. [stdout:0] python mean_power_speed.py power 102.078421 speed 5.656851 dtype: float64 Distributed diagnostics for function mean_power_speed, /Users/mean_power_speed.py (3) Data distributions: pq_table.0 1D_Block pq_index.1 1D_Block data_74 REP Parfor distributions: 0 1D_Block 1 1D_Block Distributed listing for function mean_power_speed, /Users/hadia/Bodo/testing/mean_power_speed.py (3) ---------------------------------------------------------------------| parfor_id/variable: distribution @bodo.jit | def mean_power_speed(): | df = pd.read_parquet(\"Bodo-tutorial/data/cycling_dataset.pq\")----| pq_table.0: 1D_Block, pq_index.1: 1D_Block m = df[[\"power\", \"speed\"]].mean()--------------------------------| #0: 1D_Block, #1: 1D_Block, data_74: REP return m | Setting distribution of variable 'impl_v48_data_74' to REP: output of np.asarray() call on non-array is REP Bodo compiler optimizations rename the variables. The output shows that power and speed columns of df are distributed ( 1D_Block ), but m is replicated ( REP ). This is because df is the output from read_parquet and input to mean , both of which can be distributed by Bodo. m is the output from mean , which is replicated (available on every process).","title":"Distributed Diagnostics"},{"location":"bodo_parallelism/compile_time_constants/","text":"Compile Time Constants \u00b6 Unlike regular Python, which is dynamically typed, Bodo needs to be able to type all functions at compile time. While in most cases, the output types depend solely on the input types, some APIs require knowing exact values in order to produce accurate types. As an example, consider the iloc DataFrame API. This API can be used to selected a subset of rows and columns by passing integers or slices of integers. A Bodo JIT version of a function calling this API might look like: import numpy as np import pandas as pd import bodo @bodo . jit def df_iloc ( df , rows , columns ): return df . iloc [ rows , columns ] df = pd . DataFrame ({ 'A' : np . arange ( 100 ), 'B' : [ \"A\" , \"B\" , \"C\" , \"D\" ] * 25 }) print ( df_iloc ( df , slice ( 1 , 4 ), 0 )) If we try to run this file, we will get an error message: $ python iloc_example.py Traceback (most recent call last): File \"iloc_example.py\", line 10, in <module> df_iloc(df, slice(1, 4), 0) File \"/my_path/bodo/numba_compat.py\", line 1195, in _compile_for_args raise error bodo.utils.typing.BodoError: idx2 in df.iloc[idx1, idx2] should be a constant integer or constant list of integers File \"iloc_example.py\", line 7: def df_iloc(df, rows, columns): return df.iloc[rows, columns] The relevant part of the error message is idx2 in df.iloc[idx1, idx2] should be a constant integer or constant list of integers . This error is thrown because depending on the value of columns , Bodo selects different columns with different types. When columns=0 Bodo will need to compile code for numeric values, but when columns=1 Bodo needs to compile code for strings, so it cannot properly type this function. To resolve this issue, you will need to replace columns with a literal integer. If instead the Bodo function is written as: import numpy as np import pandas as pd import bodo @bodo . jit def df_iloc ( df , rows ): return df . iloc [ rows , 0 ] df = pd . DataFrame ({ 'A' : np . arange ( 100 ), 'B' : [ \"A\" , \"B\" , \"C\" , \"D\" ] * 25 }) print ( df_iloc ( df , slice ( 1 , 4 ))) Bodo now can see that the output DataFrame should have a single int64 column and it is able to compile the code. Whenever a value needs to be known for typing purposes, Bodo will throw an error that indicates some argument requires a constant value . All of these can be resolved by making this value a literal. Alternatively, some APIs support other ways of specifying the output types, which will be indicated in the error message.","title":"Compile Time Constants"},{"location":"bodo_parallelism/compile_time_constants/#require_constants","text":"Unlike regular Python, which is dynamically typed, Bodo needs to be able to type all functions at compile time. While in most cases, the output types depend solely on the input types, some APIs require knowing exact values in order to produce accurate types. As an example, consider the iloc DataFrame API. This API can be used to selected a subset of rows and columns by passing integers or slices of integers. A Bodo JIT version of a function calling this API might look like: import numpy as np import pandas as pd import bodo @bodo . jit def df_iloc ( df , rows , columns ): return df . iloc [ rows , columns ] df = pd . DataFrame ({ 'A' : np . arange ( 100 ), 'B' : [ \"A\" , \"B\" , \"C\" , \"D\" ] * 25 }) print ( df_iloc ( df , slice ( 1 , 4 ), 0 )) If we try to run this file, we will get an error message: $ python iloc_example.py Traceback (most recent call last): File \"iloc_example.py\", line 10, in <module> df_iloc(df, slice(1, 4), 0) File \"/my_path/bodo/numba_compat.py\", line 1195, in _compile_for_args raise error bodo.utils.typing.BodoError: idx2 in df.iloc[idx1, idx2] should be a constant integer or constant list of integers File \"iloc_example.py\", line 7: def df_iloc(df, rows, columns): return df.iloc[rows, columns] The relevant part of the error message is idx2 in df.iloc[idx1, idx2] should be a constant integer or constant list of integers . This error is thrown because depending on the value of columns , Bodo selects different columns with different types. When columns=0 Bodo will need to compile code for numeric values, but when columns=1 Bodo needs to compile code for strings, so it cannot properly type this function. To resolve this issue, you will need to replace columns with a literal integer. If instead the Bodo function is written as: import numpy as np import pandas as pd import bodo @bodo . jit def df_iloc ( df , rows ): return df . iloc [ rows , 0 ] df = pd . DataFrame ({ 'A' : np . arange ( 100 ), 'B' : [ \"A\" , \"B\" , \"C\" , \"D\" ] * 25 }) print ( df_iloc ( df , slice ( 1 , 4 ))) Bodo now can see that the output DataFrame should have a single int64 column and it is able to compile the code. Whenever a value needs to be known for typing purposes, Bodo will throw an error that indicates some argument requires a constant value . All of these can be resolved by making this value a literal. Alternatively, some APIs support other ways of specifying the output types, which will be indicated in the error message.","title":"Compile Time Constants"},{"location":"bodo_parallelism/not_supported/","text":"Unsupported Python Programs \u00b6 Bodo compiles functions into efficient native parallel binaries, which requires all the operations used in the code to be supported by Bodo. This excludes some Python features explained in this section. Type Stability \u00b6 To enable type inference, the program should be type stable , which means Bodo should be able to assign a single type to every variable. DataFrame Schema \u00b6 Deterministic dataframe schemas, which are required in most data systems, is key for type stability. For example, variable df in example below could be either a single column dataframe or a two column one -- Bodo cannot determine it at compilation time: @bodo . jit def f ( a ): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) df2 = pd . DataFrame ({ \"A\" : [ 1 , 3 , 4 ], \"C\" : [ - 1 , - 2 , - 3 ]}) if len ( a ) > 3 : df = df . merge ( df2 ) return df . mean () print ( f ([ 2 , 3 ])) # TypeError: Cannot unify dataframe((array(int64, 1d, C),), RangeIndexType(none), ('A',), False) # and dataframe((array(int64, 1d, C), array(int64, 1d, C)), RangeIndexType(none), ('A', 'C'), False) for 'df' The error message means that Bodo cannot find a type that can unify the two types into a single type. This code can be refactored so that the if control flow is executed in regular Python context, but the rest of computation is in Bodo functions. For example, one could use two versions of the function: @bodo . jit def f1 (): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) return df . mean () @bodo . jit def f2 (): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) df2 = pd . DataFrame ({ \"A\" : [ 1 , 3 , 4 ], \"C\" : [ - 1 , - 2 , - 3 ]}) df = df . merge ( df2 ) return df . mean () a = [ 2 , 3 ] if len ( a ) > 3 : print ( f1 ()) else : print ( f2 ()) Another common place where schema stability may be compromised is in passing non-constant list of key column names to dataframe operations such as groupby , merge and sort_values . In these operations, Bodo should be able to deduce the list of key column names at compile time in order to determine the output dataframe schema. For example, the program below is potentially type unstable since Bodo may not be able to infer column_list during compilation: @bodo . jit def f ( a , i ): column_list = a [: i ] # some computation that cannot be inferred statically df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 1 ], \"B\" : [ 4 , 5 , 6 ]}) return df . groupby ( column_list ) . sum () a = [ \"A\" , \"B\" ] i = 1 f ( a , i ) # BodoError: groupby(): 'by' parameter only supports a constant column label or column labels. This code can be refactored so that the computation for column_list is performed in regular Python context, and the result is passed as a function argument: @bodo . jit def f ( column_list ): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 1 ], \"B\" : [ 4 , 5 , 6 ]}) return df . groupby ( column_list ) . sum () a = [ \"A\" , \"B\" ] i = 1 column_list = a [: i ] f ( column_list ) In general, Bodo can infer constants from function arguments, global variables, and constant values in the program. Furthermore, Bodo supports implicitly inferring constant lists automatically for list addition and set difference operations such as: df . groupby ([ \"A\" ] + [ \"B\" ]) . sum () df . groupby ( list ( set ( df . columns ) - set ([ \"A\" , \"C\" ]))) . sum () Bodo will support inferring more implicit constant cases in the future (e.g. more list and set operations). Referring to dataframe columns (e.g. [df[\"A\"]] ) requires constants for schema stability as well. for loops over dataframe column names such as below is not supported yet: @bodo . jit def f ( df ): s = 0 for c in df . columns : s += df [ c ] . sum () return s f ( pd . DataFrame ({ \"A\" : [ 1 , 2 , 1 ], \"B\" : [ 4 , 5 , 6 ]})) # BodoError: df[] getitem selecting a subset of columns requires providing constant column names. For more information, see https://docs.bodo.ai/latest/source/programming_with_bodo/require_constants.html Variable Types and Functions \u00b6 The example below is not type stable since variable a can be both a float and an array of floats: if flag : a = 1.0 else : a = np . ones ( 10 ) The use of isinstance operator of Python often means type instability and is not supported. Similarly, function calls should also be deterministic. The below example is not supported since the function f is not known in advance: if flag : f = np . zeros else : f = np . random . ranf A = f ( 10 ) One can usually avoid these cases in analytics codes without significant effort. Accessing individual values of nullable data \u00b6 The type of null (NA) value for most nullable data arrays is different than regular values (except float data which stores np.nan ). Therefore, accessing individual values (i.e. using [[]] with an integer index) may not be type stable. In these cases, Bodo assumes the value is not NA and returns an \"neutral\" value: @bodo . jit def f ( S , i ): return S . iloc [ i ] # not type stable S = pd . Series ([ \"A\" , None , \"CC\" ]) f ( S , 1 ) # returns \"\" The solution is to check for NA values using pd.isna to handle NA values appropriately: @bodo . jit def f ( S , i ): if pd . isna ( S . iloc [ i ]): return \"NA\" return S . iloc [ i ] S = pd . Series ([ \"A\" , None , \"CC\" ]) f ( S , 1 ) # returns \"NA\" We are working on making it possible to avoid stability issues automatically in most practical cases. Unsupported Python Constructs \u00b6 Bodo relies on Numba for supporting basic Python features. Therefore, Python constructs that are not supported by Numba should be avoided in Bodo programs. Generally, these Python features are not supported: exceptions: try .. except , raise context manager: with list, set, dict and generator comprehensions async features class definition: class jit functions cannot have **kwargs functions can be passed as arguments but not returned lists of lists cannot be passed as arguments unless Numba typed-lists are used. Numba typed-dicts are currently required for passing dictionaries as argument to jit functions. Heterogeneous types inside a data structure \u00b6 List containing values of heterogeneous type: myList = [ 1 , \"a\" , 0.1 ] Dictionary containing values of heterogeneous type myDict = { \"A\" : 1 , \"B\" : \"a\" , \"C\" : 0.1 }","title":"Unsupported Programs"},{"location":"bodo_parallelism/not_supported/#notsupported","text":"Bodo compiles functions into efficient native parallel binaries, which requires all the operations used in the code to be supported by Bodo. This excludes some Python features explained in this section.","title":"Unsupported Python Programs"},{"location":"bodo_parallelism/not_supported/#typestability","text":"To enable type inference, the program should be type stable , which means Bodo should be able to assign a single type to every variable.","title":"Type Stability"},{"location":"bodo_parallelism/not_supported/#schemastability","text":"Deterministic dataframe schemas, which are required in most data systems, is key for type stability. For example, variable df in example below could be either a single column dataframe or a two column one -- Bodo cannot determine it at compilation time: @bodo . jit def f ( a ): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) df2 = pd . DataFrame ({ \"A\" : [ 1 , 3 , 4 ], \"C\" : [ - 1 , - 2 , - 3 ]}) if len ( a ) > 3 : df = df . merge ( df2 ) return df . mean () print ( f ([ 2 , 3 ])) # TypeError: Cannot unify dataframe((array(int64, 1d, C),), RangeIndexType(none), ('A',), False) # and dataframe((array(int64, 1d, C), array(int64, 1d, C)), RangeIndexType(none), ('A', 'C'), False) for 'df' The error message means that Bodo cannot find a type that can unify the two types into a single type. This code can be refactored so that the if control flow is executed in regular Python context, but the rest of computation is in Bodo functions. For example, one could use two versions of the function: @bodo . jit def f1 (): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) return df . mean () @bodo . jit def f2 (): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ]}) df2 = pd . DataFrame ({ \"A\" : [ 1 , 3 , 4 ], \"C\" : [ - 1 , - 2 , - 3 ]}) df = df . merge ( df2 ) return df . mean () a = [ 2 , 3 ] if len ( a ) > 3 : print ( f1 ()) else : print ( f2 ()) Another common place where schema stability may be compromised is in passing non-constant list of key column names to dataframe operations such as groupby , merge and sort_values . In these operations, Bodo should be able to deduce the list of key column names at compile time in order to determine the output dataframe schema. For example, the program below is potentially type unstable since Bodo may not be able to infer column_list during compilation: @bodo . jit def f ( a , i ): column_list = a [: i ] # some computation that cannot be inferred statically df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 1 ], \"B\" : [ 4 , 5 , 6 ]}) return df . groupby ( column_list ) . sum () a = [ \"A\" , \"B\" ] i = 1 f ( a , i ) # BodoError: groupby(): 'by' parameter only supports a constant column label or column labels. This code can be refactored so that the computation for column_list is performed in regular Python context, and the result is passed as a function argument: @bodo . jit def f ( column_list ): df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 1 ], \"B\" : [ 4 , 5 , 6 ]}) return df . groupby ( column_list ) . sum () a = [ \"A\" , \"B\" ] i = 1 column_list = a [: i ] f ( column_list ) In general, Bodo can infer constants from function arguments, global variables, and constant values in the program. Furthermore, Bodo supports implicitly inferring constant lists automatically for list addition and set difference operations such as: df . groupby ([ \"A\" ] + [ \"B\" ]) . sum () df . groupby ( list ( set ( df . columns ) - set ([ \"A\" , \"C\" ]))) . sum () Bodo will support inferring more implicit constant cases in the future (e.g. more list and set operations). Referring to dataframe columns (e.g. [df[\"A\"]] ) requires constants for schema stability as well. for loops over dataframe column names such as below is not supported yet: @bodo . jit def f ( df ): s = 0 for c in df . columns : s += df [ c ] . sum () return s f ( pd . DataFrame ({ \"A\" : [ 1 , 2 , 1 ], \"B\" : [ 4 , 5 , 6 ]})) # BodoError: df[] getitem selecting a subset of columns requires providing constant column names. For more information, see https://docs.bodo.ai/latest/source/programming_with_bodo/require_constants.html","title":"DataFrame Schema"},{"location":"bodo_parallelism/not_supported/#variable-types-and-functions","text":"The example below is not type stable since variable a can be both a float and an array of floats: if flag : a = 1.0 else : a = np . ones ( 10 ) The use of isinstance operator of Python often means type instability and is not supported. Similarly, function calls should also be deterministic. The below example is not supported since the function f is not known in advance: if flag : f = np . zeros else : f = np . random . ranf A = f ( 10 ) One can usually avoid these cases in analytics codes without significant effort.","title":"Variable Types and Functions"},{"location":"bodo_parallelism/not_supported/#accessing-individual-values-of-nullable-data","text":"The type of null (NA) value for most nullable data arrays is different than regular values (except float data which stores np.nan ). Therefore, accessing individual values (i.e. using [[]] with an integer index) may not be type stable. In these cases, Bodo assumes the value is not NA and returns an \"neutral\" value: @bodo . jit def f ( S , i ): return S . iloc [ i ] # not type stable S = pd . Series ([ \"A\" , None , \"CC\" ]) f ( S , 1 ) # returns \"\" The solution is to check for NA values using pd.isna to handle NA values appropriately: @bodo . jit def f ( S , i ): if pd . isna ( S . iloc [ i ]): return \"NA\" return S . iloc [ i ] S = pd . Series ([ \"A\" , None , \"CC\" ]) f ( S , 1 ) # returns \"NA\" We are working on making it possible to avoid stability issues automatically in most practical cases.","title":"Accessing individual values of nullable data"},{"location":"bodo_parallelism/not_supported/#notsupportedpython","text":"Bodo relies on Numba for supporting basic Python features. Therefore, Python constructs that are not supported by Numba should be avoided in Bodo programs. Generally, these Python features are not supported: exceptions: try .. except , raise context manager: with list, set, dict and generator comprehensions async features class definition: class jit functions cannot have **kwargs functions can be passed as arguments but not returned lists of lists cannot be passed as arguments unless Numba typed-lists are used. Numba typed-dicts are currently required for passing dictionaries as argument to jit functions.","title":"Unsupported Python Constructs"},{"location":"bodo_parallelism/not_supported/#heterogeneousdtype","text":"List containing values of heterogeneous type: myList = [ 1 , \"a\" , 0.1 ] Dictionary containing values of heterogeneous type myDict = { \"A\" : 1 , \"B\" : \"a\" , \"C\" : 0.1 }","title":"Heterogeneous types inside a data structure"},{"location":"diagnostics_and_troubleshooting/Bodoerrors/","text":"Bodo Error Messages \u00b6 This section lists some of the compilation error messages you may encounter with your jitted functions, reasons for them and suggestions on how to proceed with resolving them. Unsupported Bodo Functionality \u00b6 BodoError: <functionality> not supported yet As the error states, this message is encountered when you are attempting to call an as yet unsupported API within a jit function. For example : @bodo . jit def unsupported_func ( pd_str_series ): return pd_str_series . str . casefold () would result in an unsupported BodoError as follows: BodoError: Series.str.casefold not supported yet Please submit a request for us to support your required functionality here . Also consider joining our community slack , where you can interact directly with fellow Bodo users to find a workaround for your requirements. For longer and more detailed discussions, please join our discourse . See Also Object Mode can be used to switch to Python interpreted context to be able to run your workload, but we strongly recommend trying to find a Bodo-native workaround. BodoError: <operation> : <parameter_name> parameter only supports default value Certain methods only support default parameter values for some of their parameters. Please see supported Pandas API for a list of supported pandas functionality and their respective parameters. We also have a list of supported Numpy , as well as ML operations. Typing Errors \u00b6 BodoError: <operation>: <operand> must be a compile time constant Bodo needs certain arguments to be known at compile time to produce an optimized binary. Please see the section on compile time constants for more details. BodoError: dtype <DataType> cannot be stored in arrays This error message is encountered when Bodo is unable to assign a supported type to elements of an array. Example: @bodo . jit def obj_in_array (): df = pd . DataFrame ({ 'col1' : [ \"1\" , \"2\" ], 'col2' : [ 3 , 4 ]}) return df . select_dtypes ( include = 'object' ) a = obj_in_array () print ( a ) Error: BodoError: dtype pyobject cannot be stored in arrays In this example, we get this error because we attempted to get Bodo to recognize col1 as a column with the datatype object , and the object type is too generic for Bodo. A workaround for this specific example would be to return df.select_dtypes(exclude='int') . Invalid Series.dt/Series.cat/Series.str, cannot handle conditional yet This error is encountered when there are conditional assignments of series functions Series.dt , Series.cat or Series.str , which Bodo cannot handle yet. Example: @bodo . jit def conditional_series_str ( flag ): s = pd . Series ([ \"Str_Series\" ]) s1 = pd . Series ([ \"Str_Series_1\" ]) . str if flag : s1 = s . str else : s1 = s1 return s1 . split ( \"_\" ) Error: BodoError: ... Invalid Series.str, cannot handle conditional yet When using these operations, you need to include the function and accessor together inside the control flow if it is absolutely necessary. For this specific case, we simply compute the str.split within the conditional: @bodo . jit def test_category ( flag ): s = pd . Series ([ \"A_Str_Series\" ]) s1 = pd . Series ([ \"test_series\" ]) . str s2 = None if flag : s2 = s . str . split ( \"_\" ) else : s2 = s1 . split ( \"_\" ) return s2 Unsupported Numba Errors \u00b6 numba.core.errors.TypingError: Compilation error This is likely due to unsupported functionality. If you encounter this error, please provide us a minimum reproducer for this error here . numba.core.errors.TypingError: Unknown attribute <attribute> of type This is an uncaught error due to unsupported functionality. If you encounter this error, please provide us a minimum reproducer for this error here .","title":"Bodo Errors"},{"location":"diagnostics_and_troubleshooting/Bodoerrors/#bodoerrors","text":"This section lists some of the compilation error messages you may encounter with your jitted functions, reasons for them and suggestions on how to proceed with resolving them.","title":"Bodo Error Messages"},{"location":"diagnostics_and_troubleshooting/Bodoerrors/#unsupported-bodo-functionality","text":"BodoError: <functionality> not supported yet As the error states, this message is encountered when you are attempting to call an as yet unsupported API within a jit function. For example : @bodo . jit def unsupported_func ( pd_str_series ): return pd_str_series . str . casefold () would result in an unsupported BodoError as follows: BodoError: Series.str.casefold not supported yet Please submit a request for us to support your required functionality here . Also consider joining our community slack , where you can interact directly with fellow Bodo users to find a workaround for your requirements. For longer and more detailed discussions, please join our discourse . See Also Object Mode can be used to switch to Python interpreted context to be able to run your workload, but we strongly recommend trying to find a Bodo-native workaround. BodoError: <operation> : <parameter_name> parameter only supports default value Certain methods only support default parameter values for some of their parameters. Please see supported Pandas API for a list of supported pandas functionality and their respective parameters. We also have a list of supported Numpy , as well as ML operations.","title":"Unsupported Bodo Functionality"},{"location":"diagnostics_and_troubleshooting/Bodoerrors/#typing-errors","text":"BodoError: <operation>: <operand> must be a compile time constant Bodo needs certain arguments to be known at compile time to produce an optimized binary. Please see the section on compile time constants for more details. BodoError: dtype <DataType> cannot be stored in arrays This error message is encountered when Bodo is unable to assign a supported type to elements of an array. Example: @bodo . jit def obj_in_array (): df = pd . DataFrame ({ 'col1' : [ \"1\" , \"2\" ], 'col2' : [ 3 , 4 ]}) return df . select_dtypes ( include = 'object' ) a = obj_in_array () print ( a ) Error: BodoError: dtype pyobject cannot be stored in arrays In this example, we get this error because we attempted to get Bodo to recognize col1 as a column with the datatype object , and the object type is too generic for Bodo. A workaround for this specific example would be to return df.select_dtypes(exclude='int') . Invalid Series.dt/Series.cat/Series.str, cannot handle conditional yet This error is encountered when there are conditional assignments of series functions Series.dt , Series.cat or Series.str , which Bodo cannot handle yet. Example: @bodo . jit def conditional_series_str ( flag ): s = pd . Series ([ \"Str_Series\" ]) s1 = pd . Series ([ \"Str_Series_1\" ]) . str if flag : s1 = s . str else : s1 = s1 return s1 . split ( \"_\" ) Error: BodoError: ... Invalid Series.str, cannot handle conditional yet When using these operations, you need to include the function and accessor together inside the control flow if it is absolutely necessary. For this specific case, we simply compute the str.split within the conditional: @bodo . jit def test_category ( flag ): s = pd . Series ([ \"A_Str_Series\" ]) s1 = pd . Series ([ \"test_series\" ]) . str s2 = None if flag : s2 = s . str . split ( \"_\" ) else : s2 = s1 . split ( \"_\" ) return s2","title":"Typing Errors"},{"location":"diagnostics_and_troubleshooting/Bodoerrors/#unsupported-numba-errors","text":"numba.core.errors.TypingError: Compilation error This is likely due to unsupported functionality. If you encounter this error, please provide us a minimum reproducer for this error here . numba.core.errors.TypingError: Unknown attribute <attribute> of type This is an uncaught error due to unsupported functionality. If you encounter this error, please provide us a minimum reproducer for this error here .","title":"Unsupported Numba Errors"},{"location":"diagnostics_and_troubleshooting/compilation/","text":"Compilation Tips and Troubleshooting \u00b6 What Code to JIT Compile \u00b6 The general recommendation is to use Bodo JIT compilation only for code that is data and/or compute intensive (e.g. Pandas code on large dataframes). In other words: Only use Bodo for data processing and analytics code such as Pandas, Numpy, and Scikit-Learn (see Bodo API reference for analytics APIs with JIT support). Refactor code that sets up infrastructure or performs initializations out of JIT functions. This reduces the risk of encountering unsupported features and also reduces compilation time. For example, the program below finds the input file name in regular Python, and uses Bodo JIT only for data load and processing: def get_filename (): if os . path . exists ( \"input.parquet\" ): return \"input.parquet\" if \"INPUT_FILE\" in os . environ : return os . environ [ \"INPUT_FILE\" ] raise Exception ( \"Input file name not found\" ) @bodo . jit def f ( fname ): df = pd . read_parquet ( fname ) print ( df . sum ()) fname = get_filename () f ( fname ) This recommendation is similar to Numba's What to compile . Compilation Errors \u00b6 First of all, let us understand why the code may fail to compile. There are three main kinds of issues: Some API is used that is not supported in Bodo JIT yet (see Bodo API Reference ). Some Python construct or data structure is used that cannot be JIT compiled (see Unsupported Python APIs ). The code has type stability issues (see type stability ). Below are some examples of the type of errors you may see due to these issues. Unsupported Functions or Methods \u00b6 If a JIT function uses an unsupported function or method (e.g. in Pandas APIs), Bodo raises BodoError explaining that the method is not supported yet: BodoError: <method> not supported yet For example: >>> @bodo . jit ... def f ( df ): ... return df . swapaxes ( 0 , 1 ) ... >>> f ( df ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1198 , in _compile_for_args raise error bodo . utils . typing . BodoError : DataFrame . swapaxes () not supported yet Unsupported Attributes \u00b6 Attempting to access an unsupported attribute in Bodo JIT functions will result in a BodoError as follows: BodoError: <attribute> not supported yet For example: >>> @bodo . jit ... def f ( df ): ... return df . flags ... >>> f ( df ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1198 , in _compile_for_args raise error bodo . utils . typing . BodoError : DataFrame . flags not supported yet Unsupported Arguments \u00b6 Supported APIs may not support all optional arguments. Supplying an unsupported argument will result in a BodoError : BodoError: <method>: <keyword> argument not supported yet For example: >>> @bodo . jit ... def f ( df ): ... return df . sort_index ( key = lambda x : x . str . lower ()) ... >>> f ( df ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1198 , in _compile_for_args raise error bodo . utils . typing . BodoError : DataFrame . sort_index (): key parameter only supports default value None Type Stability Errors \u00b6 Bodo needs to infer data types for all program variables for successful JIT compilation. A type stability issue arises when different program control flow paths assign values with different types to a variable. For example, variable a below could either be an integer or a string: >>> @bodo . jit ... def f ( flag ): ... if flag : ... a = 3 ... else : ... a = \"A\" ... return a ... >>> f ( True ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1163 , in _compile_for_args error_rewrite ( e , \"typing\" ) File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1043 , in error_rewrite raise e . with_traceback ( None ) numba . core . errors . TypingError : Cannot unify Literal [ str ]( A ) and Literal [ int ]( 3 ) for 'a.2' , defined at < stdin > ( 7 ) The error TypingError: Cannot unify <type1> and <type2> means that the two possible data types cannot be combined and therefore, the variable cannot have a single data type. Dataframe variables require their schema (column names and their types) to be consistent for type stability (see dataframe schema stability ). For example, the dataframe variable df below could either have a single column (\"A\": integer) or two columns (\"A\": integer, \"B\": float) depending on the runtime value of flag , which results in a type stability error: >>> @bodo . jit ... def f ( flag ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 ]}) ... if flag : ... df [ \"B\" ] = [ 1.2 , 0.4 , 0.7 , 121.9 ] ... print ( df ) ... >>> f ( True ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1163 , in _compile_for_args error_rewrite ( e , \"typing\" ) File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1043 , in error_rewrite raise e . with_traceback ( None ) numba . core . errors . TypingError : Cannot unify dataframe (( array ( int64 , 1 d , C ),), RangeIndexType ( none ), ( 'A' ,), 1 D_Block_Var , False ) and dataframe (( array ( int64 , 1 d , C ), array ( float64 , 1 d , C )), RangeIndexType ( none ), ( 'A' , 'B' ), 1 D_Block_Var , False ) for 'df' , defined at < stdin > ( 3 ) Additionally, some function arguments need to be constant to ensure type stability. In certain cases where it is possible, Bodo may infer the constant values. In other cases, it may throw an error indicating that the argument should be constant. For instance, axis argument in pd.concat determines whether the output is a Series type or a dataframe type in the example below. Therefore, Bodo needs to know the value at compilation time for type inference. Otherwise, an error is thrown (passing axis as argument to the JIT function fixes the error in this case): >>> import pandas as pd >>> import bodo >>> @bodo . jit ... def f ( S1 , S2 , flag ): ... axis = 0 ... if flag : ... axis = 1 ... return pd . concat ([ S1 , S2 ], axis = axis ) ... >>> S1 = pd . Series ([ 1 , 2 , 3 ], name = \"A\" ) >>> S2 = pd . Series ([ 3 , 4 , 5 ], name = \"B\" ) >>> f ( S1 , S2 , False ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/ehsan/dev/bodo/bodo/numba_compat.py\" , line 1198 , in _compile_for_args raise error bodo . utils . typing . BodoError : pd . concat (): 'axis' should be a constant integer >>> @bodo . jit ... def f ( S1 , S2 , axis ): ... return pd . concat ([ S1 , S2 ], axis = axis ) ... >>> print ( f ( S1 , S2 , 0 )) 0 1 1 2 2 3 0 3 1 4 2 5 dtype : int64 See Bodo API reference for more details on argument requirements. Troubleshooting Compilation Errors \u00b6 Now that we understand what causes the error, let's fix it! For potential unsupported APIs, Python feature gaps or type stability issues try the following: Make sure your code works in Python. In a lot of cases, a Bodo decorated function does not compile, but it does not compile in Python either. Refactor your code with supported operations if possible. For instance, the sort_index(key=lambda ...) examble above can be replaced with regular sort_values : >>> df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 , 4 ]}, index = [ 'A' , 'b' , 'C' , 'd' ]) >>> @bodo . jit ... def f ( df ): ... return df . sort_index ( key = lambda x : x . str . lower ()) ... >>> f ( df ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/ehsan/dev/bodo/bodo/numba_compat.py\" , line 1198 , in _compile_for_args raise error bodo . utils . typing . BodoError : DataFrame . sort_index (): key parameter only supports default value None >>> @bodo . jit ... def f ( df ): ... df [ \"key\" ] = df . index . map ( lambda a : a . lower ()) ... return df . sort_values ( \"key\" ) . drop ( columns = \"key\" ) ... >>> f ( df ) a A 1 b 2 C 3 d 4 Refactor your code and use regular Python for unsupported features. a. Move the code causing issues to regular Python and pass necessary data to JIT functions. b. Use Object Mode to perform some computation within JIT functions in regular Python if necessary (see Object Mode ). Refactor your code to make it type stable (see type stability ). For example: >>> flag = True >>> @bodo . jit ... def f ( flag ): ... df = pd . read_parquet ( \"in.parquet\" ) ... if flag : ... df [ \"C\" ] = 1 ... df . to_parquet ( \"out.parquet\" ) ... >>> f ( flag ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/ehsan/dev/bodo/bodo/numba_compat.py\" , line 1163 , in _compile_for_args error_rewrite ( e , \"typing\" ) File \"/Users/ehsan/dev/bodo/bodo/numba_compat.py\" , line 1043 , in error_rewrite raise e . with_traceback ( None ) numba . core . errors . TypingError : Cannot unify dataframe (( array ( int64 , 1 d , C ),), StringIndexType ( none ), ( 'a' ,), 1 D_Block_Var , True ) and dataframe (( array ( int64 , 1 d , C ), array ( int64 , 1 d , C )), StringIndexType ( none ), ( 'a' , 'C' ), 1 D_Block_Var , True ) for 'df' , defined at < stdin > ( 3 ) >>> @bodo . jit ... def f1 (): ... df = pd . read_parquet ( \"in.parquet\" ) ... return df ... >>> @bodo . jit ... def f2 ( df ): ... df [ \"C\" ] = 1 ... return df ... >>> @bodo . jit ... def f3 ( df ): ... df . to_parquet ( \"out.parquet\" ) ... >>> df = f1 () >>> if flag : ... df = f2 ( df ) ... >>> f3 ( df ) Disabling Python Output Buffering \u00b6 Sometimes standard output prints may not appear when the program fails, due to Python's I/O buffering. Therefore, setting PYTHONUNBUFFERED environment variable is recommended for debugging: export PYTHONUNBUFFERED = 1 Requesting Unsupported Functionality and Reporting Errors \u00b6 If you want to request a new feature, or report a bug you have found, please create an issue in our Feedback repository. If you encounter an error which is not covered on this page, please report it to our Feedback repository as well.","title":"Compilation Tips"},{"location":"diagnostics_and_troubleshooting/compilation/#compilation","text":"","title":"Compilation Tips and Troubleshooting"},{"location":"diagnostics_and_troubleshooting/compilation/#what-code-to-jit-compile","text":"The general recommendation is to use Bodo JIT compilation only for code that is data and/or compute intensive (e.g. Pandas code on large dataframes). In other words: Only use Bodo for data processing and analytics code such as Pandas, Numpy, and Scikit-Learn (see Bodo API reference for analytics APIs with JIT support). Refactor code that sets up infrastructure or performs initializations out of JIT functions. This reduces the risk of encountering unsupported features and also reduces compilation time. For example, the program below finds the input file name in regular Python, and uses Bodo JIT only for data load and processing: def get_filename (): if os . path . exists ( \"input.parquet\" ): return \"input.parquet\" if \"INPUT_FILE\" in os . environ : return os . environ [ \"INPUT_FILE\" ] raise Exception ( \"Input file name not found\" ) @bodo . jit def f ( fname ): df = pd . read_parquet ( fname ) print ( df . sum ()) fname = get_filename () f ( fname ) This recommendation is similar to Numba's What to compile .","title":"What Code to JIT Compile"},{"location":"diagnostics_and_troubleshooting/compilation/#whycompilationerror","text":"First of all, let us understand why the code may fail to compile. There are three main kinds of issues: Some API is used that is not supported in Bodo JIT yet (see Bodo API Reference ). Some Python construct or data structure is used that cannot be JIT compiled (see Unsupported Python APIs ). The code has type stability issues (see type stability ). Below are some examples of the type of errors you may see due to these issues.","title":"Compilation Errors"},{"location":"diagnostics_and_troubleshooting/compilation/#unsupported-functions-or-methods","text":"If a JIT function uses an unsupported function or method (e.g. in Pandas APIs), Bodo raises BodoError explaining that the method is not supported yet: BodoError: <method> not supported yet For example: >>> @bodo . jit ... def f ( df ): ... return df . swapaxes ( 0 , 1 ) ... >>> f ( df ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1198 , in _compile_for_args raise error bodo . utils . typing . BodoError : DataFrame . swapaxes () not supported yet","title":"Unsupported Functions or Methods"},{"location":"diagnostics_and_troubleshooting/compilation/#unsupported-attributes","text":"Attempting to access an unsupported attribute in Bodo JIT functions will result in a BodoError as follows: BodoError: <attribute> not supported yet For example: >>> @bodo . jit ... def f ( df ): ... return df . flags ... >>> f ( df ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1198 , in _compile_for_args raise error bodo . utils . typing . BodoError : DataFrame . flags not supported yet","title":"Unsupported Attributes"},{"location":"diagnostics_and_troubleshooting/compilation/#unsupported-arguments","text":"Supported APIs may not support all optional arguments. Supplying an unsupported argument will result in a BodoError : BodoError: <method>: <keyword> argument not supported yet For example: >>> @bodo . jit ... def f ( df ): ... return df . sort_index ( key = lambda x : x . str . lower ()) ... >>> f ( df ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1198 , in _compile_for_args raise error bodo . utils . typing . BodoError : DataFrame . sort_index (): key parameter only supports default value None","title":"Unsupported Arguments"},{"location":"diagnostics_and_troubleshooting/compilation/#type-stability-errors","text":"Bodo needs to infer data types for all program variables for successful JIT compilation. A type stability issue arises when different program control flow paths assign values with different types to a variable. For example, variable a below could either be an integer or a string: >>> @bodo . jit ... def f ( flag ): ... if flag : ... a = 3 ... else : ... a = \"A\" ... return a ... >>> f ( True ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1163 , in _compile_for_args error_rewrite ( e , \"typing\" ) File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1043 , in error_rewrite raise e . with_traceback ( None ) numba . core . errors . TypingError : Cannot unify Literal [ str ]( A ) and Literal [ int ]( 3 ) for 'a.2' , defined at < stdin > ( 7 ) The error TypingError: Cannot unify <type1> and <type2> means that the two possible data types cannot be combined and therefore, the variable cannot have a single data type. Dataframe variables require their schema (column names and their types) to be consistent for type stability (see dataframe schema stability ). For example, the dataframe variable df below could either have a single column (\"A\": integer) or two columns (\"A\": integer, \"B\": float) depending on the runtime value of flag , which results in a type stability error: >>> @bodo . jit ... def f ( flag ): ... df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 , 4 ]}) ... if flag : ... df [ \"B\" ] = [ 1.2 , 0.4 , 0.7 , 121.9 ] ... print ( df ) ... >>> f ( True ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1163 , in _compile_for_args error_rewrite ( e , \"typing\" ) File \"/Users/user/bodo/bodo/numba_compat.py\" , line 1043 , in error_rewrite raise e . with_traceback ( None ) numba . core . errors . TypingError : Cannot unify dataframe (( array ( int64 , 1 d , C ),), RangeIndexType ( none ), ( 'A' ,), 1 D_Block_Var , False ) and dataframe (( array ( int64 , 1 d , C ), array ( float64 , 1 d , C )), RangeIndexType ( none ), ( 'A' , 'B' ), 1 D_Block_Var , False ) for 'df' , defined at < stdin > ( 3 ) Additionally, some function arguments need to be constant to ensure type stability. In certain cases where it is possible, Bodo may infer the constant values. In other cases, it may throw an error indicating that the argument should be constant. For instance, axis argument in pd.concat determines whether the output is a Series type or a dataframe type in the example below. Therefore, Bodo needs to know the value at compilation time for type inference. Otherwise, an error is thrown (passing axis as argument to the JIT function fixes the error in this case): >>> import pandas as pd >>> import bodo >>> @bodo . jit ... def f ( S1 , S2 , flag ): ... axis = 0 ... if flag : ... axis = 1 ... return pd . concat ([ S1 , S2 ], axis = axis ) ... >>> S1 = pd . Series ([ 1 , 2 , 3 ], name = \"A\" ) >>> S2 = pd . Series ([ 3 , 4 , 5 ], name = \"B\" ) >>> f ( S1 , S2 , False ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/ehsan/dev/bodo/bodo/numba_compat.py\" , line 1198 , in _compile_for_args raise error bodo . utils . typing . BodoError : pd . concat (): 'axis' should be a constant integer >>> @bodo . jit ... def f ( S1 , S2 , axis ): ... return pd . concat ([ S1 , S2 ], axis = axis ) ... >>> print ( f ( S1 , S2 , 0 )) 0 1 1 2 2 3 0 3 1 4 2 5 dtype : int64 See Bodo API reference for more details on argument requirements.","title":"Type Stability Errors"},{"location":"diagnostics_and_troubleshooting/compilation/#troubleshooting-compilation-errors","text":"Now that we understand what causes the error, let's fix it! For potential unsupported APIs, Python feature gaps or type stability issues try the following: Make sure your code works in Python. In a lot of cases, a Bodo decorated function does not compile, but it does not compile in Python either. Refactor your code with supported operations if possible. For instance, the sort_index(key=lambda ...) examble above can be replaced with regular sort_values : >>> df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 , 4 ]}, index = [ 'A' , 'b' , 'C' , 'd' ]) >>> @bodo . jit ... def f ( df ): ... return df . sort_index ( key = lambda x : x . str . lower ()) ... >>> f ( df ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/ehsan/dev/bodo/bodo/numba_compat.py\" , line 1198 , in _compile_for_args raise error bodo . utils . typing . BodoError : DataFrame . sort_index (): key parameter only supports default value None >>> @bodo . jit ... def f ( df ): ... df [ \"key\" ] = df . index . map ( lambda a : a . lower ()) ... return df . sort_values ( \"key\" ) . drop ( columns = \"key\" ) ... >>> f ( df ) a A 1 b 2 C 3 d 4 Refactor your code and use regular Python for unsupported features. a. Move the code causing issues to regular Python and pass necessary data to JIT functions. b. Use Object Mode to perform some computation within JIT functions in regular Python if necessary (see Object Mode ). Refactor your code to make it type stable (see type stability ). For example: >>> flag = True >>> @bodo . jit ... def f ( flag ): ... df = pd . read_parquet ( \"in.parquet\" ) ... if flag : ... df [ \"C\" ] = 1 ... df . to_parquet ( \"out.parquet\" ) ... >>> f ( flag ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"/Users/ehsan/dev/bodo/bodo/numba_compat.py\" , line 1163 , in _compile_for_args error_rewrite ( e , \"typing\" ) File \"/Users/ehsan/dev/bodo/bodo/numba_compat.py\" , line 1043 , in error_rewrite raise e . with_traceback ( None ) numba . core . errors . TypingError : Cannot unify dataframe (( array ( int64 , 1 d , C ),), StringIndexType ( none ), ( 'a' ,), 1 D_Block_Var , True ) and dataframe (( array ( int64 , 1 d , C ), array ( int64 , 1 d , C )), StringIndexType ( none ), ( 'a' , 'C' ), 1 D_Block_Var , True ) for 'df' , defined at < stdin > ( 3 ) >>> @bodo . jit ... def f1 (): ... df = pd . read_parquet ( \"in.parquet\" ) ... return df ... >>> @bodo . jit ... def f2 ( df ): ... df [ \"C\" ] = 1 ... return df ... >>> @bodo . jit ... def f3 ( df ): ... df . to_parquet ( \"out.parquet\" ) ... >>> df = f1 () >>> if flag : ... df = f2 ( df ) ... >>> f3 ( df )","title":"Troubleshooting Compilation Errors"},{"location":"diagnostics_and_troubleshooting/compilation/#disabling-python-output-buffering","text":"Sometimes standard output prints may not appear when the program fails, due to Python's I/O buffering. Therefore, setting PYTHONUNBUFFERED environment variable is recommended for debugging: export PYTHONUNBUFFERED = 1","title":"Disabling Python Output Buffering"},{"location":"diagnostics_and_troubleshooting/compilation/#requesting-unsupported-functionality-and-reporting-errors","text":"If you want to request a new feature, or report a bug you have found, please create an issue in our Feedback repository. If you encounter an error which is not covered on this page, please report it to our Feedback repository as well.","title":"Requesting Unsupported Functionality and Reporting Errors"},{"location":"diagnostics_and_troubleshooting/troubleaws/","text":"Troubleshooting Managed Bodo Cloud Platform Issues on AWS \u00b6 Here are solutions to potential issues you may encounter while using the Bodo Cloud Platform. Cluster Creation Fails \u00b6 Most of cluster creation failures are usually due to one of the following: Your account hits AWS resource limits such as limits on the number of VPCs and EC2 instances Your AWS credentials do not have the required permissions (see how to set aws credentials ) AWS does not have enough of the requested resources (such as some of the large EC2 instances) In case of failure, the logs are made available on the platform and should provide some details regarding why the failure occurred. Even though cluster creation was not successful, some AWS resources may still have been provisioned. Click on the delete icon to remove all the created resources, otherwise you may incur charges for the provisioned AWS resources. You can try to create a cluster again after addressing the underlying issue such as increasing limits or providing AWS credentials with the required permissions. Cluster Deletion Fails \u00b6 Failures during cluster deletion are very rare and usually only occur when the provisioned resources have been manually modified in some way. In these cases, logs are provided to help you diagnose the issue. For instance, if logs indicate that some resource cannot be deleted due to a dependent resource, you can try to delete the resource manually through the AWS Management Console and try to remove the cluster through the platform again. Cleanup Shared Resources Manually \u00b6 As described in the AWS account cleanup section , an option to remove organization level shared resources provisioned by Bodo in your AWS environment is provided. If you need to remove resources manually (e.g. the process fails), below is the list of organization level resources and the order to remove them. Note Please ensure that you have removed all clusters and related resources before proceeding. Deleting the resources listed below may result in the platform losing access to those clusters for removal in the future. The resources should be easy to identify within their respective sections on the AWS Management Console since their names are all prefixed with bodo . Navigate to the AWS Management Console . Sign in if you are not already signed in. Make sure you have selected the region from which you want to remove the shared resources. Click on Services in the top-right corner. Navigate to the EC2 section (under Compute ) and then to Network Interfaces in the sidebar (under Network & Security ). You will see two Network Interfaces. One of them is required for an EFS Mount (shared storage), and the other is required by a NAT Gateway. These dependent resources need to be removed first. a. Click on Services and navigate to the EFS section (under Storage ). Click on File Systems in the sidebar. Delete the File System prefixed with bodo by selecting it and clicking on Delete . b. Click on Services and navigate to the VPC section (under Networking & Content Delivery ). Select NAT Gateways in the sidebar (under Virtual Private Cloud ). Select the NAT Gateway prefixed with bodo and delete it. Navigate back to Network Interfaces in the EC2 section and ensure that the two ENIs are deleted (or have the status available ). This may take a few minutes in some cases. Click on Services and navigate to the VPC section (under Networking & Content Delivery ). Select Your VPCs in the sidebar (under Virtual Private Cloud ). Select the VPC prefixed with bodo and delete it. If there is a dependency warning, wait for a few minutes and try again. You can also try to delete the linked dependent resources manually if it does not resolve on its own. Click on Services in the top-right corner. Navigate to the EC2 section (under Compute ) and select Elastic IPs in the sidebar (under Network & Security ). Select the EIP prefixed with bodo and select Release Elastic IP addresses under Actions . Click on Services in the top-right corner. Navigate to the Key Management Service (KMS) section (under Security, Identity, & Compliance ) and select Customer managed keys in the sidebar. Click on the key prefixed with bodoai-kms . Go to the Aliases tab. There should be a single alias defined. Select this alias and delete it. Next, click on Key actions (top-right) and select Schedule key deletion . Optional Reduce the Waiting period from 30 days to 7 days. Next, check Confirm that you want to delete this key in XX days and click on Schedule deletion . Finally, click on Services in the top-right corner and navigate to Systems Manager (under Management & Governance ). Select Parameter Store from sidebar. Look for parameters prefixed with /<EXTERNAL_ID> , where EXTERNAL_ID is the same as the External ID visible on the Settings page on the Bodo Platform (see how to create an iam role manually ). Select all these parameter entries and delete them. The steps above should remove the organization level resources provisioned by Bodo in your AWS environment.","title":"Troubleshooting on AWS"},{"location":"diagnostics_and_troubleshooting/troubleaws/#troubleshootingaws","text":"Here are solutions to potential issues you may encounter while using the Bodo Cloud Platform.","title":"Troubleshooting Managed Bodo Cloud Platform Issues on AWS"},{"location":"diagnostics_and_troubleshooting/troubleaws/#creationfail","text":"Most of cluster creation failures are usually due to one of the following: Your account hits AWS resource limits such as limits on the number of VPCs and EC2 instances Your AWS credentials do not have the required permissions (see how to set aws credentials ) AWS does not have enough of the requested resources (such as some of the large EC2 instances) In case of failure, the logs are made available on the platform and should provide some details regarding why the failure occurred. Even though cluster creation was not successful, some AWS resources may still have been provisioned. Click on the delete icon to remove all the created resources, otherwise you may incur charges for the provisioned AWS resources. You can try to create a cluster again after addressing the underlying issue such as increasing limits or providing AWS credentials with the required permissions.","title":"Cluster Creation Fails"},{"location":"diagnostics_and_troubleshooting/troubleaws/#deletionfail","text":"Failures during cluster deletion are very rare and usually only occur when the provisioned resources have been manually modified in some way. In these cases, logs are provided to help you diagnose the issue. For instance, if logs indicate that some resource cannot be deleted due to a dependent resource, you can try to delete the resource manually through the AWS Management Console and try to remove the cluster through the platform again.","title":"Cluster Deletion Fails"},{"location":"diagnostics_and_troubleshooting/troubleaws/#manualcleanup","text":"As described in the AWS account cleanup section , an option to remove organization level shared resources provisioned by Bodo in your AWS environment is provided. If you need to remove resources manually (e.g. the process fails), below is the list of organization level resources and the order to remove them. Note Please ensure that you have removed all clusters and related resources before proceeding. Deleting the resources listed below may result in the platform losing access to those clusters for removal in the future. The resources should be easy to identify within their respective sections on the AWS Management Console since their names are all prefixed with bodo . Navigate to the AWS Management Console . Sign in if you are not already signed in. Make sure you have selected the region from which you want to remove the shared resources. Click on Services in the top-right corner. Navigate to the EC2 section (under Compute ) and then to Network Interfaces in the sidebar (under Network & Security ). You will see two Network Interfaces. One of them is required for an EFS Mount (shared storage), and the other is required by a NAT Gateway. These dependent resources need to be removed first. a. Click on Services and navigate to the EFS section (under Storage ). Click on File Systems in the sidebar. Delete the File System prefixed with bodo by selecting it and clicking on Delete . b. Click on Services and navigate to the VPC section (under Networking & Content Delivery ). Select NAT Gateways in the sidebar (under Virtual Private Cloud ). Select the NAT Gateway prefixed with bodo and delete it. Navigate back to Network Interfaces in the EC2 section and ensure that the two ENIs are deleted (or have the status available ). This may take a few minutes in some cases. Click on Services and navigate to the VPC section (under Networking & Content Delivery ). Select Your VPCs in the sidebar (under Virtual Private Cloud ). Select the VPC prefixed with bodo and delete it. If there is a dependency warning, wait for a few minutes and try again. You can also try to delete the linked dependent resources manually if it does not resolve on its own. Click on Services in the top-right corner. Navigate to the EC2 section (under Compute ) and select Elastic IPs in the sidebar (under Network & Security ). Select the EIP prefixed with bodo and select Release Elastic IP addresses under Actions . Click on Services in the top-right corner. Navigate to the Key Management Service (KMS) section (under Security, Identity, & Compliance ) and select Customer managed keys in the sidebar. Click on the key prefixed with bodoai-kms . Go to the Aliases tab. There should be a single alias defined. Select this alias and delete it. Next, click on Key actions (top-right) and select Schedule key deletion . Optional Reduce the Waiting period from 30 days to 7 days. Next, check Confirm that you want to delete this key in XX days and click on Schedule deletion . Finally, click on Services in the top-right corner and navigate to Systems Manager (under Management & Governance ). Select Parameter Store from sidebar. Look for parameters prefixed with /<EXTERNAL_ID> , where EXTERNAL_ID is the same as the External ID visible on the Settings page on the Bodo Platform (see how to create an iam role manually ). Select all these parameter entries and delete them. The steps above should remove the organization level resources provisioned by Bodo in your AWS environment.","title":"Cleanup Shared Resources Manually"},{"location":"diagnostics_and_troubleshooting/troubleazure/","text":"Troubleshooting Managed Bodo Cloud Platform Issues on Azure \u00b6 Here are solutions to potential issues you may encounter while using the Bodo Cloud Platform. Cluster Creation Fails \u00b6 Most of cluster creation failures are usually due to one of the following: Your account hits Azure resource limits such as limits on the number of VNets and virtual machines Your Azure credentials do not have the required permissions (see how to set azure credentials ) Azure does not have enough of the requested resources (such as some of the large virtual machine sizes) In case of failure, the logs are made available on the platform and should provide some details regarding why the failure occurred. Even though cluster creation was not successful, some Azure resources may still have been provisioned. Click on the delete icon to remove all the created resources, otherwise you may incur charges for the provisioned Azure resources. You can try to create a cluster again after addressing the underlying issue such as increasing limits or providing Azure credentials with the required permissions. Cluster Deletion Fails \u00b6 Failures during cluster deletion are very rare and usually only occur when the provisioned resources have been manually modified in some way. In these cases, logs are provided to help you diagnose the issue. For instance, if logs indicate that some resource cannot be deleted due to a dependent resource, you can try to delete the resource manually through the Azure Portal and try to remove the cluster through the platform again. The resources provisioned for the cluster are tagged with the Cluster-ID in the resource group, making them easy to identify and remove. Cleanup Shared Resources Manually \u00b6 As described in Azure account cleanup , an option to remove organization level shared resources provisioned by Bodo in your Azure environment is provided. If you need to remove resources manually (e.g. the process fails), you can simply remove all the resources in the designated resource group whose name contains bodo . Note Please ensure that you have removed all clusters and related resources before proceeding. Deleting the resources listed below may result in the platform losing access to those clusters for removal in the future. The resources should be easy to identify within their respective sections on the Azure Portal since their names all contain bodo }. See here for a list of resources the platform creates. Navigate to the Azure Portal . Sign in if you are not already signed in. Navigate to the resource group that you're using for your Bodo resources (you can find the name on the Settings page). If all cluster specific resources have been deleted properly, the resource group should look something like: Look for all resources with bodo in their name, and delete them. Next, you will need to purge the key vault manually. Follow the instructions here . The steps above should remove the organization level resources provisioned by Bodo in your Azure environment.","title":"Troubleshooting on Azure"},{"location":"diagnostics_and_troubleshooting/troubleazure/#troubleshootingazure","text":"Here are solutions to potential issues you may encounter while using the Bodo Cloud Platform.","title":"Troubleshooting Managed Bodo Cloud Platform Issues on Azure"},{"location":"diagnostics_and_troubleshooting/troubleazure/#creationfail","text":"Most of cluster creation failures are usually due to one of the following: Your account hits Azure resource limits such as limits on the number of VNets and virtual machines Your Azure credentials do not have the required permissions (see how to set azure credentials ) Azure does not have enough of the requested resources (such as some of the large virtual machine sizes) In case of failure, the logs are made available on the platform and should provide some details regarding why the failure occurred. Even though cluster creation was not successful, some Azure resources may still have been provisioned. Click on the delete icon to remove all the created resources, otherwise you may incur charges for the provisioned Azure resources. You can try to create a cluster again after addressing the underlying issue such as increasing limits or providing Azure credentials with the required permissions.","title":"Cluster Creation Fails"},{"location":"diagnostics_and_troubleshooting/troubleazure/#deletionfail","text":"Failures during cluster deletion are very rare and usually only occur when the provisioned resources have been manually modified in some way. In these cases, logs are provided to help you diagnose the issue. For instance, if logs indicate that some resource cannot be deleted due to a dependent resource, you can try to delete the resource manually through the Azure Portal and try to remove the cluster through the platform again. The resources provisioned for the cluster are tagged with the Cluster-ID in the resource group, making them easy to identify and remove.","title":"Cluster Deletion Fails"},{"location":"diagnostics_and_troubleshooting/troubleazure/#manualcleanup","text":"As described in Azure account cleanup , an option to remove organization level shared resources provisioned by Bodo in your Azure environment is provided. If you need to remove resources manually (e.g. the process fails), you can simply remove all the resources in the designated resource group whose name contains bodo . Note Please ensure that you have removed all clusters and related resources before proceeding. Deleting the resources listed below may result in the platform losing access to those clusters for removal in the future. The resources should be easy to identify within their respective sections on the Azure Portal since their names all contain bodo }. See here for a list of resources the platform creates. Navigate to the Azure Portal . Sign in if you are not already signed in. Navigate to the resource group that you're using for your Bodo resources (you can find the name on the Settings page). If all cluster specific resources have been deleted properly, the resource group should look something like: Look for all resources with bodo in their name, and delete them. Next, you will need to purge the key vault manually. Follow the instructions here . The steps above should remove the organization level resources provisioned by Bodo in your Azure environment.","title":"Cleanup Shared Resources Manually"},{"location":"help_and_reference/eula/","text":"End User License Agreement \u00b6 THIS ONLINE END-USER LICENSE AGREEMENT (\"AGREEMENT\") IS A BINDING LEGAL CONTRACT BETWEEN YOU (THE USER) AND BODO INC. (\"WE\", \"US\", OR \"BODO\"). BY DOWNLOADING, INSTALLING, ACCESSING OR USING THE SOFTWARE, SERVICES, AND ANY OTHER MATERIALS MADE AVAILABLE BY BODO ON THIS SITE OR IN ANY OTHER FORMAT (COLLECTIVELY, THE \"SERVICES\"), YOU (A) AGREE TO BE BOUND BY THIS AGREEMENT; (B) ACKNOWLEDGE AND AGREE YOU HAVE INDEPENDENTLY EVALUATED THE DESIRABILITY OF USING THE SERVICES AND ARE NOT RELYING ON ANY REPRESENTATION, GUARANTEE, OR STATEMENT OTHER THAN AS EXPRESSLY PROVIDED IN THIS AGREEMENT; AND (C) REPRESENT YOU ARE LAWFULLY ABLE TO ENTER INTO CONTRACTS AND ARE OF THE LEGAL AGE OF MAJORITY IN THE JURISDICTION IN WHICH YOU RESIDE (AT LEAST EIGHTEEN YEARS OF AGE IN MANY COUNTRIES/JURISDICTIONS). IF THIS AGREEMENT IS BEING AGREED TO BY A COMPANY OR OTHER LEGAL ENTITY, THEN THE PERSON AGREEING TO THIS AGREEMENT ON BEHALF OF THAT COMPANY OR ENTITY REPRESENTS AND WARRANTS THAT HE OR SHE IS AUTHORIZED AND LAWFULLY ABLE TO BIND THAT COMPANY OR ENTITY TO THIS AGREEMENT. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOU MAY NOT USE THE SERVICES. Services. Subject to the terms and conditions of this Agreement and, if applicable, your payment of all relevant fees, we grant you a non-exclusive, non-transferable, limited license to access and use our software services, content, and other materials provided by Bodo or its third-party vendors through this Web site or in other format (the \" Services \") for your internal use only. Certain third-party services may have their own terms and conditions, which will be presented to you in your use of the Services. Your use of those third-party services will indicate your acceptance of the additional terms and conditions. In connection with the Services, we may afford you the ability to interface and interoperate with certain third-party software and to upload data from that software. This functionality is dependent on the operation of the third-party software and is provided on an entirely as-is basis. We may change, modify, or discontinue all or any portion of the Services at any time, without prior notice. Restrictions. You may only use the Services as described in the documentation we make generally available from time to time to our customers for use of the Services (the \" Documentation \"). Any breach of this Agreement by your employees or agents will constitute a breach by you. Except as expressly authorized by this Agreement, you will not (and will not allow any third-party to): (i) permit any third-party to access and/or use the Services; (ii) decompile, disassemble, or reverse engineer the Services, or attempt to derive the source code, underlying ideas, algorithm or structure of software provided to you in object code form; (iii) use the Services or any of our Confidential Information (as defined below) to develop a competing product or service; (iv) sell, transfer, assign, distribute, rent, loan, lease, sublicense or otherwise make available the software associated with the Services or its functionality to third parties; (v) modify, translate or otherwise create any derivative works of any software used and made available by Bodo in connection with the Services; (vi) provide, lease, lend, use for timesharing or service bureau purposes or otherwise use or allow others to use the Services for the benefit of any third party; (vii) use the Services, or allow the transfer, transmission, export, or re-export of the Services, including by way of a \"deemed export,\" in violation of any export control laws or regulations administered by the U.S. Commerce Department or any other government agency; or (viii) remove any copyright, trademark, proprietary rights, disclaimer or warning notice included on or embedded in any part of the Services or Documentation. Nothing in this Agreement shall be construed to give you a right to use, or otherwise obtain access to, any source code from which the software used in connection with the Services or any portion thereof is compiled or interpreted. Under no circumstances, will we be liable or responsible for any use, or any results obtained by the use, of the Services in conjunction with any other software or third-party products. All such use will be at your sole risk. Proprietary Rights. You acknowledge that all Services are protected by intellectual property rights of Bodo and its vendors/licensors and that you have no rights to transfer or reproduce the Services or prepare any derivative works with respect to, or disclose Confidential Information pertaining to, the Services. Under no circumstances will you be deemed to receive title to any portion of any Services, title to which at all times will vest exclusively in us and our licensors. This is not a \"work made for hire\" agreement, as that term is defined in Section 101 of Title 17 of the United States Code (\" the Copyright Act \"). You will preserve all Services from any liens, encumbrances, and claims of any individual or entity. You will not use any of our information or data to contest the validity of any of our intellectual property or our licensors. Any such use of our information and data will constitute a material, non-curable breach of this Agreement. To the extent you provide us with any content (e.g., graphics, logos, artwork, text, data) for use in connection with the Services (collectively, the \" Customer Content \"), you grant us a non-exclusive, world-wide, royalty-free license to use the Customer Content for purposes of performing this Agreement. You are responsible for obtaining all rights, permissions, licenses, and consents required to furnish the Customer Content to us for use as described above. You are also responsible for preserving and making adequate backups of the Customer Content and will not rely on us to preserve or make adequate backups of data used in connection with the Services, or to maintain a record of your usage of any part or all of the Services. Your rights in and to the Services and related software are limited to those expressly granted under this Agreement and no other licenses are granted whether by implication, estoppel or otherwise. Bodo reserves all rights, title and interest in and to the Services and related software not expressly granted under this Agreement. Third Party Software. The Services may come bundled with, or otherwise include or be distributed with, third party software licensed by a Bodo supplier and/or open source software provided under an open source license (Open Source Software) (collectively, \" Third Party Software \"). Notwithstanding anything to the contrary herein, Third Party Software is licensed to you subject to the terms and conditions of the software license agreement accompanying such Third Party Software whether in the form of a discrete agreement, click-through license, or electronic license terms accepted at the time of installation and any additional terms or agreements provided by the third party licensor (\" Third Party License Terms \"). Use of the Third Party Software by you shall be governed by such Third Party License Terms, or if no Third Party License Terms apply, then the Third Party Software is provided to you as-is, as available, for use in or with the Services and not otherwise used separately. Copyright to Third Party Software is held by the copyright holders indicated in the Third Party License Terms. Feedback. You may provide us with suggestions, comments or other feedback (collectively, \" Feedback \") with respect to our products and services, including the Services. Feedback is voluntary and we are not required to hold it in confidence. We may use Feedback for any purpose without obligation of any kind. To the extent a license is required under your intellectual property rights to make use of the Feedback, you grant us an irrevocable, non-exclusive, perpetual, royalty-free license to use the Feedback in connection with our business, products, and services, including the enhancement of the Services. Aggregated Data. You grant us a non-exclusive, perpetual, irrevocable, fully-paid-up, royalty free license to use data derived from your use of the Services (the \" Aggregated Data \") for our business purposes, including the provision of products and services to our customers; provided the Aggregated Data is combined with similar data from our other customers. \" Aggregated Data \" does not include (directly or by inference) any information identifying you or any identifiable individual. You further grant us the right to (i) use the Aggregated Data in any aggregate or statistical products or reports, (ii) transfer and/or disclose the Aggregated Data upon a sale of our company or its assets or other form of reorganization, (iii) disclose Aggregated Data in a summary report that does not show, display or indicate customer specific or customer identifying information, (iv) provide Aggregated Data to a third party service provider, for analytical purposes, and (v) use the Aggregated Data (without personally identifiable information) to compare with other organizations within the same industry or group. The Aggregated Data will not be considered your Confidential Information. Fees. You will promptly pay Bodo all applicable fees and, as described below, taxes associated with the Services. Except as expressly provided otherwise in this Agreement, all fees (if any) are non-refundable. Payments not made within such time period will be subject to late charges equal to the lesser of (i) one and one-half percent (1.5%) per month of the overdue amount or (ii) the maximum amount permitted under applicable law. You are responsible for paying all personal property, sales, use and other taxes (excluding taxes based upon our net income) and license and registration fees and other assessments or charges levied or imposed by any governmental body or agency as a result of the execution or performance of this Agreement, including your receipt of the Services. On notice of not less than sixty (60) days, we may, in our discretion, adjust any or all fees for the Services. You may terminate this Agreement on written notice to us within thirty (30) days of its receipt of our notice to adjust the fees; provided, however, that if you do not object to the adjustment in writing within the foregoing thirty (30) day period then you will be deemed to have agreed to the adjustment. Your Warranties. You represent and warrant that (i) you have full power, capacity, and authority to enter into this Agreement and to grant the license in Section 4 (Proprietary Rights); and (ii) your use of the Services will be in compliance with all applicable local, state, and federal laws and regulations. Indemnification. You will defend and indemnify Bodo and hold it and its affiliates, officers, directors, employees, and agents harmless from any and all claims, actions, proceedings, losses, deficiencies, damages, liabilities, costs, and expenses (including but not limited to reasonable attorneys' fees and all related costs and expenses) incurred by them as a result of any claim, judgment, or adjudication related to or arising from any or all of the following: (i) your use of the Services; (ii) breach of any of your obligations, representations, or warranties in this Agreement; or (iii) your failure to comply with applicable laws and regulations. Beta Services. We may designate certain new functionality or services to be made available in connection with the Services as \" Beta Services .\" The Beta Services will not be ready for use in a production environment. Because they will be at an early stage of development, operation and use of the Beta Services may be unpredictable and lead to erroneous results. You acknowledge and agree that: (i) the Beta Services will be experimental and will not have been fully tested; (ii) the Beta Services may not meet your requirements; (iii) the use or operation of the Beta Services may not be uninterrupted or error free; and (iv) your use of the Beta Services will be for purposes of evaluating and testing the new functionality and services and providing feedback to us. Your use of the Beta Services will be subject to all of the terms and conditions of this Agreement relating to the Services. You agree to promptly report any errors, defects, or other deficiencies in the Beta Services to us. NOTWITHSTANDING ANY OTHER PROVISION OF THIS AGREEMENT, ALL BETA SERVICES ARE PROVIDED \"AS-IS\" AND \"AS-AVAILABLE,\" WITHOUT WARRANTIES OF ANY KIND. You waive any and all claims, now known or later discovered, that you may have against us and our suppliers and licensors arising out of the Beta Services. Suspension or Termination of Services and Removal of Customer Content. We may, in our sole discretion, suspend your access to the Services for any of the following reasons: (i) to prevent disruption of or damages to, or degradation of, the Services and our systems; (ii) to comply with any law, regulation, court order, or other governmental request; (iii) to otherwise protect us from potential legal liability; (iv) to remove Customer Content that is illegal, offensive, or otherwise inappropriate, in our sole discretion, or (iv) in the event an invoice remains unpaid for more than forty-five (45) or more days from the invoice date. We will restore access to the Services as soon as the event giving rise to suspension has been resolved. This Section will not be construed as imposing any obligation or duty on us to monitor use of the Services. Confidentiality. 12.1 \"Confidential Information\" means all information or material which (i) gives a party some competitive business advantage or the opportunity of obtaining such advantage or the disclosure of which could be detrimental to the interests of that party; or (ii) which from all the relevant circumstances should reasonably be assumed to be confidential and proprietary. Each party's Confidential Information will remain the sole and exclusive property of that party. Confidential Information includes, but is not limited to, the Services. Neither party will have any obligation with respect to confidential information which: (i) is or becomes generally known to the public by any means other than a breach of the obligations of a receiving party; (ii) was previously known to the receiving party or rightly received by the receiving party from a third party; (iii) is independently developed by the receiving party; or (iv) subject to disclosure under court order or other lawful process. 12.2 Treatment of Confidential Information. Each party recognizes the importance of the other party's Confidential Information. In particular, each party recognizes and agrees that the Confidential Information of the other is critical to their respective businesses and that neither party would enter into this Agreement without assurance that the information will be protected as provided in this Section 12 and elsewhere in this Agreement. Accordingly, each party agrees as follows: (a) Each party will hold any and all Confidential Information it obtains in strictest confidence and will use and permit use of Confidential Information solely as permitted under this Agreement; and (b) Each party may disclose or provide access to its responsible employees and agents or as otherwise permitted under this Agreement, and may make copies, of Confidential Information only to the extent permitted under this Agreement. 12.3 Non-Exclusive Equitable Remedy. Each party acknowledges and agrees that due to the unique nature of the Confidential Information there can be no adequate remedy at law for any breach of its obligations hereunder, and therefore, that upon any such breach or any threat thereof, each party will be entitled to appropriate equitable relief from a court of competent jurisdiction in addition to whatever remedies either of them might have at law or equity. 12.4 You agree not to use any Confidential Information of Bodo, and shall restrict your affiliates and sublicensees from using the Confidential Information of Bodo, for purposes of challenging the validity of such Confidential Information, or Bodo's ability to use and exploit such Confidential Information. Limited Warranty; Exclusive Remedy. During the Term, Bodo warrants the Services will materially comply with the requirements of this Agreement and Documentation. In the event of a breach of the foregoing warranty, Bodo's sole and exclusive liability and your sole and exclusive remedy will be to use reasonable efforts to correct the non-conformity. In the event Bodo is unable through reasonable efforts to correct the defective Service, you may elect to terminate this Agreement and, if applicable, receive a prorated refund of any pre-paid, unused recurring fees. Disclaimer of Warranties. EXCEPT AS PROVIDED IN SECTION 13 (LIMITED WARRANTY), THE SERVICES ARE PROVIDED \"AS IS\" AND \"AS-AVAILABLE,\" WITH ALL FAULTS, AND WITHOUT WARRANTY OF ANY KIND. BODO AND ITS VENDORS AND LICENSORS DISCLAIM ALL OTHER WARRANTIES, EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT, QUALITY OF INFORMATION, OR TITLE/NON-INFRINGEMENT AND ALL SUCH WARRANTIES ARE HEREBY SPECIFICALLY DISCLAIMED. YOU EXPRESSLY AGREE AND ACKNOWLEDGE THAT USE OF SERVICES, IS AT YOUR SOLE RISK. NO ORAL OR WRITTEN INFORMATION OR ADVICE GIVEN BY BODO OR ITS AUTHORIZED REPRESENTATIVES WILL CREATE A WARRANTY OR IN ANY WAY INCREASE THE SCOPE OF BODO'S OBLIGATIONS HEREUNDER. THE SERVICES MAY BE USED TO ACCESS AND TRANSFER INFORMATION OVER THE INTERNET. YOU ACKNOWLEDGE AND AGREE THAT BODO AND ITS VENDORS AND LICENSORS DO NOT OPERATE OR CONTROL THE INTERNET AND THAT: (I) VIRUSES, WORMS, TROJAN HORSES, OR OTHER UNDESIRABLE DATA OR SOFTWARE; OR (II) UNAUTHORIZED USERS (E.G., HACKERS) MAY ATTEMPT TO OBTAIN ACCESS TO AND DAMAGE THE CUSTOMER CONTENT, WEB-SITES, COMPUTERS, OR NETWORKS. WE WILL NOT BE RESPONSIBLE FOR THOSE ACTIVITIES. Limitation of Liability and Damages. NEITHER BODO NOR ITS VENDORS AND LICENSORS WILL HAVE ANY LIABILITY TO YOU OR ANY THIRD PARTY FOR ANY LOSS OF PROFITS, BUSINESS, DATA, OR OTHER INCIDENTAL, CONSEQUENTIAL, OR SPECIAL LOSS OR DAMAGE, INCLUDING EXEMPLARY AND PUNITIVE, OF ANY KIND OR NATURE RESULTING FROM OR ARISING OUT OF THIS AGREEMENT, INCLUDING USE OF THE SERVICES EVEN IF BODO HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. THE TOTAL LIABILITY OF BODO AND ITS VENDORS AND LICENSORS TO YOU OR ANY THIRD PARTY ARISING OUT OF THIS AGREEMENT OR USE OF THE SERVICES IN CONNECTION WITH ANY CLAIM OR TYPE OF DAMAGE (WHETHER IN CONTRACT OR TORT) WILL NOT EXCEED THE TOTAL FEES YOU PAID, IF ANY, DURING THE SIX (6) MONTHS IMMEDIATELY PRECEDING THE EVENT GIVING RISE TO THE LIABILITY. THIS LIMITATION OF LIABILITY WILL APPLY EVEN IF THE EXPRESS WARRANTIES PROVIDED ABOVE FAIL OF THEIR ESSENTIAL PURPOSE. Term and Termination. Unless otherwise agreed by the parties, the Agreement shall be on-going until terminated by either party on thirty (30) days prior notice to the other party. In the event we terminate this Agreement for reasons other than breach of contract, any prepaid but unused fees will be refunded. Government Restrictions. Any software or other programming provided by us in connection with this Agreement is commercial computer software as described in DFARS 252.227-7014(a)(1) and FAR 2.101. If acquired by or on behalf of the United States Department of Defense or any component thereof, the United States Government acquires this commercial computer software and commercial computer software documentation subject to the terms of this Agreement as specified in DFARS 227.7202-3, Rights in Commercial Computer Software or Commercial Computer Software Documentation. If acquired by or on behalf of any civilian agency, the United States Government acquires this commercial computer software and commercial computer software documentation subject to the terms of this Agreement as specified in FAR 12.212, Computer Software. USA Patriot Act Notice. The U.S. federal USA Patriot Act (\" USA Patriot Act \") provides generally for the operator of a communication host and law enforcement to be able to monitor any content, upon request of the operator. We anticipate fully complying with our obligations and availing ourselves of all rights under the USA Patriot Act. General. Except for the payment of fees, if applicable, neither party will be liable for any failure or delay in performance under this Agreement which is due to any event beyond the reasonable control of such party, including without limitation, fire, explosion, unavailability of utilities or raw materials, Internet delays and failures, telecommunications failures, unavailability of components, labor difficulties, war, riot, act of God, export control regulation, laws, judgments or government instructions. This Agreement provides the entire agreement between the parties with regard to its subject matter. Except as provided below, this Agreement may not be amended without a writing signed by both parties. We may, at any time and from time-to-time, change the terms of this Agreement. Any changes will be posted on our Web site. In addition, we may also send you a notice about the amended terms via email. If you do not accept the terms of any modification, your only recourse is to terminate this Agreement by sending a termination notice us before the effective date of the amendments. The termination will be effective on the date we receive the notice. The most current version of the Agreement will be available on our Web site and will supersede all previous versions of the Agreement. Your continued use of the Services will constitute your acceptance of the changes. This Agreement will be construed according to, and the rights of the parties will be governed by, the law of the State of California, without reference to its conflict of laws rules. Any action at law or in equity arising out of or directly or indirectly relating to this Agreement may be instituted only in the Federal or state courts located in San Francisco, California. You consent and submit to the personal jurisdiction of those courts for the purposes of any action related to this Agreement, and to extra-territorial service of process. No action, regardless of form, arising out of this Agreement, may be brought by either party more than one (1) year after the cause of action has arisen. You may not assign this Agreement without the prior written consent of Bodo. If any of the provisions of this Agreement are found or deemed by a court to be invalid or unenforceable, they will be severable from the remainder of this Agreement and will not cause the invalidity or unenforceability of the remainder of this Agreement. Neither party will by mere lapse of time without giving notice or taking other action hereunder be deemed to have waived any breach by the other party of any of the provisions of this Agreement. The following provisions will survive termination or expiration of this Agreement: 4 (Proprietary Rights), 9 (Indemnification), 12 (Confidentiality), 13 (Limited Warranty; Exclusive Remedy); 14 (Disclaimer of Warranties), 15 (Limitation of Liability and Damages), 17 (Government Restrictions), 18 (USA Patriot Act Notice), and 19 (General Provisions). This Agreement may be accepted in electronic form (e.g., by an electronic or other means of demonstrating assent) and your acceptance will be deemed binding between us. Neither of us will contest the validity or enforceability of this Agreement and any related documents, including under any applicable statute of frauds, because they were accepted or signed in electronic form.","title":"End User License Agreement"},{"location":"help_and_reference/eula/#eula","text":"THIS ONLINE END-USER LICENSE AGREEMENT (\"AGREEMENT\") IS A BINDING LEGAL CONTRACT BETWEEN YOU (THE USER) AND BODO INC. (\"WE\", \"US\", OR \"BODO\"). BY DOWNLOADING, INSTALLING, ACCESSING OR USING THE SOFTWARE, SERVICES, AND ANY OTHER MATERIALS MADE AVAILABLE BY BODO ON THIS SITE OR IN ANY OTHER FORMAT (COLLECTIVELY, THE \"SERVICES\"), YOU (A) AGREE TO BE BOUND BY THIS AGREEMENT; (B) ACKNOWLEDGE AND AGREE YOU HAVE INDEPENDENTLY EVALUATED THE DESIRABILITY OF USING THE SERVICES AND ARE NOT RELYING ON ANY REPRESENTATION, GUARANTEE, OR STATEMENT OTHER THAN AS EXPRESSLY PROVIDED IN THIS AGREEMENT; AND (C) REPRESENT YOU ARE LAWFULLY ABLE TO ENTER INTO CONTRACTS AND ARE OF THE LEGAL AGE OF MAJORITY IN THE JURISDICTION IN WHICH YOU RESIDE (AT LEAST EIGHTEEN YEARS OF AGE IN MANY COUNTRIES/JURISDICTIONS). IF THIS AGREEMENT IS BEING AGREED TO BY A COMPANY OR OTHER LEGAL ENTITY, THEN THE PERSON AGREEING TO THIS AGREEMENT ON BEHALF OF THAT COMPANY OR ENTITY REPRESENTS AND WARRANTS THAT HE OR SHE IS AUTHORIZED AND LAWFULLY ABLE TO BIND THAT COMPANY OR ENTITY TO THIS AGREEMENT. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOU MAY NOT USE THE SERVICES. Services. Subject to the terms and conditions of this Agreement and, if applicable, your payment of all relevant fees, we grant you a non-exclusive, non-transferable, limited license to access and use our software services, content, and other materials provided by Bodo or its third-party vendors through this Web site or in other format (the \" Services \") for your internal use only. Certain third-party services may have their own terms and conditions, which will be presented to you in your use of the Services. Your use of those third-party services will indicate your acceptance of the additional terms and conditions. In connection with the Services, we may afford you the ability to interface and interoperate with certain third-party software and to upload data from that software. This functionality is dependent on the operation of the third-party software and is provided on an entirely as-is basis. We may change, modify, or discontinue all or any portion of the Services at any time, without prior notice. Restrictions. You may only use the Services as described in the documentation we make generally available from time to time to our customers for use of the Services (the \" Documentation \"). Any breach of this Agreement by your employees or agents will constitute a breach by you. Except as expressly authorized by this Agreement, you will not (and will not allow any third-party to): (i) permit any third-party to access and/or use the Services; (ii) decompile, disassemble, or reverse engineer the Services, or attempt to derive the source code, underlying ideas, algorithm or structure of software provided to you in object code form; (iii) use the Services or any of our Confidential Information (as defined below) to develop a competing product or service; (iv) sell, transfer, assign, distribute, rent, loan, lease, sublicense or otherwise make available the software associated with the Services or its functionality to third parties; (v) modify, translate or otherwise create any derivative works of any software used and made available by Bodo in connection with the Services; (vi) provide, lease, lend, use for timesharing or service bureau purposes or otherwise use or allow others to use the Services for the benefit of any third party; (vii) use the Services, or allow the transfer, transmission, export, or re-export of the Services, including by way of a \"deemed export,\" in violation of any export control laws or regulations administered by the U.S. Commerce Department or any other government agency; or (viii) remove any copyright, trademark, proprietary rights, disclaimer or warning notice included on or embedded in any part of the Services or Documentation. Nothing in this Agreement shall be construed to give you a right to use, or otherwise obtain access to, any source code from which the software used in connection with the Services or any portion thereof is compiled or interpreted. Under no circumstances, will we be liable or responsible for any use, or any results obtained by the use, of the Services in conjunction with any other software or third-party products. All such use will be at your sole risk. Proprietary Rights. You acknowledge that all Services are protected by intellectual property rights of Bodo and its vendors/licensors and that you have no rights to transfer or reproduce the Services or prepare any derivative works with respect to, or disclose Confidential Information pertaining to, the Services. Under no circumstances will you be deemed to receive title to any portion of any Services, title to which at all times will vest exclusively in us and our licensors. This is not a \"work made for hire\" agreement, as that term is defined in Section 101 of Title 17 of the United States Code (\" the Copyright Act \"). You will preserve all Services from any liens, encumbrances, and claims of any individual or entity. You will not use any of our information or data to contest the validity of any of our intellectual property or our licensors. Any such use of our information and data will constitute a material, non-curable breach of this Agreement. To the extent you provide us with any content (e.g., graphics, logos, artwork, text, data) for use in connection with the Services (collectively, the \" Customer Content \"), you grant us a non-exclusive, world-wide, royalty-free license to use the Customer Content for purposes of performing this Agreement. You are responsible for obtaining all rights, permissions, licenses, and consents required to furnish the Customer Content to us for use as described above. You are also responsible for preserving and making adequate backups of the Customer Content and will not rely on us to preserve or make adequate backups of data used in connection with the Services, or to maintain a record of your usage of any part or all of the Services. Your rights in and to the Services and related software are limited to those expressly granted under this Agreement and no other licenses are granted whether by implication, estoppel or otherwise. Bodo reserves all rights, title and interest in and to the Services and related software not expressly granted under this Agreement. Third Party Software. The Services may come bundled with, or otherwise include or be distributed with, third party software licensed by a Bodo supplier and/or open source software provided under an open source license (Open Source Software) (collectively, \" Third Party Software \"). Notwithstanding anything to the contrary herein, Third Party Software is licensed to you subject to the terms and conditions of the software license agreement accompanying such Third Party Software whether in the form of a discrete agreement, click-through license, or electronic license terms accepted at the time of installation and any additional terms or agreements provided by the third party licensor (\" Third Party License Terms \"). Use of the Third Party Software by you shall be governed by such Third Party License Terms, or if no Third Party License Terms apply, then the Third Party Software is provided to you as-is, as available, for use in or with the Services and not otherwise used separately. Copyright to Third Party Software is held by the copyright holders indicated in the Third Party License Terms. Feedback. You may provide us with suggestions, comments or other feedback (collectively, \" Feedback \") with respect to our products and services, including the Services. Feedback is voluntary and we are not required to hold it in confidence. We may use Feedback for any purpose without obligation of any kind. To the extent a license is required under your intellectual property rights to make use of the Feedback, you grant us an irrevocable, non-exclusive, perpetual, royalty-free license to use the Feedback in connection with our business, products, and services, including the enhancement of the Services. Aggregated Data. You grant us a non-exclusive, perpetual, irrevocable, fully-paid-up, royalty free license to use data derived from your use of the Services (the \" Aggregated Data \") for our business purposes, including the provision of products and services to our customers; provided the Aggregated Data is combined with similar data from our other customers. \" Aggregated Data \" does not include (directly or by inference) any information identifying you or any identifiable individual. You further grant us the right to (i) use the Aggregated Data in any aggregate or statistical products or reports, (ii) transfer and/or disclose the Aggregated Data upon a sale of our company or its assets or other form of reorganization, (iii) disclose Aggregated Data in a summary report that does not show, display or indicate customer specific or customer identifying information, (iv) provide Aggregated Data to a third party service provider, for analytical purposes, and (v) use the Aggregated Data (without personally identifiable information) to compare with other organizations within the same industry or group. The Aggregated Data will not be considered your Confidential Information. Fees. You will promptly pay Bodo all applicable fees and, as described below, taxes associated with the Services. Except as expressly provided otherwise in this Agreement, all fees (if any) are non-refundable. Payments not made within such time period will be subject to late charges equal to the lesser of (i) one and one-half percent (1.5%) per month of the overdue amount or (ii) the maximum amount permitted under applicable law. You are responsible for paying all personal property, sales, use and other taxes (excluding taxes based upon our net income) and license and registration fees and other assessments or charges levied or imposed by any governmental body or agency as a result of the execution or performance of this Agreement, including your receipt of the Services. On notice of not less than sixty (60) days, we may, in our discretion, adjust any or all fees for the Services. You may terminate this Agreement on written notice to us within thirty (30) days of its receipt of our notice to adjust the fees; provided, however, that if you do not object to the adjustment in writing within the foregoing thirty (30) day period then you will be deemed to have agreed to the adjustment. Your Warranties. You represent and warrant that (i) you have full power, capacity, and authority to enter into this Agreement and to grant the license in Section 4 (Proprietary Rights); and (ii) your use of the Services will be in compliance with all applicable local, state, and federal laws and regulations. Indemnification. You will defend and indemnify Bodo and hold it and its affiliates, officers, directors, employees, and agents harmless from any and all claims, actions, proceedings, losses, deficiencies, damages, liabilities, costs, and expenses (including but not limited to reasonable attorneys' fees and all related costs and expenses) incurred by them as a result of any claim, judgment, or adjudication related to or arising from any or all of the following: (i) your use of the Services; (ii) breach of any of your obligations, representations, or warranties in this Agreement; or (iii) your failure to comply with applicable laws and regulations. Beta Services. We may designate certain new functionality or services to be made available in connection with the Services as \" Beta Services .\" The Beta Services will not be ready for use in a production environment. Because they will be at an early stage of development, operation and use of the Beta Services may be unpredictable and lead to erroneous results. You acknowledge and agree that: (i) the Beta Services will be experimental and will not have been fully tested; (ii) the Beta Services may not meet your requirements; (iii) the use or operation of the Beta Services may not be uninterrupted or error free; and (iv) your use of the Beta Services will be for purposes of evaluating and testing the new functionality and services and providing feedback to us. Your use of the Beta Services will be subject to all of the terms and conditions of this Agreement relating to the Services. You agree to promptly report any errors, defects, or other deficiencies in the Beta Services to us. NOTWITHSTANDING ANY OTHER PROVISION OF THIS AGREEMENT, ALL BETA SERVICES ARE PROVIDED \"AS-IS\" AND \"AS-AVAILABLE,\" WITHOUT WARRANTIES OF ANY KIND. You waive any and all claims, now known or later discovered, that you may have against us and our suppliers and licensors arising out of the Beta Services. Suspension or Termination of Services and Removal of Customer Content. We may, in our sole discretion, suspend your access to the Services for any of the following reasons: (i) to prevent disruption of or damages to, or degradation of, the Services and our systems; (ii) to comply with any law, regulation, court order, or other governmental request; (iii) to otherwise protect us from potential legal liability; (iv) to remove Customer Content that is illegal, offensive, or otherwise inappropriate, in our sole discretion, or (iv) in the event an invoice remains unpaid for more than forty-five (45) or more days from the invoice date. We will restore access to the Services as soon as the event giving rise to suspension has been resolved. This Section will not be construed as imposing any obligation or duty on us to monitor use of the Services. Confidentiality. 12.1 \"Confidential Information\" means all information or material which (i) gives a party some competitive business advantage or the opportunity of obtaining such advantage or the disclosure of which could be detrimental to the interests of that party; or (ii) which from all the relevant circumstances should reasonably be assumed to be confidential and proprietary. Each party's Confidential Information will remain the sole and exclusive property of that party. Confidential Information includes, but is not limited to, the Services. Neither party will have any obligation with respect to confidential information which: (i) is or becomes generally known to the public by any means other than a breach of the obligations of a receiving party; (ii) was previously known to the receiving party or rightly received by the receiving party from a third party; (iii) is independently developed by the receiving party; or (iv) subject to disclosure under court order or other lawful process. 12.2 Treatment of Confidential Information. Each party recognizes the importance of the other party's Confidential Information. In particular, each party recognizes and agrees that the Confidential Information of the other is critical to their respective businesses and that neither party would enter into this Agreement without assurance that the information will be protected as provided in this Section 12 and elsewhere in this Agreement. Accordingly, each party agrees as follows: (a) Each party will hold any and all Confidential Information it obtains in strictest confidence and will use and permit use of Confidential Information solely as permitted under this Agreement; and (b) Each party may disclose or provide access to its responsible employees and agents or as otherwise permitted under this Agreement, and may make copies, of Confidential Information only to the extent permitted under this Agreement. 12.3 Non-Exclusive Equitable Remedy. Each party acknowledges and agrees that due to the unique nature of the Confidential Information there can be no adequate remedy at law for any breach of its obligations hereunder, and therefore, that upon any such breach or any threat thereof, each party will be entitled to appropriate equitable relief from a court of competent jurisdiction in addition to whatever remedies either of them might have at law or equity. 12.4 You agree not to use any Confidential Information of Bodo, and shall restrict your affiliates and sublicensees from using the Confidential Information of Bodo, for purposes of challenging the validity of such Confidential Information, or Bodo's ability to use and exploit such Confidential Information. Limited Warranty; Exclusive Remedy. During the Term, Bodo warrants the Services will materially comply with the requirements of this Agreement and Documentation. In the event of a breach of the foregoing warranty, Bodo's sole and exclusive liability and your sole and exclusive remedy will be to use reasonable efforts to correct the non-conformity. In the event Bodo is unable through reasonable efforts to correct the defective Service, you may elect to terminate this Agreement and, if applicable, receive a prorated refund of any pre-paid, unused recurring fees. Disclaimer of Warranties. EXCEPT AS PROVIDED IN SECTION 13 (LIMITED WARRANTY), THE SERVICES ARE PROVIDED \"AS IS\" AND \"AS-AVAILABLE,\" WITH ALL FAULTS, AND WITHOUT WARRANTY OF ANY KIND. BODO AND ITS VENDORS AND LICENSORS DISCLAIM ALL OTHER WARRANTIES, EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT, QUALITY OF INFORMATION, OR TITLE/NON-INFRINGEMENT AND ALL SUCH WARRANTIES ARE HEREBY SPECIFICALLY DISCLAIMED. YOU EXPRESSLY AGREE AND ACKNOWLEDGE THAT USE OF SERVICES, IS AT YOUR SOLE RISK. NO ORAL OR WRITTEN INFORMATION OR ADVICE GIVEN BY BODO OR ITS AUTHORIZED REPRESENTATIVES WILL CREATE A WARRANTY OR IN ANY WAY INCREASE THE SCOPE OF BODO'S OBLIGATIONS HEREUNDER. THE SERVICES MAY BE USED TO ACCESS AND TRANSFER INFORMATION OVER THE INTERNET. YOU ACKNOWLEDGE AND AGREE THAT BODO AND ITS VENDORS AND LICENSORS DO NOT OPERATE OR CONTROL THE INTERNET AND THAT: (I) VIRUSES, WORMS, TROJAN HORSES, OR OTHER UNDESIRABLE DATA OR SOFTWARE; OR (II) UNAUTHORIZED USERS (E.G., HACKERS) MAY ATTEMPT TO OBTAIN ACCESS TO AND DAMAGE THE CUSTOMER CONTENT, WEB-SITES, COMPUTERS, OR NETWORKS. WE WILL NOT BE RESPONSIBLE FOR THOSE ACTIVITIES. Limitation of Liability and Damages. NEITHER BODO NOR ITS VENDORS AND LICENSORS WILL HAVE ANY LIABILITY TO YOU OR ANY THIRD PARTY FOR ANY LOSS OF PROFITS, BUSINESS, DATA, OR OTHER INCIDENTAL, CONSEQUENTIAL, OR SPECIAL LOSS OR DAMAGE, INCLUDING EXEMPLARY AND PUNITIVE, OF ANY KIND OR NATURE RESULTING FROM OR ARISING OUT OF THIS AGREEMENT, INCLUDING USE OF THE SERVICES EVEN IF BODO HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. THE TOTAL LIABILITY OF BODO AND ITS VENDORS AND LICENSORS TO YOU OR ANY THIRD PARTY ARISING OUT OF THIS AGREEMENT OR USE OF THE SERVICES IN CONNECTION WITH ANY CLAIM OR TYPE OF DAMAGE (WHETHER IN CONTRACT OR TORT) WILL NOT EXCEED THE TOTAL FEES YOU PAID, IF ANY, DURING THE SIX (6) MONTHS IMMEDIATELY PRECEDING THE EVENT GIVING RISE TO THE LIABILITY. THIS LIMITATION OF LIABILITY WILL APPLY EVEN IF THE EXPRESS WARRANTIES PROVIDED ABOVE FAIL OF THEIR ESSENTIAL PURPOSE. Term and Termination. Unless otherwise agreed by the parties, the Agreement shall be on-going until terminated by either party on thirty (30) days prior notice to the other party. In the event we terminate this Agreement for reasons other than breach of contract, any prepaid but unused fees will be refunded. Government Restrictions. Any software or other programming provided by us in connection with this Agreement is commercial computer software as described in DFARS 252.227-7014(a)(1) and FAR 2.101. If acquired by or on behalf of the United States Department of Defense or any component thereof, the United States Government acquires this commercial computer software and commercial computer software documentation subject to the terms of this Agreement as specified in DFARS 227.7202-3, Rights in Commercial Computer Software or Commercial Computer Software Documentation. If acquired by or on behalf of any civilian agency, the United States Government acquires this commercial computer software and commercial computer software documentation subject to the terms of this Agreement as specified in FAR 12.212, Computer Software. USA Patriot Act Notice. The U.S. federal USA Patriot Act (\" USA Patriot Act \") provides generally for the operator of a communication host and law enforcement to be able to monitor any content, upon request of the operator. We anticipate fully complying with our obligations and availing ourselves of all rights under the USA Patriot Act. General. Except for the payment of fees, if applicable, neither party will be liable for any failure or delay in performance under this Agreement which is due to any event beyond the reasonable control of such party, including without limitation, fire, explosion, unavailability of utilities or raw materials, Internet delays and failures, telecommunications failures, unavailability of components, labor difficulties, war, riot, act of God, export control regulation, laws, judgments or government instructions. This Agreement provides the entire agreement between the parties with regard to its subject matter. Except as provided below, this Agreement may not be amended without a writing signed by both parties. We may, at any time and from time-to-time, change the terms of this Agreement. Any changes will be posted on our Web site. In addition, we may also send you a notice about the amended terms via email. If you do not accept the terms of any modification, your only recourse is to terminate this Agreement by sending a termination notice us before the effective date of the amendments. The termination will be effective on the date we receive the notice. The most current version of the Agreement will be available on our Web site and will supersede all previous versions of the Agreement. Your continued use of the Services will constitute your acceptance of the changes. This Agreement will be construed according to, and the rights of the parties will be governed by, the law of the State of California, without reference to its conflict of laws rules. Any action at law or in equity arising out of or directly or indirectly relating to this Agreement may be instituted only in the Federal or state courts located in San Francisco, California. You consent and submit to the personal jurisdiction of those courts for the purposes of any action related to this Agreement, and to extra-territorial service of process. No action, regardless of form, arising out of this Agreement, may be brought by either party more than one (1) year after the cause of action has arisen. You may not assign this Agreement without the prior written consent of Bodo. If any of the provisions of this Agreement are found or deemed by a court to be invalid or unenforceable, they will be severable from the remainder of this Agreement and will not cause the invalidity or unenforceability of the remainder of this Agreement. Neither party will by mere lapse of time without giving notice or taking other action hereunder be deemed to have waived any breach by the other party of any of the provisions of this Agreement. The following provisions will survive termination or expiration of this Agreement: 4 (Proprietary Rights), 9 (Indemnification), 12 (Confidentiality), 13 (Limited Warranty; Exclusive Remedy); 14 (Disclaimer of Warranties), 15 (Limitation of Liability and Damages), 17 (Government Restrictions), 18 (USA Patriot Act Notice), and 19 (General Provisions). This Agreement may be accepted in electronic form (e.g., by an electronic or other means of demonstrating assent) and your acceptance will be deemed binding between us. Neither of us will contest the validity or enforceability of this Agreement and any related documents, including under any applicable statute of frauds, because they were accepted or signed in electronic form.","title":"End User License Agreement"},{"location":"help_and_reference/releases/","text":"Release Notes \u00b6 Bodo.ai 2022.1 Release Bodo.ai 2021.12 Release Bodo.ai 2021.11 Release Bodo.ai 2021.10 Release Bodo.ai 2021.9 Release Bodo.ai 2021.8 Release Bodo.ai 2021.7 Release Bodo.ai 2021.5 Release Bodo.ai 2021.4 Release Bodo.ai 2021.3 Release Bodo.ai 2021.2 Release Bodo.ai 2021.1 Release Bodo.ai 2020.12 Release Bodo.ai 2020.11 Release Bodo.ai 2020.10 Release Bodo.ai 2020.9 Release Bodo.ai 2020.8 Release Bodo.ai 2020.7 Release Bodo.ai 2020.6 Release Bodo.ai 2020.5 Release Bodo.ai 2020.4 Release Bodo.ai 2020.2 Release","title":"Release Notes"},{"location":"help_and_reference/releases/#releases","text":"Bodo.ai 2022.1 Release Bodo.ai 2021.12 Release Bodo.ai 2021.11 Release Bodo.ai 2021.10 Release Bodo.ai 2021.9 Release Bodo.ai 2021.8 Release Bodo.ai 2021.7 Release Bodo.ai 2021.5 Release Bodo.ai 2021.4 Release Bodo.ai 2021.3 Release Bodo.ai 2021.2 Release Bodo.ai 2021.1 Release Bodo.ai 2020.12 Release Bodo.ai 2020.11 Release Bodo.ai 2020.10 Release Bodo.ai 2020.9 Release Bodo.ai 2020.8 Release Bodo.ai 2020.7 Release Bodo.ai 2020.6 Release Bodo.ai 2020.5 Release Bodo.ai 2020.4 Release Bodo.ai 2020.2 Release","title":"Release Notes"},{"location":"installation_and_setup/bodo_platform/","text":"Using Bodo Cloud Platform \u00b6 Creating Clusters \u00b6 In the left bar click on Clusters (or click on the second step in the Onboarding list): This will take you to the Clusters page. At the top right corner, click on Create Cluster which opens the cluster creation form. First, choose a name for your cluster and check the EFA checkbox if you want to use EFA-enabled nodes (only available on AWS). Then, select the type of nodes in the cluster to be created from the Instance type dropdown list. Note If the Instance type dropdown list does not populate, either the credentials are not entered properly or they are not valid. Please see how to set your AWS or Azure credentials and make sure your credentials are valid. Next, enter the number of nodes for your cluster in Number of Instances . and choose the Bodo Version to be installed on your cluster. Typically the three latest Bodo Releases are available. Note If the Bodo Version dropdown list does not populate, either the credentials are not entered properly or the permissions to Bodo's Images have not been granted to your account. Please repeat the steps to set your AWS or Azure credentials and make sure you complete themwith valid credentials and that images have been successfully shared with your AWS or Azure account. Then, select a value for Cluster auto shutdown . This is the amount of time of inactivity after which the platform will remove the cluster automatically. Activity is determined through attached notebooks (see how to attach a notebook to a cluster ) and jobs (see how to run a job ). Therefore, if you don't plan to attach a notebook or a job to this cluster (and use it via ssh instead), it's recommended to set this to Never , since otherwise the cluster will be removed after the set time. Finally click on CREATE . You will see that a new task for creating the cluster has been created. The status is updated to INPROGRESS when the task starts executing and cluster creation is in progress. You can click on the Details drop down to monitor the progress for the cluster creation. Once the cluster is successfully created and ready to use, the status is updated to FINISHED . Attaching a Notebook to a Cluster \u00b6 Go to the notebooks page by clicking on Notebooks in the left bar (or on the third green step in the Onboarding list at the top). This will take you to the Notebooks page. At the top right corner, click on the Create Notebook button which opens the notebook creation form. Choose a name for your notebook and select the type of node that will host the notebook from the Instance type drop down list. Note that this node is for running the Jupyter notebook itself, and will not run cluster workloads. Lastly, select a cluster for attaching the notebook from the Cluster drop down menu and and click on CREATE . After clicking CREATE , a new task for creating the notebook and its corresponding node is created. The status updates to INPROGRESS when the task starts executing. After creating the notebook, the platform runs readiness probe checks: The notebook is ready to use after all checks are complete. OPEN NOTEBOOK will open the notebook in the current browser page, while the dropdown allows opening the notebook in a new tab. Connecting to a Cluster \u00b6 We recommend interacting with clusters primarily through Jupyter notebooks and Jobs. However, it may be necessary to connect directly to a cluster in some cases. You can either connect through a notebook terminal (recommended), or ssh directly from your machine. The latter requires providing your ssh public key during cluster creation. Connecting with a Notebook Terminal \u00b6 First, you need to create a cluster and attach a notebook to the cluster . Then, go the cluster tab and find your cluster. Click on DETAILS and copy the cluster UUID . Next, go to the notebooks tab and select OPEN NOTEBOOK . In the Launcher , click on Terminal . Through this terminal, you can interact with the /shared folder, which is shared by all the instances in the cluster and the Notebook instance. Verify your connection to interact directly with your cluster. SSH From Your Machine \u00b6 First, navigate to the clusters tabs and select Create a Cluster . Click on Show Advanced and add your public key in SSH Public Key . Then, click on Add your IP in the Access from IP address section to enable accessing your cluster from your machine. Fill the rest of the form by following the steps to create clusters . In the clusters tab, select your cluster and click on DETAILS to find the list of IP addresses for your cluster nodes. Use any of the IP addresses as the ssh destination. In addition, also copy the cluster UUID which will be needed to execute commands across the cluster. In any ssh agent, you can connect to one of your nodes with: ssh -i <path_to_private_key> bodo@<IP_ADDRESS> To add additional ssh options please refer to the documentation for your ssh agent. Verify your Connection \u00b6 Once you have connected to a node in your cluster, you should verify that you can run operations across all the instances in the cluster. Verify the path to the hostfile for your cluster. You can find it by running: ls -la /shared/.hostfile-<CLUSTER UUID> Check that you can run a command across you cluster. To do this, run: mpiexec -n <TOTAL_CORE_COUNT> -f /shared/.hostfile-<CLUSTER UUID> hostname This will print one line per each core in the cluster, with one unique hostname per cluster node. Your cluster's TOTAL_CORE_COUNT is usually half the number of vCPUs on each instance times the number of instances in your cluster. For example, if you have a 4 instance cluster of c5.4xlarge, then your TOTAL_CORE_COUNT is 32. Verify that you can run a python command across your cluster. For example, run: mpiexec -n <TOTAL_CORE_COUNT> -f /shared/.hostfile-<CLUSTER_UUID> python --version If all commands succeed, you should be able to execute workloads across your cluster. You can place scripts and small data that are shared across cluster nodes in /shared . However, external storage, such as S3, should be used for reading and writing large data. Running a Job \u00b6 Bodo Cloud Platform has support for running scheduled (and immediate) Python jobs without the need for Jupyter Notebooks. To create a Job, navigate to the Jobs page by selecting Jobs in the left bar. This pages displays any INPROGRESS jobs you have previously scheduled and allows you to schedule new Jobs. At the top right corner, click on CREATE JOB . This opens a job creation form. First, select a name for your job and specify the cluster on which you want to deploy your job. If you have an existing cluster that is not currently bound to a notebook or another job, you can select this cluster from the dropdown menu. Alternatively, you can create a cluster specifically for this job by selecting the NEW button next to the cluster dropdown menu. When creating a cluster specifically for a job, note that the cluster is only used for that job and is removed once the job completes. After selecting your cluster, indicate when you want your job to be executed in the Schedule section. Then, enter the Command that you want to execute inside this cluster. Note This command is automatically prepended with mpiexec -n <CORE_COUNT> python . For example, to run a file ex.py with the argument 1, you would enter the command ex.py 1 . To specify your source code location, fill in the Path line with a valid Git URL or S3 URI (only available on AWS) that leads to a repository containing your code. Note When selecting a GitHub URL, you should select the URL available at the top of your web browser and NOT the path when cloning the repository, i.e. your path SHOULD NOT end in .git . If selecting an S3 URI, your S3 bucket must be in the same region as your cluster. If you are cloning a private repository, you need to provide the platform with valid Git credentials to download your repository. To do so, select Show advanced in the bottom right of the form. Then in Workspace username , enter your Git username and in Workspace password enter either your password or a valid Github Access Token. The advanced options also allow you to specify a particular commit or branch with Workspace reference and to load other custom environment variables in Other . Note If your Github Account uses 2FA please use a Github Access Token to avoid any possible authentication issues. Once your form is complete, select CREATE to begin your job. Once you've provided all the necessary details, select CREATE to begin your job. You will see a NEW task created in your jobs page. If you created a cluster specifically for this job, a new cluster will also appear in your clusters page. Your job will begin once it reaches its scheduled time and any necessary clusters have been created. Then your job will transition to being INPROGRESS . At this point your job will execute your desired command. Once it finishes executing, your job will transition to FINISHED status. You can find any stdout information that you may need by pressing DETAILS followed by SHOW LOGS . If a cluster was specifically created for this job, it will be deleted after the job finishes. Note Bodo DOES NOT preserve artifacts written to local storage. If you have any information that you need to persist and later review, you should write to external storage, such as Amazon S3. You may also write to stdout/stderr, but output logs may be truncated, so it should not be considered reliable for large outputs that need to be read later.","title":"Using Bodo Cloud Platform"},{"location":"installation_and_setup/bodo_platform/#bodo_platform","text":"","title":"Using Bodo Cloud Platform"},{"location":"installation_and_setup/bodo_platform/#creating_clusters","text":"In the left bar click on Clusters (or click on the second step in the Onboarding list): This will take you to the Clusters page. At the top right corner, click on Create Cluster which opens the cluster creation form. First, choose a name for your cluster and check the EFA checkbox if you want to use EFA-enabled nodes (only available on AWS). Then, select the type of nodes in the cluster to be created from the Instance type dropdown list. Note If the Instance type dropdown list does not populate, either the credentials are not entered properly or they are not valid. Please see how to set your AWS or Azure credentials and make sure your credentials are valid. Next, enter the number of nodes for your cluster in Number of Instances . and choose the Bodo Version to be installed on your cluster. Typically the three latest Bodo Releases are available. Note If the Bodo Version dropdown list does not populate, either the credentials are not entered properly or the permissions to Bodo's Images have not been granted to your account. Please repeat the steps to set your AWS or Azure credentials and make sure you complete themwith valid credentials and that images have been successfully shared with your AWS or Azure account. Then, select a value for Cluster auto shutdown . This is the amount of time of inactivity after which the platform will remove the cluster automatically. Activity is determined through attached notebooks (see how to attach a notebook to a cluster ) and jobs (see how to run a job ). Therefore, if you don't plan to attach a notebook or a job to this cluster (and use it via ssh instead), it's recommended to set this to Never , since otherwise the cluster will be removed after the set time. Finally click on CREATE . You will see that a new task for creating the cluster has been created. The status is updated to INPROGRESS when the task starts executing and cluster creation is in progress. You can click on the Details drop down to monitor the progress for the cluster creation. Once the cluster is successfully created and ready to use, the status is updated to FINISHED .","title":"Creating Clusters"},{"location":"installation_and_setup/bodo_platform/#attaching_notebook_to_cluster","text":"Go to the notebooks page by clicking on Notebooks in the left bar (or on the third green step in the Onboarding list at the top). This will take you to the Notebooks page. At the top right corner, click on the Create Notebook button which opens the notebook creation form. Choose a name for your notebook and select the type of node that will host the notebook from the Instance type drop down list. Note that this node is for running the Jupyter notebook itself, and will not run cluster workloads. Lastly, select a cluster for attaching the notebook from the Cluster drop down menu and and click on CREATE . After clicking CREATE , a new task for creating the notebook and its corresponding node is created. The status updates to INPROGRESS when the task starts executing. After creating the notebook, the platform runs readiness probe checks: The notebook is ready to use after all checks are complete. OPEN NOTEBOOK will open the notebook in the current browser page, while the dropdown allows opening the notebook in a new tab.","title":"Attaching a Notebook to a Cluster"},{"location":"installation_and_setup/bodo_platform/#connecting_to_a_cluster","text":"We recommend interacting with clusters primarily through Jupyter notebooks and Jobs. However, it may be necessary to connect directly to a cluster in some cases. You can either connect through a notebook terminal (recommended), or ssh directly from your machine. The latter requires providing your ssh public key during cluster creation.","title":"Connecting to a Cluster"},{"location":"installation_and_setup/bodo_platform/#connecting-with-a-notebook-terminal","text":"First, you need to create a cluster and attach a notebook to the cluster . Then, go the cluster tab and find your cluster. Click on DETAILS and copy the cluster UUID . Next, go to the notebooks tab and select OPEN NOTEBOOK . In the Launcher , click on Terminal . Through this terminal, you can interact with the /shared folder, which is shared by all the instances in the cluster and the Notebook instance. Verify your connection to interact directly with your cluster.","title":"Connecting with a Notebook Terminal"},{"location":"installation_and_setup/bodo_platform/#ssh-from-your-machine","text":"First, navigate to the clusters tabs and select Create a Cluster . Click on Show Advanced and add your public key in SSH Public Key . Then, click on Add your IP in the Access from IP address section to enable accessing your cluster from your machine. Fill the rest of the form by following the steps to create clusters . In the clusters tab, select your cluster and click on DETAILS to find the list of IP addresses for your cluster nodes. Use any of the IP addresses as the ssh destination. In addition, also copy the cluster UUID which will be needed to execute commands across the cluster. In any ssh agent, you can connect to one of your nodes with: ssh -i <path_to_private_key> bodo@<IP_ADDRESS> To add additional ssh options please refer to the documentation for your ssh agent.","title":"SSH From Your Machine"},{"location":"installation_and_setup/bodo_platform/#verify_your_connection","text":"Once you have connected to a node in your cluster, you should verify that you can run operations across all the instances in the cluster. Verify the path to the hostfile for your cluster. You can find it by running: ls -la /shared/.hostfile-<CLUSTER UUID> Check that you can run a command across you cluster. To do this, run: mpiexec -n <TOTAL_CORE_COUNT> -f /shared/.hostfile-<CLUSTER UUID> hostname This will print one line per each core in the cluster, with one unique hostname per cluster node. Your cluster's TOTAL_CORE_COUNT is usually half the number of vCPUs on each instance times the number of instances in your cluster. For example, if you have a 4 instance cluster of c5.4xlarge, then your TOTAL_CORE_COUNT is 32. Verify that you can run a python command across your cluster. For example, run: mpiexec -n <TOTAL_CORE_COUNT> -f /shared/.hostfile-<CLUSTER_UUID> python --version If all commands succeed, you should be able to execute workloads across your cluster. You can place scripts and small data that are shared across cluster nodes in /shared . However, external storage, such as S3, should be used for reading and writing large data.","title":"Verify your Connection"},{"location":"installation_and_setup/bodo_platform/#running_a_job","text":"Bodo Cloud Platform has support for running scheduled (and immediate) Python jobs without the need for Jupyter Notebooks. To create a Job, navigate to the Jobs page by selecting Jobs in the left bar. This pages displays any INPROGRESS jobs you have previously scheduled and allows you to schedule new Jobs. At the top right corner, click on CREATE JOB . This opens a job creation form. First, select a name for your job and specify the cluster on which you want to deploy your job. If you have an existing cluster that is not currently bound to a notebook or another job, you can select this cluster from the dropdown menu. Alternatively, you can create a cluster specifically for this job by selecting the NEW button next to the cluster dropdown menu. When creating a cluster specifically for a job, note that the cluster is only used for that job and is removed once the job completes. After selecting your cluster, indicate when you want your job to be executed in the Schedule section. Then, enter the Command that you want to execute inside this cluster. Note This command is automatically prepended with mpiexec -n <CORE_COUNT> python . For example, to run a file ex.py with the argument 1, you would enter the command ex.py 1 . To specify your source code location, fill in the Path line with a valid Git URL or S3 URI (only available on AWS) that leads to a repository containing your code. Note When selecting a GitHub URL, you should select the URL available at the top of your web browser and NOT the path when cloning the repository, i.e. your path SHOULD NOT end in .git . If selecting an S3 URI, your S3 bucket must be in the same region as your cluster. If you are cloning a private repository, you need to provide the platform with valid Git credentials to download your repository. To do so, select Show advanced in the bottom right of the form. Then in Workspace username , enter your Git username and in Workspace password enter either your password or a valid Github Access Token. The advanced options also allow you to specify a particular commit or branch with Workspace reference and to load other custom environment variables in Other . Note If your Github Account uses 2FA please use a Github Access Token to avoid any possible authentication issues. Once your form is complete, select CREATE to begin your job. Once you've provided all the necessary details, select CREATE to begin your job. You will see a NEW task created in your jobs page. If you created a cluster specifically for this job, a new cluster will also appear in your clusters page. Your job will begin once it reaches its scheduled time and any necessary clusters have been created. Then your job will transition to being INPROGRESS . At this point your job will execute your desired command. Once it finishes executing, your job will transition to FINISHED status. You can find any stdout information that you may need by pressing DETAILS followed by SHOW LOGS . If a cluster was specifically created for this job, it will be deleted after the job finishes. Note Bodo DOES NOT preserve artifacts written to local storage. If you have any information that you need to persist and later review, you should write to external storage, such as Amazon S3. You may also write to stdout/stderr, but output logs may be truncated, so it should not be considered reliable for large outputs that need to be read later.","title":"Running a Job"},{"location":"installation_and_setup/bodo_platform_aws/","text":"Bodo Managed Cloud Platform on AWS \u00b6 Registration \u00b6 a. Subscribe through the AWS Marketplace . b. After confirming your subscription, you'll be directed to Bodo Platform's registration page. c. Fill out the fields with your information. If this is your individual account, use a unique name such as firstname_lastname for the Organization Name field. d. Check the box for accepting terms and conditions and click on SIGN UP : e. A page confirming that an activation link was sent to your email will appear. Please open the email and click on the activation link: Clicking on the confirmation link will take you to the bodo platform page where you can use your newly created credentials to sign in: Setting AWS Credentials \u00b6 To use Bodo on AWS, you need to link your AWS account to the Bodo platform. This can be done either using the Settings page in the left bar or the first item in the Onboarding list highlighted in green as shown in the picture below: To be able to use the Bodo Platform to launch clusters and notebooks, you must grant it permission to access your AWS account and provision the required resources in it. This can be done through an AWS Cross Account IAM Role for the Bodo Platform. Create a Cross-Account IAM Role \u00b6 There are two ways to create such an IAM Role, (a) you can create it manually, or (b) you can provide us with Access Keys and we can create an IAM role in your AWS account. We provide directions for both these methods below. Create the IAM Role Manually \u00b6 Create the IAM Role Manually Log in to the AWS Management Console and navigate to the IAM Service. Select the Roles tab in the sidebar, and click Create Role . In Select type of trusted entity , select Another AWS Account . Enter the Bodo Platform Account ID 481633624848 in the Account ID field. Check the Require external ID option. In the External ID field, copy over the External ID from the Settings page on the Bodo Platform. Click the Next: Permissions button. Click the Next: Tags button. Click the Next: Review button. In the Role name field, enter a role name, e.g. BodoPlatformUser . Click Create Role . You will be taken back to the list of IAM Roles in your account. In the list of IAM Roles, click on the role you just created. Click on Add inline policy . Click the JSON tab. Bodo Cloud Platform requires a specific set of AWS permissions which are documented in Bodo-Platform Policy . Paste the contents of the linked JSON file into the policy editor. Click on Review policy . In the Name field, add a policy name, e.g. Bodo-Platform-User-Policy . Click on Create policy . You will be taken back to the Role Summary. From the role summary, copy the Role ARN . This is the value that you will enter into the Role ARN field on the Setting Page on the Bodo Platform. Let the Bodo Platform create the IAM Role \u00b6 Let the Bodo Platform create the IAM Role Follow the instructions from AWS Account and Access Keys guide to create/retrieve your AWS access key ID and secret access key. Click on Create Role For Me below the Role ARN field on the Setting page. This will open up a panel. Enter the Access Keys created in step 1 in the form and click on CREATE ROLE . Note We will not save the provided Access Keys for security reasons. Click OK on the popup confirmation box. We will use the provided Access Keys to create an IAM Role in your AWS Account. The created Role ARN will be displayed on the same form. Copy the generated Role ARN. This is the value that you will enter into the Role ARN field on the Setting Page on the Bodo Platform. In some cases, this role creation might fail. This could happen due to various reasons. a. A role already exists : In this case, please open the AWS Management Console , and navigate to the IAM Service. Click on Roles in the sidebar. Look for a Role named BodoPlatformUser . Click on the role, and copy over the Role ARN from the role summary. Alternatively, you can delete the existing role from the AWS Console and then try to create an IAM role again via the Bodo Platform. This will ensure you have the role set up with the correct permissions. Note If this is a shared AWS Account, ensure that no one else is actively using this IAM Role before deleting it. b. Provided access keys are not valid : Please ensure that valid access keys are provided. c. Provided access keys don't have the right permissions to create a role : Please ensure that the provided access keys have the permissions required to create an IAM Role. If none of these work, try creating the IAM Role manually as described in earlier . Once you have generated an IAM Role using either of the methods described above, you are now ready to fill the Settings Form on the Bodo Platform. Enter the Role ARN created using one of the above options into the Role ARN field in the Settings Form. Select a Region from the dropdown list. This is the region that your resources will be deployed in by default. Click on SAVE . You can see the progress on granting AMI launch permissions to your account ID in the AMI Share Status field. Your account is ready when it turns green. Note We grant AMI launch permissions to your account in the following AWS regions: us-east-1, us-east-2, us-west-1 & us-west-2. Important We highly recommend that you ensure sufficient limits on your AWS account to launch resources. See here for details on the resources required for Bodo Cloud Platform. See Also Bodo Cloud Platform Resources Created in Your AWS Environment \u00b6 Bodo deploys cluster/notebook resources in your own AWS environment to ensure security of your data. Below is a list of AWS resources that the Bodo Platform creates in your account to enable clusters and notebooks. AWS Service Purpose EC2 Instances Cluster/notebook workers EFS Shared file system for clusters VPC , Subnets , NAT Gateway , Elastic IP , ENI , Security Groups , ... Secure networking for clusters/notebooks S3 and Dynamo DB Resource states AWS Systems Manager Managing EC2 instances KMS Cluster secrets (e.g. SSH keys) IAM Role for Clusters Allow cluster workers to access resources above Note These resources incur additional AWS infrastructure charges and are not included in the Bodo Platform charges. AWS Account Cleanup \u00b6 As explained earlier , the platform creates two types of resources in the users' AWS environments: organization level resources and cluster specific resources. The organization level resources are created by the platform to set up shared resources (such as a VPC, an EFS Mount, etc) that are used later by all created resources. The cluster specific resources (such as EC2 instances, ENIs, etc) are created by the platform to host/manage a specific cluster. This includes notebooks and corresponding resources as well. The cluster specific resources are removed when you request a cluster to be removed. The organization level resources persist in the user account so they can be used by clusters deployed in the future. However, if you need to remove these resources for any reason (AWS limits, etc.), an option to do so is provided. Navigate to the Settings page and click on Show Advanced in the bottom-right corner. This will bring up a section called AWS Resource Cleanup . Select the region from which you would like to remove these resources (i.e. the region in which the resources you want to delete have been created), and click CLEANUP AWS RESOURCES . Note that this will only work if you don't have any active clusters in that region deployed through the platform. Else, the request will be rejected, and you'll be asked to remove all clusters in that region before trying again. Removing active clusters (including clusters with a FAILED status) is necessary because this process will make them inaccessible to the platform. See Also Troubleshooting Managed Bodo Cloud Platform Issues on AWS Billing \u00b6 Users subscribed to the Bodo Platform through the AWS Marketplace will be charged for their use of the platform as part of their regular AWS bill. The platform charges are based on the type of instances deployed and the duration of their usage (to the nearest minute). The hourly rate for the supported instance types can be found on our website . For any cluster deployed through the platform, users are charged starting from when the cluster has been successfully deployed, until the time the user requests the cluster to be removed. Note Users are not charged in case of failures in cluster creation. As mentioned previously , the AWS resources set up by the platform in your AWS environment incur additional AWS infrastructure charges, and are not included in the Bodo Platform charges.","title":"Bodo Managed Cloud Platform on AWS"},{"location":"installation_and_setup/bodo_platform_aws/#bodo_platform_aws","text":"","title":"Bodo Managed Cloud Platform on AWS"},{"location":"installation_and_setup/bodo_platform_aws/#registration","text":"a. Subscribe through the AWS Marketplace . b. After confirming your subscription, you'll be directed to Bodo Platform's registration page. c. Fill out the fields with your information. If this is your individual account, use a unique name such as firstname_lastname for the Organization Name field. d. Check the box for accepting terms and conditions and click on SIGN UP : e. A page confirming that an activation link was sent to your email will appear. Please open the email and click on the activation link: Clicking on the confirmation link will take you to the bodo platform page where you can use your newly created credentials to sign in:","title":"Registration"},{"location":"installation_and_setup/bodo_platform_aws/#setting_aws_credentials","text":"To use Bodo on AWS, you need to link your AWS account to the Bodo platform. This can be done either using the Settings page in the left bar or the first item in the Onboarding list highlighted in green as shown in the picture below: To be able to use the Bodo Platform to launch clusters and notebooks, you must grant it permission to access your AWS account and provision the required resources in it. This can be done through an AWS Cross Account IAM Role for the Bodo Platform.","title":"Setting AWS Credentials"},{"location":"installation_and_setup/bodo_platform_aws/#create_iam_role","text":"There are two ways to create such an IAM Role, (a) you can create it manually, or (b) you can provide us with Access Keys and we can create an IAM role in your AWS account. We provide directions for both these methods below.","title":"Create a Cross-Account IAM Role"},{"location":"installation_and_setup/bodo_platform_aws/#create_iam_role_manually","text":"Create the IAM Role Manually Log in to the AWS Management Console and navigate to the IAM Service. Select the Roles tab in the sidebar, and click Create Role . In Select type of trusted entity , select Another AWS Account . Enter the Bodo Platform Account ID 481633624848 in the Account ID field. Check the Require external ID option. In the External ID field, copy over the External ID from the Settings page on the Bodo Platform. Click the Next: Permissions button. Click the Next: Tags button. Click the Next: Review button. In the Role name field, enter a role name, e.g. BodoPlatformUser . Click Create Role . You will be taken back to the list of IAM Roles in your account. In the list of IAM Roles, click on the role you just created. Click on Add inline policy . Click the JSON tab. Bodo Cloud Platform requires a specific set of AWS permissions which are documented in Bodo-Platform Policy . Paste the contents of the linked JSON file into the policy editor. Click on Review policy . In the Name field, add a policy name, e.g. Bodo-Platform-User-Policy . Click on Create policy . You will be taken back to the Role Summary. From the role summary, copy the Role ARN . This is the value that you will enter into the Role ARN field on the Setting Page on the Bodo Platform.","title":"Create the IAM Role Manually"},{"location":"installation_and_setup/bodo_platform_aws/#create_iam_role_using_platform","text":"Let the Bodo Platform create the IAM Role Follow the instructions from AWS Account and Access Keys guide to create/retrieve your AWS access key ID and secret access key. Click on Create Role For Me below the Role ARN field on the Setting page. This will open up a panel. Enter the Access Keys created in step 1 in the form and click on CREATE ROLE . Note We will not save the provided Access Keys for security reasons. Click OK on the popup confirmation box. We will use the provided Access Keys to create an IAM Role in your AWS Account. The created Role ARN will be displayed on the same form. Copy the generated Role ARN. This is the value that you will enter into the Role ARN field on the Setting Page on the Bodo Platform. In some cases, this role creation might fail. This could happen due to various reasons. a. A role already exists : In this case, please open the AWS Management Console , and navigate to the IAM Service. Click on Roles in the sidebar. Look for a Role named BodoPlatformUser . Click on the role, and copy over the Role ARN from the role summary. Alternatively, you can delete the existing role from the AWS Console and then try to create an IAM role again via the Bodo Platform. This will ensure you have the role set up with the correct permissions. Note If this is a shared AWS Account, ensure that no one else is actively using this IAM Role before deleting it. b. Provided access keys are not valid : Please ensure that valid access keys are provided. c. Provided access keys don't have the right permissions to create a role : Please ensure that the provided access keys have the permissions required to create an IAM Role. If none of these work, try creating the IAM Role manually as described in earlier . Once you have generated an IAM Role using either of the methods described above, you are now ready to fill the Settings Form on the Bodo Platform. Enter the Role ARN created using one of the above options into the Role ARN field in the Settings Form. Select a Region from the dropdown list. This is the region that your resources will be deployed in by default. Click on SAVE . You can see the progress on granting AMI launch permissions to your account ID in the AMI Share Status field. Your account is ready when it turns green. Note We grant AMI launch permissions to your account in the following AWS regions: us-east-1, us-east-2, us-west-1 & us-west-2. Important We highly recommend that you ensure sufficient limits on your AWS account to launch resources. See here for details on the resources required for Bodo Cloud Platform. See Also Bodo Cloud Platform","title":"Let the Bodo Platform create the IAM Role"},{"location":"installation_and_setup/bodo_platform_aws/#resources_created_in_aws_env","text":"Bodo deploys cluster/notebook resources in your own AWS environment to ensure security of your data. Below is a list of AWS resources that the Bodo Platform creates in your account to enable clusters and notebooks. AWS Service Purpose EC2 Instances Cluster/notebook workers EFS Shared file system for clusters VPC , Subnets , NAT Gateway , Elastic IP , ENI , Security Groups , ... Secure networking for clusters/notebooks S3 and Dynamo DB Resource states AWS Systems Manager Managing EC2 instances KMS Cluster secrets (e.g. SSH keys) IAM Role for Clusters Allow cluster workers to access resources above Note These resources incur additional AWS infrastructure charges and are not included in the Bodo Platform charges.","title":"Resources Created in Your AWS Environment"},{"location":"installation_and_setup/bodo_platform_aws/#aws_account_cleanup","text":"As explained earlier , the platform creates two types of resources in the users' AWS environments: organization level resources and cluster specific resources. The organization level resources are created by the platform to set up shared resources (such as a VPC, an EFS Mount, etc) that are used later by all created resources. The cluster specific resources (such as EC2 instances, ENIs, etc) are created by the platform to host/manage a specific cluster. This includes notebooks and corresponding resources as well. The cluster specific resources are removed when you request a cluster to be removed. The organization level resources persist in the user account so they can be used by clusters deployed in the future. However, if you need to remove these resources for any reason (AWS limits, etc.), an option to do so is provided. Navigate to the Settings page and click on Show Advanced in the bottom-right corner. This will bring up a section called AWS Resource Cleanup . Select the region from which you would like to remove these resources (i.e. the region in which the resources you want to delete have been created), and click CLEANUP AWS RESOURCES . Note that this will only work if you don't have any active clusters in that region deployed through the platform. Else, the request will be rejected, and you'll be asked to remove all clusters in that region before trying again. Removing active clusters (including clusters with a FAILED status) is necessary because this process will make them inaccessible to the platform. See Also Troubleshooting Managed Bodo Cloud Platform Issues on AWS","title":"AWS Account Cleanup"},{"location":"installation_and_setup/bodo_platform_aws/#aws_billing","text":"Users subscribed to the Bodo Platform through the AWS Marketplace will be charged for their use of the platform as part of their regular AWS bill. The platform charges are based on the type of instances deployed and the duration of their usage (to the nearest minute). The hourly rate for the supported instance types can be found on our website . For any cluster deployed through the platform, users are charged starting from when the cluster has been successfully deployed, until the time the user requests the cluster to be removed. Note Users are not charged in case of failures in cluster creation. As mentioned previously , the AWS resources set up by the platform in your AWS environment incur additional AWS infrastructure charges, and are not included in the Bodo Platform charges.","title":"Billing"},{"location":"installation_and_setup/bodo_platform_azure/","text":"Bodo Managed Cloud Platform on Azure \u00b6 Registration \u00b6 a. Contact Bodo to be onboarded onto Bodo Cloud Platform on Azure. You will be provided with an onboarding link. b. The provided link will take you to Bodo Platform's registration page. c. Fill out the fields with your information. If this is your individual account, use a unique name such as firstname_lastname for the Organization Name field. d. Check the box for accepting terms and conditions and click on SIGN UP : e. A page confirming that an activation link was sent to your email will appear. Please open the email and click on the activation link: Clicking on the confirmation link will take you to the bodo platform page where you can use your newly created credentials to sign in: Setting Azure Credentials \u00b6 To use Bodo on Azure, you need to link your Azure account to the Bodo platform. This can be done either using the Settings page in the left bar or the first item in the Onboarding list highlighted in green as shown in the picture below: In order to use the Bodo Platform to launch clusters and notebooks, you must grant it permission to access your Azure account and provision the required resources in it. You can do this by creating a Service Principal for the Bodo Platform application and assigning a role to it. Create a Service Principal \u00b6 Login to your Azure Portal. Click on the icon next to the search bar to open a Cloud-Shell . Execute the following command to create a service principal: az ad sp create --id APP_ID where APP_ID is the Application ID for Bodo-Platform which is displayed on the Settings Page. Once you have created a service principal, you need to assign a role to it. You can assign a role to this service principal at either a subscription level or a resource group level. Subscription level permissions are only required if you want Bodo to create a new resource group. If you provide an existing resource group, only permissions at the resource group level are required. As shown below, go to the IAM section of your subscription or resource group and add a Contributor Role to the service principal you created for the Bodo Platform Application. See Also Required Azure resource providers Once you have created the service principal and assigned a role to it, you are now ready to fill the Settings Form on the Bodo Platform. Enter your Azure subscription ID in the Subscription ID field. You can find this in the Subscription Overview . Enter your Azure Tenant ID in the Tenant ID field. You can find this in Azure AD . If you've given Bodo subscription level permissions and want Bodo to create a new resource group in your Azure subscription, enter the name of the resource group you want it to create in the Resource Group field. A suggested name is pre-filled for you. If you've given Bodo resource group level permissions to an existing resource group, enter the name of this resource group. Select a region from the dropdown list. This is the region that all Bodo resources will be deployed in. If you're providing an existing resource group, this must be the region this resource group is located in. Click on SAVE . Note We highly recommend that you ensure sufficient limits on your Azure subscription to launch resources. See here for the resources required for Bodo Cloud Platform. Required Resource Providers on Azure subscription \u00b6 Ensure that the following resource providers are registered on your Azure subscription: Microsoft.Authorization Microsoft.Compute Microsoft.KeyVault Microsoft.ManagedIdentity Microsoft.Network Microsoft.Resources Microsoft.Storage See Also Bodo Cloud Platform Resources Created in Your Azure Environment \u00b6 Bodo deploys cluster/notebook resources in your own Azure environment to ensure security of your data. Below is a list of Azure resources that the Bodo Platform creates in your account to enable clusters and notebooks. Azure Service Purpose Virtual Machines Cluster/notebook workers Storage Accounts , File-Shares Shared file system for clusters Virtual Network with Subnets and NAT Gateway , Public IP , NIC , Proximity Placement Groups , Availability Sets , Security Groups , ... Secure networking for clusters/notebooks Blob Containers , Resource states KeyVault Cluster secrets (e.g. SSH keys) VM Identity for Clusters Allow cluster workers to access resources above Note These resources incur additional Azure infrastructure charges and are not included in the Bodo Platform charges. Azure Account Cleanup \u00b6 As explained in earlier , the platform creates two types of resources in the users' Azure environments: organization level resources and cluster specific resources. Organization level resources are created by the platform to set up shared resources (such as a VNets, File-Share, etc) that are used later by all created resources. Cluster specific resources (such as virtual machines, NICs, etc) are created by the platform to host/manage a specific cluster. This includes notebooks and corresponding resources as well. The cluster specific resources are removed when you request a cluster to be removed. The organization level resources persist in the user account so they can be used by clusters deployed in the future. However, if you need to remove these resources for any reason (Azure resource limits, etc.), an option to do so is provided. Navigate to the Settings page and click on Show Advanced in the bottom-right corner. This will bring up a section called Azure Resource Cleanup . Select the region from which you would like to remove these resources (i.e. the region in which the resources you want to delete have been created), and click CLEANUP AZURE RESOURCES . Note that this will only work if you don't have any active clusters in that region deployed through the platform. Else, the request will be rejected, and you'll be asked to remove all clusters in that region before trying again. Removing active clusters (including clusters with a FAILED status) is necessary because this process will make them inaccessible to the platform. The KeyVault deleted as part of this process needs to be purged manually through the Azure Portal if you plan to create resources on the platform again. See how to manually purge Azure KeyVault . Manually Purge Azure Keyvault \u00b6 Purging key vaults requires subscription level permissions. You can read more about this here and here . To avoid having to assign subscription level roles to Bodo Platform's service principal, we require users to do this step manually. Navigate to Key vaults on your Azure Portal . Click on Manage deleted vaults . In the form, select the subscription associated with Bodo KeyVault to see a list of deleted key vaults. Select the key vault with bodo in its name, click on Purge and confirm by clicking Delete . As shown in the notification, the purge process can take up to 10 minutes to complete. The purged key vault may continue to show up on the list of deleted key vaults until it has been successfully purged. Once the key vault has been successfully purged, the list of deleted keyvaults should not feature it. At this point you can use the Bodo Platform again to provision clusters, etc. See Also Troubleshooting Managed Bodo Cloud Platform Issues on Azure","title":"Bodo Managed Cloud Platform on Azure"},{"location":"installation_and_setup/bodo_platform_azure/#bodo_platform_azure","text":"","title":"Bodo Managed Cloud Platform on Azure"},{"location":"installation_and_setup/bodo_platform_azure/#registration","text":"a. Contact Bodo to be onboarded onto Bodo Cloud Platform on Azure. You will be provided with an onboarding link. b. The provided link will take you to Bodo Platform's registration page. c. Fill out the fields with your information. If this is your individual account, use a unique name such as firstname_lastname for the Organization Name field. d. Check the box for accepting terms and conditions and click on SIGN UP : e. A page confirming that an activation link was sent to your email will appear. Please open the email and click on the activation link: Clicking on the confirmation link will take you to the bodo platform page where you can use your newly created credentials to sign in:","title":"Registration"},{"location":"installation_and_setup/bodo_platform_azure/#setting_azure_credentials","text":"To use Bodo on Azure, you need to link your Azure account to the Bodo platform. This can be done either using the Settings page in the left bar or the first item in the Onboarding list highlighted in green as shown in the picture below: In order to use the Bodo Platform to launch clusters and notebooks, you must grant it permission to access your Azure account and provision the required resources in it. You can do this by creating a Service Principal for the Bodo Platform application and assigning a role to it.","title":"Setting Azure Credentials"},{"location":"installation_and_setup/bodo_platform_azure/#create_service_principal","text":"Login to your Azure Portal. Click on the icon next to the search bar to open a Cloud-Shell . Execute the following command to create a service principal: az ad sp create --id APP_ID where APP_ID is the Application ID for Bodo-Platform which is displayed on the Settings Page. Once you have created a service principal, you need to assign a role to it. You can assign a role to this service principal at either a subscription level or a resource group level. Subscription level permissions are only required if you want Bodo to create a new resource group. If you provide an existing resource group, only permissions at the resource group level are required. As shown below, go to the IAM section of your subscription or resource group and add a Contributor Role to the service principal you created for the Bodo Platform Application. See Also Required Azure resource providers Once you have created the service principal and assigned a role to it, you are now ready to fill the Settings Form on the Bodo Platform. Enter your Azure subscription ID in the Subscription ID field. You can find this in the Subscription Overview . Enter your Azure Tenant ID in the Tenant ID field. You can find this in Azure AD . If you've given Bodo subscription level permissions and want Bodo to create a new resource group in your Azure subscription, enter the name of the resource group you want it to create in the Resource Group field. A suggested name is pre-filled for you. If you've given Bodo resource group level permissions to an existing resource group, enter the name of this resource group. Select a region from the dropdown list. This is the region that all Bodo resources will be deployed in. If you're providing an existing resource group, this must be the region this resource group is located in. Click on SAVE . Note We highly recommend that you ensure sufficient limits on your Azure subscription to launch resources. See here for the resources required for Bodo Cloud Platform.","title":"Create a Service Principal"},{"location":"installation_and_setup/bodo_platform_azure/#required_az_resource_providers","text":"Ensure that the following resource providers are registered on your Azure subscription: Microsoft.Authorization Microsoft.Compute Microsoft.KeyVault Microsoft.ManagedIdentity Microsoft.Network Microsoft.Resources Microsoft.Storage See Also Bodo Cloud Platform","title":"Required Resource Providers on Azure subscription"},{"location":"installation_and_setup/bodo_platform_azure/#resources_created_in_azure_env","text":"Bodo deploys cluster/notebook resources in your own Azure environment to ensure security of your data. Below is a list of Azure resources that the Bodo Platform creates in your account to enable clusters and notebooks. Azure Service Purpose Virtual Machines Cluster/notebook workers Storage Accounts , File-Shares Shared file system for clusters Virtual Network with Subnets and NAT Gateway , Public IP , NIC , Proximity Placement Groups , Availability Sets , Security Groups , ... Secure networking for clusters/notebooks Blob Containers , Resource states KeyVault Cluster secrets (e.g. SSH keys) VM Identity for Clusters Allow cluster workers to access resources above Note These resources incur additional Azure infrastructure charges and are not included in the Bodo Platform charges.","title":"Resources Created in Your Azure Environment"},{"location":"installation_and_setup/bodo_platform_azure/#azure_account_cleanup","text":"As explained in earlier , the platform creates two types of resources in the users' Azure environments: organization level resources and cluster specific resources. Organization level resources are created by the platform to set up shared resources (such as a VNets, File-Share, etc) that are used later by all created resources. Cluster specific resources (such as virtual machines, NICs, etc) are created by the platform to host/manage a specific cluster. This includes notebooks and corresponding resources as well. The cluster specific resources are removed when you request a cluster to be removed. The organization level resources persist in the user account so they can be used by clusters deployed in the future. However, if you need to remove these resources for any reason (Azure resource limits, etc.), an option to do so is provided. Navigate to the Settings page and click on Show Advanced in the bottom-right corner. This will bring up a section called Azure Resource Cleanup . Select the region from which you would like to remove these resources (i.e. the region in which the resources you want to delete have been created), and click CLEANUP AZURE RESOURCES . Note that this will only work if you don't have any active clusters in that region deployed through the platform. Else, the request will be rejected, and you'll be asked to remove all clusters in that region before trying again. Removing active clusters (including clusters with a FAILED status) is necessary because this process will make them inaccessible to the platform. The KeyVault deleted as part of this process needs to be purged manually through the Azure Portal if you plan to create resources on the platform again. See how to manually purge Azure KeyVault .","title":"Azure Account Cleanup"},{"location":"installation_and_setup/bodo_platform_azure/#manually_purge_azure_kayvault","text":"Purging key vaults requires subscription level permissions. You can read more about this here and here . To avoid having to assign subscription level roles to Bodo Platform's service principal, we require users to do this step manually. Navigate to Key vaults on your Azure Portal . Click on Manage deleted vaults . In the form, select the subscription associated with Bodo KeyVault to see a list of deleted key vaults. Select the key vault with bodo in its name, click on Purge and confirm by clicking Delete . As shown in the notification, the purge process can take up to 10 minutes to complete. The purged key vault may continue to show up on the list of deleted key vaults until it has been successfully purged. Once the key vault has been successfully purged, the list of deleted keyvaults should not feature it. At this point you can use the Bodo Platform again to provision clusters, etc. See Also Troubleshooting Managed Bodo Cloud Platform Issues on Azure","title":"Manually Purge Azure Keyvault"},{"location":"installation_and_setup/enterprise/","text":"Configuring Bodo Enterprise Edition \u00b6 Bodo Enterprise Edition allows unrestricted use of Bodo on any number of cores. Ensure you have installed Bodo before configuring Bodo Enterprise Edition. License Key \u00b6 Bodo Enterprise Edition requires a license key to run. The key can be provided in two ways: Through the environment variable BODO_LICENSE A file called bodo.lic in the current working directory In both cases, the file or environment variable must contain the key exactly as provided. If Bodo cannot find the license, you will only be able to run Bodo on up to 8 cores. If you try to run Bodo on more than 8 cores and if Bodo cannot find the license (the environment variable does not exist or is empty, and no license file is found), it will exit with the Bodo license not found error. If the contents of the license key are invalid, Bodo will exit with the Invalid license error. This typically means that the key is missing data or contains extraneous characters. Please make sure the license file has not been modified, or that the environment variable contains the key verbatim. Note that some shells might append extra characters when displaying the file contents. A good way to export the key is this: export BODO_LICENSE = ` cat bodo.lic ` Automated BODO_LICENSE environment variable Setup \u00b6 You can automate setting of the BODO_LICENSE environment variable in your ~/.bashrc script (or the ~/.zshrc script for macOS) using: echo 'export BODO_LICENSE=\"<COPY_PASTE_THE_LICENSE_HERE>\"' >> ~/.bashrc For more fine-grained control and usage with the Bodo conda environment as created when installing bodo , we recommend the following steps to automate setting the BODO_LICENSE environment variable (very similar to these steps): Ensure that you are in the correct conda environment. Navigate to the $CONDA_PREFIX directory and create some additional conda environment activation and deactivation steps: cd $CONDA_PREFIX mkdir -p ./etc/conda/activate.d mkdir -p ./etc/conda/deactivate.d touch ./etc/conda/activate.d/env_vars.sh touch ./etc/conda/deactivate.d/env_vars.sh Edit ./etc/conda/activate.d/env_vars.sh as follows: #!/bin/sh export BODO_LICENSE = \"<COPY_PASTE_THE_LICENSE_HERE>\" Similarly, edit ./etc/conda/deactivate.d/env_vars.sh as follows: #!/bin/sh unset BODO_LICENSE Deactivate ( conda deactivate ) and reactivate the Bodo conda environment ( conda activate Bodo ) to ensure that the environment variable BODO_LICENSE is automatically added when the environment is activated. Using MPI in Clusters with Bodo Enterprise Edition \u00b6 MPI can be configured on clusters easily. The cluster nodes need to have passwordless SSH enabled between them, and there should be a host file listing their addresses (see an example tutorial here ). MPI usually needs to be configured to launch one process per physical core for best performance. This avoids potential resource contention between processes due to the high efficiency of MPI. For example, a cluster of four nodes, each with 16 physical cores, would use 64 MPI processes: mpiexec -n 64 python example.py For cloud instances, one physical core usually corresponds to two vCPUs. For example, an instance with 32 vCPUs has 16 physical cores. See Also Interactive Bodo Cluster Setup using IPyParallel Setting up passwordless SSH on your multi-node cluster \u00b6 Using MPI on a multi-node cluster requires setting up passwordless SSH between the hosts. There are multiple ways to do this. Here is one way: Generate an SSH key pair using a tool like ssh-keygen , for instance: ssh-keygen -b 2048 -f cluster_ssh_key -N \"\" Copy over the generated private key ( cluster_ssh_key ) and public key ( cluster_ssh_key.pub ) to all the hosts and store them in ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub respectively. Add the public key to ~/.ssh/authorized_keys on all hosts. To disable host key checking, add the following to ~/.ssh/config on each host: Host * StrictHostKeyChecking no","title":"Configuring Bodo Enterprise Edition"},{"location":"installation_and_setup/enterprise/#enterprise","text":"Bodo Enterprise Edition allows unrestricted use of Bodo on any number of cores. Ensure you have installed Bodo before configuring Bodo Enterprise Edition.","title":"Configuring Bodo Enterprise Edition"},{"location":"installation_and_setup/enterprise/#licensekey","text":"Bodo Enterprise Edition requires a license key to run. The key can be provided in two ways: Through the environment variable BODO_LICENSE A file called bodo.lic in the current working directory In both cases, the file or environment variable must contain the key exactly as provided. If Bodo cannot find the license, you will only be able to run Bodo on up to 8 cores. If you try to run Bodo on more than 8 cores and if Bodo cannot find the license (the environment variable does not exist or is empty, and no license file is found), it will exit with the Bodo license not found error. If the contents of the license key are invalid, Bodo will exit with the Invalid license error. This typically means that the key is missing data or contains extraneous characters. Please make sure the license file has not been modified, or that the environment variable contains the key verbatim. Note that some shells might append extra characters when displaying the file contents. A good way to export the key is this: export BODO_LICENSE = ` cat bodo.lic `","title":"License Key"},{"location":"installation_and_setup/enterprise/#automated-bodo_license-environment-variable-setup","text":"You can automate setting of the BODO_LICENSE environment variable in your ~/.bashrc script (or the ~/.zshrc script for macOS) using: echo 'export BODO_LICENSE=\"<COPY_PASTE_THE_LICENSE_HERE>\"' >> ~/.bashrc For more fine-grained control and usage with the Bodo conda environment as created when installing bodo , we recommend the following steps to automate setting the BODO_LICENSE environment variable (very similar to these steps): Ensure that you are in the correct conda environment. Navigate to the $CONDA_PREFIX directory and create some additional conda environment activation and deactivation steps: cd $CONDA_PREFIX mkdir -p ./etc/conda/activate.d mkdir -p ./etc/conda/deactivate.d touch ./etc/conda/activate.d/env_vars.sh touch ./etc/conda/deactivate.d/env_vars.sh Edit ./etc/conda/activate.d/env_vars.sh as follows: #!/bin/sh export BODO_LICENSE = \"<COPY_PASTE_THE_LICENSE_HERE>\" Similarly, edit ./etc/conda/deactivate.d/env_vars.sh as follows: #!/bin/sh unset BODO_LICENSE Deactivate ( conda deactivate ) and reactivate the Bodo conda environment ( conda activate Bodo ) to ensure that the environment variable BODO_LICENSE is automatically added when the environment is activated.","title":"Automated BODO_LICENSE environment variable Setup"},{"location":"installation_and_setup/enterprise/#mpienterpriseclusters","text":"MPI can be configured on clusters easily. The cluster nodes need to have passwordless SSH enabled between them, and there should be a host file listing their addresses (see an example tutorial here ). MPI usually needs to be configured to launch one process per physical core for best performance. This avoids potential resource contention between processes due to the high efficiency of MPI. For example, a cluster of four nodes, each with 16 physical cores, would use 64 MPI processes: mpiexec -n 64 python example.py For cloud instances, one physical core usually corresponds to two vCPUs. For example, an instance with 32 vCPUs has 16 physical cores. See Also Interactive Bodo Cluster Setup using IPyParallel","title":"Using MPI in Clusters with Bodo Enterprise Edition"},{"location":"installation_and_setup/enterprise/#passwordless_ssh","text":"Using MPI on a multi-node cluster requires setting up passwordless SSH between the hosts. There are multiple ways to do this. Here is one way: Generate an SSH key pair using a tool like ssh-keygen , for instance: ssh-keygen -b 2048 -f cluster_ssh_key -N \"\" Copy over the generated private key ( cluster_ssh_key ) and public key ( cluster_ssh_key.pub ) to all the hosts and store them in ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub respectively. Add the public key to ~/.ssh/authorized_keys on all hosts. To disable host key checking, add the following to ~/.ssh/config on each host: Host * StrictHostKeyChecking no","title":"Setting up passwordless SSH on your multi-node cluster"},{"location":"installation_and_setup/install/","text":"Installing Bodo Community Edition \u00b6 Bodo is available as a Python package on pip , and can be installed as follows: pip install bodo Bodo can also be installed as a using the conda command (see how to install conda below). If you are installing bodo through conda, we recommend creating a conda environment and installing Bodo and its dependencies in it as shown below: conda create -n Bodo python=3.9 -c conda-forge conda activate Bodo conda install bodo -c bodo.ai -c conda-forge Bodo uses MPI for parallelization, which is automatically installed as part of the conda install command above. This command installs Bodo Community Edition by default, which is free and works on up to 8 cores. For information on Bodo Enterprise Edition and pricing, please contact us . See Also Configuring Bodo Enterprise Edition How to Install Conda \u00b6 Install Conda using the instructions below. On Linux \u00b6 wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh chmod +x miniconda.sh ./miniconda.sh -b export PATH = $HOME /miniconda3/bin: $PATH On MacOS \u00b6 curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -L -o miniconda.sh chmod +x miniconda.sh ./miniconda.sh -b export PATH = $HOME /miniconda3/bin: $PATH On Windows \u00b6 start /wait \"\" Miniconda3-latest-Windows-x86_64.exe /InstallationType=JustMe /RegisterPython=0 /S /D=%UserProfile%\\Miniconda3 Open the Anaconda Prompt to use Bodo (click Start, select Anaconda Prompt). You may use other terminals if you have already added Anaconda to your PATH. Optional Dependencies \u00b6 Some Bodo functionality may require other dependencies, as summarized in the table below. All optional dependencies except Hadoop can be installed using the commands conda install gcsfs sqlalchemy snowflake-connector-python hdf5='*=*mpich*' openjdk -c conda-forge and pip install deltalake Functionality Dependency pd.read_sql / df.to_sql sqlalchemy Snowflake I/O snowflake-connector-python GCS I/O gcsfs Delta Lake deltalake HDFS or ADLS Gen2 hadoop (only the Hadoop client is needed) HDF5 hdf5 (MPI version) Testing your Installation \u00b6 Once you have activated your conda environment and installed Bodo in it, you can test it using the example program below. This program has two functions: The function gen_data creates a sample dataset with 20,000 rows and writes to a parquet file called example1.pq . The function test reads example1.pq and performs multiple computations on it. import bodo import pandas as pd import numpy as np import time @bodo . jit def gen_data (): NUM_GROUPS = 30 NUM_ROWS = 20_000_000 df = pd . DataFrame ({ \"A\" : np . arange ( NUM_ROWS ) % NUM_GROUPS , \"B\" : np . arange ( NUM_ROWS ) }) df . to_parquet ( \"example1.pq\" ) @bodo . jit def test (): df = pd . read_parquet ( \"example1.pq\" ) t0 = time . time () df2 = df . groupby ( \"A\" )[ \"B\" ] . agg ( ( lambda a : ( a == 1 ) . sum (), lambda a : ( a == 2 ) . sum (), lambda a : ( a == 3 ) . sum ()) ) m = df2 . mean () print ( \"Result:\" , m , \" \\n Compute time:\" , time . time () - t0 , \"secs\" ) gen_data () test () Save this code in a file called example.py , and run it on a single core as follows: python example.py Alternatively, to run the code on four cores, you can use mpiexec : mpiexec -n 8 python example.py Note You may need to delete example1.pq between consecutive runs. See Also Interactive Bodo Cluster Setup using IPyParallel","title":"Installing Bodo Community Edition"},{"location":"installation_and_setup/install/#install","text":"Bodo is available as a Python package on pip , and can be installed as follows: pip install bodo Bodo can also be installed as a using the conda command (see how to install conda below). If you are installing bodo through conda, we recommend creating a conda environment and installing Bodo and its dependencies in it as shown below: conda create -n Bodo python=3.9 -c conda-forge conda activate Bodo conda install bodo -c bodo.ai -c conda-forge Bodo uses MPI for parallelization, which is automatically installed as part of the conda install command above. This command installs Bodo Community Edition by default, which is free and works on up to 8 cores. For information on Bodo Enterprise Edition and pricing, please contact us . See Also Configuring Bodo Enterprise Edition","title":"Installing Bodo Community Edition"},{"location":"installation_and_setup/install/#conda","text":"Install Conda using the instructions below.","title":"How to Install Conda"},{"location":"installation_and_setup/install/#on-linux","text":"wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh chmod +x miniconda.sh ./miniconda.sh -b export PATH = $HOME /miniconda3/bin: $PATH","title":"On Linux"},{"location":"installation_and_setup/install/#on-macos","text":"curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -L -o miniconda.sh chmod +x miniconda.sh ./miniconda.sh -b export PATH = $HOME /miniconda3/bin: $PATH","title":"On MacOS"},{"location":"installation_and_setup/install/#on-windows","text":"start /wait \"\" Miniconda3-latest-Windows-x86_64.exe /InstallationType=JustMe /RegisterPython=0 /S /D=%UserProfile%\\Miniconda3 Open the Anaconda Prompt to use Bodo (click Start, select Anaconda Prompt). You may use other terminals if you have already added Anaconda to your PATH.","title":"On Windows"},{"location":"installation_and_setup/install/#optionaldep","text":"Some Bodo functionality may require other dependencies, as summarized in the table below. All optional dependencies except Hadoop can be installed using the commands conda install gcsfs sqlalchemy snowflake-connector-python hdf5='*=*mpich*' openjdk -c conda-forge and pip install deltalake Functionality Dependency pd.read_sql / df.to_sql sqlalchemy Snowflake I/O snowflake-connector-python GCS I/O gcsfs Delta Lake deltalake HDFS or ADLS Gen2 hadoop (only the Hadoop client is needed) HDF5 hdf5 (MPI version)","title":"Optional Dependencies"},{"location":"installation_and_setup/install/#testinstall","text":"Once you have activated your conda environment and installed Bodo in it, you can test it using the example program below. This program has two functions: The function gen_data creates a sample dataset with 20,000 rows and writes to a parquet file called example1.pq . The function test reads example1.pq and performs multiple computations on it. import bodo import pandas as pd import numpy as np import time @bodo . jit def gen_data (): NUM_GROUPS = 30 NUM_ROWS = 20_000_000 df = pd . DataFrame ({ \"A\" : np . arange ( NUM_ROWS ) % NUM_GROUPS , \"B\" : np . arange ( NUM_ROWS ) }) df . to_parquet ( \"example1.pq\" ) @bodo . jit def test (): df = pd . read_parquet ( \"example1.pq\" ) t0 = time . time () df2 = df . groupby ( \"A\" )[ \"B\" ] . agg ( ( lambda a : ( a == 1 ) . sum (), lambda a : ( a == 2 ) . sum (), lambda a : ( a == 3 ) . sum ()) ) m = df2 . mean () print ( \"Result:\" , m , \" \\n Compute time:\" , time . time () - t0 , \"secs\" ) gen_data () test () Save this code in a file called example.py , and run it on a single core as follows: python example.py Alternatively, to run the code on four cores, you can use mpiexec : mpiexec -n 8 python example.py Note You may need to delete example1.pq between consecutive runs. See Also Interactive Bodo Cluster Setup using IPyParallel","title":"Testing your Installation"},{"location":"installation_and_setup/ipyparallel/","text":"Interactive Bodo Cluster Setup using IPyParallel \u00b6 Bodo can be used with IPyParallel to allow interactive code execution on a local or remote cluster. Getting started on your machine \u00b6 Install IPyParallel, JupyterLab, and Bodo in your conda environment: conda install bodo ipyparallel = 8 .1 jupyterlab = 3 -c bodo.ai -c conda-forge Start a JupyterLab server: jupyter lab Start a new notebook and run the following code in a cell to start an IPyParallel cluster: import ipyparallel as ipp import psutil ; n = min ( psutil . cpu_count ( logical = False ), 8 ) rc = ipp . Cluster ( engines = 'mpi' , n = n ) . start_and_connect_sync ( activate = True ) This starts a local N-core MPI cluster on your machine, where N is the minimum of the number of cores on your machine and 8. You can now start using the %%px cell magic to parallelize your code execution, or use %autopx to run all cells on the IPyParallel cluster by default. Read more here . Verifying your setup \u00b6 Run the following code to verify that your IPyParallel cluster is set up correctly: %% px import bodo print ( f \"Hello World from rank { bodo . get_rank () } . Total ranks= { bodo . get_size () } \" ) The correct output is: Hello World from rank 0. Total ranks=N Hello World from rank 1. Total ranks=N ... Hello World from rank N-1. Total ranks=N Where N is the minimum of the number of cores on your machine and 8. Running on multiple hosts \u00b6 To start an IPyParallel cluster across multiple hosts: Install IPyParallel and Bodo on all hosts: conda install bodo ipyparallel=8.1 -c bodo.ai -c conda-forge Install JupyterLab on one of the hosts. Let's call it the controller node: conda install jupyterlab=3 -c bodo.ai -c conda-forge Set up passwordless SSH between each of these hosts (this is needed for mpiexec ). See the section on passwordless ssh for instructions. The controller node must be able to connect to all engines via TCP on any port. If you have a restricted network, please refer to the IPyParallel documentation for other options such as SSH tunneling. Create a hostfile that contains list of IP addresses or host names where you want to launch engines. Note Make sure your hostfile is in the following format: ip_1 ip_2 ... You can find more information about hostfiles here . It is important to note that other MPI systems and launchers (such as QSUB/PBS) may use a different user interface for the allocation of computational nodes. Create the default IPython profile on all nodes by executing the following from the controller node: mpiexec -ppn 1 -f <PATH_TO_HOSTFILE> ipython profile create Now you can start a JupyterLab server on the controller node: jupyter lab Starting an IPyParallel cluster across multiple hosts requires setting a couple of additional configuration options. Start a new notebook and run the following code in a cell: import ipyparallel as ipp c = ipp . Cluster ( engines = 'mpi' , n = 8 , # Number of engines: Set this to the total number of physical cores in your cluster controller_ip = '*' , controller_args = [ \"--nodb\" ]) c . engine_launcher_class . mpi_args = [ \"-f\" , < PATH_TO_HOSTFILE > ] rc = c . start_and_connect_sync () view = rc . broadcast_view ( block = True ) view . activate () You have now successfully started an IPyParallel cluster across multiple hosts. Verifying your setup \u00b6 Run the following code to verify that your IPyParallel cluster is set up correctly: %% px import bodo import socket print ( f \"Hello World from rank { bodo . get_rank () } on host { socket . gethostname () } . Total ranks= { bodo . get_size () } \" ) On a cluster with two hosts running 4 engines, the correct output is: Hello World from rank 0 on host A. Total ranks=4 Hello World from rank 1 on host A. Total ranks=4 Hello World from rank 2 on host B. Total ranks=4 Hello World from rank 3 on host B. Total ranks=4 Running Bodo on your IPyParallel Cluster \u00b6 You are now ready to run your Bodo code. Here is an example function with Bodo: %% px import bodo @bodo . jit def process_data ( n ): df = pd . DataFrame ({ \"A\" : np . arange ( n ), \"B\" : np . arange ( n ) ** 2 }) df [ \"C\" ] = df . apply ( lambda r : 2 * r . A + r . B if r . A > 10 else 0 , axis = 1 ) return df [ \"C\" ] . sum () process_data ( 100000000 ) Running from a Python Script \u00b6 You can run code on an IPyParallel cluster from a python script instead of IPython or JupyterLab as follows: Setup the cluster using the same steps as above. Define the function you want to run on the cluster: import inspect import bodo @bodo . jit def process_data ( n ): df = pd . DataFrame ({ \"A\" : np . arange ( n ), \"B\" : np . arange ( n ) ** 2 }) df [ \"C\" ] = df . apply ( lambda r : 2 * r . A + r . B if r . A > 10 else 0 , axis = 1 ) return df [ \"C\" ] . sum () process_data ( 100000000 ) We define a Python wrapper for process_data called bodo_exec which will be sent to the engines to compute. This wrapper will call the Bodo function on the engines, collect the result and send it back to the client. def bodo_exec ( points ): return process_data ( points ) We can send the source code to be executed at the engines, using the execute method of ipyparallel's DirectView object. After the imports and code definitions are sent to the engines, the computation is started by actually calling the process_data function (now defined on the engines) and returning the result to the client. def main (): # remote code execution: import required modules on engines view . execute ( \"import pandas as pd\" ) view . execute ( \"import numpy as np\" ) view . execute ( \"import bodo\" ) view . execute ( \"import time\" ) # send code of Bodo functions to engines bodo_funcs = [ process_data ] for f in bodo_funcs : # get source code of Bodo function f_src = inspect . getsource ( f ) # execute the source code thus defining the function on engines view . execute ( f_src ) . get () points = 200000000 ar = view . apply ( bodo_exec , points ) result = ar . get () print ( \"Result is\" , result ) rc . close () main () Useful IPyParallel References \u00b6 IPyParallel Documentation Using MPI with IPython IPython Parallel in 2021","title":"Interactive Bodo Cluster Setup using IPyParallel"},{"location":"installation_and_setup/ipyparallel/#ipyparallelsetup","text":"Bodo can be used with IPyParallel to allow interactive code execution on a local or remote cluster.","title":"Interactive Bodo Cluster Setup using IPyParallel"},{"location":"installation_and_setup/ipyparallel/#quickstart_local","text":"Install IPyParallel, JupyterLab, and Bodo in your conda environment: conda install bodo ipyparallel = 8 .1 jupyterlab = 3 -c bodo.ai -c conda-forge Start a JupyterLab server: jupyter lab Start a new notebook and run the following code in a cell to start an IPyParallel cluster: import ipyparallel as ipp import psutil ; n = min ( psutil . cpu_count ( logical = False ), 8 ) rc = ipp . Cluster ( engines = 'mpi' , n = n ) . start_and_connect_sync ( activate = True ) This starts a local N-core MPI cluster on your machine, where N is the minimum of the number of cores on your machine and 8. You can now start using the %%px cell magic to parallelize your code execution, or use %autopx to run all cells on the IPyParallel cluster by default. Read more here .","title":"Getting started on your machine"},{"location":"installation_and_setup/ipyparallel/#setupverify_local","text":"Run the following code to verify that your IPyParallel cluster is set up correctly: %% px import bodo print ( f \"Hello World from rank { bodo . get_rank () } . Total ranks= { bodo . get_size () } \" ) The correct output is: Hello World from rank 0. Total ranks=N Hello World from rank 1. Total ranks=N ... Hello World from rank N-1. Total ranks=N Where N is the minimum of the number of cores on your machine and 8.","title":"Verifying your setup"},{"location":"installation_and_setup/ipyparallel/#quickstart_multiple_hosts","text":"To start an IPyParallel cluster across multiple hosts: Install IPyParallel and Bodo on all hosts: conda install bodo ipyparallel=8.1 -c bodo.ai -c conda-forge Install JupyterLab on one of the hosts. Let's call it the controller node: conda install jupyterlab=3 -c bodo.ai -c conda-forge Set up passwordless SSH between each of these hosts (this is needed for mpiexec ). See the section on passwordless ssh for instructions. The controller node must be able to connect to all engines via TCP on any port. If you have a restricted network, please refer to the IPyParallel documentation for other options such as SSH tunneling. Create a hostfile that contains list of IP addresses or host names where you want to launch engines. Note Make sure your hostfile is in the following format: ip_1 ip_2 ... You can find more information about hostfiles here . It is important to note that other MPI systems and launchers (such as QSUB/PBS) may use a different user interface for the allocation of computational nodes. Create the default IPython profile on all nodes by executing the following from the controller node: mpiexec -ppn 1 -f <PATH_TO_HOSTFILE> ipython profile create Now you can start a JupyterLab server on the controller node: jupyter lab Starting an IPyParallel cluster across multiple hosts requires setting a couple of additional configuration options. Start a new notebook and run the following code in a cell: import ipyparallel as ipp c = ipp . Cluster ( engines = 'mpi' , n = 8 , # Number of engines: Set this to the total number of physical cores in your cluster controller_ip = '*' , controller_args = [ \"--nodb\" ]) c . engine_launcher_class . mpi_args = [ \"-f\" , < PATH_TO_HOSTFILE > ] rc = c . start_and_connect_sync () view = rc . broadcast_view ( block = True ) view . activate () You have now successfully started an IPyParallel cluster across multiple hosts.","title":"Running on multiple hosts"},{"location":"installation_and_setup/ipyparallel/#setupverify_multiple_hosts","text":"Run the following code to verify that your IPyParallel cluster is set up correctly: %% px import bodo import socket print ( f \"Hello World from rank { bodo . get_rank () } on host { socket . gethostname () } . Total ranks= { bodo . get_size () } \" ) On a cluster with two hosts running 4 engines, the correct output is: Hello World from rank 0 on host A. Total ranks=4 Hello World from rank 1 on host A. Total ranks=4 Hello World from rank 2 on host B. Total ranks=4 Hello World from rank 3 on host B. Total ranks=4","title":"Verifying your setup"},{"location":"installation_and_setup/ipyparallel/#run_bodo_ipyparallel","text":"You are now ready to run your Bodo code. Here is an example function with Bodo: %% px import bodo @bodo . jit def process_data ( n ): df = pd . DataFrame ({ \"A\" : np . arange ( n ), \"B\" : np . arange ( n ) ** 2 }) df [ \"C\" ] = df . apply ( lambda r : 2 * r . A + r . B if r . A > 10 else 0 , axis = 1 ) return df [ \"C\" ] . sum () process_data ( 100000000 )","title":"Running Bodo on your IPyParallel Cluster"},{"location":"installation_and_setup/ipyparallel/#run_from_python_script","text":"You can run code on an IPyParallel cluster from a python script instead of IPython or JupyterLab as follows: Setup the cluster using the same steps as above. Define the function you want to run on the cluster: import inspect import bodo @bodo . jit def process_data ( n ): df = pd . DataFrame ({ \"A\" : np . arange ( n ), \"B\" : np . arange ( n ) ** 2 }) df [ \"C\" ] = df . apply ( lambda r : 2 * r . A + r . B if r . A > 10 else 0 , axis = 1 ) return df [ \"C\" ] . sum () process_data ( 100000000 ) We define a Python wrapper for process_data called bodo_exec which will be sent to the engines to compute. This wrapper will call the Bodo function on the engines, collect the result and send it back to the client. def bodo_exec ( points ): return process_data ( points ) We can send the source code to be executed at the engines, using the execute method of ipyparallel's DirectView object. After the imports and code definitions are sent to the engines, the computation is started by actually calling the process_data function (now defined on the engines) and returning the result to the client. def main (): # remote code execution: import required modules on engines view . execute ( \"import pandas as pd\" ) view . execute ( \"import numpy as np\" ) view . execute ( \"import bodo\" ) view . execute ( \"import time\" ) # send code of Bodo functions to engines bodo_funcs = [ process_data ] for f in bodo_funcs : # get source code of Bodo function f_src = inspect . getsource ( f ) # execute the source code thus defining the function on engines view . execute ( f_src ) . get () points = 200000000 ar = view . apply ( bodo_exec , points ) result = ar . get () print ( \"Result is\" , result ) rc . close () main ()","title":"Running from a Python Script"},{"location":"installation_and_setup/ipyparallel/#useful-ipyparallel-references","text":"IPyParallel Documentation Using MPI with IPython IPython Parallel in 2021","title":"Useful IPyParallel References"},{"location":"installation_and_setup/recommended_cluster_config/","text":"Recommended Cluster Configuration \u00b6 This section describes best practices for configuring compute clusters for Bodo applications. Minimizing Communication Overheads \u00b6 Communication across cores is usually the largest overhead in parallel applications including Bodo. To minimize it: For a given number of physical cores, use fewer large nodes with high core count rather than many small nodes with a low core count. This ensures that more cross core communication happens inside nodes. For example, a cluster with two c5n.18xlarge AWS instances will generally perform better than a cluster with four c5n.9xlarge instances, even though the two options have equivalent cost and compute power. Use node types that support high bandwidth networking. AWS instance types with n in their name, such as c5n.18xlarge , m5n.24xlarge and r5n.24xlarge provide high bandwidth. On Azure, use virtual machines that support Accelerated Networking . Use instance types that support RDMA networking. Examples of such instance types are Elastic Fabric Adapter (EFA) (AWS) and Infiniband (Azure). In our empirical testing, we found that EFA can significantly accelerate inter-node communication during expensive operations such as shuffle (which is used in join, groupby, sorting and others). List of AWS EC2 instance types that support EFA . For more information about EFA refer to the section on Recommended AWS Network Interface . RDMA capable Azure VM Sizes . Ensure that the server nodes are located physically close to each other. On AWS this can be done by adding all instances to a placement group with the cluster strategy. Similarly on Azure, you can use Proximity Placement Groups . For most applications, we recommend using c5n.18xlarge instances on AWS for best performance. For memory intensive use cases r5n.24xlarge instances are a good alternative. Both instance types support 100 Gbps networking as well as EFA. Other Best Practices \u00b6 Ensure that the file descriptor limit ( ulimit -n ) is set to a large number like 65000 . This is especially useful when using IPyParallel which opens direct connections between ipengine and ipcontroller processes. Avoid unnecessary threading inside the application since it can conflict with MPI parallelism. You can set the following environment variables in your shell (e.g. in bashrc ) to avoid threading: export OPENBLAS_NUM_THREADS = 1 export OMP_NUM_THREADS = 1 export MKL_NUM_THREADS = 1 Use Intel MPI for best performance. See our recommended MPI settings for more details.","title":"Recommended Cluster Configuration"},{"location":"installation_and_setup/recommended_cluster_config/#recommended_cluster_config","text":"This section describes best practices for configuring compute clusters for Bodo applications.","title":"Recommended Cluster Configuration"},{"location":"installation_and_setup/recommended_cluster_config/#minimizing-communication-overheads","text":"Communication across cores is usually the largest overhead in parallel applications including Bodo. To minimize it: For a given number of physical cores, use fewer large nodes with high core count rather than many small nodes with a low core count. This ensures that more cross core communication happens inside nodes. For example, a cluster with two c5n.18xlarge AWS instances will generally perform better than a cluster with four c5n.9xlarge instances, even though the two options have equivalent cost and compute power. Use node types that support high bandwidth networking. AWS instance types with n in their name, such as c5n.18xlarge , m5n.24xlarge and r5n.24xlarge provide high bandwidth. On Azure, use virtual machines that support Accelerated Networking . Use instance types that support RDMA networking. Examples of such instance types are Elastic Fabric Adapter (EFA) (AWS) and Infiniband (Azure). In our empirical testing, we found that EFA can significantly accelerate inter-node communication during expensive operations such as shuffle (which is used in join, groupby, sorting and others). List of AWS EC2 instance types that support EFA . For more information about EFA refer to the section on Recommended AWS Network Interface . RDMA capable Azure VM Sizes . Ensure that the server nodes are located physically close to each other. On AWS this can be done by adding all instances to a placement group with the cluster strategy. Similarly on Azure, you can use Proximity Placement Groups . For most applications, we recommend using c5n.18xlarge instances on AWS for best performance. For memory intensive use cases r5n.24xlarge instances are a good alternative. Both instance types support 100 Gbps networking as well as EFA.","title":"Minimizing Communication Overheads"},{"location":"installation_and_setup/recommended_cluster_config/#other-best-practices","text":"Ensure that the file descriptor limit ( ulimit -n ) is set to a large number like 65000 . This is especially useful when using IPyParallel which opens direct connections between ipengine and ipcontroller processes. Avoid unnecessary threading inside the application since it can conflict with MPI parallelism. You can set the following environment variables in your shell (e.g. in bashrc ) to avoid threading: export OPENBLAS_NUM_THREADS = 1 export OMP_NUM_THREADS = 1 export MKL_NUM_THREADS = 1 Use Intel MPI for best performance. See our recommended MPI settings for more details.","title":"Other Best Practices"},{"location":"installation_and_setup/recommended_mpi_settings/","text":"Recommended MPI Settings \u00b6 These are our recommendations to tune your application environment and achieve the best possible performance with Bodo. Important These recommendations are only applicable when you are running your workload on a cluster. You do not need to do any of this on your laptop. Intel-MPI library is the preferred distribution for message passing interface (MPI) specification. Note that Bodo automatically installs mpich . Hence, after installing Intel-MPI, remove [mpich] using this command: conda remove -y --force mpich mpi Intel-MPI provides different tuning collective algorithms. Based on our internal benchmarking, we recommend setting these environment variables as follows: export I_MPI_ALLREDUCE = 4 export I_MPI_REDUCE = 3 MPI Process Placement \u00b6 Bodo assigns chunks of data and computation to MPI processes, also called ranks . For example, for a dataframe with a billion rows on a 1000-core cluster, the first one million rows are assigned to rank 0, the second one million rows to rank 1, and so on. MPI placement indicates how these ranks are assigned to physical cores across the cluster, and can significantly impact performance depending on hardware configuration and application behavior. We recommend trying block mapping and round-robin mapping options below for your application to achieve the best performance. Block Mapping \u00b6 In block mapping, cores of each node in the hostfile are filled with ranks before moving on to the next node. For example, for a cluster with 50-core nodes, the first 50 ranks will be on node 0, the second 50 ranks on node 1 and so on. This mapping has the advantage of fast communication between neighboring ranks on the same node. We provide instructions on setting block placement for MPICH and Intel MPI below. The following assumes the hostfile only contains a list of hosts (e.g. it does not specify number of processes per host) and the number of cores on each host is the same. Block Mapping with MPICH and Intel MPI : mpiexec -n <N> -f <hostfile> -ppn <P> python bodo_file.py where N is the number of MPI processes, hostfile contains the list of hosts, and P the number of processes (cores) per node. Round-Robin Mapping \u00b6 In round-robin mapping, MPI assigns one rank per node in hostfile and starts over when it reaches end of the host list. For example, for a cluster with 50-core nodes, rank 0 is assigned to node 0, rank 1 is assigned to node 1 and so on. Rank 50 is assigned to node 0, 51 to node 1, and so on. This mapping has the advantage of avoiding communication hotspots in the network and tends to make large shuffles faster. We provide instructions on setting round-robin placement for MPICH and Intel MPI below. The following assumes the hostfile only contains a list of hosts (e.g. it does not specify number of processes per host) and the number of cores on each host is the same. Round-Robin with MPICH : mpiexec -n <N> -f <hostfile> python bodo_file.py Round-Robin with Intel MPI : mpiexec -n <N> -f <hostfile> -rr python bodo_file.py Useful References \u00b6 More information on controlling process placement with Intel MPI can be found here . See how to use the Hydra Process Manager for MPICH here . Recommended AWS Network Interface \u00b6 Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that has shown better inter-node communications at scale on AWS. To enable EFA with Intel-MPI on your cluster, follow instructions here . Some points to note in addition to the referenced instructions: All instances must be in the same subnet. For more information, see the \"EFA Limitations\" section here . All instances must be part of a security group that allows all inbound and outbound traffic to and from the security group itself. Follow these instructions to set up the security group correctly. For use with Intel-MPI, a minimal installation of the EFA drivers is sufficient and recommended: sudo ./efa_installer.sh -y --minimal Depending on where the drivers were downloaded from, you might need to include a --no-verify flag: sudo ./efa_installer.sh -y --minimal --no-verify We recommend the following versions for the EFA installer and Intel-MPI: EFA_INSTALLER_VERSION: 1.13.0 Intel-MPI: v3.1 (2021.3.1.315) Other version combinations are not guaranteed to work as they have not been tested. For EFA installer versions >= 1.12.0 , enabling fork is required by setting environment variable FI_EFA_FORK_SAFE=1 . To confirm correct settings are enabled, run following mpiexec with I_MPI_DEBUG=5 : I_MPI_DEBUG = 5 mpiexec -f hostfile -rr -n <CORES> python -u -c \"from mpi4py import MPI\" Check that libfabric provider is efa and environment variables are set as shown below: [0] MPI startup(): Intel(R) MPI Library, Version 2021.3.1 Build 20210719 (id: 48425b416) [0] MPI startup(): Copyright (C) 2003-2021 Intel Corporation. All rights reserved. [0] MPI startup(): library kind: release [0] MPI startup(): libfabric version: 1.13.0rc1-impi [0] MPI startup(): libfabric provider: efa ... [0] MPI startup(): I_MPI_ADJUST_ALLREDUCE=4 [0] MPI startup(): I_MPI_ADJUST_REDUCE=3 [0] MPI startup(): I_MPI_DEBUG=5","title":"Recommended MPI Settings"},{"location":"installation_and_setup/recommended_mpi_settings/#recommended_mpi_settings","text":"These are our recommendations to tune your application environment and achieve the best possible performance with Bodo. Important These recommendations are only applicable when you are running your workload on a cluster. You do not need to do any of this on your laptop. Intel-MPI library is the preferred distribution for message passing interface (MPI) specification. Note that Bodo automatically installs mpich . Hence, after installing Intel-MPI, remove [mpich] using this command: conda remove -y --force mpich mpi Intel-MPI provides different tuning collective algorithms. Based on our internal benchmarking, we recommend setting these environment variables as follows: export I_MPI_ALLREDUCE = 4 export I_MPI_REDUCE = 3","title":"Recommended MPI Settings"},{"location":"installation_and_setup/recommended_mpi_settings/#mpi-process-placement","text":"Bodo assigns chunks of data and computation to MPI processes, also called ranks . For example, for a dataframe with a billion rows on a 1000-core cluster, the first one million rows are assigned to rank 0, the second one million rows to rank 1, and so on. MPI placement indicates how these ranks are assigned to physical cores across the cluster, and can significantly impact performance depending on hardware configuration and application behavior. We recommend trying block mapping and round-robin mapping options below for your application to achieve the best performance.","title":"MPI Process Placement"},{"location":"installation_and_setup/recommended_mpi_settings/#block-mapping","text":"In block mapping, cores of each node in the hostfile are filled with ranks before moving on to the next node. For example, for a cluster with 50-core nodes, the first 50 ranks will be on node 0, the second 50 ranks on node 1 and so on. This mapping has the advantage of fast communication between neighboring ranks on the same node. We provide instructions on setting block placement for MPICH and Intel MPI below. The following assumes the hostfile only contains a list of hosts (e.g. it does not specify number of processes per host) and the number of cores on each host is the same. Block Mapping with MPICH and Intel MPI : mpiexec -n <N> -f <hostfile> -ppn <P> python bodo_file.py where N is the number of MPI processes, hostfile contains the list of hosts, and P the number of processes (cores) per node.","title":"Block Mapping"},{"location":"installation_and_setup/recommended_mpi_settings/#round-robin-mapping","text":"In round-robin mapping, MPI assigns one rank per node in hostfile and starts over when it reaches end of the host list. For example, for a cluster with 50-core nodes, rank 0 is assigned to node 0, rank 1 is assigned to node 1 and so on. Rank 50 is assigned to node 0, 51 to node 1, and so on. This mapping has the advantage of avoiding communication hotspots in the network and tends to make large shuffles faster. We provide instructions on setting round-robin placement for MPICH and Intel MPI below. The following assumes the hostfile only contains a list of hosts (e.g. it does not specify number of processes per host) and the number of cores on each host is the same. Round-Robin with MPICH : mpiexec -n <N> -f <hostfile> python bodo_file.py Round-Robin with Intel MPI : mpiexec -n <N> -f <hostfile> -rr python bodo_file.py","title":"Round-Robin Mapping"},{"location":"installation_and_setup/recommended_mpi_settings/#useful-references","text":"More information on controlling process placement with Intel MPI can be found here . See how to use the Hydra Process Manager for MPICH here .","title":"Useful References"},{"location":"installation_and_setup/recommended_mpi_settings/#recommended_aws_nic","text":"Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that has shown better inter-node communications at scale on AWS. To enable EFA with Intel-MPI on your cluster, follow instructions here . Some points to note in addition to the referenced instructions: All instances must be in the same subnet. For more information, see the \"EFA Limitations\" section here . All instances must be part of a security group that allows all inbound and outbound traffic to and from the security group itself. Follow these instructions to set up the security group correctly. For use with Intel-MPI, a minimal installation of the EFA drivers is sufficient and recommended: sudo ./efa_installer.sh -y --minimal Depending on where the drivers were downloaded from, you might need to include a --no-verify flag: sudo ./efa_installer.sh -y --minimal --no-verify We recommend the following versions for the EFA installer and Intel-MPI: EFA_INSTALLER_VERSION: 1.13.0 Intel-MPI: v3.1 (2021.3.1.315) Other version combinations are not guaranteed to work as they have not been tested. For EFA installer versions >= 1.12.0 , enabling fork is required by setting environment variable FI_EFA_FORK_SAFE=1 . To confirm correct settings are enabled, run following mpiexec with I_MPI_DEBUG=5 : I_MPI_DEBUG = 5 mpiexec -f hostfile -rr -n <CORES> python -u -c \"from mpi4py import MPI\" Check that libfabric provider is efa and environment variables are set as shown below: [0] MPI startup(): Intel(R) MPI Library, Version 2021.3.1 Build 20210719 (id: 48425b416) [0] MPI startup(): Copyright (C) 2003-2021 Intel Corporation. All rights reserved. [0] MPI startup(): library kind: release [0] MPI startup(): libfabric version: 1.13.0rc1-impi [0] MPI startup(): libfabric provider: efa ... [0] MPI startup(): I_MPI_ADJUST_ALLREDUCE=4 [0] MPI startup(): I_MPI_ADJUST_REDUCE=3 [0] MPI startup(): I_MPI_DEBUG=5","title":"Recommended AWS Network Interface"},{"location":"integrating_bodo/data_visualization/","text":"Data Visualization \u00b6 Bodo supports Matplotlib visualization natively inside JIT functions. This section specifies the supported Matplotlib APIs and classes. In general, these APIs support all arguments except for the restrictions specified in each section. Plotting APIs \u00b6 Currently, Bodo automatically supports the following plotting APIs. matplotlib.pyplot.plot matplotlib.pyplot.scatter matplotlib.pyplot.bar matplotlib.pyplot.contour matplotlib.pyplot.contourf matplotlib.pyplot.quiver matplotlib.pyplot.pie ( autopct must be a constant boolean or omitted) matplotlib.pyplot.fill matplotlib.pyplot.fill_between matplotlib.pyplot.step matplotlib.pyplot.errorbar matplotlib.pyplot.barbs matplotlib.pyplot.eventplot matplotlib.pyplot.hexbin matplotlib.pyplot.xcorr ( autopct must be a constant boolean or omitted) matplotlib.pyplot.imshow matplotlib.pyplot.plot matplotlib.pyplot.scatter matplotlib.pyplot.bar matplotlib.axes.Axes.contour matplotlib.axes.Axes.contourf matplotlib.axes.Axes.quiver matplotlib.axes.Axes.pie ( usevlines must be a constant boolean or omitted) matplotlib.axes.Axes.fill matplotlib.axes.Axes.fill_between matplotlib.axes.Axes.step matplotlib.axes.Axes.errorbar matplotlib.axes.Axes.barbs matplotlib.axes.Axes.eventplot matplotlib.axes.Axes.hexbin matplotlib.axes.Axes.xcorr ( usevlines must be a constant boolean or omitted) matplotlib.axes.Axes.imshow These APIs have the following restrictions: The data being plotted must be Numpy arrays and not Pandas data structures. Use of lists is not currently supported. If you need to plot multiple arrays use a tuple or a 2D Numpy array. These functions work by automatically gathering all of the data onto one machine and then plotting the data. If there is not enough memory on your machine, a sample of the data can be selected. The example code below demonstrates calling plot with a sample of the data: import matplotlib.pyplot as plt % matplotlib inline @bodo . jit def dist_plot ( n ): X = np . arange ( n ) Y = np . exp ( - X / 3.0 ) plt . plot ( X [:: 10 ], Y [:: 10 ]) # gather every 10th element plt . show () dist_plot ( 100 ) [output:0] Formatting APIs \u00b6 In addition to plotting, we also support a variety of formatting APIs to modify your figures. matplotlib.pyplot.gca matplotlib.pyplot.gcf matplotlib.pyplot.text matplotlib.pyplot.subplots ( nrows and ncols must be constant integers) matplotlib.pyplot.suptitle matplotlib.pyplot.tight_layout matplotlib.pyplot.savefig matplotlib.pyplot.draw matplotlib.pyplot.show (Output is only displayed on rank 0) matplotlib.figure.Figure.suptitle matplotlib.figure.Figure.tight_layout matplotlib.figure.Figure.subplots ( nrows and ncols must be constant integers) matplotlib.figure.Figure.show (Output is only displayed on rank 0) matplotlib.axes.Axes.annotate matplotlib.axes.Axes.text matplotlib.axes.Axes.set_xlabel matplotlib.axes.Axes.set_ylabel matplotlib.axes.Axes.set_xscale matplotlib.axes.Axes.set_yscale matplotlib.axes.Axes.set_xticklabels matplotlib.axes.Axes.set_yticklabels matplotlib.axes.Axes.set_xlim matplotlib.axes.Axes.set_ylim matplotlib.axes.Axes.set_xticks matplotlib.axes.Axes.set_yticks matplotlib.axes.Axes.set_axis_on matplotlib.axes.Axes.set_axis_off matplotlib.axes.Axes.draw matplotlib.axes.Axes.set_title matplotlib.axes.Axes.legend matplotlib.axes.Axes.grid In general these APIs support all arguments except for the restrictions specified. In addition, APIs have the following restrictions: Use of lists is not currently supported. If you need to provide a list, please use a tuple instead. Formatting functions execute on all ranks by default. If you need to execute further Matplotlib code on all of your processes, please close any figures you opened inside Bodo. Matplotlib Classes \u00b6 Bodo supports the following Matplotlib classes when used with the previously mentioned APIs: matplotlib.figure.Figure matplotlib.axes.Axes matplotlib.text.Text matplotlib.text.Annotation matplotlib.lines.Line2D matplotlib.collections.PathCollection matplotlib.container.BarContainer matplotlib.contour.QuadContourSet matplotlib.quiver.Quiver matplotlib.patches.Wedge matplotlib.patches.Polygon matplotlib.collections.PolyCollection matplotlib.image.AxesImage matplotlib.container.ErrorbarContainer matplotlib.quiver.Barbs matplotlib.collections.EventCollection matplotlib.collections.LineCollection Working with Unsupported APIs \u00b6 For other visualization functions, you can call them from regular Python and manually gather the data. If the data does not fit in a single machine's memory, you may need to take a sample. The example code below demonstrates gathering a portion of data in Bodo and calling polar (which Bodo doesn't support yet) in regular Python: import bodo import numpy as np import matplotlib.pyplot as plt @bodo . jit () def dist_gather_test ( n ): X = np . arange ( n ) Y = 3 - np . cos ( X ) return bodo . gatherv ( X [:: 10 ]), bodo . gatherv ( Y [:: 10 ]) # gather every 10th element X_Sample , Y_Sample = dist_gather_test ( 1000 ) if bodo . get_rank () == 0 : plt . polar ( X_Sample , Y_Sample ) plt . show ()","title":"Data Visualization"},{"location":"integrating_bodo/data_visualization/#data_visualization","text":"Bodo supports Matplotlib visualization natively inside JIT functions. This section specifies the supported Matplotlib APIs and classes. In general, these APIs support all arguments except for the restrictions specified in each section.","title":"Data Visualization"},{"location":"integrating_bodo/data_visualization/#plotting-apis","text":"Currently, Bodo automatically supports the following plotting APIs. matplotlib.pyplot.plot matplotlib.pyplot.scatter matplotlib.pyplot.bar matplotlib.pyplot.contour matplotlib.pyplot.contourf matplotlib.pyplot.quiver matplotlib.pyplot.pie ( autopct must be a constant boolean or omitted) matplotlib.pyplot.fill matplotlib.pyplot.fill_between matplotlib.pyplot.step matplotlib.pyplot.errorbar matplotlib.pyplot.barbs matplotlib.pyplot.eventplot matplotlib.pyplot.hexbin matplotlib.pyplot.xcorr ( autopct must be a constant boolean or omitted) matplotlib.pyplot.imshow matplotlib.pyplot.plot matplotlib.pyplot.scatter matplotlib.pyplot.bar matplotlib.axes.Axes.contour matplotlib.axes.Axes.contourf matplotlib.axes.Axes.quiver matplotlib.axes.Axes.pie ( usevlines must be a constant boolean or omitted) matplotlib.axes.Axes.fill matplotlib.axes.Axes.fill_between matplotlib.axes.Axes.step matplotlib.axes.Axes.errorbar matplotlib.axes.Axes.barbs matplotlib.axes.Axes.eventplot matplotlib.axes.Axes.hexbin matplotlib.axes.Axes.xcorr ( usevlines must be a constant boolean or omitted) matplotlib.axes.Axes.imshow These APIs have the following restrictions: The data being plotted must be Numpy arrays and not Pandas data structures. Use of lists is not currently supported. If you need to plot multiple arrays use a tuple or a 2D Numpy array. These functions work by automatically gathering all of the data onto one machine and then plotting the data. If there is not enough memory on your machine, a sample of the data can be selected. The example code below demonstrates calling plot with a sample of the data: import matplotlib.pyplot as plt % matplotlib inline @bodo . jit def dist_plot ( n ): X = np . arange ( n ) Y = np . exp ( - X / 3.0 ) plt . plot ( X [:: 10 ], Y [:: 10 ]) # gather every 10th element plt . show () dist_plot ( 100 ) [output:0]","title":"Plotting APIs"},{"location":"integrating_bodo/data_visualization/#formatting-apis","text":"In addition to plotting, we also support a variety of formatting APIs to modify your figures. matplotlib.pyplot.gca matplotlib.pyplot.gcf matplotlib.pyplot.text matplotlib.pyplot.subplots ( nrows and ncols must be constant integers) matplotlib.pyplot.suptitle matplotlib.pyplot.tight_layout matplotlib.pyplot.savefig matplotlib.pyplot.draw matplotlib.pyplot.show (Output is only displayed on rank 0) matplotlib.figure.Figure.suptitle matplotlib.figure.Figure.tight_layout matplotlib.figure.Figure.subplots ( nrows and ncols must be constant integers) matplotlib.figure.Figure.show (Output is only displayed on rank 0) matplotlib.axes.Axes.annotate matplotlib.axes.Axes.text matplotlib.axes.Axes.set_xlabel matplotlib.axes.Axes.set_ylabel matplotlib.axes.Axes.set_xscale matplotlib.axes.Axes.set_yscale matplotlib.axes.Axes.set_xticklabels matplotlib.axes.Axes.set_yticklabels matplotlib.axes.Axes.set_xlim matplotlib.axes.Axes.set_ylim matplotlib.axes.Axes.set_xticks matplotlib.axes.Axes.set_yticks matplotlib.axes.Axes.set_axis_on matplotlib.axes.Axes.set_axis_off matplotlib.axes.Axes.draw matplotlib.axes.Axes.set_title matplotlib.axes.Axes.legend matplotlib.axes.Axes.grid In general these APIs support all arguments except for the restrictions specified. In addition, APIs have the following restrictions: Use of lists is not currently supported. If you need to provide a list, please use a tuple instead. Formatting functions execute on all ranks by default. If you need to execute further Matplotlib code on all of your processes, please close any figures you opened inside Bodo.","title":"Formatting APIs"},{"location":"integrating_bodo/data_visualization/#matplotlib_classes","text":"Bodo supports the following Matplotlib classes when used with the previously mentioned APIs: matplotlib.figure.Figure matplotlib.axes.Axes matplotlib.text.Text matplotlib.text.Annotation matplotlib.lines.Line2D matplotlib.collections.PathCollection matplotlib.container.BarContainer matplotlib.contour.QuadContourSet matplotlib.quiver.Quiver matplotlib.patches.Wedge matplotlib.patches.Polygon matplotlib.collections.PolyCollection matplotlib.image.AxesImage matplotlib.container.ErrorbarContainer matplotlib.quiver.Barbs matplotlib.collections.EventCollection matplotlib.collections.LineCollection","title":"Matplotlib Classes"},{"location":"integrating_bodo/data_visualization/#working-with-unsupported-apis","text":"For other visualization functions, you can call them from regular Python and manually gather the data. If the data does not fit in a single machine's memory, you may need to take a sample. The example code below demonstrates gathering a portion of data in Bodo and calling polar (which Bodo doesn't support yet) in regular Python: import bodo import numpy as np import matplotlib.pyplot as plt @bodo . jit () def dist_gather_test ( n ): X = np . arange ( n ) Y = 3 - np . cos ( X ) return bodo . gatherv ( X [:: 10 ]), bodo . gatherv ( Y [:: 10 ]) # gather every 10th element X_Sample , Y_Sample = dist_gather_test ( 1000 ) if bodo . get_rank () == 0 : plt . polar ( X_Sample , Y_Sample ) plt . show ()","title":"Working with Unsupported APIs"},{"location":"integrating_bodo/dl/","text":"Deep Learning \u00b6 Bodo works seamlessly with Horovod to support large-scale distributed deep learning with PyTorch and TensorFlow. Prerequisites \u00b6 You will need to install Horovod and a deep learning framework in your Bodo conda environment. Horovod needs to be compiled and linked with the same MPI library that Bodo uses. Installing Horovod and PyTorch \u00b6 Here are simple instructions for installing Horovod and PyTorch in your Bodo environment without CUDA support: # Activate the Bodo conda environment conda install -c pytorch -c conda-forge -c defaults bokeh pytorch=1.5 torchvision=0.6 pip install horovod[pytorch] For information on setting up Horovod in a conda environment with CUDA see here . How it works \u00b6 Bodo works seamlessly with Horovod for distributed deep learning. The main thing to consider if you are using GPUs for deep learning is that a Bodo application typically uses all CPU cores on a cluster (there is one worker or process per core), whereas for deep learning only a subset of Bodo workers are pinned to a GPU (one worker per GPU). This means that data processed and generated by Bodo will need to be distributed to the GPU workers before training. Note Bodo can automatically assign a subset of workers in your cluster to GPU devices, initialize Horovod with these workers, and distribute data for deep learning to these workers. To ensure that Bodo automatically handles all of the above call bodo.dl.start() before starting training and bodo.dl.prepare_data(X) to distribute the data. API \u00b6 bodo.dl.start \u00b6 bodo.dl.start(framework) framework is a string specifying the DL framework to use (\"torch\" or \"tensorflow\"). Note that this must be called before starting deep learning. It initializes Horovod and pins workers to GPUs. bodo.dl.prepare_data \u00b6 bodo.dl.prepare_data(X) Redistributes the given data to DL workers. bodo.dl.end \u00b6 bodo.dl.end() On calling this function, non-DL workers will wait for DL workers. They will become idle to free up computational resources for DL workers. This has to be called by every process. Example \u00b6 The code snippet below shows how deep learning can be integrated in a Bodo application: # Deep learning code in regular Python usign Horovod def deep_learning ( X , y ): # Note: X and y have already been distributed by Bodo and there is no # need to partition data with Horovod if hvd . initialized (): # do deep learning with Horovod and your DL framework of choice ... use_cuda = bodo . dl . is_cuda_available () ... else : # this rank does not participate in DL (not pinned to GPU) pass @bodo . jit def main () ... X = ... # distributed NumPy array generated with Bodo y = ... # distributed NumPy array generated with Bodo bodo . dl . start ( \"torch\" ) # Initialize Horovod with PyTorch X = bodo . dl . prepare_data ( X ) y = bodo . dl . prepare_data ( y ) with bodo . objmode : deep_learning () # DL user code bodo . dl . end () As we can see, the deep learning code is not compiled by Bodo. It runs in Python (in objmode or outside Bodo jitted functions) and must use Horovod. Note that data coming from Bodo has already been partitioned and distributed by Bodo (in bodo.dl.prepare_data ), and that you don't have to initialize Horovod. A full distributed training example with the MNIST dataset can be found here .","title":"Deep Learning"},{"location":"integrating_bodo/dl/#dl","text":"Bodo works seamlessly with Horovod to support large-scale distributed deep learning with PyTorch and TensorFlow.","title":"Deep Learning"},{"location":"integrating_bodo/dl/#prerequisites","text":"You will need to install Horovod and a deep learning framework in your Bodo conda environment. Horovod needs to be compiled and linked with the same MPI library that Bodo uses.","title":"Prerequisites"},{"location":"integrating_bodo/dl/#installing-horovod-and-pytorch","text":"Here are simple instructions for installing Horovod and PyTorch in your Bodo environment without CUDA support: # Activate the Bodo conda environment conda install -c pytorch -c conda-forge -c defaults bokeh pytorch=1.5 torchvision=0.6 pip install horovod[pytorch] For information on setting up Horovod in a conda environment with CUDA see here .","title":"Installing Horovod and PyTorch"},{"location":"integrating_bodo/dl/#how-it-works","text":"Bodo works seamlessly with Horovod for distributed deep learning. The main thing to consider if you are using GPUs for deep learning is that a Bodo application typically uses all CPU cores on a cluster (there is one worker or process per core), whereas for deep learning only a subset of Bodo workers are pinned to a GPU (one worker per GPU). This means that data processed and generated by Bodo will need to be distributed to the GPU workers before training. Note Bodo can automatically assign a subset of workers in your cluster to GPU devices, initialize Horovod with these workers, and distribute data for deep learning to these workers. To ensure that Bodo automatically handles all of the above call bodo.dl.start() before starting training and bodo.dl.prepare_data(X) to distribute the data.","title":"How it works"},{"location":"integrating_bodo/dl/#api","text":"","title":"API"},{"location":"integrating_bodo/dl/#bododlstart","text":"bodo.dl.start(framework) framework is a string specifying the DL framework to use (\"torch\" or \"tensorflow\"). Note that this must be called before starting deep learning. It initializes Horovod and pins workers to GPUs.","title":"bodo.dl.start"},{"location":"integrating_bodo/dl/#bododlprepare_data","text":"bodo.dl.prepare_data(X) Redistributes the given data to DL workers.","title":"bodo.dl.prepare_data"},{"location":"integrating_bodo/dl/#bododlend","text":"bodo.dl.end() On calling this function, non-DL workers will wait for DL workers. They will become idle to free up computational resources for DL workers. This has to be called by every process.","title":"bodo.dl.end"},{"location":"integrating_bodo/dl/#example","text":"The code snippet below shows how deep learning can be integrated in a Bodo application: # Deep learning code in regular Python usign Horovod def deep_learning ( X , y ): # Note: X and y have already been distributed by Bodo and there is no # need to partition data with Horovod if hvd . initialized (): # do deep learning with Horovod and your DL framework of choice ... use_cuda = bodo . dl . is_cuda_available () ... else : # this rank does not participate in DL (not pinned to GPU) pass @bodo . jit def main () ... X = ... # distributed NumPy array generated with Bodo y = ... # distributed NumPy array generated with Bodo bodo . dl . start ( \"torch\" ) # Initialize Horovod with PyTorch X = bodo . dl . prepare_data ( X ) y = bodo . dl . prepare_data ( y ) with bodo . objmode : deep_learning () # DL user code bodo . dl . end () As we can see, the deep learning code is not compiled by Bodo. It runs in Python (in objmode or outside Bodo jitted functions) and must use Horovod. Note that data coming from Bodo has already been partitioned and distributed by Bodo (in bodo.dl.prepare_data ), and that you don't have to initialize Horovod. A full distributed training example with the MNIST dataset can be found here .","title":"Example"},{"location":"integrating_bodo/front_end/","text":"Integrating Bodo with Front-End Tools \u00b6 Bodo can be integrated with front-end tools to build real-time analytics dashboards. This section provides a walk-through of creating a Streamlit app with Bodo on your laptop or VM. All the code used in this section is available here , and the steps to running the app are provided below . The Taxi Pickup App \u00b6 This app is based on a demo from the official Streamlit documentation, which explores a public Uber dataset for pickups and drop-offs in New York City. We will essentially read a parquet file into a dataframe, convert the string date/time column to datetime data, and return the dataframe to be plotted in the app: def load_data_pandas ( pq_file_path , date_col = 'date/time' ): data = pd . read_parquet ( pq_file_path ) data [ date_col ] = pd . to_datetime ( data [ date_col ]) return data Bodo version of the Taxi Pickup App \u00b6 To run the app using Bodo, we will use the same process as running the app on an IPyParallel cluster . For this app, we want to visualize all the data, so in the Bodo version of this function, we disable automatic data distribution using the returns_maybe_distributed flag, and use bodo.gatherv to gather all the data onto a single process: @bodo . jit ( returns_maybe_distributed = False , cache = True ) def load_data_bodo ( pq_file_path , date_col = 'date/time' ): data = pd . read_parquet ( pq_file_path ) data [ date_col ] = pd . to_datetime ( data [ date_col ]) return bodo . gatherv ( data ) We define a Python wrapper for load_data_bodo called build_main : def build_main ( pq_file_path , date_col = 'date/time' ): op_df = load_data_bodo ( pq_file_path , date_col = 'Date/Time' ) return op_df Finally, we need a function to send the imports and code definitions to the mpi engines, call the load_data_bodo function, and then return the result to the client: def initialize_bodo ( pq_file_path , date_col = 'date/time' ): t0 = time . time () client = ipp . Client ( profile = 'mpi' ) dview = client [:] # import libraries dview . execute ( \"import numpy as np\" ) dview . execute ( \"import pandas as pd\" ) dview . execute ( \"import bodo\" ) dview . execute ( \"import time\" ) dview . execute ( \"import os\" ) dview . execute ( \"import datetime as dt\" ) dview . execute ( \"import sys\" ) bodo_funcs = [ load_data_bodo ] for f in bodo_funcs : # get source code of Bodo function f_src = inspect . getsource ( f ) # execute the source code thereby defining the function on engines dview . execute ( f_src ) . get () op_df = dview . apply ( build_main , pq_file_path , 'Date/Time' ) . get () t1 = time . time () print ( \"Total Exec + Compilation time:\" , t1 - t0 ) client . close () return op_df [ 0 ] Building the Streamlit Visualization \u00b6 We create the Streamlit App by adding the title, creating some headers and printing out some basic information about our app: st . title ( 'Scale up your datasets and make Pandas fly with Bodo!' ) st . subheader ( 'Based on Streamlit example for Uber pickups in NYC' ) st . subheader ( ' - > Basic Info' ) st . subheader ( 'Number of physical cores/ranks available on system: %s ' % psutil . cpu_count ( logical = False )) We first run the Pandas app and see how long it takes: t0 = time . time () pdf = load_data_pandas ( pq_file_path , date_col = 'Date/Time' ) t1 = time . time () st . subheader ( 'Pandas df' ) st . subheader ( 'Time taken for one op with Pandas:' ) st . subheader ( t1 - t0 ) st . write ( pdf . head ( 2 )) # print two rows to check output. We do the same with Bodo: t2 = time . time () bdf = initialize_bodo ( pq_file_path , date_col = 'Date/Time' ) t3 = time . time () st . subheader ( 'Bodo df' ) st . subheader ( 'Total Compilation and Execution time taken for one op with Bodo:' ) st . subheader ( t3 - t2 ) st . write ( bdf . head ( 2 )) We can also visualize the data in a histogram showing the pickups by hour: DATE_COLUMN = 'date/time' lowercase = lambda x : str ( x ) . lower () bdf . rename ( lowercase , axis = 'columns' , inplace = True ) st . subheader ( 'Number of pickups by hour' ) hist_values = np . histogram ( bdf [ DATE_COLUMN ] . dt . hour , bins = 24 , range = ( 0 , 24 ))[ 0 ] st . bar_chart ( hist_values ) Running the Taxi Pickup App \u00b6 Clone the Bodo Examples repository and navigate to the streamlit directory. The directory has the following structure: streamlit \u251c\u2500\u2500 README.md \u251c\u2500\u2500 app.py \u251c\u2500\u2500 config.py \u251c\u2500\u2500 environment.yml \u251c\u2500\u2500 pd_vs_Bodo.png \u251c\u2500\u2500 sample_parquet_file.pq We have provided an environment.yml file to create a conda environment with all the required dependencies. The app code is stored in app.py , and some configuration parameters such as the input file, and path to current directory are set in config.py . We have provided a sample parquet file sample_parquet_file.pq to test the app with. Note Please ensure that the path to current directory is set in the config.py file. Start the IPyParallel controller and engines \u00b6 Create a conda environment from the provided environment.yml file, and activate the conda environment: conda env create -f environment.yml conda activate stlbodo Append the current directory to your Python Path: export PYTHONPATH = \" ${ PYTHONPATH } :<path_to_directory>\" Now you can start ipcontroller: ipcontroller --profile mpi --ip '*' Open a new terminal and activate the stlbodo conda environment. You will need to append the current directory to your Python Path again. Use the following command to start a set of MPI engines: mpiexec -n 4 python -m ipyparallel.engine --mpi --profile-dir ~/.ipython/profile_mpi --cluster-id '' --log-level = DEBUG Run the Streamlit App \u00b6 Open another terminal and activate the stlbodo conda environment. Navigate to the streamlit directory, and then run: streamlit run app.py You should now be able to open up the app in a browser window and see the output for yourself. Note that it will take roughly around one and a half minute for the Pandas output to show up, and including compilation time, and following that, less than a minute for for the Bodo output and visualization to show up. If you face any issues while running the app, please let us know through our Feedback repository, or join our community slack to communicate directly with Bodo engineers.","title":"Front End Tools"},{"location":"integrating_bodo/front_end/#front_end","text":"Bodo can be integrated with front-end tools to build real-time analytics dashboards. This section provides a walk-through of creating a Streamlit app with Bodo on your laptop or VM. All the code used in this section is available here , and the steps to running the app are provided below .","title":"Integrating Bodo with Front-End Tools"},{"location":"integrating_bodo/front_end/#the-taxi-pickup-app","text":"This app is based on a demo from the official Streamlit documentation, which explores a public Uber dataset for pickups and drop-offs in New York City. We will essentially read a parquet file into a dataframe, convert the string date/time column to datetime data, and return the dataframe to be plotted in the app: def load_data_pandas ( pq_file_path , date_col = 'date/time' ): data = pd . read_parquet ( pq_file_path ) data [ date_col ] = pd . to_datetime ( data [ date_col ]) return data","title":"The Taxi Pickup App"},{"location":"integrating_bodo/front_end/#bodo-version-of-the-taxi-pickup-app","text":"To run the app using Bodo, we will use the same process as running the app on an IPyParallel cluster . For this app, we want to visualize all the data, so in the Bodo version of this function, we disable automatic data distribution using the returns_maybe_distributed flag, and use bodo.gatherv to gather all the data onto a single process: @bodo . jit ( returns_maybe_distributed = False , cache = True ) def load_data_bodo ( pq_file_path , date_col = 'date/time' ): data = pd . read_parquet ( pq_file_path ) data [ date_col ] = pd . to_datetime ( data [ date_col ]) return bodo . gatherv ( data ) We define a Python wrapper for load_data_bodo called build_main : def build_main ( pq_file_path , date_col = 'date/time' ): op_df = load_data_bodo ( pq_file_path , date_col = 'Date/Time' ) return op_df Finally, we need a function to send the imports and code definitions to the mpi engines, call the load_data_bodo function, and then return the result to the client: def initialize_bodo ( pq_file_path , date_col = 'date/time' ): t0 = time . time () client = ipp . Client ( profile = 'mpi' ) dview = client [:] # import libraries dview . execute ( \"import numpy as np\" ) dview . execute ( \"import pandas as pd\" ) dview . execute ( \"import bodo\" ) dview . execute ( \"import time\" ) dview . execute ( \"import os\" ) dview . execute ( \"import datetime as dt\" ) dview . execute ( \"import sys\" ) bodo_funcs = [ load_data_bodo ] for f in bodo_funcs : # get source code of Bodo function f_src = inspect . getsource ( f ) # execute the source code thereby defining the function on engines dview . execute ( f_src ) . get () op_df = dview . apply ( build_main , pq_file_path , 'Date/Time' ) . get () t1 = time . time () print ( \"Total Exec + Compilation time:\" , t1 - t0 ) client . close () return op_df [ 0 ]","title":"Bodo version of the Taxi Pickup App"},{"location":"integrating_bodo/front_end/#building-the-streamlit-visualization","text":"We create the Streamlit App by adding the title, creating some headers and printing out some basic information about our app: st . title ( 'Scale up your datasets and make Pandas fly with Bodo!' ) st . subheader ( 'Based on Streamlit example for Uber pickups in NYC' ) st . subheader ( ' - > Basic Info' ) st . subheader ( 'Number of physical cores/ranks available on system: %s ' % psutil . cpu_count ( logical = False )) We first run the Pandas app and see how long it takes: t0 = time . time () pdf = load_data_pandas ( pq_file_path , date_col = 'Date/Time' ) t1 = time . time () st . subheader ( 'Pandas df' ) st . subheader ( 'Time taken for one op with Pandas:' ) st . subheader ( t1 - t0 ) st . write ( pdf . head ( 2 )) # print two rows to check output. We do the same with Bodo: t2 = time . time () bdf = initialize_bodo ( pq_file_path , date_col = 'Date/Time' ) t3 = time . time () st . subheader ( 'Bodo df' ) st . subheader ( 'Total Compilation and Execution time taken for one op with Bodo:' ) st . subheader ( t3 - t2 ) st . write ( bdf . head ( 2 )) We can also visualize the data in a histogram showing the pickups by hour: DATE_COLUMN = 'date/time' lowercase = lambda x : str ( x ) . lower () bdf . rename ( lowercase , axis = 'columns' , inplace = True ) st . subheader ( 'Number of pickups by hour' ) hist_values = np . histogram ( bdf [ DATE_COLUMN ] . dt . hour , bins = 24 , range = ( 0 , 24 ))[ 0 ] st . bar_chart ( hist_values )","title":"Building the Streamlit Visualization"},{"location":"integrating_bodo/front_end/#runtaxipickup","text":"Clone the Bodo Examples repository and navigate to the streamlit directory. The directory has the following structure: streamlit \u251c\u2500\u2500 README.md \u251c\u2500\u2500 app.py \u251c\u2500\u2500 config.py \u251c\u2500\u2500 environment.yml \u251c\u2500\u2500 pd_vs_Bodo.png \u251c\u2500\u2500 sample_parquet_file.pq We have provided an environment.yml file to create a conda environment with all the required dependencies. The app code is stored in app.py , and some configuration parameters such as the input file, and path to current directory are set in config.py . We have provided a sample parquet file sample_parquet_file.pq to test the app with. Note Please ensure that the path to current directory is set in the config.py file.","title":"Running the Taxi Pickup App"},{"location":"integrating_bodo/front_end/#start-the-ipyparallel-controller-and-engines","text":"Create a conda environment from the provided environment.yml file, and activate the conda environment: conda env create -f environment.yml conda activate stlbodo Append the current directory to your Python Path: export PYTHONPATH = \" ${ PYTHONPATH } :<path_to_directory>\" Now you can start ipcontroller: ipcontroller --profile mpi --ip '*' Open a new terminal and activate the stlbodo conda environment. You will need to append the current directory to your Python Path again. Use the following command to start a set of MPI engines: mpiexec -n 4 python -m ipyparallel.engine --mpi --profile-dir ~/.ipython/profile_mpi --cluster-id '' --log-level = DEBUG","title":"Start the IPyParallel controller and engines"},{"location":"integrating_bodo/front_end/#run-the-streamlit-app","text":"Open another terminal and activate the stlbodo conda environment. Navigate to the streamlit directory, and then run: streamlit run app.py You should now be able to open up the app in a browser window and see the output for yourself. Note that it will take roughly around one and a half minute for the Pandas output to show up, and including compilation time, and following that, less than a minute for for the Bodo output and visualization to show up. If you face any issues while running the app, please let us know through our Feedback repository, or join our community slack to communicate directly with Bodo engineers.","title":"Run the Streamlit App"},{"location":"integrating_bodo/spark/","text":"Spark Examples \u00b6 Bodo offers simplicity and maintainability of Python codes while unlocking orders of magnitude performance improvement. Spark APIs are usually equivalent to simpler Python/Pandas APIs, which are automatically parallelized by Bodo. This section aims to assist spark users with their transition to Bodo. Here, we show the most common data wrangling methods in PySpark and Pandas through brief code examples. We used the COVID-19 World Vaccination Progress dataset that can be downloaded from Kaggle . If you want to execute the code as shown below, make sure that you have Bodo installed. Here is a list of examples. Environment Setup \u00b6 With Bodo: import bodo import pandas as pd import numpy as np With PySpark: from pyspark.sql import SparkSession spark = SparkSession \\ . builder \\ . appName ( \"Migration From Spark\" ) \\ . getOrCreate () Load Data \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' ]) def load_data (): df = pd . read_csv ( 'country_vaccinations_by_manufacturer.csv' ) return df df = load_data () With PySpark: data = spark . read . csv ( 'country_vaccinations_by_manufacturer.csv' , header = True ) Display the Schema of the DataFrame \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' ]) def schema ( df ): print ( df . dtypes ) schema ( df ) With PySpark: print ( data . printSchema ()) Change Data Types of the DataFrame \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' ]) def load_data (): df = pd . read_csv ( 'country_vaccinations_by_manufacturer.csv' , dtype = { 'location' : 'str' , 'vaccine' : 'str' , 'total_vaccinations' : 'Int64' }, parse_dates = [ 'date' ]) print ( df . info ()) return df df = load_data () With PySpark: from pyspark.sql.types import StructField , IntegerType , StringType , DateType , StructType new_schema = [ StructField ( 'location' , StringType (), True ), StructField ( 'date' , DateType (), True ), StructField ( 'vaccine' , StringType (), True ), StructField ( 'total_vaccinations' , IntegerType (), True )] data = spark . read . csv ( 'country_vaccinations_by_manufacturer.csv' , header = True , schema = StructType ( fields = new_schema )) data . printSchema () Display the Head of the DataFrame \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' ]) def head_data ( df ): print ( df . head ()) head_data ( df ) With PySpark: data . show ( 5 ) data . take ( 5 ) Select Columns from the DataFrame \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_columns' ]) def load_data ( df ): df_columns = df [[ 'location' , 'vaccine' ]] return df_columns df_columns = load_data ( df ) With PySpark : data_columns = data . select ( 'location' , 'vaccine' ) . show () Show the Statistics of the DataFrame \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' ]) def get_describe ( df ): print ( df . describe ()) get_describe ( df ) With Pyspark: data . describe () . show () Drop Duplicate Values \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_cleaned' ]) def drop ( df ): df_cleaned = df . drop_duplicates () return df_cleaned df_cleaned = drop ( df ) With Pyspark: data . dropDuplicates () . show () Missing Values \u00b6 Count NA \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' ]) def count_na ( df ): print ( df . isnull () . sum ()) count_na ( df ) With Pyspark: from pyspark.sql.functions import isnan , when , count , col data . select ([ count ( when ( isnan ( c ) | col ( c ) . isNull (), c )) . alias ( c ) for c in df_s . columns ]) . show () Drop NA \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_valid' ]) def drop_na ( df ): df_valid = df . dropna ( how = 'any' ) return df_valid df_valid = drop_na ( df ) With Pyspark: data_valid = data . dropna ( how = 'any' ) Replace NA \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_filled' ]) def replace_na ( df ): df_filled = df . fillna ( 0 ) return df_filled df_filled = replace_na ( df ) With Pyspark: data_replaced = data . na . fill ( value = 0 ) DateTime Manipulation \u00b6 Convert String to Datetime : With Bodo: @bodo . jit ( distributed = [ 'df' ]) def convert_date ( df ): df [ 'record_date' ] = pd . to_datetime ( df [ 'date' ]) return df df = convert_date ( df ) With Pyspark: from pyspark.sql.types import DateType data = data . withColumn ( \"record_date\" , data [ \"date\" ] . cast ( DateType ())) Extract Day / Month / Year from Datetime : With Bodo: @bodo . jit ( distributed = [ 'df' ]) def extract_date ( df ): print ( df [ 'record_date' ] . dt . year ) extract_date ( df ) With Pyspark: from pyspark.sql.functions import year data . select ( year ( df_s . record_date )) . show () Filter Data Based on Conditions \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_filtered' ]) def sort_data ( df ): df_filtered = df [ df . vaccine == 'Pfizer/BioNTech' ] return df_filtered df_filtered = sort_data ( df ) With Pyspark: data_filtered = data . where ( data . vaccine == 'Pfizer/BioNTech' ) Aggregation Functions: (sum, count, mean, max, min, etc) \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' ]) def group_by ( df ): print ( df . groupby ( 'location' ) . agg ({ 'total_vaccinations' : 'sum' })) group_by ( df ) With Pyspark: data . groupBy ( 'location' ) . agg ({ 'total_vaccinations' : 'sum' }) . show () Sort Data \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_sorted' ]) def sort_data ( df ): df_sorted = df . sort_values ( by = [ 'total_vaccinations' ], ascending = False ) return df_sorted df_sorted = sort_data ( df ) With Pyspark: from pyspark.sql.types import IntegerType from pyspark.sql.functions import col from pyspark.sql.functions import desc data_sorted = data . withColumn ( \"total_vaccinations\" , col ( \"total_vaccinations\" ) . cast ( IntegerType ())) . select ( \"total_vaccinations\" ) . sort ( desc ( 'total_vaccinations' )) . show () Rename Columns \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_renamed' ]) def rename_column ( df ): df_renamed = df . rename ( columns = { 'location' : 'country' }, inplace = True ) return data_renamed df_renamed = rename_column ( df ) With Pyspark: data_renamed = data . withColumnRenamed ( \"location\" , \"country\" ) . show () Create New Columns \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' ]) def create_column ( df ): df [ 'doubled' ] = 2 * df [ 'total_vaccinations' ] return df df = create_column ( df ) With Pyspark: from pyspark.sql.functions import col data = data . withColumn ( \"doubled\" , 2 * col ( \"total_vaccinations\" )) . show () User-Defined Functions \u00b6 With Bodo: @bodo . jit ( distributed = [ 'df' ]) def udf ( df ): df [ 'new_column' ] = df [ 'location' ] . apply ( lambda x : x . upper ()) return df df = udf ( df ) With Pyspark: from pyspark.sql.functions import udf from pyspark.sql.types import StringType pyspark_udf = udf ( lambda x : x . upper (), StringType ()) data = data . withColumn ( \"new_column\" , pyspark_udf ( df_s . location )) . show () Create a DataFrame \u00b6 With Bodo: @bodo . jit () def create (): df = pd . DataFrame ({ 'id' : [ 1 , 2 ], 'label' : [ \"one\" , \"two\" ]}) return df df = create () With Pyspark: data = spark . createDataFrame ([( 1 , \"one\" ),( 2 , \"two\" ),],[ \"id\" , \"label\" ]) Export the Data \u00b6 With Bodo: @bodo . jit () def export_data (): df = pd . DataFrame ({ 'id' : [ 1 , 2 ], 'label' : [ \"one\" , \"two\" ]}) df_pandas = df . to_csv ( 'pandas_data.csv' ) return df_pandas export_data () With Pyspark: df = spark . createDataFrame ([( 1 , \"one\" ),( 2 , \"two\" ),],[ \"id\" , \"label\" ]) df_spark . write . csv ( \"df_spark.csv\" , header = True )","title":"Spark Examples"},{"location":"integrating_bodo/spark/#sparkexamples","text":"Bodo offers simplicity and maintainability of Python codes while unlocking orders of magnitude performance improvement. Spark APIs are usually equivalent to simpler Python/Pandas APIs, which are automatically parallelized by Bodo. This section aims to assist spark users with their transition to Bodo. Here, we show the most common data wrangling methods in PySpark and Pandas through brief code examples. We used the COVID-19 World Vaccination Progress dataset that can be downloaded from Kaggle . If you want to execute the code as shown below, make sure that you have Bodo installed. Here is a list of examples.","title":"Spark Examples"},{"location":"integrating_bodo/spark/#Environment","text":"With Bodo: import bodo import pandas as pd import numpy as np With PySpark: from pyspark.sql import SparkSession spark = SparkSession \\ . builder \\ . appName ( \"Migration From Spark\" ) \\ . getOrCreate ()","title":"Environment Setup"},{"location":"integrating_bodo/spark/#Load","text":"With Bodo: @bodo . jit ( distributed = [ 'df' ]) def load_data (): df = pd . read_csv ( 'country_vaccinations_by_manufacturer.csv' ) return df df = load_data () With PySpark: data = spark . read . csv ( 'country_vaccinations_by_manufacturer.csv' , header = True )","title":"Load Data"},{"location":"integrating_bodo/spark/#Display","text":"With Bodo: @bodo . jit ( distributed = [ 'df' ]) def schema ( df ): print ( df . dtypes ) schema ( df ) With PySpark: print ( data . printSchema ())","title":"Display the Schema of the DataFrame"},{"location":"integrating_bodo/spark/#Change","text":"With Bodo: @bodo . jit ( distributed = [ 'df' ]) def load_data (): df = pd . read_csv ( 'country_vaccinations_by_manufacturer.csv' , dtype = { 'location' : 'str' , 'vaccine' : 'str' , 'total_vaccinations' : 'Int64' }, parse_dates = [ 'date' ]) print ( df . info ()) return df df = load_data () With PySpark: from pyspark.sql.types import StructField , IntegerType , StringType , DateType , StructType new_schema = [ StructField ( 'location' , StringType (), True ), StructField ( 'date' , DateType (), True ), StructField ( 'vaccine' , StringType (), True ), StructField ( 'total_vaccinations' , IntegerType (), True )] data = spark . read . csv ( 'country_vaccinations_by_manufacturer.csv' , header = True , schema = StructType ( fields = new_schema )) data . printSchema ()","title":"Change Data Types of the DataFrame"},{"location":"integrating_bodo/spark/#Display","text":"With Bodo: @bodo . jit ( distributed = [ 'df' ]) def head_data ( df ): print ( df . head ()) head_data ( df ) With PySpark: data . show ( 5 ) data . take ( 5 )","title":"Display the Schema of the DataFrame"},{"location":"integrating_bodo/spark/#Select","text":"With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_columns' ]) def load_data ( df ): df_columns = df [[ 'location' , 'vaccine' ]] return df_columns df_columns = load_data ( df ) With PySpark : data_columns = data . select ( 'location' , 'vaccine' ) . show ()","title":"Select Columns from the DataFrame"},{"location":"integrating_bodo/spark/#Show","text":"With Bodo: @bodo . jit ( distributed = [ 'df' ]) def get_describe ( df ): print ( df . describe ()) get_describe ( df ) With Pyspark: data . describe () . show ()","title":"Show the Statistics of the DataFrame"},{"location":"integrating_bodo/spark/#Drop","text":"With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_cleaned' ]) def drop ( df ): df_cleaned = df . drop_duplicates () return df_cleaned df_cleaned = drop ( df ) With Pyspark: data . dropDuplicates () . show ()","title":"Drop Duplicate Values"},{"location":"integrating_bodo/spark/#Missing","text":"","title":"Missing Values"},{"location":"integrating_bodo/spark/#count-na","text":"With Bodo: @bodo . jit ( distributed = [ 'df' ]) def count_na ( df ): print ( df . isnull () . sum ()) count_na ( df ) With Pyspark: from pyspark.sql.functions import isnan , when , count , col data . select ([ count ( when ( isnan ( c ) | col ( c ) . isNull (), c )) . alias ( c ) for c in df_s . columns ]) . show ()","title":"Count NA"},{"location":"integrating_bodo/spark/#drop-na","text":"With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_valid' ]) def drop_na ( df ): df_valid = df . dropna ( how = 'any' ) return df_valid df_valid = drop_na ( df ) With Pyspark: data_valid = data . dropna ( how = 'any' )","title":"Drop NA"},{"location":"integrating_bodo/spark/#replace-na","text":"With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_filled' ]) def replace_na ( df ): df_filled = df . fillna ( 0 ) return df_filled df_filled = replace_na ( df ) With Pyspark: data_replaced = data . na . fill ( value = 0 )","title":"Replace NA"},{"location":"integrating_bodo/spark/#DateTime","text":"Convert String to Datetime : With Bodo: @bodo . jit ( distributed = [ 'df' ]) def convert_date ( df ): df [ 'record_date' ] = pd . to_datetime ( df [ 'date' ]) return df df = convert_date ( df ) With Pyspark: from pyspark.sql.types import DateType data = data . withColumn ( \"record_date\" , data [ \"date\" ] . cast ( DateType ())) Extract Day / Month / Year from Datetime : With Bodo: @bodo . jit ( distributed = [ 'df' ]) def extract_date ( df ): print ( df [ 'record_date' ] . dt . year ) extract_date ( df ) With Pyspark: from pyspark.sql.functions import year data . select ( year ( df_s . record_date )) . show ()","title":"DateTime Manipulation"},{"location":"integrating_bodo/spark/#Filter","text":"With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_filtered' ]) def sort_data ( df ): df_filtered = df [ df . vaccine == 'Pfizer/BioNTech' ] return df_filtered df_filtered = sort_data ( df ) With Pyspark: data_filtered = data . where ( data . vaccine == 'Pfizer/BioNTech' )","title":"Filter Data Based on Conditions"},{"location":"integrating_bodo/spark/#Aggregation","text":"With Bodo: @bodo . jit ( distributed = [ 'df' ]) def group_by ( df ): print ( df . groupby ( 'location' ) . agg ({ 'total_vaccinations' : 'sum' })) group_by ( df ) With Pyspark: data . groupBy ( 'location' ) . agg ({ 'total_vaccinations' : 'sum' }) . show ()","title":"Aggregation Functions: (sum, count, mean, max, min, etc)"},{"location":"integrating_bodo/spark/#Sort","text":"With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_sorted' ]) def sort_data ( df ): df_sorted = df . sort_values ( by = [ 'total_vaccinations' ], ascending = False ) return df_sorted df_sorted = sort_data ( df ) With Pyspark: from pyspark.sql.types import IntegerType from pyspark.sql.functions import col from pyspark.sql.functions import desc data_sorted = data . withColumn ( \"total_vaccinations\" , col ( \"total_vaccinations\" ) . cast ( IntegerType ())) . select ( \"total_vaccinations\" ) . sort ( desc ( 'total_vaccinations' )) . show ()","title":"Sort Data"},{"location":"integrating_bodo/spark/#Rename","text":"With Bodo: @bodo . jit ( distributed = [ 'df' , 'df_renamed' ]) def rename_column ( df ): df_renamed = df . rename ( columns = { 'location' : 'country' }, inplace = True ) return data_renamed df_renamed = rename_column ( df ) With Pyspark: data_renamed = data . withColumnRenamed ( \"location\" , \"country\" ) . show ()","title":"Rename Columns"},{"location":"integrating_bodo/spark/#Create","text":"With Bodo: @bodo . jit ( distributed = [ 'df' ]) def create_column ( df ): df [ 'doubled' ] = 2 * df [ 'total_vaccinations' ] return df df = create_column ( df ) With Pyspark: from pyspark.sql.functions import col data = data . withColumn ( \"doubled\" , 2 * col ( \"total_vaccinations\" )) . show ()","title":"Create New Columns"},{"location":"integrating_bodo/spark/#User-Defined","text":"With Bodo: @bodo . jit ( distributed = [ 'df' ]) def udf ( df ): df [ 'new_column' ] = df [ 'location' ] . apply ( lambda x : x . upper ()) return df df = udf ( df ) With Pyspark: from pyspark.sql.functions import udf from pyspark.sql.types import StringType pyspark_udf = udf ( lambda x : x . upper (), StringType ()) data = data . withColumn ( \"new_column\" , pyspark_udf ( df_s . location )) . show ()","title":"User-Defined Functions"},{"location":"integrating_bodo/spark/#Create","text":"With Bodo: @bodo . jit () def create (): df = pd . DataFrame ({ 'id' : [ 1 , 2 ], 'label' : [ \"one\" , \"two\" ]}) return df df = create () With Pyspark: data = spark . createDataFrame ([( 1 , \"one\" ),( 2 , \"two\" ),],[ \"id\" , \"label\" ])","title":"Create New Columns"},{"location":"integrating_bodo/spark/#Export","text":"With Bodo: @bodo . jit () def export_data (): df = pd . DataFrame ({ 'id' : [ 1 , 2 ], 'label' : [ \"one\" , \"two\" ]}) df_pandas = df . to_csv ( 'pandas_data.csv' ) return df_pandas export_data () With Pyspark: df = spark . createDataFrame ([( 1 , \"one\" ),( 2 , \"two\" ),],[ \"id\" , \"label\" ]) df_spark . write . csv ( \"df_spark.csv\" , header = True )","title":"Export the Data"},{"location":"integrating_bodo/sparkcheatsheet/","text":"PySpark Bodo Cheatsheet \u00b6 References of PySpark methods and their Python equivalents supported by Bodo. pyspark.sql.SparkSession \u00b6 The table below is a reference of SparkSession methods and their equivalents in Python, which are supported by Bodo. PySpark Method Python Equivalent pyspark.sql.SparkSession.read.csv - pd.read_csv() pyspark.sql.SparkSession.read.text pd.read_csv(\"file.txt\", sep=\"\\n\", names=[\"value\"], dtype={\"value\": \"str\"}) pyspark.sql.SparkSession.read.parquet pd.read_parquet() pyspark.sql.SparkSession.read.json pd.read_json() pyspark.sql.DataFrame \u00b6 The table below is a reference of Spark DataFrame methods and their equivalents in Python, which are supported by Bodo. PySpark Method Python Equivalent pyspark.sql.DataFrame.alias alias = df pyspark.sql.DataFrame.approxQuantile df[['A', 'B', 'C']].quantile(q) pyspark.sql.DataFrame.columns df.columns pyspark.sql.DataFrame.corr df[['A', 'B']].corr() pyspark.sql.DataFrame.count df.count() pyspark.sql.DataFrame.cov df[['A', 'B']].cov() pyspark.sql.DataFrame.crossJoin df1.assign(key=1).merge(df2.assign(key=1), on=\"key\").drop(\"key\", axis=1) pyspark.sql.DataFrame.describe df.describe() pyspark.sql.DataFrame.distinct df.distinct() pyspark.sql.DataFrame.drop df.drop(col, axis=1) pyspark.sql.DataFrame.dropDuplicates df.drop_duplicates() pyspark.sql.DataFrame.drop_duplicates df.drop_duplicates() pyspark.sql.DataFrame.dropna df.dropna() pyspark.sql.DataFrame.fillna df.fillna(value) pyspark.sql.DataFrame.filter df[cond] pyspark.sql.DataFrame.first df.head(1) pyspark.sql.DataFrame.foreach df.apply(f, axis=1) pyspark.sql.DataFrame.groupBy df.groupby(\"col\") pyspark.sql.DataFrame.groupby df.groupby(\"col\") pyspark.sql.DataFrame.head df.head(n) pyspark.sql.DataFrame.intersect pd.merge(df1[['col1', 'col2']].drop_duplicates(), df2[['col1', 'col2']].drop_duplicates(), on =['col1', 'col2']) pyspark.sql.DataFrame.intersectAll pd.merge(df1[['col1', 'col2']], df2[['col1', 'col2']].drop_duplicates(), on =['col1', 'col2']) pyspark.sql.DataFrame.join df1.join(df2) pyspark.sql.DataFrame.orderBy df.sort_values('colname') pyspark.sql.DataFrame.show print(df.head(n)) pyspark.sql.DataFrame.sort df.sort_values('colname') pyspark.sql.functions \u00b6 The table below is a reference of Spark SQL functions and their equivalents in Python, which are supported by Bodo. PySpark Function Python Equivalent pyspark.sql.functions.abs df.col.abs() pyspark.sql.functions.acos np.arccos(df.col) pyspark.sql.functions.acosh np.arccosh(df.col) pyspark.sql.functions.add_months df.col + pd.DateOffset(months=num_months) pyspark.sql.functions.approx_count_distinct df.col.nunique() pyspark.sql.functions.array_contains df.col.apply(lambda a, value: value in a, value=value) pyspark.sql.functions.array_distinct df.col.map(lambda x: np.unique(x)) pyspark.sql.functions.array_except df[['col1', 'col2']].apply(lambda x: np.setdiff1d(x[0], x[1]), axis=1) pyspark.sql.functions.array_join df.col.apply(lambda x, sep: sep.join(x), sep=sep) pyspark.sql.functions.array_max df.col.map(lambda x: np.nanmax(x)) pyspark.sql.functions.array_min df.col.map(lambda x: np.nanmin(x)) pyspark.sql.functions.array_position df.col.apply(lambda x, value: np.append(np.where(x == value)[0], -1)[0], value=value) pyspark.sql.functions.array_repeat df.col.apply(lambda x, count: np.repeat(x, count), count=count) pyspark.sql.functions.array_sort df.col.map(lambda x: np.sort(x)) pyspark.sql.functions.array_union df[['col1', 'col2']].apply(lambda x: np.union1d(x[0], x[1]), axis=1) pyspark.sql.functions.array_overlap df[['A', 'B']].apply(lambda x: len(np.intersect1d(x[0], x[1])) > 0, axis=1) pyspark.sql.functions.asc df.sort_values('col') pyspark.sql.functions.asc_nulls_first df.sort_values('col', na_position='first') pyspark.sql.functions.asc_nulls_last df.sort_values('col') pyspark.sql.functions.ascii df.col.map(lambda x: ord(x[0])) pyspark.sql.functions.asin np.arcsin(df.col) pyspark.sql.functions.asinh np.arcsinh(df.col) pyspark.sql.functions.atan np.arctan(df.col) pyspark.sql.functions.atanh np.arctanh(df.col) pyspark.sql.functions.atan2 df[['col1', 'col2']].apply(lambda x: np.arctan2(x[0], x[1]), axis=1) pyspark.sql.functions.avg df.col.mean() pyspark.sql.functions.bin df.col.map(lambda x: \"{0:b}\".format(x)) pyspark.sql.functions.bitwiseNOT np.invert(df.col) pyspark.sql.functions.bround df.col.apply(lambda x, scale: np.round(x, scale), scale=scale) pyspark.sql.functions.cbrt df.col.map(lambda x: np.cbrt(x)) pyspark.sql.functions.ceil np.ceil(df.col) pyspark.sql.functions.col df.col pyspark.sql.functions.collect_list df.col.to_numpy() pyspark.sql.functions.collect_set np.unique(df.col.to_numpy()) pyspark.sql.functions.column df.col pyspark.sql.functions.corr df[['col1', 'col2']].corr(method = 'pearson') pyspark.sql.functions.cos np.cos(df.col) pyspark.sql.functions.cosh np.cosh(df.col) pyspark.sql.functions.count df.col.count() pyspark.sql.functions.countDistinct df.col.drop_duplicates().count() pyspark.sql.functions.current_date datetime.datetime.now().date() pyspark.sql.functions.current_timestamp datetime.datetime.now() pyspark.sql.functions.date_add df.col + pd.tseries.offsets.DateOffset(num_days) pyspark.sql.functions.date_format df.col.dt.strftime(format_str) pyspark.sql.functions.date_sub df.col - pd.tseries.offsets.DateOffset(num_days) pyspark.sql.functions.datediff (df.col1 - df.col2).dt.days pyspark.sql.functions.dayofmonth df.col.dt.day pyspark.sql.functions.dayofweek df.col.dt.dayofweek pyspark.sql.functions.dayofyear df.col.dt.dayofyear pyspark.sql.functions.degrees np.degrees(df.col) pyspark.sql.functions.desc df.sort_values('col', ascending=False) pyspark.sql.functions.desc_nulls_first df.sort_values('col', ascending=False, na_position='first') pyspark.sql.functions.desc_nulls_last df.sort_values('col', ascending=False) pyspark.sql.functions.exp np.exp(df.col) pyspark.sql.functions.expm1 np.exp(df.col) - 1 pyspark.sql.functions.factorial df.col.map(lambda x: math.factorial(x)) pyspark.sql.functions.filter df.filter() pyspark.sql.functions.floor np.floor(df.col) pyspark.sql.functions.format_number df.col.apply(lambda x,d : (\"{:,.\" + str(d) + \"f}\").format(np.round(x, d)), d=d) pyspark.sql.functions.format_string df.col.apply(lambda x, format_str : format_str.format(x), format_str=format_str) pyspark.sql.functions.from_unixtime df.col.map(lambda x: pd.Timestamp(x, 's')).dt.strftime(format_str) pyspark.sql.functions.greatest df[['col1', 'col2']].apply(lambda x: np.nanmax(x), axis=1) pyspark.sql.functions.hash df.col.map(lambda x: hash(x)) pyspark.sql.functions.hour df.col.dt.hour pyspark.sql.functions.hypot df[['col1', 'col2']].apply(lambda x: np.hypot(x[0], x[1]), axis=1) pyspark.sql.functions.initcap df.col.str.title() pyspark.sql.functions.instr df.col.str.find(sub=substr) pyspark.sql.functions.isnan np.isnan(df.col) pyspark.sql.functions.isnull df.col.isna() pyspark.sql.functions.kurtosis df.col.kurtosis() pyspark.sql.functions.last_day df.col + pd.tseries.offsets.MonthEnd() pyspark.sql.functions.least df.min(axis=1) pyspark.sql.functions.locate df.col.str.find(sub=substr, start=start) pyspark.sql.functions.log np.log(df.col) / np.log(base) pyspark.sql.functions.log10 np.log10(df.col) pyspark.sql.functions.log1p np.log(df.col) + 1 pyspark.sql.functions.log2 np.log2(df.col) pyspark.sql.functions.lower df.col.str.lower() pyspark.sql.functions.lpad df.col.str.pad(len, flllchar=char) pyspark.sql.functions.ltrim df.col.str.lstrip() pyspark.sql.functions.max df.col.max() pyspark.sql.functions.mean df.col.mean() pyspark.sql.functions.min df.col.min() pyspark.sql.functions.minute df.col.dt.minute pyspark.sql.functions.monotonically_increasing_id pd.Series(np.arange(len(df))) pyspark.sql.functions.month df.col.dt.month pyspark.sql.functions.nanvl df[['A', 'B']].apply(lambda x: x[0] if not pd.isna(x[0]) else x[1], axis=1) pyspark.sql.functions.overlay df.A.str.slice_replace(start=index, stop=index+len, repl=repl_str) pyspark.sql.functions.pandas_udf df.apply(f) or df.col.map(f) pyspark.sql.functions.pow np.power(df.col1, df.col2) pyspark.sql.functions.quarter df.col.dt.quarter pyspark.sql.functions.radians np.radians(df.col) pyspark.sql.functions.rand pd.Series(np.random.rand(1, num_cols)) pyspark.sql.functions.randn pd.Series(np.random.randn(num_cols)) pyspark.sql.functions.regexp_replace df.col.str.replace(pattern, repl_string) pyspark.sql.functions.repeat df.col.str.repeat(count) pyspark.sql.functions.reverse df.col.map(lambda x: x[::-1]) pyspark.sql.functions.rint df.col.map(lambda x: int(np.round(x, 0))) pyspark.sql.functions.round df.col.apply(lambda x, decimal_places: np.round(x, decimal_places), decimal_places=decimal_places) pyspark.sql.functions.rpad df.col.str.pad(len, side='right', flllchar=char) pyspark.sql.functions.rtrim df.col.str.rstrip() pyspark.sql.functions.second df.col.dt.second pyspark.sql.functions.sequence df[['col1', 'col2', 'col3']].apply(lambda x: np.arange(x[0], x[1], x[2]), axis=1) pyspark.sql.functions.shuffle df.col.map(lambda x: np.random.permutation(x)) pyspark.sql.functions.signum np.sign(df.col) pyspark.sql.functions.sin np.sin(df.col) pyspark.sql.functions.sinh np.sinh(df.col) pyspark.sql.functions.size df.col.map(lambda x: len(x)) pyspark.sql.functions.skewness df.col.skew() pyspark.sql.functions.slice df.col.map(lambda x: x[start : end]) pyspark.sql.functions.split df.col.str.split(pat, num_splits) pyspark.sql.functions.sqrt np.sqrt(df.col) pyspark.sql.functions.stddev df.col.std() pyspark.sql.functions.stddev_pop df.col.std(ddof=0) pyspark.sql.functions.stddev_samp df.col.std() pyspark.sql.functions.substring df.col.str.slice(start, start+len) pyspark.sql.functions.substring_index df.col.apply(lambda x, sep, count: sep.join(x.split(sep)[:count]), sep=sep, count=count) pyspark.sql.functions.sum df.col.sum() pyspark.sql.functions.sumDistinct df.col.drop_duplicates().sum() pyspark.sql.functions.tan np.tan(df.col) pyspark.sql.functions.tanh np.tanh(df.col) pyspark.sql.functions.timestamp_seconds pd.to_datetime(\"now\") pyspark.sql.functions.to_date df.col.apply(lambda x, format_str: pd.to_datetime(x, format=format_str).date(), format_str=format_str) pyspark.sql.functions.to_timestamp df.A.apply(lambda x, format_str: pd.to_datetime(x, format=format_str), format_str=format_str) pyspark.sql.functions.translate df.col.str.split(\"\").apply(lambda x: \"\".join(pd.Series(x).replace(to_replace, values).tolist()), to_replace=to_replace, values=values) pyspark.sql.functions.trim df.col.str.strip() pyspark.sql.functions.udf df.apply or df.col.map pyspark.sql.functions.unix_timestamp df.col.apply(lambda x, format_str: (pd.to_datetime(x, format=format_str) - pd.Timestamp(\"1970-01-01\")).total_seconds(), format_str=format_str) pyspark.sql.functions.upper df.col.str.upper() pyspark.sql.functions.var_pop df.col.var(ddof=0) pyspark.sql.functions.var_samp df.col.var() pyspark.sql.functions.variance df.col.var() pyspark.sql.functions.weekofyear df.col.dt.isocalendar().week pyspark.sql.functions.when df.A.apply(lambda a, cond, val, other: val if cond(a) else other, cond=cond, val=val, other=other) pyspark.sql.functions.year df.col.dt.year Special Cases \u00b6 pyspark.sql.functions.concat \u00b6 pyspark.sql.functions.concat for Arrays : df[['col1', 'col2', 'col3']].apply(lambda x: np.hstack(x), axis=1) for Strings : df[['col1', 'col2', 'col3']].apply(lambda x: \"\".join(x), axis=1) pyspark.sql.functions.conv \u00b6 pyspark.sql.functions.conv pandas equivalent: base_map = { 2 : \" {0:b} \" , 8 : \" {0:o} \" , 10 : \" {0:d} \" , 16 : \" {0:x} \" } new_format = base_map [ new_base ] df . col . apply ( lambda x , old_base , new_format : new_format . format ( int ( x , old_base )), old_base = old_base , new_format = new_format ) pyspark.sql.functions.date_trunc \u00b6 pyspark.sql.functions.date_trunc For frequencies day and below: df.col.dt.floor(freq=trunc_val) For month: df.col.map(lambda x: pd.Timestamp(year=x.year, month=x.month, day=1)) For year: df.col.map(lambda x: pd.Timestamp(year=x.year, month=1, day=1)) pyspark.sql.functions.regexp_extract \u00b6 pyspark.sql.functions.regexp_extract Here's a small pandas function equivalent: def f ( x , pat ): res = re . search ( pat , x ) return \"\" if res is None else res [ 0 ] df . col . apply ( f , pat = pat ) pyspark.sql.functions.shiftLeft \u00b6 pyspark.sql.functions.shiftLeft If the type is uint64 np.left_shift(df.col.astype(np.int64), numbits).astype(np.uint64)) Other integer types: np.left_shift(df.col, numbits) pyspark.sql.functions.shiftRight \u00b6 pyspark.sql.functions.shiftRight If the type is uint64 use shiftRightUnsigned Other integer types: np.right_shift(df.col, numbits) pyspark.sql.functions.shiftRightUnsigned \u00b6 pyspark.sql.functions.shiftRightUnsigned Here's a small pandas function equivalent: def shiftRightUnsigned ( col , num_bits ): bits_minus_1 = max (( num_bits - 1 ), 0 ) mask_bits = ( np . int64 ( 1 ) << bits_minus_1 ) - 1 mask = ~ ( mask_bits << ( 63 - bits_minus_1 )) return np . right_shift ( col . astype ( np . int64 ), num_bits ) & mask ) . astype ( np . uint64 ) shiftRightUnsigned ( df . col , numbits ) pyspark.sql.functions.sort_array \u00b6 pyspark.sql.functions.sort_array Ascending: df.col.map(lambda x: np.sort(x)) Descending: df.col.map(lambda x: np.sort(x)[::-1]) pyspark.sql.functions.trunc \u00b6 pyspark.sql.functions.trunc def f ( date , trunc_str ): if trunc_str == 'year' : return pd . Timestamp ( year = date . year , month = 1 , day = 1 ) if trunc_str == 'month' : return pd . Timestamp ( year = date . year , month = date . month , day = 1 ) df . A . apply ( f , trunc_str = trunc_str )","title":"Bodo Spark Cheatsheet"},{"location":"integrating_bodo/sparkcheatsheet/#pscheatsheet","text":"References of PySpark methods and their Python equivalents supported by Bodo.","title":"PySpark Bodo Cheatsheet"},{"location":"integrating_bodo/sparkcheatsheet/#pssession","text":"The table below is a reference of SparkSession methods and their equivalents in Python, which are supported by Bodo. PySpark Method Python Equivalent pyspark.sql.SparkSession.read.csv - pd.read_csv() pyspark.sql.SparkSession.read.text pd.read_csv(\"file.txt\", sep=\"\\n\", names=[\"value\"], dtype={\"value\": \"str\"}) pyspark.sql.SparkSession.read.parquet pd.read_parquet() pyspark.sql.SparkSession.read.json pd.read_json()","title":"pyspark.sql.SparkSession"},{"location":"integrating_bodo/sparkcheatsheet/#psdataframe","text":"The table below is a reference of Spark DataFrame methods and their equivalents in Python, which are supported by Bodo. PySpark Method Python Equivalent pyspark.sql.DataFrame.alias alias = df pyspark.sql.DataFrame.approxQuantile df[['A', 'B', 'C']].quantile(q) pyspark.sql.DataFrame.columns df.columns pyspark.sql.DataFrame.corr df[['A', 'B']].corr() pyspark.sql.DataFrame.count df.count() pyspark.sql.DataFrame.cov df[['A', 'B']].cov() pyspark.sql.DataFrame.crossJoin df1.assign(key=1).merge(df2.assign(key=1), on=\"key\").drop(\"key\", axis=1) pyspark.sql.DataFrame.describe df.describe() pyspark.sql.DataFrame.distinct df.distinct() pyspark.sql.DataFrame.drop df.drop(col, axis=1) pyspark.sql.DataFrame.dropDuplicates df.drop_duplicates() pyspark.sql.DataFrame.drop_duplicates df.drop_duplicates() pyspark.sql.DataFrame.dropna df.dropna() pyspark.sql.DataFrame.fillna df.fillna(value) pyspark.sql.DataFrame.filter df[cond] pyspark.sql.DataFrame.first df.head(1) pyspark.sql.DataFrame.foreach df.apply(f, axis=1) pyspark.sql.DataFrame.groupBy df.groupby(\"col\") pyspark.sql.DataFrame.groupby df.groupby(\"col\") pyspark.sql.DataFrame.head df.head(n) pyspark.sql.DataFrame.intersect pd.merge(df1[['col1', 'col2']].drop_duplicates(), df2[['col1', 'col2']].drop_duplicates(), on =['col1', 'col2']) pyspark.sql.DataFrame.intersectAll pd.merge(df1[['col1', 'col2']], df2[['col1', 'col2']].drop_duplicates(), on =['col1', 'col2']) pyspark.sql.DataFrame.join df1.join(df2) pyspark.sql.DataFrame.orderBy df.sort_values('colname') pyspark.sql.DataFrame.show print(df.head(n)) pyspark.sql.DataFrame.sort df.sort_values('colname')","title":"pyspark.sql.DataFrame"},{"location":"integrating_bodo/sparkcheatsheet/#psfunctions","text":"The table below is a reference of Spark SQL functions and their equivalents in Python, which are supported by Bodo. PySpark Function Python Equivalent pyspark.sql.functions.abs df.col.abs() pyspark.sql.functions.acos np.arccos(df.col) pyspark.sql.functions.acosh np.arccosh(df.col) pyspark.sql.functions.add_months df.col + pd.DateOffset(months=num_months) pyspark.sql.functions.approx_count_distinct df.col.nunique() pyspark.sql.functions.array_contains df.col.apply(lambda a, value: value in a, value=value) pyspark.sql.functions.array_distinct df.col.map(lambda x: np.unique(x)) pyspark.sql.functions.array_except df[['col1', 'col2']].apply(lambda x: np.setdiff1d(x[0], x[1]), axis=1) pyspark.sql.functions.array_join df.col.apply(lambda x, sep: sep.join(x), sep=sep) pyspark.sql.functions.array_max df.col.map(lambda x: np.nanmax(x)) pyspark.sql.functions.array_min df.col.map(lambda x: np.nanmin(x)) pyspark.sql.functions.array_position df.col.apply(lambda x, value: np.append(np.where(x == value)[0], -1)[0], value=value) pyspark.sql.functions.array_repeat df.col.apply(lambda x, count: np.repeat(x, count), count=count) pyspark.sql.functions.array_sort df.col.map(lambda x: np.sort(x)) pyspark.sql.functions.array_union df[['col1', 'col2']].apply(lambda x: np.union1d(x[0], x[1]), axis=1) pyspark.sql.functions.array_overlap df[['A', 'B']].apply(lambda x: len(np.intersect1d(x[0], x[1])) > 0, axis=1) pyspark.sql.functions.asc df.sort_values('col') pyspark.sql.functions.asc_nulls_first df.sort_values('col', na_position='first') pyspark.sql.functions.asc_nulls_last df.sort_values('col') pyspark.sql.functions.ascii df.col.map(lambda x: ord(x[0])) pyspark.sql.functions.asin np.arcsin(df.col) pyspark.sql.functions.asinh np.arcsinh(df.col) pyspark.sql.functions.atan np.arctan(df.col) pyspark.sql.functions.atanh np.arctanh(df.col) pyspark.sql.functions.atan2 df[['col1', 'col2']].apply(lambda x: np.arctan2(x[0], x[1]), axis=1) pyspark.sql.functions.avg df.col.mean() pyspark.sql.functions.bin df.col.map(lambda x: \"{0:b}\".format(x)) pyspark.sql.functions.bitwiseNOT np.invert(df.col) pyspark.sql.functions.bround df.col.apply(lambda x, scale: np.round(x, scale), scale=scale) pyspark.sql.functions.cbrt df.col.map(lambda x: np.cbrt(x)) pyspark.sql.functions.ceil np.ceil(df.col) pyspark.sql.functions.col df.col pyspark.sql.functions.collect_list df.col.to_numpy() pyspark.sql.functions.collect_set np.unique(df.col.to_numpy()) pyspark.sql.functions.column df.col pyspark.sql.functions.corr df[['col1', 'col2']].corr(method = 'pearson') pyspark.sql.functions.cos np.cos(df.col) pyspark.sql.functions.cosh np.cosh(df.col) pyspark.sql.functions.count df.col.count() pyspark.sql.functions.countDistinct df.col.drop_duplicates().count() pyspark.sql.functions.current_date datetime.datetime.now().date() pyspark.sql.functions.current_timestamp datetime.datetime.now() pyspark.sql.functions.date_add df.col + pd.tseries.offsets.DateOffset(num_days) pyspark.sql.functions.date_format df.col.dt.strftime(format_str) pyspark.sql.functions.date_sub df.col - pd.tseries.offsets.DateOffset(num_days) pyspark.sql.functions.datediff (df.col1 - df.col2).dt.days pyspark.sql.functions.dayofmonth df.col.dt.day pyspark.sql.functions.dayofweek df.col.dt.dayofweek pyspark.sql.functions.dayofyear df.col.dt.dayofyear pyspark.sql.functions.degrees np.degrees(df.col) pyspark.sql.functions.desc df.sort_values('col', ascending=False) pyspark.sql.functions.desc_nulls_first df.sort_values('col', ascending=False, na_position='first') pyspark.sql.functions.desc_nulls_last df.sort_values('col', ascending=False) pyspark.sql.functions.exp np.exp(df.col) pyspark.sql.functions.expm1 np.exp(df.col) - 1 pyspark.sql.functions.factorial df.col.map(lambda x: math.factorial(x)) pyspark.sql.functions.filter df.filter() pyspark.sql.functions.floor np.floor(df.col) pyspark.sql.functions.format_number df.col.apply(lambda x,d : (\"{:,.\" + str(d) + \"f}\").format(np.round(x, d)), d=d) pyspark.sql.functions.format_string df.col.apply(lambda x, format_str : format_str.format(x), format_str=format_str) pyspark.sql.functions.from_unixtime df.col.map(lambda x: pd.Timestamp(x, 's')).dt.strftime(format_str) pyspark.sql.functions.greatest df[['col1', 'col2']].apply(lambda x: np.nanmax(x), axis=1) pyspark.sql.functions.hash df.col.map(lambda x: hash(x)) pyspark.sql.functions.hour df.col.dt.hour pyspark.sql.functions.hypot df[['col1', 'col2']].apply(lambda x: np.hypot(x[0], x[1]), axis=1) pyspark.sql.functions.initcap df.col.str.title() pyspark.sql.functions.instr df.col.str.find(sub=substr) pyspark.sql.functions.isnan np.isnan(df.col) pyspark.sql.functions.isnull df.col.isna() pyspark.sql.functions.kurtosis df.col.kurtosis() pyspark.sql.functions.last_day df.col + pd.tseries.offsets.MonthEnd() pyspark.sql.functions.least df.min(axis=1) pyspark.sql.functions.locate df.col.str.find(sub=substr, start=start) pyspark.sql.functions.log np.log(df.col) / np.log(base) pyspark.sql.functions.log10 np.log10(df.col) pyspark.sql.functions.log1p np.log(df.col) + 1 pyspark.sql.functions.log2 np.log2(df.col) pyspark.sql.functions.lower df.col.str.lower() pyspark.sql.functions.lpad df.col.str.pad(len, flllchar=char) pyspark.sql.functions.ltrim df.col.str.lstrip() pyspark.sql.functions.max df.col.max() pyspark.sql.functions.mean df.col.mean() pyspark.sql.functions.min df.col.min() pyspark.sql.functions.minute df.col.dt.minute pyspark.sql.functions.monotonically_increasing_id pd.Series(np.arange(len(df))) pyspark.sql.functions.month df.col.dt.month pyspark.sql.functions.nanvl df[['A', 'B']].apply(lambda x: x[0] if not pd.isna(x[0]) else x[1], axis=1) pyspark.sql.functions.overlay df.A.str.slice_replace(start=index, stop=index+len, repl=repl_str) pyspark.sql.functions.pandas_udf df.apply(f) or df.col.map(f) pyspark.sql.functions.pow np.power(df.col1, df.col2) pyspark.sql.functions.quarter df.col.dt.quarter pyspark.sql.functions.radians np.radians(df.col) pyspark.sql.functions.rand pd.Series(np.random.rand(1, num_cols)) pyspark.sql.functions.randn pd.Series(np.random.randn(num_cols)) pyspark.sql.functions.regexp_replace df.col.str.replace(pattern, repl_string) pyspark.sql.functions.repeat df.col.str.repeat(count) pyspark.sql.functions.reverse df.col.map(lambda x: x[::-1]) pyspark.sql.functions.rint df.col.map(lambda x: int(np.round(x, 0))) pyspark.sql.functions.round df.col.apply(lambda x, decimal_places: np.round(x, decimal_places), decimal_places=decimal_places) pyspark.sql.functions.rpad df.col.str.pad(len, side='right', flllchar=char) pyspark.sql.functions.rtrim df.col.str.rstrip() pyspark.sql.functions.second df.col.dt.second pyspark.sql.functions.sequence df[['col1', 'col2', 'col3']].apply(lambda x: np.arange(x[0], x[1], x[2]), axis=1) pyspark.sql.functions.shuffle df.col.map(lambda x: np.random.permutation(x)) pyspark.sql.functions.signum np.sign(df.col) pyspark.sql.functions.sin np.sin(df.col) pyspark.sql.functions.sinh np.sinh(df.col) pyspark.sql.functions.size df.col.map(lambda x: len(x)) pyspark.sql.functions.skewness df.col.skew() pyspark.sql.functions.slice df.col.map(lambda x: x[start : end]) pyspark.sql.functions.split df.col.str.split(pat, num_splits) pyspark.sql.functions.sqrt np.sqrt(df.col) pyspark.sql.functions.stddev df.col.std() pyspark.sql.functions.stddev_pop df.col.std(ddof=0) pyspark.sql.functions.stddev_samp df.col.std() pyspark.sql.functions.substring df.col.str.slice(start, start+len) pyspark.sql.functions.substring_index df.col.apply(lambda x, sep, count: sep.join(x.split(sep)[:count]), sep=sep, count=count) pyspark.sql.functions.sum df.col.sum() pyspark.sql.functions.sumDistinct df.col.drop_duplicates().sum() pyspark.sql.functions.tan np.tan(df.col) pyspark.sql.functions.tanh np.tanh(df.col) pyspark.sql.functions.timestamp_seconds pd.to_datetime(\"now\") pyspark.sql.functions.to_date df.col.apply(lambda x, format_str: pd.to_datetime(x, format=format_str).date(), format_str=format_str) pyspark.sql.functions.to_timestamp df.A.apply(lambda x, format_str: pd.to_datetime(x, format=format_str), format_str=format_str) pyspark.sql.functions.translate df.col.str.split(\"\").apply(lambda x: \"\".join(pd.Series(x).replace(to_replace, values).tolist()), to_replace=to_replace, values=values) pyspark.sql.functions.trim df.col.str.strip() pyspark.sql.functions.udf df.apply or df.col.map pyspark.sql.functions.unix_timestamp df.col.apply(lambda x, format_str: (pd.to_datetime(x, format=format_str) - pd.Timestamp(\"1970-01-01\")).total_seconds(), format_str=format_str) pyspark.sql.functions.upper df.col.str.upper() pyspark.sql.functions.var_pop df.col.var(ddof=0) pyspark.sql.functions.var_samp df.col.var() pyspark.sql.functions.variance df.col.var() pyspark.sql.functions.weekofyear df.col.dt.isocalendar().week pyspark.sql.functions.when df.A.apply(lambda a, cond, val, other: val if cond(a) else other, cond=cond, val=val, other=other) pyspark.sql.functions.year df.col.dt.year","title":"pyspark.sql.functions"},{"location":"integrating_bodo/sparkcheatsheet/#special-cases","text":"","title":"Special Cases"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsconcat","text":"pyspark.sql.functions.concat for Arrays : df[['col1', 'col2', 'col3']].apply(lambda x: np.hstack(x), axis=1) for Strings : df[['col1', 'col2', 'col3']].apply(lambda x: \"\".join(x), axis=1)","title":"pyspark.sql.functions.concat"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsconv","text":"pyspark.sql.functions.conv pandas equivalent: base_map = { 2 : \" {0:b} \" , 8 : \" {0:o} \" , 10 : \" {0:d} \" , 16 : \" {0:x} \" } new_format = base_map [ new_base ] df . col . apply ( lambda x , old_base , new_format : new_format . format ( int ( x , old_base )), old_base = old_base , new_format = new_format )","title":"pyspark.sql.functions.conv"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsdate_trunc","text":"pyspark.sql.functions.date_trunc For frequencies day and below: df.col.dt.floor(freq=trunc_val) For month: df.col.map(lambda x: pd.Timestamp(year=x.year, month=x.month, day=1)) For year: df.col.map(lambda x: pd.Timestamp(year=x.year, month=1, day=1))","title":"pyspark.sql.functions.date_trunc"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsregexp_extract","text":"pyspark.sql.functions.regexp_extract Here's a small pandas function equivalent: def f ( x , pat ): res = re . search ( pat , x ) return \"\" if res is None else res [ 0 ] df . col . apply ( f , pat = pat )","title":"pyspark.sql.functions.regexp_extract"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsshiftleft","text":"pyspark.sql.functions.shiftLeft If the type is uint64 np.left_shift(df.col.astype(np.int64), numbits).astype(np.uint64)) Other integer types: np.left_shift(df.col, numbits)","title":"pyspark.sql.functions.shiftLeft"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsshiftright","text":"pyspark.sql.functions.shiftRight If the type is uint64 use shiftRightUnsigned Other integer types: np.right_shift(df.col, numbits)","title":"pyspark.sql.functions.shiftRight"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsshiftrightunsigned","text":"pyspark.sql.functions.shiftRightUnsigned Here's a small pandas function equivalent: def shiftRightUnsigned ( col , num_bits ): bits_minus_1 = max (( num_bits - 1 ), 0 ) mask_bits = ( np . int64 ( 1 ) << bits_minus_1 ) - 1 mask = ~ ( mask_bits << ( 63 - bits_minus_1 )) return np . right_shift ( col . astype ( np . int64 ), num_bits ) & mask ) . astype ( np . uint64 ) shiftRightUnsigned ( df . col , numbits )","title":"pyspark.sql.functions.shiftRightUnsigned"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionssort_array","text":"pyspark.sql.functions.sort_array Ascending: df.col.map(lambda x: np.sort(x)) Descending: df.col.map(lambda x: np.sort(x)[::-1])","title":"pyspark.sql.functions.sort_array"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionstrunc","text":"pyspark.sql.functions.trunc def f ( date , trunc_str ): if trunc_str == 'year' : return pd . Timestamp ( year = date . year , month = 1 , day = 1 ) if trunc_str == 'month' : return pd . Timestamp ( year = date . year , month = date . month , day = 1 ) df . A . apply ( f , trunc_str = trunc_str )","title":"pyspark.sql.functions.trunc"},{"location":"performance/caching/","text":"Caching \u00b6 In many situations, Bodo can save the binary resulting from the compilation of a function to disk, to be reused in future runs. This avoids the need to recompile functions the next time that you run your application. Recompiling a function is only necessary when it is called with new input types, and the same applies to caching. In other words, an application can be run multiple times and process different data without having to recompile any code if the data types remain the same (which is the most common situation). Warning Caching works in most (but not all) situations, and is disabled by default. See caching limitations below for more information. Caching Example \u00b6 To cache a function, we only need to add the option cache=True to the JIT decorator: import time import pandas as pd import bodo @bodo . jit ( cache = True ) def mean_power_speed (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) return df [[ \"power\" , \"speed\" ]] . mean () t0 = time . time () result = mean_power_speed () if bodo . get_rank () == 0 : print ( result ) print ( \"Total execution time:\" , round ( time . time () - t0 , 3 ), \"secs\" ) The first time that the above code runs, Bodo compiles the function and caches it to disk. The code times the whole function call, which includes compilation time the first time the function is run: power 102.078421 speed 5.656851 dtype: float64 Total execution time: 4.614 secs In subsequent runs, it will recover the function from cache and as a result, the execution time will be much faster: power 102.078421 speed 5.656851 dtype: float64 Total execution time: 0.518 secs Note data/cycling_dataset.pq is located in the Bodo tutorial repo . Cache Location and Portability \u00b6 In most cases, the cache is saved in the __pycache__ directory inside the directory where the source files are located. The variable NUMBA_DEBUG_CACHE can be set to 1 in order to see where exactly the cache is and whether it is being written to or read from. On Jupyter notebooks, the cache directory is called numba_cache and is located in IPython.paths.get_ipython_cache_dir() . See here for more information on these and other alternate cache locations. For example, when running in a notebook: import os import IPython cache_dir = IPython . paths . get_ipython_cache_dir () + \"/numba_cache\" print ( \"Cache files:\" ) os . listdir ( cache_dir ) Cache files: ['ipython-input-bce41f829e09.mean_power_speed-4444615264.py38.nbi', 'ipython-input-bce41f829e09.mean_power_speed-4444615264.py38.1.nbc'] Cached objects work across systems with the same CPU model and CPU features. Therefore, it is safe to share and reuse the contents in the cache directory on a different machine. See here for more information. Cache Invalidation \u00b6 The cache is invalidated automatically when the corresponding source code is modified. One way to observe this behavior is to modify the above example after it has been cached a first time, by changing the name of the variable df . The next time that we run the code, Bodo will determine that the source code has been modified, invalidate the cache and recompile the function. Warning It is sometimes necessary to clear the cache manually (see caching limitations below). To clear the cache, the cache files can simply be removed. Tips for Reusing the Cache \u00b6 As explained above, caching is invalidated for a function any time any of the source code in the file changes. If we define a function and call it in the same file, and modify the arguments passed to the function, the cache will be invalidated. Caching File IO \u00b6 For example: a typical use case is calling an IO function with a different file name. @bodo . jit ( cache = True ) def io_call ( file_name ): ... io_call ( \"mydata.parquet\" ) The above function would need to be recompiled if the argument to io_call changes from mydata.parquet . By separating into separate files the function call from the function definition, the function definition does not need to be recompiled for each function call with new arguments. The cached IO function will work for a change in file name so long as the file schema is the same. For example, the below code snippet import IO_function from IO_functions IO_function ( file_name ) would not need to recompile IO_function each time file_name is modified since IO_function is isolated from that code change. Caching Notebook Cells \u00b6 For IPython notebooks the function to be cached should be in a separate cell from the function call. @bodo . jit ( cache = True ) def io_call ( file_name ): ... io_call ( file_name ) io_call ( another_file_name ) ... If a cell with a cached function is modified, then its cache is invalidated and the function must be compiled again. Current Caching Limitations \u00b6 Caching does not recognize changes in Bodo versions, and cached files from different versions may not work, thus requiring manual clearing of the cache. Changes in compiled functions are not seen across files. For example, if we have a cached Bodo function that calls a cached Bodo function in a different file, and modify the latter, Bodo will not update its cache (and therefore run with the old version of the function). Global variables are treated as compile-time constants. When a function is compiled, the value of any globals that the function uses are embedded in the binary at compilation time and remain constant. If the value of the global changes in the source code after compilation, the compiled object (and cache) will not rebind to the new value. Troubleshooting \u00b6 During execution, Bodo will print information on caching if the environment variable NUMBA_DEBUG_CACHE is set to 1 . For example, on first run it will show if the cache is being saved to and where, and on subsequent runs it will show if the compiler is successfully loading from cache. If the compiler reports that it is not able to cache a function, or load a function from cache, please report the issue on our feedback respository .","title":"Caching"},{"location":"performance/caching/#caching","text":"In many situations, Bodo can save the binary resulting from the compilation of a function to disk, to be reused in future runs. This avoids the need to recompile functions the next time that you run your application. Recompiling a function is only necessary when it is called with new input types, and the same applies to caching. In other words, an application can be run multiple times and process different data without having to recompile any code if the data types remain the same (which is the most common situation). Warning Caching works in most (but not all) situations, and is disabled by default. See caching limitations below for more information.","title":"Caching"},{"location":"performance/caching/#caching-example","text":"To cache a function, we only need to add the option cache=True to the JIT decorator: import time import pandas as pd import bodo @bodo . jit ( cache = True ) def mean_power_speed (): df = pd . read_parquet ( \"data/cycling_dataset.pq\" ) return df [[ \"power\" , \"speed\" ]] . mean () t0 = time . time () result = mean_power_speed () if bodo . get_rank () == 0 : print ( result ) print ( \"Total execution time:\" , round ( time . time () - t0 , 3 ), \"secs\" ) The first time that the above code runs, Bodo compiles the function and caches it to disk. The code times the whole function call, which includes compilation time the first time the function is run: power 102.078421 speed 5.656851 dtype: float64 Total execution time: 4.614 secs In subsequent runs, it will recover the function from cache and as a result, the execution time will be much faster: power 102.078421 speed 5.656851 dtype: float64 Total execution time: 0.518 secs Note data/cycling_dataset.pq is located in the Bodo tutorial repo .","title":"Caching Example"},{"location":"performance/caching/#cache-location-and-portability","text":"In most cases, the cache is saved in the __pycache__ directory inside the directory where the source files are located. The variable NUMBA_DEBUG_CACHE can be set to 1 in order to see where exactly the cache is and whether it is being written to or read from. On Jupyter notebooks, the cache directory is called numba_cache and is located in IPython.paths.get_ipython_cache_dir() . See here for more information on these and other alternate cache locations. For example, when running in a notebook: import os import IPython cache_dir = IPython . paths . get_ipython_cache_dir () + \"/numba_cache\" print ( \"Cache files:\" ) os . listdir ( cache_dir ) Cache files: ['ipython-input-bce41f829e09.mean_power_speed-4444615264.py38.nbi', 'ipython-input-bce41f829e09.mean_power_speed-4444615264.py38.1.nbc'] Cached objects work across systems with the same CPU model and CPU features. Therefore, it is safe to share and reuse the contents in the cache directory on a different machine. See here for more information.","title":"Cache Location and Portability"},{"location":"performance/caching/#cache-invalidation","text":"The cache is invalidated automatically when the corresponding source code is modified. One way to observe this behavior is to modify the above example after it has been cached a first time, by changing the name of the variable df . The next time that we run the code, Bodo will determine that the source code has been modified, invalidate the cache and recompile the function. Warning It is sometimes necessary to clear the cache manually (see caching limitations below). To clear the cache, the cache files can simply be removed.","title":"Cache Invalidation"},{"location":"performance/caching/#tips-for-reusing-the-cache","text":"As explained above, caching is invalidated for a function any time any of the source code in the file changes. If we define a function and call it in the same file, and modify the arguments passed to the function, the cache will be invalidated.","title":"Tips for Reusing the Cache"},{"location":"performance/caching/#caching-file-io","text":"For example: a typical use case is calling an IO function with a different file name. @bodo . jit ( cache = True ) def io_call ( file_name ): ... io_call ( \"mydata.parquet\" ) The above function would need to be recompiled if the argument to io_call changes from mydata.parquet . By separating into separate files the function call from the function definition, the function definition does not need to be recompiled for each function call with new arguments. The cached IO function will work for a change in file name so long as the file schema is the same. For example, the below code snippet import IO_function from IO_functions IO_function ( file_name ) would not need to recompile IO_function each time file_name is modified since IO_function is isolated from that code change.","title":"Caching File IO"},{"location":"performance/caching/#caching-notebook-cells","text":"For IPython notebooks the function to be cached should be in a separate cell from the function call. @bodo . jit ( cache = True ) def io_call ( file_name ): ... io_call ( file_name ) io_call ( another_file_name ) ... If a cell with a cached function is modified, then its cache is invalidated and the function must be compiled again.","title":"Caching Notebook Cells"},{"location":"performance/caching/#current-caching-limitations","text":"Caching does not recognize changes in Bodo versions, and cached files from different versions may not work, thus requiring manual clearing of the cache. Changes in compiled functions are not seen across files. For example, if we have a cached Bodo function that calls a cached Bodo function in a different file, and modify the latter, Bodo will not update its cache (and therefore run with the old version of the function). Global variables are treated as compile-time constants. When a function is compiled, the value of any globals that the function uses are embedded in the binary at compilation time and remain constant. If the value of the global changes in the source code after compilation, the compiled object (and cache) will not rebind to the new value.","title":"Current Caching Limitations"},{"location":"performance/caching/#troubleshooting","text":"During execution, Bodo will print information on caching if the environment variable NUMBA_DEBUG_CACHE is set to 1 . For example, on first run it will show if the cache is being saved to and where, and on subsequent runs it will show if the compiler is successfully loading from cache. If the compiler reports that it is not able to cache a function, or load a function from cache, please report the issue on our feedback respository .","title":"Troubleshooting"},{"location":"performance/inlining/","text":"Inlining \u00b6 Inlining allows the compiler to perform optimizations across functions, at the cost of increased compilation time. Use inlining when you have your code split into multiple Bodo functions and there are important optimizations that need to performed on some functions, that are dependent on the code of other functions. We will explain this with examples below. Danger Inlining should be used sparingly as it can cause increased compilation time. We strongly recommend against inlining functions with 10 or more lines of code. Bodo's compiler translates high-level code inside bodo.jit decorated functions to highly optimized lower level code. It can perform many optimizations on the generated code based on the structure of the code inside the function being compiled. Let's consider the following example where data.pq is a dataset with 1000 columns: @bodo . jit def example (): df = pd . read_parquet ( \"data.pq\" ) return df . groupby ( \"A\" )[ \"B\" , \"C\" ] . sum () To execute the query inside the example function, Bodo doesn't need to read all the columns from the file. It only needs three columns ( A , B and C ), and can save a lot of time and memory by just reading those. When compiling example , Bodo automatically optimizes the read_parquet call to only read the three required columns. Warning If you have separate Bodo functions and their code needs to be optimized jointly, you need to use inlining. Any code that needs to be optimized jointly needs to be compiled as part of the same JIT compilation. If we have the following: @bodo . jit def read_data ( fname ): return pd . read_parquet ( fname ) @bodo . jit def query (): df = read_data ( \"data.pq\" ) return df . groupby ( \"A\" )[ \"B\" , \"C\" ] . sum () Bodo will compile the functions separately, and won't be able to optimize the read_parquet call because it doesn't know how the return value of read_data is used. To structure the code into different functions and still allow the compiler to do holistic optimizations, you can specify the inline=\"always\" option to the jit decorator to tell the compiler to include that function during compilation of another one. For example: @bodo . jit ( inline = \"always\" ) def read_data ( fname ): return pd . read_parquet ( fname ) @bodo . jit def query (): df = read_data ( \"data.pq\" ) return df . groupby ( \"A\" )[ \"B\" , \"C\" ] . sum () The option inline=\"always\" in this example tells the compiler to compile and include read_data when it is compiling query .","title":"Inlining"},{"location":"performance/inlining/#inlining","text":"Inlining allows the compiler to perform optimizations across functions, at the cost of increased compilation time. Use inlining when you have your code split into multiple Bodo functions and there are important optimizations that need to performed on some functions, that are dependent on the code of other functions. We will explain this with examples below. Danger Inlining should be used sparingly as it can cause increased compilation time. We strongly recommend against inlining functions with 10 or more lines of code. Bodo's compiler translates high-level code inside bodo.jit decorated functions to highly optimized lower level code. It can perform many optimizations on the generated code based on the structure of the code inside the function being compiled. Let's consider the following example where data.pq is a dataset with 1000 columns: @bodo . jit def example (): df = pd . read_parquet ( \"data.pq\" ) return df . groupby ( \"A\" )[ \"B\" , \"C\" ] . sum () To execute the query inside the example function, Bodo doesn't need to read all the columns from the file. It only needs three columns ( A , B and C ), and can save a lot of time and memory by just reading those. When compiling example , Bodo automatically optimizes the read_parquet call to only read the three required columns. Warning If you have separate Bodo functions and their code needs to be optimized jointly, you need to use inlining. Any code that needs to be optimized jointly needs to be compiled as part of the same JIT compilation. If we have the following: @bodo . jit def read_data ( fname ): return pd . read_parquet ( fname ) @bodo . jit def query (): df = read_data ( \"data.pq\" ) return df . groupby ( \"A\" )[ \"B\" , \"C\" ] . sum () Bodo will compile the functions separately, and won't be able to optimize the read_parquet call because it doesn't know how the return value of read_data is used. To structure the code into different functions and still allow the compiler to do holistic optimizations, you can specify the inline=\"always\" option to the jit decorator to tell the compiler to include that function during compilation of another one. For example: @bodo . jit ( inline = \"always\" ) def read_data ( fname ): return pd . read_parquet ( fname ) @bodo . jit def query (): df = read_data ( \"data.pq\" ) return df . groupby ( \"A\" )[ \"B\" , \"C\" ] . sum () The option inline=\"always\" in this example tells the compiler to compile and include read_data when it is compiling query .","title":"Inlining"},{"location":"performance/performance/","text":"Performance Measurement \u00b6 This section provides tips on measuring performance of Bodo programs. It is important to keep the following in mind when measuring program run time: Every program has some overhead, so large data sets may be necessary for useful measurements. Performance can vary from one run to another. Several measurements are always needed. It is important to use a sequence of tests with increasing input size, which helps understand the impact of problem size on program performance. Testing with different data (in terms statistical distribution and skew) can be useful to see the impact of data skew on performance and scaling. Simple programs are useful to study performance factors. Complex programs are impacted by multiple factors and their performance is harder to understand. Longer computations typically provide more reliable run time information. Measuring execution time of Bodo functions \u00b6 Since Bodo-decorated functions are JIT-compiled , the compilation time is non-negligible but it only happens the first time a function is compiled. Compiled functions stay in memory and don't need to be re-compiled, and they can also be cached to disk (see caching ) to be reused across different executions. To avoid measuring compilation time, place timers inside the functions. For example: \"\"\" calc_pi.py: computes the value of Pi using Monte-Carlo Integration \"\"\" import numpy as np import bodo import time n = 2 * 10 ** 8 def calc_pi ( n ): t1 = time . time () x = 2 * np . random . ranf ( n ) - 1 y = 2 * np . random . ranf ( n ) - 1 pi = 4 * np . sum ( x ** 2 + y ** 2 < 1 ) / n print ( \"Execution time:\" , time . time () - t1 , \" \\n result:\" , pi ) return pi bodo_calc_pi = bodo . jit ( calc_pi ) print ( \"python:\" ) calc_pi ( n ) print ( \" \\n bodo:\" ) bodo_calc_pi ( n ) The output of this code is as follows: python: Execution time: 5.060443162918091 result: 3.14165914 bodo: Execution time: 2.165610068012029 result: 3.14154512 Bodo's parallel speedup can be measured similarly: \"\"\" calc_pi.py: computes the value of Pi using Monte-Carlo Integration \"\"\" import numpy as np import bodo import time @bodo . jit def calc_pi ( n ): t1 = time . time () x = 2 * np . random . ranf ( n ) - 1 y = 2 * np . random . ranf ( n ) - 1 pi = 4 * np . sum ( x ** 2 + y ** 2 < 1 ) / n print ( \"Execution time:\" , time . time () - t1 , \" \\n result:\" , pi ) return pi calc_pi ( 2 * 10 ** 8 ) Launched on eight parallel cores: $ mpiexec -n 8 python calc_pi.py Execution time: 0.5736249439651147 result: 3.14161474 And the time it takes can be compared with Python performance. Here, we have a 5.06/0.57 ~= 9x speedup (from parallelism and sequential optimizations). Measuring sections inside Bodo functions \u00b6 We can add multiple timers inside a function to see how much time each section takes: \"\"\" calc_pi.py: computes the value of Pi using Monte-Carlo Integration \"\"\" import numpy as np import bodo import time n = 2 * 10 ** 8 def calc_pi ( n ): t1 = time . time () x = 2 * np . random . ranf ( n ) - 1 y = 2 * np . random . ranf ( n ) - 1 t2 = time . time () print ( \"Initializing x,y takes: \" , t2 - t1 ) pi = 4 * np . sum ( x ** 2 + y ** 2 < 1 ) / n print ( \"calculation takes:\" , time . time () - t2 , \" \\n result:\" , pi ) return pi bodo_calc_pi = bodo . jit ( calc_pi ) print ( \"python: ------------------\" ) calc_pi ( n ) print ( \" \\n bodo: ------------------\" ) bodo_calc_pi ( n ) The output is as follows: python: ------------------ Initializing x,y takes: 3.9832258224487305 calculation takes: 1.1460411548614502 result: 3.14156454 bodo: ------------------ Initializing x,y takes: 3.0611653940286487 calculation takes: 0.35728363902308047 result: 3.14155538 Note Note that Bodo execution took longer in the last example than previous ones, since the presence of timers in the middle of computation can inhibit some code optimizations (e.g. code reordering and fusion). Therefore, one should be cautious about adding timers in the middle of computation. Disabling JIT Compilation \u00b6 Sometimes it is convenient to disable JIT compilation without removing the jit decorators in the code, to enable easy performance comparison with regular Python or perform debugging. This can be done by setting the environment variable NUMBA_DISABLE_JIT to 1 , which makes the jit decorators act as if they perform no operation. In this case, the invocation of decorated functions calls the original Python functions instead of compiled versions. Load Imbalance \u00b6 Bodo distributes and processes equal amounts of data across cores as much as possible. There are certain cases, however, where depending on the statistical properties of the data and the operation being performed on it, some cores will need to process much more data than others at certain points in the application, which limits the scaling that can be achieved. How much this impacts performance depends on the degree of imbalance and the impact the affected operation has on overall execution time. For example, consider the following operation: df . groupby ( \"A\" )[ \"B\" ] . nunique () Where df has one billion rows, A only has 3 unique values, and we are running this on a cluster with 1000 cores. Although the work can be distributed to a certain extent, the final result for each group of A has to be computed on a single core. Because there are only 3 groups, during computation of the final result there will only be at most three cores active. Expected Scaling \u00b6 Scaling can be measured as the speedup achieved with n cores compared to running on a single core, that is, the ratio of execution time with 1 core vs n cores. For a fixed input size, the speed up achieved by Bodo with increasing number of cores (also known as strong scaling ) depends on a combination of various factors: size of the input data (problem size), properties of the data, compute operations used, and the hardware platform's attributes (such as effective network throughput). For example, the program above can scale almost linearly (e.g. 100x speed up on 100 cores) for large enough problem sizes, since the only communication overhead is parallel summation of the partial sums obtained by np.sum on each processor. On the other hand, some operations such as join and groupby may require communicating significant amounts of data across the network, depending on the characteristics of the data and the exact operation (e.g. groupby.sum , groupby.nunique , groupy.apply , inner vs outer join , etc.), requiring fast cluster interconnection networks to scale to large number of cores. Load imbalance, as described above, can also significantly impair scaling in certain situations.","title":"Measuring Performance"},{"location":"performance/performance/#performance","text":"This section provides tips on measuring performance of Bodo programs. It is important to keep the following in mind when measuring program run time: Every program has some overhead, so large data sets may be necessary for useful measurements. Performance can vary from one run to another. Several measurements are always needed. It is important to use a sequence of tests with increasing input size, which helps understand the impact of problem size on program performance. Testing with different data (in terms statistical distribution and skew) can be useful to see the impact of data skew on performance and scaling. Simple programs are useful to study performance factors. Complex programs are impacted by multiple factors and their performance is harder to understand. Longer computations typically provide more reliable run time information.","title":"Performance Measurement"},{"location":"performance/performance/#measuring-execution-time-of-bodo-functions","text":"Since Bodo-decorated functions are JIT-compiled , the compilation time is non-negligible but it only happens the first time a function is compiled. Compiled functions stay in memory and don't need to be re-compiled, and they can also be cached to disk (see caching ) to be reused across different executions. To avoid measuring compilation time, place timers inside the functions. For example: \"\"\" calc_pi.py: computes the value of Pi using Monte-Carlo Integration \"\"\" import numpy as np import bodo import time n = 2 * 10 ** 8 def calc_pi ( n ): t1 = time . time () x = 2 * np . random . ranf ( n ) - 1 y = 2 * np . random . ranf ( n ) - 1 pi = 4 * np . sum ( x ** 2 + y ** 2 < 1 ) / n print ( \"Execution time:\" , time . time () - t1 , \" \\n result:\" , pi ) return pi bodo_calc_pi = bodo . jit ( calc_pi ) print ( \"python:\" ) calc_pi ( n ) print ( \" \\n bodo:\" ) bodo_calc_pi ( n ) The output of this code is as follows: python: Execution time: 5.060443162918091 result: 3.14165914 bodo: Execution time: 2.165610068012029 result: 3.14154512 Bodo's parallel speedup can be measured similarly: \"\"\" calc_pi.py: computes the value of Pi using Monte-Carlo Integration \"\"\" import numpy as np import bodo import time @bodo . jit def calc_pi ( n ): t1 = time . time () x = 2 * np . random . ranf ( n ) - 1 y = 2 * np . random . ranf ( n ) - 1 pi = 4 * np . sum ( x ** 2 + y ** 2 < 1 ) / n print ( \"Execution time:\" , time . time () - t1 , \" \\n result:\" , pi ) return pi calc_pi ( 2 * 10 ** 8 ) Launched on eight parallel cores: $ mpiexec -n 8 python calc_pi.py Execution time: 0.5736249439651147 result: 3.14161474 And the time it takes can be compared with Python performance. Here, we have a 5.06/0.57 ~= 9x speedup (from parallelism and sequential optimizations).","title":"Measuring execution time of Bodo functions"},{"location":"performance/performance/#measuring-sections-inside-bodo-functions","text":"We can add multiple timers inside a function to see how much time each section takes: \"\"\" calc_pi.py: computes the value of Pi using Monte-Carlo Integration \"\"\" import numpy as np import bodo import time n = 2 * 10 ** 8 def calc_pi ( n ): t1 = time . time () x = 2 * np . random . ranf ( n ) - 1 y = 2 * np . random . ranf ( n ) - 1 t2 = time . time () print ( \"Initializing x,y takes: \" , t2 - t1 ) pi = 4 * np . sum ( x ** 2 + y ** 2 < 1 ) / n print ( \"calculation takes:\" , time . time () - t2 , \" \\n result:\" , pi ) return pi bodo_calc_pi = bodo . jit ( calc_pi ) print ( \"python: ------------------\" ) calc_pi ( n ) print ( \" \\n bodo: ------------------\" ) bodo_calc_pi ( n ) The output is as follows: python: ------------------ Initializing x,y takes: 3.9832258224487305 calculation takes: 1.1460411548614502 result: 3.14156454 bodo: ------------------ Initializing x,y takes: 3.0611653940286487 calculation takes: 0.35728363902308047 result: 3.14155538 Note Note that Bodo execution took longer in the last example than previous ones, since the presence of timers in the middle of computation can inhibit some code optimizations (e.g. code reordering and fusion). Therefore, one should be cautious about adding timers in the middle of computation.","title":"Measuring sections inside Bodo functions"},{"location":"performance/performance/#disable-jit","text":"Sometimes it is convenient to disable JIT compilation without removing the jit decorators in the code, to enable easy performance comparison with regular Python or perform debugging. This can be done by setting the environment variable NUMBA_DISABLE_JIT to 1 , which makes the jit decorators act as if they perform no operation. In this case, the invocation of decorated functions calls the original Python functions instead of compiled versions.","title":"Disabling JIT Compilation"},{"location":"performance/performance/#load-imbalance","text":"Bodo distributes and processes equal amounts of data across cores as much as possible. There are certain cases, however, where depending on the statistical properties of the data and the operation being performed on it, some cores will need to process much more data than others at certain points in the application, which limits the scaling that can be achieved. How much this impacts performance depends on the degree of imbalance and the impact the affected operation has on overall execution time. For example, consider the following operation: df . groupby ( \"A\" )[ \"B\" ] . nunique () Where df has one billion rows, A only has 3 unique values, and we are running this on a cluster with 1000 cores. Although the work can be distributed to a certain extent, the final result for each group of A has to be computed on a single core. Because there are only 3 groups, during computation of the final result there will only be at most three cores active.","title":"Load Imbalance"},{"location":"performance/performance/#expected-scaling","text":"Scaling can be measured as the speedup achieved with n cores compared to running on a single core, that is, the ratio of execution time with 1 core vs n cores. For a fixed input size, the speed up achieved by Bodo with increasing number of cores (also known as strong scaling ) depends on a combination of various factors: size of the input data (problem size), properties of the data, compute operations used, and the hardware platform's attributes (such as effective network throughput). For example, the program above can scale almost linearly (e.g. 100x speed up on 100 cores) for large enough problem sizes, since the only communication overhead is parallel summation of the partial sums obtained by np.sum on each processor. On the other hand, some operations such as join and groupby may require communicating significant amounts of data across the network, depending on the characteristics of the data and the exact operation (e.g. groupby.sum , groupby.nunique , groupy.apply , inner vs outer join , etc.), requiring fast cluster interconnection networks to scale to large number of cores. Load imbalance, as described above, can also significantly impair scaling in certain situations.","title":"Expected Scaling"},{"location":"release_notes/Apr_2020/","text":"Bodo 2020.04 Release (Date: 04/08/2020) \u00b6 New Features and Improvements \u00b6 Support for scatterv operation Improved memory management for DataFrame and Series data Initial support for pandas.read_sql() pandas.read_csv() reads a directory of csv files pandas.read_csv() reads from S3, and Hadoop Distributed File System (HDFS) pandas.read_parquet() now reads all integer types (like int16) and gets nullable information for integer columns from pandas metadata pandas.read_parquet() now supports reading columns of list of string elements avoid type error for unselected columns in Parquet files support pandas.RangeIndex when reading a non-partitioned parquet dataset pandas.Dataframe.to_parquet() to Hadoop Distributed File System (HDFS) pandas.Dataframe.to_parquet() always writes pandas.RangeIndex to Parquet metadata support pandas.Dataframe.to_parquet() writing datetime64 (default in Pandas) and datatime.date types to Parquet files support decimal.Decimal type in dataframes and Parquet I/O Support for & , | , and pandas.Series.dt in pandas.Dataframe.query() Support added for groupby last operation min , max , and sum support in groupby() for string columns non-constant list of column names as argument support for functions like groupby() MultiIndex support for groupby(...).agg(as_index=False) pandas.Dataframe.merge() one dataframe on index, and the other on a column sorting compilation time improvement supports for integer, float, string, string list, datetime.date , datetime.datetime , and datetime.timedelta types in pandas.Series.cummin() , pandas.DataFrame.cummin() , pandas.Series.cummax() , and pandas.DataFrame.cummax() NA s in datetime.date array better datetime.timedelta support Support for min and max in pandas.Timestamp and datetime.date pandas.DataFrame.all() for boolean series pandas.Series.astype() to float, int, str Convert string columns to float using astype() NA support for Series.str.split() refactored and improved Dataframe indexing: pandas.loc() , pandas.Dataframe.iloc() , and pandas.Dataframe.iat() better support for pandas.Series.shift() , pandas.Series.pct_change() , pandas.Dataframe.drop() set dataframe column using a scalar support for Index.values Addition support for String columns Bug Fix \u00b6 pandas.join() produce the correct index. pandas.groupby() use the latest schema groupby(...).cumsum() preserves index groupby(...).agg() when passing a dictionary of functions: support mix of multi-function lists and single functions Fixed Numpy slicing error in a corner case when the slice is equivalent to array and array size is a constant proper construction of dataframe from slicing Numpy 2D array pandas.read_csv reads a dataframe containing only datetime like columns When using pandas.merge() and pandas.join() integer columns which can have a missing value NA are returned as nullable integer array (as opposed to 0 and -1 before) avoid errors in comparing Pandas and Numpy","title":"Apr 2020"},{"location":"release_notes/Apr_2020/#Apr_2020","text":"","title":"Bodo 2020.04 Release (Date: 04/08/2020)"},{"location":"release_notes/Apr_2020/#new-features-and-improvements","text":"Support for scatterv operation Improved memory management for DataFrame and Series data Initial support for pandas.read_sql() pandas.read_csv() reads a directory of csv files pandas.read_csv() reads from S3, and Hadoop Distributed File System (HDFS) pandas.read_parquet() now reads all integer types (like int16) and gets nullable information for integer columns from pandas metadata pandas.read_parquet() now supports reading columns of list of string elements avoid type error for unselected columns in Parquet files support pandas.RangeIndex when reading a non-partitioned parquet dataset pandas.Dataframe.to_parquet() to Hadoop Distributed File System (HDFS) pandas.Dataframe.to_parquet() always writes pandas.RangeIndex to Parquet metadata support pandas.Dataframe.to_parquet() writing datetime64 (default in Pandas) and datatime.date types to Parquet files support decimal.Decimal type in dataframes and Parquet I/O Support for & , | , and pandas.Series.dt in pandas.Dataframe.query() Support added for groupby last operation min , max , and sum support in groupby() for string columns non-constant list of column names as argument support for functions like groupby() MultiIndex support for groupby(...).agg(as_index=False) pandas.Dataframe.merge() one dataframe on index, and the other on a column sorting compilation time improvement supports for integer, float, string, string list, datetime.date , datetime.datetime , and datetime.timedelta types in pandas.Series.cummin() , pandas.DataFrame.cummin() , pandas.Series.cummax() , and pandas.DataFrame.cummax() NA s in datetime.date array better datetime.timedelta support Support for min and max in pandas.Timestamp and datetime.date pandas.DataFrame.all() for boolean series pandas.Series.astype() to float, int, str Convert string columns to float using astype() NA support for Series.str.split() refactored and improved Dataframe indexing: pandas.loc() , pandas.Dataframe.iloc() , and pandas.Dataframe.iat() better support for pandas.Series.shift() , pandas.Series.pct_change() , pandas.Dataframe.drop() set dataframe column using a scalar support for Index.values Addition support for String columns","title":"New Features and Improvements"},{"location":"release_notes/Apr_2020/#bug-fix","text":"pandas.join() produce the correct index. pandas.groupby() use the latest schema groupby(...).cumsum() preserves index groupby(...).agg() when passing a dictionary of functions: support mix of multi-function lists and single functions Fixed Numpy slicing error in a corner case when the slice is equivalent to array and array size is a constant proper construction of dataframe from slicing Numpy 2D array pandas.read_csv reads a dataframe containing only datetime like columns When using pandas.merge() and pandas.join() integer columns which can have a missing value NA are returned as nullable integer array (as opposed to 0 and -1 before) avoid errors in comparing Pandas and Numpy","title":"Bug Fix"},{"location":"release_notes/April_2021/","text":"Bodo 2021.4 Release (Date: 4/19/2021) \u00b6 This release includes many new features, bug fixes and usability improvements. Overall, 98 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is available for Windows as a Conda package (similar to Linux and macOS) Removed boost library dependency Many improvements to error checking and reporting, including: Internal compiler errors and stack traces are now avoided more effectively (clear errors are thrown) Ensure that an error is thrown if user specifies an argument as distributed but it must be replicated Improvements in error checking for user-defined functions (UDFs) Connectors: Support for writing partitioned Parquet datasets ( df.to_parquet with partition_cols parameter) Support for S3 anonymous access with storage_options={\"anon\": True} in pd.read_parquet() Parquet read: optimized metadata collection for nested parquet directories (includes hive-partitioned dataset) To reduce Parquet read time, schema validation of multi-file parquet datasets can be disabled with bodo.parquet_validate_schema=False Reduced compilation time for Pandas APIs Improved compilation time for df.head/tail Support for format spec in f-strings, for example: f\"{a:0.0%}\" Support for arrays in bodo.rebalance() Pandas coverage: Support for df.filter for filtering columns Support for indicator=True in pd.merge() Support for DataFrame/Series/GroupBy.pipe() Support for setting dataframe columns using a 2D array Support for string and nullable arrays (e.g. pd.Int64Dtype) in DataFrame/Series.shift() Support for pandas.tseries.offsets.MonthBegin Series.where and Series.mask : support for nullable arrays (e.g. pd.Int64Dtype) Scikit-learn: Support for sklearn.ensemble.RandomForestRegressor","title":"April 2021"},{"location":"release_notes/April_2021/#April_2021","text":"This release includes many new features, bug fixes and usability improvements. Overall, 98 code patches were merged since the last release.","title":"Bodo 2021.4 Release (Date: 4/19/2021)"},{"location":"release_notes/April_2021/#new-features-and-improvements","text":"Bodo is available for Windows as a Conda package (similar to Linux and macOS) Removed boost library dependency Many improvements to error checking and reporting, including: Internal compiler errors and stack traces are now avoided more effectively (clear errors are thrown) Ensure that an error is thrown if user specifies an argument as distributed but it must be replicated Improvements in error checking for user-defined functions (UDFs) Connectors: Support for writing partitioned Parquet datasets ( df.to_parquet with partition_cols parameter) Support for S3 anonymous access with storage_options={\"anon\": True} in pd.read_parquet() Parquet read: optimized metadata collection for nested parquet directories (includes hive-partitioned dataset) To reduce Parquet read time, schema validation of multi-file parquet datasets can be disabled with bodo.parquet_validate_schema=False Reduced compilation time for Pandas APIs Improved compilation time for df.head/tail Support for format spec in f-strings, for example: f\"{a:0.0%}\" Support for arrays in bodo.rebalance() Pandas coverage: Support for df.filter for filtering columns Support for indicator=True in pd.merge() Support for DataFrame/Series/GroupBy.pipe() Support for setting dataframe columns using a 2D array Support for string and nullable arrays (e.g. pd.Int64Dtype) in DataFrame/Series.shift() Support for pandas.tseries.offsets.MonthBegin Series.where and Series.mask : support for nullable arrays (e.g. pd.Int64Dtype) Scikit-learn: Support for sklearn.ensemble.RandomForestRegressor","title":"New Features and Improvements"},{"location":"release_notes/August_2020/","text":"Bodo 2020.08 Release (Date: 08/21/2020) \u00b6 This release includes many new features, bug fixes and performance improvements. Overall, 112 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is updated to use the latest versions of Numba, pandas and Arrow: Numba 0.51.0 pandas 1.1.0 Arrow 1.0 Support reading and writing Parquet files with columns where values are arrays or structs, which can contain other arrays/structs with arbitrary nesting. S3 I/O: automatically determine the region of the S3 bucket when reading and writing. Initial support for scikit-learn RandomForestClassifier (fit, predict and score methods) Support sklearn.metrics.precision_score , sklearn.metrics.recall_score and sklearn.metrics.f1_score . Improved caching support (caching @bodo.jit functions with cache=True) Initial support for arrays of map data structures Support count and offset arguments of np.fromfile New bodo.rebalance() function for load balancing dataframes manually if desired Support setting dataframe column as attribute, for example: df.B = \"AA\" Support DataFrame min/max/sum/prod/mean/median functions with axis=1 Support df.loc[:,columns] indexing pd.concat support for mix of Numpy and nullable integer/bool arrays Support parallel append to dataframes (concatenation reduction) Support GroupBy.idxmin and GroupBy.idxmax Improvements and optimizations in user-defined function (UDF) handling Basic support for Series.where() Support calling bodo.jit functions inside prange loops Support DataFrame.select_dtypes with constant strings Support DataFrame.sample Support Series.replace() and df.replace() (scalars and lists) Support for Series.dt methods: total_seconds() and to_pytimedelta() Improved support for Categorical data types Support for pandas.Timestamp.isocalendar() Support np.digitize() Improved error handling during I/O when input CSV or Parquet file does not exist Support pd.concat(axis=1) for dataframes Significant improvements in compilation time for dataframes with large number of columns bodo.is_jit_execution() can be used to know if a function is running with Bodo.","title":"August 2020"},{"location":"release_notes/August_2020/#August_2020","text":"This release includes many new features, bug fixes and performance improvements. Overall, 112 code patches were merged since the last release.","title":"Bodo 2020.08 Release (Date: 08/21/2020)"},{"location":"release_notes/August_2020/#new-features-and-improvements","text":"Bodo is updated to use the latest versions of Numba, pandas and Arrow: Numba 0.51.0 pandas 1.1.0 Arrow 1.0 Support reading and writing Parquet files with columns where values are arrays or structs, which can contain other arrays/structs with arbitrary nesting. S3 I/O: automatically determine the region of the S3 bucket when reading and writing. Initial support for scikit-learn RandomForestClassifier (fit, predict and score methods) Support sklearn.metrics.precision_score , sklearn.metrics.recall_score and sklearn.metrics.f1_score . Improved caching support (caching @bodo.jit functions with cache=True) Initial support for arrays of map data structures Support count and offset arguments of np.fromfile New bodo.rebalance() function for load balancing dataframes manually if desired Support setting dataframe column as attribute, for example: df.B = \"AA\" Support DataFrame min/max/sum/prod/mean/median functions with axis=1 Support df.loc[:,columns] indexing pd.concat support for mix of Numpy and nullable integer/bool arrays Support parallel append to dataframes (concatenation reduction) Support GroupBy.idxmin and GroupBy.idxmax Improvements and optimizations in user-defined function (UDF) handling Basic support for Series.where() Support calling bodo.jit functions inside prange loops Support DataFrame.select_dtypes with constant strings Support DataFrame.sample Support Series.replace() and df.replace() (scalars and lists) Support for Series.dt methods: total_seconds() and to_pytimedelta() Improved support for Categorical data types Support for pandas.Timestamp.isocalendar() Support np.digitize() Improved error handling during I/O when input CSV or Parquet file does not exist Support pd.concat(axis=1) for dataframes Significant improvements in compilation time for dataframes with large number of columns bodo.is_jit_execution() can be used to know if a function is running with Bodo.","title":"New Features and Improvements"},{"location":"release_notes/August_2021/","text":"Bodo 2021.8 Release (Date: 8/30/2021) \u00b6 This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 74 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is updated to use pandas 1.3 and Arrow 5.0 (latest) Updated bodo.jit flag handling to remove the need for the distributed flag in most cases. Arguments and return types are now automatically inferred as distributed in many cases. Automatic inference can be disabled using the new returns_maybe_distributed and args_maybe_distributed flags if necessary. Connectors: Improved pd.read_sql performance on Snowflake by using the Snowflake connector APIs directly. Improved performance of pd.read_parquet when reading large partitioned datasets Performance improvements: Reduced compilation time for some DataFrame operations General performance improvements in Bodo's execution engine resulting in better speed and memory efficiency for a wide range of operations Improved performance of merge and join operations Improved performance and scalability of groupby operations Improved performance of groupby.apply Improved performance of groupby.transform Significantly optimized Series.str.contains(..., regex=True) Improved performance of filtering operations involving string arrays Pandas: Support for passing string function names to Series.apply . The string can refer to a Series method or a Numpy ufunc. Support for passing string function names to DataFrame.apply . The string can refer to a DataFrame method. axis can be provided if the method takes an axis argument. Enhanced support for binary arrays, including within series/dataframes astype() support for casting strings to nullable integers Support for operator.mul between a Timedelta scalar and integers Series Support for std in groupby.transform . Scikit-learn: Support for sklearn.feature_extraction.text.CountVectorizer Support for coef_ attribute for Ridge BodoSQL 2021.8beta Release (Date: 8/30/2021) This release adds more SQL coverage, introduces new BodoSQL specific features, and fixes bugs. Overall, 53 code patches were merged since the last release. New Features and Improvements \u00b6 Parameterized Queries: Parameterized queries allow replacing scalars with Python variables during runtime execution. This enables caching more complex BodoSQL queries When paired with Bodo jit: a query parameter can change without the need to recompile the query. More information and example usage can be found in our documentation. SQL Coverage: This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage. Support for != and <=operators Support for CAST Support for LEAST Support for [NOT] IN with lists of literals Support for the offset optional argument in queries with LIMIT (i.e. SELECT A from table LIMIT 1, 4) Initial support for YEAR/MONTH interval literals/scalars. Currently these are only supported with addition and subtraction operators and cannot be used as a column type. Support the following string functions: CHAR (to convert a value to a string) LENGTH Support for the following Timestamp functions: ADDDATE SUBDATE TIMESTAMPDIFF WEEKDAY YEARWEEK LAST_DAY","title":"August 2021"},{"location":"release_notes/August_2021/#August_2021","text":"This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 74 code patches were merged since the last release.","title":"Bodo 2021.8 Release (Date: 8/30/2021)"},{"location":"release_notes/August_2021/#new-features-and-improvements","text":"Bodo is updated to use pandas 1.3 and Arrow 5.0 (latest) Updated bodo.jit flag handling to remove the need for the distributed flag in most cases. Arguments and return types are now automatically inferred as distributed in many cases. Automatic inference can be disabled using the new returns_maybe_distributed and args_maybe_distributed flags if necessary. Connectors: Improved pd.read_sql performance on Snowflake by using the Snowflake connector APIs directly. Improved performance of pd.read_parquet when reading large partitioned datasets Performance improvements: Reduced compilation time for some DataFrame operations General performance improvements in Bodo's execution engine resulting in better speed and memory efficiency for a wide range of operations Improved performance of merge and join operations Improved performance and scalability of groupby operations Improved performance of groupby.apply Improved performance of groupby.transform Significantly optimized Series.str.contains(..., regex=True) Improved performance of filtering operations involving string arrays Pandas: Support for passing string function names to Series.apply . The string can refer to a Series method or a Numpy ufunc. Support for passing string function names to DataFrame.apply . The string can refer to a DataFrame method. axis can be provided if the method takes an axis argument. Enhanced support for binary arrays, including within series/dataframes astype() support for casting strings to nullable integers Support for operator.mul between a Timedelta scalar and integers Series Support for std in groupby.transform . Scikit-learn: Support for sklearn.feature_extraction.text.CountVectorizer Support for coef_ attribute for Ridge BodoSQL 2021.8beta Release (Date: 8/30/2021) This release adds more SQL coverage, introduces new BodoSQL specific features, and fixes bugs. Overall, 53 code patches were merged since the last release.","title":"New Features and Improvements"},{"location":"release_notes/August_2021/#new-features-and-improvements_1","text":"Parameterized Queries: Parameterized queries allow replacing scalars with Python variables during runtime execution. This enables caching more complex BodoSQL queries When paired with Bodo jit: a query parameter can change without the need to recompile the query. More information and example usage can be found in our documentation. SQL Coverage: This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage. Support for != and <=operators Support for CAST Support for LEAST Support for [NOT] IN with lists of literals Support for the offset optional argument in queries with LIMIT (i.e. SELECT A from table LIMIT 1, 4) Initial support for YEAR/MONTH interval literals/scalars. Currently these are only supported with addition and subtraction operators and cannot be used as a column type. Support the following string functions: CHAR (to convert a value to a string) LENGTH Support for the following Timestamp functions: ADDDATE SUBDATE TIMESTAMPDIFF WEEKDAY YEARWEEK LAST_DAY","title":"New Features and Improvements"},{"location":"release_notes/December_2020/","text":"Bodo 2020.12 Release (Date: 12/30/2020) \u00b6 This release includes many new features, bug fixes and performance improvements. Overall, 60 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is updated to use Numba 0.52 (latest) Support for reading CSV and Parquet from Azure Data Lake Storage (ADLS) Improved support for UDFs More robust user function handling Improved support for date/time data types in UDFs Improved support for rolling window functions Support raw argument of apply() Support column selection from rolling objects Support for nullable int values Pandas coverage: Support for groupby.apply Support for groupby rolling functions Improved support for dataframe indexing using df.loc/iloc Improve dtype handling in read_csv Support for Series.mask Improved robustness for highly skewed string data (e.g. most of string data is on a few processes due to uneven data distribution) Support for dataframes with repeated column names Support for datetime.date arrays as Index in pivot_table and as argument to pd.DatetimeIndex Improved error checking in Pandas implementations Unroll constant loops for type stability in more cases Numpy coverage: Support for np.hstack Scikit-learn: Support for sklearn.preprocessing.StandardScaler inside jit functions.","title":"December 2020"},{"location":"release_notes/December_2020/#December_2020","text":"This release includes many new features, bug fixes and performance improvements. Overall, 60 code patches were merged since the last release.","title":"Bodo 2020.12 Release (Date: 12/30/2020)"},{"location":"release_notes/December_2020/#new-features-and-improvements","text":"Bodo is updated to use Numba 0.52 (latest) Support for reading CSV and Parquet from Azure Data Lake Storage (ADLS) Improved support for UDFs More robust user function handling Improved support for date/time data types in UDFs Improved support for rolling window functions Support raw argument of apply() Support column selection from rolling objects Support for nullable int values Pandas coverage: Support for groupby.apply Support for groupby rolling functions Improved support for dataframe indexing using df.loc/iloc Improve dtype handling in read_csv Support for Series.mask Improved robustness for highly skewed string data (e.g. most of string data is on a few processes due to uneven data distribution) Support for dataframes with repeated column names Support for datetime.date arrays as Index in pivot_table and as argument to pd.DatetimeIndex Improved error checking in Pandas implementations Unroll constant loops for type stability in more cases Numpy coverage: Support for np.hstack Scikit-learn: Support for sklearn.preprocessing.StandardScaler inside jit functions.","title":"New Features and Improvements"},{"location":"release_notes/December_2021/","text":"Bodo 2021.12 Release (Date: 12/29/2021) \u00b6 This release includes many new features and usability improvements. Overall, 67 code patches were merged since the last release. New Features and Improvements \u00b6 Significantly upgrades to the Bodo documentation to improve the developer experience Improvements to documentation and unsupported attribute handling for Pandas APIs Significant enhancements to objmode user experience and robustness, such as automatic output data type checking and automatic conversion if possible Improved support for re package, such as support for re flags, better support for returning None when necessary, and better catching of unsupported corner cases Support caching functions that take a string as input and create a file path using concatenation. For example: @bodo . jit ( cache = True ) def f ( folder ): return pd . read_parquet ( folder + \"/example.pq\" ) Connectors: Improved read_parquet runtime performance when reading from S3 Decreased compilation time for read_csv on DataFrames with large number of columns (100) Improved compilation time for dataframes with large number of columns (>10,000) Improved NA handling in User Defined Functions with df.apply when functions are not inlined Support for using logging.RootLogger.info when passing the logger as an argument to a JIT function Support for datetime.datetime.today Simpler bodo.scatterv usage from regular Python. Other ranks are ignored but not required to have None as their data Improved support for map arrays in various operations Support feature_importances_ of XGBoost Support predict_proba and predict_log_proba in Scikit-learn classifier algorithms Pandas: Support for Bodo specific argument _bodo_upcast_to_float64 in pd.read_csv. This can be used when all data is numeric but schema inference cannot accurate predict data types. Support for using DataFrame.to_parquet with \"wide\" DataFrames with large number of columns Support for storing a DateTimeIndex with DataFrame.to_parquet Support for the 'method' argument in DataFrame.fillna and Series.fillna Support for Series.bfill , Series.ffill , Series.pad , and Series.backfill Support for Series.keys Support for Series.infer_objects and DataFrame.infer_objects Decreased runtime when calling .astype(\"categorical\") on Series with large numbers of categories","title":"December 2021"},{"location":"release_notes/December_2021/#December_2021","text":"This release includes many new features and usability improvements. Overall, 67 code patches were merged since the last release.","title":"Bodo 2021.12 Release (Date: 12/29/2021)"},{"location":"release_notes/December_2021/#new-features-and-improvements","text":"Significantly upgrades to the Bodo documentation to improve the developer experience Improvements to documentation and unsupported attribute handling for Pandas APIs Significant enhancements to objmode user experience and robustness, such as automatic output data type checking and automatic conversion if possible Improved support for re package, such as support for re flags, better support for returning None when necessary, and better catching of unsupported corner cases Support caching functions that take a string as input and create a file path using concatenation. For example: @bodo . jit ( cache = True ) def f ( folder ): return pd . read_parquet ( folder + \"/example.pq\" ) Connectors: Improved read_parquet runtime performance when reading from S3 Decreased compilation time for read_csv on DataFrames with large number of columns (100) Improved compilation time for dataframes with large number of columns (>10,000) Improved NA handling in User Defined Functions with df.apply when functions are not inlined Support for using logging.RootLogger.info when passing the logger as an argument to a JIT function Support for datetime.datetime.today Simpler bodo.scatterv usage from regular Python. Other ranks are ignored but not required to have None as their data Improved support for map arrays in various operations Support feature_importances_ of XGBoost Support predict_proba and predict_log_proba in Scikit-learn classifier algorithms Pandas: Support for Bodo specific argument _bodo_upcast_to_float64 in pd.read_csv. This can be used when all data is numeric but schema inference cannot accurate predict data types. Support for using DataFrame.to_parquet with \"wide\" DataFrames with large number of columns Support for storing a DateTimeIndex with DataFrame.to_parquet Support for the 'method' argument in DataFrame.fillna and Series.fillna Support for Series.bfill , Series.ffill , Series.pad , and Series.backfill Support for Series.keys Support for Series.infer_objects and DataFrame.infer_objects Decreased runtime when calling .astype(\"categorical\") on Series with large numbers of categories","title":"New Features and Improvements"},{"location":"release_notes/Feb_2020/","text":"Bodo 2020.02 Release (Date: 02/14/2020) \u00b6 New Features and Improvements \u00b6 Bodo now utilizes the following packages: - pandas >= 1.0.0 - numba 0.48.0 - Apache Arrow 0.16.0 Custom S3 endpoint is supported as well as S3-like object storage systems such as MinIO Reading and writing of parquet files with S3 is more robust Parquet read now supports reading columns where elements are list of strings pandas.read_csv() now also accepts a list of column names for the parse_date parameter pandas groupby.agg() supports list of functions for a column: df = pd.DataFrame( {\"A\": [2, 1, 1, 1, 2, 2, 1], \"B\": [\"a\", \"b\", \"c\", \"c\", \"b\", \"c\", \"a\"]} ) gb = df.groupby(\"B\").agg({\"A\": [\"sum\", \"mean\"]}) pandas groupby.agg() now supports a tuple of built-in functions: gb = df.groupby(\"B\")[\"A\"].agg((\"sum\", \"median\")) User-defined functions can now be used with groupby.agg() and constant dict: gb = df.groupby(\"B\").agg({\"A\": my_function}) The compilation time and run time have been improved for pandas groupby with median , cumsum , and cumprod . pandas groupby now supports cumsum , max , min , prod , sum functions for string columns. pandas groupby.agg() now supports mixing median and nunique with other functions, and use of multiple \"cumulative\" operations in the same groupby (example: cumsum, cumprod, etc). Selecting groupby columns using attribute is now possible: df = pd.DataFrame( {\"A\": [2, 1, 1, 1, 2, 2, 1], \"B\": [3, 5, 6, 5, 4, 4, 3]} ) df.groupby('A').B.sum() pandas Series.str.extractall , Series.all() and Series.any() are supported Support for returning MultiIndex in groupby operations Various forms of UDFs in df.apply and Series.map are supported Comparison of datetime fields with datetime constants is now possible Converting date and datetime of Python datetime module to pandas Timestamp is now supported Conversion to float using float class as dtype for pandas Series.astype() is now supported: S = pd.Series(['1', '2', '3']) S.astype(float) Bug Fix \u00b6 Fixed a memory leak issue when returning a dataframe from a Bodo function pandas DataFrame.sort_values() now returns correct output for input cases that contain NA Groupby.agg: explicit column selection when using constant dictionary is no longer required Fixed an issue that Bodo always dropped the index in reset_index()","title":"Feb 2020"},{"location":"release_notes/Feb_2020/#Feb_2020","text":"","title":"Bodo 2020.02 Release (Date: 02/14/2020)"},{"location":"release_notes/Feb_2020/#new-features-and-improvements","text":"Bodo now utilizes the following packages: - pandas >= 1.0.0 - numba 0.48.0 - Apache Arrow 0.16.0 Custom S3 endpoint is supported as well as S3-like object storage systems such as MinIO Reading and writing of parquet files with S3 is more robust Parquet read now supports reading columns where elements are list of strings pandas.read_csv() now also accepts a list of column names for the parse_date parameter pandas groupby.agg() supports list of functions for a column: df = pd.DataFrame( {\"A\": [2, 1, 1, 1, 2, 2, 1], \"B\": [\"a\", \"b\", \"c\", \"c\", \"b\", \"c\", \"a\"]} ) gb = df.groupby(\"B\").agg({\"A\": [\"sum\", \"mean\"]}) pandas groupby.agg() now supports a tuple of built-in functions: gb = df.groupby(\"B\")[\"A\"].agg((\"sum\", \"median\")) User-defined functions can now be used with groupby.agg() and constant dict: gb = df.groupby(\"B\").agg({\"A\": my_function}) The compilation time and run time have been improved for pandas groupby with median , cumsum , and cumprod . pandas groupby now supports cumsum , max , min , prod , sum functions for string columns. pandas groupby.agg() now supports mixing median and nunique with other functions, and use of multiple \"cumulative\" operations in the same groupby (example: cumsum, cumprod, etc). Selecting groupby columns using attribute is now possible: df = pd.DataFrame( {\"A\": [2, 1, 1, 1, 2, 2, 1], \"B\": [3, 5, 6, 5, 4, 4, 3]} ) df.groupby('A').B.sum() pandas Series.str.extractall , Series.all() and Series.any() are supported Support for returning MultiIndex in groupby operations Various forms of UDFs in df.apply and Series.map are supported Comparison of datetime fields with datetime constants is now possible Converting date and datetime of Python datetime module to pandas Timestamp is now supported Conversion to float using float class as dtype for pandas Series.astype() is now supported: S = pd.Series(['1', '2', '3']) S.astype(float)","title":"New Features and Improvements"},{"location":"release_notes/Feb_2020/#bug-fix","text":"Fixed a memory leak issue when returning a dataframe from a Bodo function pandas DataFrame.sort_values() now returns correct output for input cases that contain NA Groupby.agg: explicit column selection when using constant dictionary is no longer required Fixed an issue that Bodo always dropped the index in reset_index()","title":"Bug Fix"},{"location":"release_notes/February_2021/","text":"Bodo 2021.2 Release (Date: 2/16/2021) \u00b6 This release includes many new features, bug fixes and usability improvements. Overall, 70 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is updated to use pandas 1.2 and Arrow 3.0 (latest) Many improvements to error checking and reporting Several documentation improvements Support tuple return from Bodo functions where elements of the tuple have a mix of distributed and replicated distributions Improvements in automatic loop unrolling to support column names generated in loops, e.g. pd.DataFrame(X, columns=[\"y\"] + [\"x{}\".format(i) for i in range(m)]) Improvements in caching to cover missing cases Pandas coverage: Support column indices in read_csv() dtype argument. For example: df = pd.read_csv(fname, dtype={3: str}) Support for df.to_string() Initial support for pd.Categorical() Support Series.min and Series.max for categorical data Support pd.to_datetime() with categorical string input Support pd.Series() constructor without data argument specified Support dtype=\"str\" in Series constructor Support for Series.to_dict() Support for Series.between() Support Series.loc[] setitem with boolean array index, such as S.loc[idx] = val where idx is a boolean array or Series Support dictionary input in Series.map() , such as S.map({1.0: \"A\", 4.0: \"DD\"}) Support for pd.TimedeltaIndex min and max Support for pd.tseries.offsets.Week Numpy coverage: Support axis=1 in distributed np.concatenate Initial support for np.random.multivariate_normal Scikit-learn: Add coef_ attribute to SGDClassifier model. Add coef_ attribute to LinearRegression model. Support for sklearn.preprocessing.LabelEncoder inside jit functions. Support for sklearn.metrics.r2_score inside jit functions.","title":"February 2021"},{"location":"release_notes/February_2021/#February_2021","text":"This release includes many new features, bug fixes and usability improvements. Overall, 70 code patches were merged since the last release.","title":"Bodo 2021.2 Release (Date: 2/16/2021)"},{"location":"release_notes/February_2021/#new-features-and-improvements","text":"Bodo is updated to use pandas 1.2 and Arrow 3.0 (latest) Many improvements to error checking and reporting Several documentation improvements Support tuple return from Bodo functions where elements of the tuple have a mix of distributed and replicated distributions Improvements in automatic loop unrolling to support column names generated in loops, e.g. pd.DataFrame(X, columns=[\"y\"] + [\"x{}\".format(i) for i in range(m)]) Improvements in caching to cover missing cases Pandas coverage: Support column indices in read_csv() dtype argument. For example: df = pd.read_csv(fname, dtype={3: str}) Support for df.to_string() Initial support for pd.Categorical() Support Series.min and Series.max for categorical data Support pd.to_datetime() with categorical string input Support pd.Series() constructor without data argument specified Support dtype=\"str\" in Series constructor Support for Series.to_dict() Support for Series.between() Support Series.loc[] setitem with boolean array index, such as S.loc[idx] = val where idx is a boolean array or Series Support dictionary input in Series.map() , such as S.map({1.0: \"A\", 4.0: \"DD\"}) Support for pd.TimedeltaIndex min and max Support for pd.tseries.offsets.Week Numpy coverage: Support axis=1 in distributed np.concatenate Initial support for np.random.multivariate_normal Scikit-learn: Add coef_ attribute to SGDClassifier model. Add coef_ attribute to LinearRegression model. Support for sklearn.preprocessing.LabelEncoder inside jit functions. Support for sklearn.metrics.r2_score inside jit functions.","title":"New Features and Improvements"},{"location":"release_notes/January_2021/","text":"Bodo 2021.1 Release (Date: 1/26/2021) \u00b6 This release includes many new features, bug fixes and performance improvements. Overall, 61 code patches were merged since the last release. New Features and Improvements \u00b6 Connectors: Support filter pushdown when reading partitioned parquet datasets: at compile time, Bodo detects if filters are applied to a dataframe after read_parquet , and generates code that applies those filters at read time so that only the required parquet files are read. Support for Series.to_csv() Supports passing file and dtype arguments of np.fromfile as kwargs. Support for f-strings in Bodo jitted functions Support passing Bodo distributed JIT functions to other Bodo JIT functions Pandas coverage: Support groupby with pd.NamedAgg() Support for groupby.size Support for groupby.shift Match input row order of pandas in groupby.apply when applicable Support min_periods in rolling calls Support passing a dictionary of data types to df.astype() Support dataframe setitem of multiple columns. For example: df[[\"A\", \"B\"]] = 1.3 Support for Index.get_loc() Support ddof argument (delta degrees of freedom) of Series.cov Support Series.is_monotonic property Initial support for dictionaries in Series.replace Support Series.reset_index(drop=True) Support level argument with all levels in reset_index() Several documentation improvements Scikit-learn: Support for sklearn.model_selection.train_test_split inside jit functions. Support for sklearn.preprocessing.MinMaxScaler inside jit functions.","title":"January 2021"},{"location":"release_notes/January_2021/#January_2021","text":"This release includes many new features, bug fixes and performance improvements. Overall, 61 code patches were merged since the last release.","title":"Bodo 2021.1 Release (Date: 1/26/2021)"},{"location":"release_notes/January_2021/#new-features-and-improvements","text":"Connectors: Support filter pushdown when reading partitioned parquet datasets: at compile time, Bodo detects if filters are applied to a dataframe after read_parquet , and generates code that applies those filters at read time so that only the required parquet files are read. Support for Series.to_csv() Supports passing file and dtype arguments of np.fromfile as kwargs. Support for f-strings in Bodo jitted functions Support passing Bodo distributed JIT functions to other Bodo JIT functions Pandas coverage: Support groupby with pd.NamedAgg() Support for groupby.size Support for groupby.shift Match input row order of pandas in groupby.apply when applicable Support min_periods in rolling calls Support passing a dictionary of data types to df.astype() Support dataframe setitem of multiple columns. For example: df[[\"A\", \"B\"]] = 1.3 Support for Index.get_loc() Support ddof argument (delta degrees of freedom) of Series.cov Support Series.is_monotonic property Initial support for dictionaries in Series.replace Support Series.reset_index(drop=True) Support level argument with all levels in reset_index() Several documentation improvements Scikit-learn: Support for sklearn.model_selection.train_test_split inside jit functions. Support for sklearn.preprocessing.MinMaxScaler inside jit functions.","title":"New Features and Improvements"},{"location":"release_notes/January_2022/","text":"Bodo 2022.1 Release (Date: 1/31/2022) \u00b6 This release includes many new features and usability improvements. Overall, 71 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is now available with pip on both Linux and Windows Bodo is upgraded to use Numba 0.55.0 (the latest release) Bodo can now evaluate JIT functions at compilation time if possible to extract constant values. This improves user experience by simplifying type stability requirements. For example, the function below can be refactored to be type stable easily: @bodo . jit def f ( df ): df . columns = [ f \"m_ { c } \" if c not in [ \"B\" , \"C\" ] else c for c in df . columns ] return df @bodo . jit def g ( df_cols ): return [ f \"m_ { c } \" if c not in [ \"B\" , \"C\" ] else c for c in df_cols ] > @bodo . jit def f ( df ): df . columns = g ( df . columns ) return df Connectors: read_csv now skips hidden files when reading from a directory. read_parquet now supports reading a list of files. Improved error handling for both read_csv and read_sql Improved null value handling in user-defined-functions that aren't inlined. Truncated error messages with DataFrames with large numbers of columns to improve readability. Improved support for the logging standard library: Support regular logging.Logger in addition to the logging.RootLogger . Supports passing a logger as a constant global. Supports the attributes: level , name , propagate , disabled , and parent . Supports the methods: debug , warning , warn , error , exception , critical , log , and setLevel . Improvments to global value handling of the compiler to avoid memory leaks in corner cases. Pandas: Support for DataFrame.pivot() and DataFrame.pivot_table() without requiring a constant list of output columns. Bodo currently only supports limited operations on output DataFrames of pivot, so users are recommended to immediately return these DataFrames to Python before doing any further processing in Bodo. Support for Index.rename Support for Index.is_monotonic , Index.is_montonic_increasing , and Index.is_monotonic_decreasing Support for Index.notna and Index.notnull Support for Index.drop_duplicates Support for groupby.min , groupby.max , groupby.first , and groupby.last on DataFrames with Categorical columns Support for column slice assignment with df.iloc (e.g. df.iloc[0,:] = 0 ) Support for Series.first , Series.last , DataFrame.first , and DataFrame.last","title":"January 2022"},{"location":"release_notes/January_2022/#January_2022","text":"This release includes many new features and usability improvements. Overall, 71 code patches were merged since the last release.","title":"Bodo 2022.1 Release (Date: 1/31/2022)"},{"location":"release_notes/January_2022/#new-features-and-improvements","text":"Bodo is now available with pip on both Linux and Windows Bodo is upgraded to use Numba 0.55.0 (the latest release) Bodo can now evaluate JIT functions at compilation time if possible to extract constant values. This improves user experience by simplifying type stability requirements. For example, the function below can be refactored to be type stable easily: @bodo . jit def f ( df ): df . columns = [ f \"m_ { c } \" if c not in [ \"B\" , \"C\" ] else c for c in df . columns ] return df @bodo . jit def g ( df_cols ): return [ f \"m_ { c } \" if c not in [ \"B\" , \"C\" ] else c for c in df_cols ] > @bodo . jit def f ( df ): df . columns = g ( df . columns ) return df Connectors: read_csv now skips hidden files when reading from a directory. read_parquet now supports reading a list of files. Improved error handling for both read_csv and read_sql Improved null value handling in user-defined-functions that aren't inlined. Truncated error messages with DataFrames with large numbers of columns to improve readability. Improved support for the logging standard library: Support regular logging.Logger in addition to the logging.RootLogger . Supports passing a logger as a constant global. Supports the attributes: level , name , propagate , disabled , and parent . Supports the methods: debug , warning , warn , error , exception , critical , log , and setLevel . Improvments to global value handling of the compiler to avoid memory leaks in corner cases. Pandas: Support for DataFrame.pivot() and DataFrame.pivot_table() without requiring a constant list of output columns. Bodo currently only supports limited operations on output DataFrames of pivot, so users are recommended to immediately return these DataFrames to Python before doing any further processing in Bodo. Support for Index.rename Support for Index.is_monotonic , Index.is_montonic_increasing , and Index.is_monotonic_decreasing Support for Index.notna and Index.notnull Support for Index.drop_duplicates Support for groupby.min , groupby.max , groupby.first , and groupby.last on DataFrames with Categorical columns Support for column slice assignment with df.iloc (e.g. df.iloc[0,:] = 0 ) Support for Series.first , Series.last , DataFrame.first , and DataFrame.last","title":"New Features and Improvements"},{"location":"release_notes/July_2020/","text":"Bodo 2020.07 Release (Date: 07/16/2020) \u00b6 This release includes many new features and bug fixes. Overall, 59 code patches were merged since the last release, including the major addition of support for columns of array and struct values with arbitrary nesting. New Features and Improvements \u00b6 Bodo is updated to use the latest version of Numba (Numba 0.50.1) Series and dataframe columns can have values that are arrays. For example: A B 0 0 [1, 2, 3] 1 1 [4, 5] 2 2 [6] 3 3 [7, 8, 9] Series and dataframe columns can have values that are structs. For example: A B 0 0 {'A': 1, 'B': 2.1} 1 1 {'A': 3, 'B': 4.3} 2 2 {'A': 5, 'B': 6.5} 3 3 {'A': 7, 'B': 8.4} Array and Struct values can contain other arrays/structs with arbitrary nesting. For example: A B 0 0 {'A': [1, 2], 'B': [3]} 1 1 {'A': [4, 5, 6], 'B': [7]} 2 2 {'A': [8, 9, 10, 11], 'B': [12, 13]} 3 3 {'A': [14], 'B': [15]} df.drop_duplicates() and df.merge() is supported for nested array/struct columns. Added support for categorical array data type without explicit categories. Added support for Series.astype(cat_dtype) and Series.astype(\"category\") . Generalized df.dropna() to all arrays, and added support for 'how' and 'subset' options. Support for Series.explode() series.median() : support 'skipna' option and Decimal type, and bug fixes. Added Series.radd/rsub/rmul/rdiv/rpow/rmod Support for Series.dot/kurt/kurtosis/skew/sem Added Series.mad (mean absolute deviation) Series.var() and Series.std() : Added support for 'skipna' and 'ddof' options Support Series.equals Series product/sum: added support for 'min_count' and 'skipna' options Support Index.map() for all Index type Support all Bodo array types as output of Series.map/apply, df.apply Support df.values with nullable int columns Bodo release builds now enforce licensing (expiration date and maximum core count) via license keys provided as a file or an environment variable called \"BODO_LICENSE\".","title":"July 2020"},{"location":"release_notes/July_2020/#July_2020","text":"This release includes many new features and bug fixes. Overall, 59 code patches were merged since the last release, including the major addition of support for columns of array and struct values with arbitrary nesting.","title":"Bodo 2020.07 Release (Date: 07/16/2020)"},{"location":"release_notes/July_2020/#new-features-and-improvements","text":"Bodo is updated to use the latest version of Numba (Numba 0.50.1) Series and dataframe columns can have values that are arrays. For example: A B 0 0 [1, 2, 3] 1 1 [4, 5] 2 2 [6] 3 3 [7, 8, 9] Series and dataframe columns can have values that are structs. For example: A B 0 0 {'A': 1, 'B': 2.1} 1 1 {'A': 3, 'B': 4.3} 2 2 {'A': 5, 'B': 6.5} 3 3 {'A': 7, 'B': 8.4} Array and Struct values can contain other arrays/structs with arbitrary nesting. For example: A B 0 0 {'A': [1, 2], 'B': [3]} 1 1 {'A': [4, 5, 6], 'B': [7]} 2 2 {'A': [8, 9, 10, 11], 'B': [12, 13]} 3 3 {'A': [14], 'B': [15]} df.drop_duplicates() and df.merge() is supported for nested array/struct columns. Added support for categorical array data type without explicit categories. Added support for Series.astype(cat_dtype) and Series.astype(\"category\") . Generalized df.dropna() to all arrays, and added support for 'how' and 'subset' options. Support for Series.explode() series.median() : support 'skipna' option and Decimal type, and bug fixes. Added Series.radd/rsub/rmul/rdiv/rpow/rmod Support for Series.dot/kurt/kurtosis/skew/sem Added Series.mad (mean absolute deviation) Series.var() and Series.std() : Added support for 'skipna' and 'ddof' options Support Series.equals Series product/sum: added support for 'min_count' and 'skipna' options Support Index.map() for all Index type Support all Bodo array types as output of Series.map/apply, df.apply Support df.values with nullable int columns Bodo release builds now enforce licensing (expiration date and maximum core count) via license keys provided as a file or an environment variable called \"BODO_LICENSE\".","title":"New Features and Improvements"},{"location":"release_notes/July_2021/","text":"Bodo 2021.7 Release (Date: 7/23/2021) \u00b6 This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 109 code patches were merged since the last release. New Features and Improvements \u00b6 Documentation has been reorganized and updated, with improved navigation and a detailed walkthrough of Pandas equivalents of PySpark functions. Improvements to enable BodoSQL features Connectors: Improved performance of pd.read_parquet when reading from remote storage systems like S3 Support reading categorical columns of Parquet Performance improvements: Improved performance and scalability of sort_values Optimized pd.Series.isin(values) performance for long list of values . UDFs in Series.apply and Dataframe.apply: the Bodo compiler transforms the code to pass main function values referenced in the UDF (\"free variables\") as arguments to apply() automatically if possible (to simplify UDF usage). Support passing Bodo data types to objmode directly (in addition to string representation of the data types). For example, the following code sets the return type an int64 type: @bodo.jit def f(a, b): with bodo.objmode(res=bodo.int64): res = random.randint(a, b) return res Compilation time improvements for some dataframe operations Distributed support for pd.RangeIndex calls Pandas coverage: Initial support for binary arrays, including within series/dataframes - Support for `groupby.transform` - Groupby: support repeated input columns. For example: df.groupby(\"A\").agg( D=pd.NamedAgg(column=\"B\", aggfunc=lambda A: A.sum()), F=pd.NamedAgg(column=\"C\", aggfunc=\"max\"), E=pd.NamedAgg(column=\"B\", aggfunc=\"min\"), ) - Support Groupby with `dropna=False` - Support for `dropna` in `Series.nunique`, `DataFrame.nunique`, and `groupby.nunique` - Support for `DataFrame.insert()` - Support `tolist()` for string and numpy arrays - Expanded `astype` support: : - str to timedelta64/datetime64 - timedelta64/datetime64 to int64 - date arrays - Numeric-like inputs to datetime/timedelta - Support for `pd.StringDtype()` in `astype` - numeric-like to nullable integer types - Support for `pd.Timestamp.now()` - Support Timestamp in `pd.to_datetime` - Support for Timestamp/Timedelta as the scalar value for a Series - Support for `Series.dt.month_name`, `Timestamp.month_name` - Support for min/max on timedelta64 series/arrays Python coverage: Support for bytes.fromhex() Support for bytes.__hash__ Support for min and max for string values","title":"July 2021"},{"location":"release_notes/July_2021/#July_2021","text":"This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 109 code patches were merged since the last release.","title":"Bodo 2021.7 Release (Date: 7/23/2021)"},{"location":"release_notes/July_2021/#new-features-and-improvements","text":"Documentation has been reorganized and updated, with improved navigation and a detailed walkthrough of Pandas equivalents of PySpark functions. Improvements to enable BodoSQL features Connectors: Improved performance of pd.read_parquet when reading from remote storage systems like S3 Support reading categorical columns of Parquet Performance improvements: Improved performance and scalability of sort_values Optimized pd.Series.isin(values) performance for long list of values . UDFs in Series.apply and Dataframe.apply: the Bodo compiler transforms the code to pass main function values referenced in the UDF (\"free variables\") as arguments to apply() automatically if possible (to simplify UDF usage). Support passing Bodo data types to objmode directly (in addition to string representation of the data types). For example, the following code sets the return type an int64 type: @bodo.jit def f(a, b): with bodo.objmode(res=bodo.int64): res = random.randint(a, b) return res Compilation time improvements for some dataframe operations Distributed support for pd.RangeIndex calls Pandas coverage: Initial support for binary arrays, including within series/dataframes - Support for `groupby.transform` - Groupby: support repeated input columns. For example: df.groupby(\"A\").agg( D=pd.NamedAgg(column=\"B\", aggfunc=lambda A: A.sum()), F=pd.NamedAgg(column=\"C\", aggfunc=\"max\"), E=pd.NamedAgg(column=\"B\", aggfunc=\"min\"), ) - Support Groupby with `dropna=False` - Support for `dropna` in `Series.nunique`, `DataFrame.nunique`, and `groupby.nunique` - Support for `DataFrame.insert()` - Support `tolist()` for string and numpy arrays - Expanded `astype` support: : - str to timedelta64/datetime64 - timedelta64/datetime64 to int64 - date arrays - Numeric-like inputs to datetime/timedelta - Support for `pd.StringDtype()` in `astype` - numeric-like to nullable integer types - Support for `pd.Timestamp.now()` - Support Timestamp in `pd.to_datetime` - Support for Timestamp/Timedelta as the scalar value for a Series - Support for `Series.dt.month_name`, `Timestamp.month_name` - Support for min/max on timedelta64 series/arrays Python coverage: Support for bytes.fromhex() Support for bytes.__hash__ Support for min and max for string values","title":"New Features and Improvements"},{"location":"release_notes/June_2020/","text":"Bodo 2020.06 Release (Date: 06/12/2020) \u00b6 New Features and Improvements \u00b6 Bodo is updated to use the latest minor releases of Numba and Apache Arrow packages: numba 0.49.1 Apache Arrow 0.17.1 Significant optimizations in read CSV/JSON/Parquet to reduce number of requests, files opened and overall load on the filesystem (for local filesystems, S3 and HDFS). Improvements in pandas.read_csv() and pandas.read_json() : Support reading compressed JSON and CSV files (gzip and bz2) Can read directories containing files with any extension Correctly handle CSV files with headers when reading a directory of CSV files Support automatic data type inference of JSON files when orient='records' and lines=True Bodo can now automatically infer the required constant values (e.g. list of key names for groupby) from the program in many cases. In addition, Bodo raises informative errors for cases that are not possible to infer automatically. Various improvements to support caching of Bodo functions, including adding support for caching inside Jupyter Notebook (see here for more information) Support NA value check with pandas.isna(A[i]) Support creating empty dataframes and setting columns on empty dataframes More balanced workload distribution across processor cores Support for user-defined functions calling other JIT functions, and improved error messages for invalid cases pandas.read_parquet() : support reading columns of list of integers/floats Support bodo.scatterv() for arrays of list of strings/integers/floats. Improved support for pd.to_datetime() to handle optional arguments and cases such as string and integer array/Series inputs Improved pd.concat support to handle arrays of list, Decimal and datetime.date values Improved array indexing (getitem/setitem) support for various data types such as date/time cases and Decimals Support sorting of Decimal series Support Dataframe merge and groupby with Decimal columns Groupby: ignore non-numeric columns for numeric-only operations like sum (same behavior as pandas). Support for comparison of Timedelta data types ( datetime.timedelta and timedelta64 ) Parallelization of numpy.full() Support glob.glob(...) inside Bodo functions Error messages and warnings: Improvements to clarity and conciseness of error messages Can use numba syntax highlighting for Bodo errors (enable with NUMBA_COLOR_SCHEME environment variable) Documentation: New theme and style Revamped introductory material and guide Improved documentation for pd.read_csv() and pd.read_json() Documented Bodo's coverage of data types Overall, 82 code patches are merged since the last release.","title":"June 2020"},{"location":"release_notes/June_2020/#June_2020","text":"","title":"Bodo 2020.06 Release (Date: 06/12/2020)"},{"location":"release_notes/June_2020/#new-features-and-improvements","text":"Bodo is updated to use the latest minor releases of Numba and Apache Arrow packages: numba 0.49.1 Apache Arrow 0.17.1 Significant optimizations in read CSV/JSON/Parquet to reduce number of requests, files opened and overall load on the filesystem (for local filesystems, S3 and HDFS). Improvements in pandas.read_csv() and pandas.read_json() : Support reading compressed JSON and CSV files (gzip and bz2) Can read directories containing files with any extension Correctly handle CSV files with headers when reading a directory of CSV files Support automatic data type inference of JSON files when orient='records' and lines=True Bodo can now automatically infer the required constant values (e.g. list of key names for groupby) from the program in many cases. In addition, Bodo raises informative errors for cases that are not possible to infer automatically. Various improvements to support caching of Bodo functions, including adding support for caching inside Jupyter Notebook (see here for more information) Support NA value check with pandas.isna(A[i]) Support creating empty dataframes and setting columns on empty dataframes More balanced workload distribution across processor cores Support for user-defined functions calling other JIT functions, and improved error messages for invalid cases pandas.read_parquet() : support reading columns of list of integers/floats Support bodo.scatterv() for arrays of list of strings/integers/floats. Improved support for pd.to_datetime() to handle optional arguments and cases such as string and integer array/Series inputs Improved pd.concat support to handle arrays of list, Decimal and datetime.date values Improved array indexing (getitem/setitem) support for various data types such as date/time cases and Decimals Support sorting of Decimal series Support Dataframe merge and groupby with Decimal columns Groupby: ignore non-numeric columns for numeric-only operations like sum (same behavior as pandas). Support for comparison of Timedelta data types ( datetime.timedelta and timedelta64 ) Parallelization of numpy.full() Support glob.glob(...) inside Bodo functions Error messages and warnings: Improvements to clarity and conciseness of error messages Can use numba syntax highlighting for Bodo errors (enable with NUMBA_COLOR_SCHEME environment variable) Documentation: New theme and style Revamped introductory material and guide Improved documentation for pd.read_csv() and pd.read_json() Documented Bodo's coverage of data types Overall, 82 code patches are merged since the last release.","title":"New Features and Improvements"},{"location":"release_notes/March_2021/","text":"Bodo 2021.3 Release (Date: 3/25/2021) \u00b6 This release includes many new features, bug fixes and usability improvements. Overall, 148 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is updated to use Numba 0.53 (latest) and support Python 3.9 Many improvements to error checking and reporting Compilation time is reduced, especially for user-defined functions (UDFs) Reduced initialization time when importing Bodo Distributed diagnostics improvements: Show distributed diagnostics when raising errors for distributed flag Only show user defined variables in diagnostics level one Performance optimizations: Faster groupby nunique with improved scaling Faster setitem for categorical arrays Connectors: Google Cloud Storage (GCS) support with Parquet Support reading Delta Lake tables Improved Snowflake support Removed s3fs dependency (Bodo now fully relies on Apache Arrow for S3 connectivity) Change default parallelism semantics of unique() to replicated output to match user expectations better Support objmode in groupby apply UDFs Pandas coverage: Support pd.DataFrame.duplicated() with categorical data Groupby support for min/max on categorical data Support for categorical in pd.Series.dropna Support nullable int array in pd.Categorical constructor Support for pd.Series.where and pd.Series.mask with categorical data and a scalar value. Support for pd.Series.diff() Support for pd.DataFrame.diff() Support for pd.Series.repeat() Support list of functions in groupby.agg() Support tuple of UDFs inside groupby.agg() dictionary case Support single row and scalar UDF output in groupby.apply() Support Categorical values in Groupby.shift Support case=False in Series.str.contains Support mapper with axis=1 for pd.DataFrame.rename . Support Timedelta64 data in pd.Groupby Support for datetime.date arrays in Series.max and Series.min Support for pd.timedelta_range Support equality between datetime64 / pd.Timestamp and timedelta64 / pd.Timedelta Support for iterating across most index types Support getting the name attribute of data inside df.apply Support Series.reset_index(drop=False) for common cases Support == and != on Dataframe and a scalar with a different type Sequential support for `pd.Series.idxmax`, `pd.Series.idxmin`, : `pd.DataFrame.idxmax`, and `pd.DataFrame.idxmin` with Nullable and Categorical arrays. Python coverage: Support datetime.date.replace() Improved support for datetime.date.strftime() Support for calendar.month_abbr SciPy: Initial support for scipy.sparse.csr_matrix Scikit-learn: Support for sklearn.feature_extraction.text.HashingVectorizer","title":"March 2021"},{"location":"release_notes/March_2021/#March_2021","text":"This release includes many new features, bug fixes and usability improvements. Overall, 148 code patches were merged since the last release.","title":"Bodo 2021.3 Release (Date: 3/25/2021)"},{"location":"release_notes/March_2021/#new-features-and-improvements","text":"Bodo is updated to use Numba 0.53 (latest) and support Python 3.9 Many improvements to error checking and reporting Compilation time is reduced, especially for user-defined functions (UDFs) Reduced initialization time when importing Bodo Distributed diagnostics improvements: Show distributed diagnostics when raising errors for distributed flag Only show user defined variables in diagnostics level one Performance optimizations: Faster groupby nunique with improved scaling Faster setitem for categorical arrays Connectors: Google Cloud Storage (GCS) support with Parquet Support reading Delta Lake tables Improved Snowflake support Removed s3fs dependency (Bodo now fully relies on Apache Arrow for S3 connectivity) Change default parallelism semantics of unique() to replicated output to match user expectations better Support objmode in groupby apply UDFs Pandas coverage: Support pd.DataFrame.duplicated() with categorical data Groupby support for min/max on categorical data Support for categorical in pd.Series.dropna Support nullable int array in pd.Categorical constructor Support for pd.Series.where and pd.Series.mask with categorical data and a scalar value. Support for pd.Series.diff() Support for pd.DataFrame.diff() Support for pd.Series.repeat() Support list of functions in groupby.agg() Support tuple of UDFs inside groupby.agg() dictionary case Support single row and scalar UDF output in groupby.apply() Support Categorical values in Groupby.shift Support case=False in Series.str.contains Support mapper with axis=1 for pd.DataFrame.rename . Support Timedelta64 data in pd.Groupby Support for datetime.date arrays in Series.max and Series.min Support for pd.timedelta_range Support equality between datetime64 / pd.Timestamp and timedelta64 / pd.Timedelta Support for iterating across most index types Support getting the name attribute of data inside df.apply Support Series.reset_index(drop=False) for common cases Support == and != on Dataframe and a scalar with a different type Sequential support for `pd.Series.idxmax`, `pd.Series.idxmin`, : `pd.DataFrame.idxmax`, and `pd.DataFrame.idxmin` with Nullable and Categorical arrays. Python coverage: Support datetime.date.replace() Improved support for datetime.date.strftime() Support for calendar.month_abbr SciPy: Initial support for scipy.sparse.csr_matrix Scikit-learn: Support for sklearn.feature_extraction.text.HashingVectorizer","title":"New Features and Improvements"},{"location":"release_notes/May_2020/","text":"Bodo 2020.05 Release (Date: 05/06/2020) \u00b6 New Features and Improvements \u00b6 Bodo is updated to use the latest versions of Numba and Apache Arrow packages: numba 0.49.0 Apache Arrow 0.17.0 Various improvements to clarity and conciseness of error messages Initial support for pandas.DataFrame.to_sql() pandas.read_sql() support sql and con passed to Bodo-decorated functions Added support for pandas.read_json() and pandas.DataFrame.to_json() from & to POSIX, S3, and Hadoop File Systems. Initial support for pandas.read_excel() numpy.fromfile() and numpy.tofile() from and to S3, and Hadoop File Systems. Reduction in number of requests in I/O read calls Initial support for array of lists of fixed sized values List of strings data type support for pandas.DataFrame.join() , pandas.DataFrame.drop_duplicates() , and pandas.DataFrame.groupby() pandas.Timestamp subtraction, min and max Improved support for null values in datetime and timedelta operations Support copy() function for Series of decimal.Decimal and datetime.date data types and most Index types Improved support for Series decimal.Decimal dtype String Series and Dataframe Column are now mutable and support inplace fillna() pandas.Series.round() pandas.Dataframe.assign() Support groupby(...).first() operation pandas.Dataframe.iloc support for extracting a subset of columns numpy.array.sum(axis=0) numpy.reshape() multi-dimensional distributed arrays Initial implementation of experimental legacy mode Proper error when using unsupported pandas.(...) & pandas.Series.(...) functions Improved robustness of pandas.DataFrame inplace operations Memory usage improvements Type safety improvements Compilation time improvements Bug Fixes \u00b6 Fixed an issue in pandas.read_csv() reading a large CSV file in specific distributed cases numpy.dot() with empty vector/matrix input","title":"May 2020"},{"location":"release_notes/May_2020/#May_2020","text":"","title":"Bodo 2020.05 Release (Date: 05/06/2020)"},{"location":"release_notes/May_2020/#new-features-and-improvements","text":"Bodo is updated to use the latest versions of Numba and Apache Arrow packages: numba 0.49.0 Apache Arrow 0.17.0 Various improvements to clarity and conciseness of error messages Initial support for pandas.DataFrame.to_sql() pandas.read_sql() support sql and con passed to Bodo-decorated functions Added support for pandas.read_json() and pandas.DataFrame.to_json() from & to POSIX, S3, and Hadoop File Systems. Initial support for pandas.read_excel() numpy.fromfile() and numpy.tofile() from and to S3, and Hadoop File Systems. Reduction in number of requests in I/O read calls Initial support for array of lists of fixed sized values List of strings data type support for pandas.DataFrame.join() , pandas.DataFrame.drop_duplicates() , and pandas.DataFrame.groupby() pandas.Timestamp subtraction, min and max Improved support for null values in datetime and timedelta operations Support copy() function for Series of decimal.Decimal and datetime.date data types and most Index types Improved support for Series decimal.Decimal dtype String Series and Dataframe Column are now mutable and support inplace fillna() pandas.Series.round() pandas.Dataframe.assign() Support groupby(...).first() operation pandas.Dataframe.iloc support for extracting a subset of columns numpy.array.sum(axis=0) numpy.reshape() multi-dimensional distributed arrays Initial implementation of experimental legacy mode Proper error when using unsupported pandas.(...) & pandas.Series.(...) functions Improved robustness of pandas.DataFrame inplace operations Memory usage improvements Type safety improvements Compilation time improvements","title":"New Features and Improvements"},{"location":"release_notes/May_2020/#bug-fixes","text":"Fixed an issue in pandas.read_csv() reading a large CSV file in specific distributed cases numpy.dot() with empty vector/matrix input","title":"Bug Fixes"},{"location":"release_notes/May_2021/","text":"Bodo 2021.5 Release (Date: 5/19/2021) \u00b6 This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 70 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is updated to use Arrow 4.0 (latest) Connectors: Improved performance of pd.read_parquet significantly for large multi-file datasets by optimizing Parquet metadata collection Bodo nows reads only the first few rows from a Parquet dataset if the program only requires df.head(n) and/or df.shape . This helps with exploring large datasets without the need for a large cluster to load the full data in memory. Visualization: Bodo now supports calling many Matplotlib plotting functions directly from JIT code. See the \"Data Visualization\" section of our documentation for more details. The current support gathers the data into one process but this will be avoided in future releases. Improved compilation time for dataframe functions Improved the performance and scalability of groupby.nunique Many improvements to error checking and reporting Bodo now avoids printing empty slices of distributed data to make print output easier to read. Pandas coverage: Support for DataFrame.info() Support for memory_usage() for DataFrame and Series Support for nbytes for array and Index types Support for df.describe() with datetime data (assumes datetime_is_numeric=True ) Support for groupby.value_counts() Support for pd.NamedAgg with nunique in groupby Initial support for CategoricalIndex type and categorical keys in groupby Support for groupby idxmin and idxmax with nullable Integer and Boolean arrays Support for timedelta64 in Groupby.agg Support for bins and other optional arguments in Series.value_counts() Support for df.dtypes Support passing df.dtypes to df.astype() , for example: df1.astype(df2.dtypes) Support for boolean pd.Index Support for Series.sort_index() Support for Timestamp.day_name() and Series.dt.day_name() Support for Series.quantile() with datetime Support for passing list of quantile values to Series.quantile() Support for Series.to_frame() Support for sum() method of Boolean Arrays Initial support for MultiIndex.from_product String array comparison returns a Pandas nullable boolean array instead of a Numpy boolean array","title":"May 2021"},{"location":"release_notes/May_2021/#May_2021","text":"This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 70 code patches were merged since the last release.","title":"Bodo 2021.5 Release (Date: 5/19/2021)"},{"location":"release_notes/May_2021/#new-features-and-improvements","text":"Bodo is updated to use Arrow 4.0 (latest) Connectors: Improved performance of pd.read_parquet significantly for large multi-file datasets by optimizing Parquet metadata collection Bodo nows reads only the first few rows from a Parquet dataset if the program only requires df.head(n) and/or df.shape . This helps with exploring large datasets without the need for a large cluster to load the full data in memory. Visualization: Bodo now supports calling many Matplotlib plotting functions directly from JIT code. See the \"Data Visualization\" section of our documentation for more details. The current support gathers the data into one process but this will be avoided in future releases. Improved compilation time for dataframe functions Improved the performance and scalability of groupby.nunique Many improvements to error checking and reporting Bodo now avoids printing empty slices of distributed data to make print output easier to read. Pandas coverage: Support for DataFrame.info() Support for memory_usage() for DataFrame and Series Support for nbytes for array and Index types Support for df.describe() with datetime data (assumes datetime_is_numeric=True ) Support for groupby.value_counts() Support for pd.NamedAgg with nunique in groupby Initial support for CategoricalIndex type and categorical keys in groupby Support for groupby idxmin and idxmax with nullable Integer and Boolean arrays Support for timedelta64 in Groupby.agg Support for bins and other optional arguments in Series.value_counts() Support for df.dtypes Support passing df.dtypes to df.astype() , for example: df1.astype(df2.dtypes) Support for boolean pd.Index Support for Series.sort_index() Support for Timestamp.day_name() and Series.dt.day_name() Support for Series.quantile() with datetime Support for passing list of quantile values to Series.quantile() Support for Series.to_frame() Support for sum() method of Boolean Arrays Initial support for MultiIndex.from_product String array comparison returns a Pandas nullable boolean array instead of a Numpy boolean array","title":"New Features and Improvements"},{"location":"release_notes/November_2020/","text":"Bodo 2020.11 Release (Date: 11/19/2020) \u00b6 This release includes many new features, bug fixes and performance improvements. Overall, 126 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is updated to use Apache Arrow 2.0 (latest) Performance and memory optimizations Significant memory usage optimizations for several operations involving string arrays Up to 2x speedup for many string operations such as Series.str.replace/get/contains and groupby.sum() User-defined functions (UDFs) Support for returning datafarames from DataFrame.apply() and Series.apply() Support for returning nested arrays Caching: for Bodo functions that receive CSV and JSON file names as string arguments, the cache will now be reused when file name arguments differ but have the same dataset type (schema). Support for distributed deep learning with Tensorflow and PyTorch: https://docs.bodo.ai/latest/source/dl.html Pandas coverage: Support for tuple values in Series and DataFrame columns Improvements to error checking and handling Automatic unrolling of loops over dataframe columns when necessary for type stability Support integer column names for Dataframes Support for pd.Timedelta values Support for pd.tseries.offsets.DateOffset and pd.tseries.offsets.Monthend Support for Series.dt, Timestamp, and DateTimeIndex attributes ( is_month_start , is_month_end , is_quarter_start , is_quarter_end , is_year_start , is_year_end , week , weekofyear , weekday ) Support for Series.dt and Timestamp normalize method Support for Timestamp.components and Timestamp.strftime Support for Series.dt.ceil and Series.dt.round Support for pd.to_timedelta Support Series.replace for categorical arrays where value and to_replace are scalars or lists Support for comparison operators on Decimal types Support for Series.add() with String, datetime, and timedelta Support for Series.mul() with string and int literal Support for setting values in categorical arrays Initial support for pd.get_dummies() Support for Series.groupby() Scikit-learn: the following classes and functions are supported inside jit functions: sklearn.linear_model.LinearRegression sklearn.linear_model.LogisticRegression sklearn.linear_model.Ridge sklearn.linear_model.Lasso sklearn.svm.LinearSVC sklearn.naive_bayes.MultinomialNB sklearn.metrics.accuracy_score sklearn.metrics.mean_squared_error sklearn.metrics.mean_absolute_error XGBoost: Training XGBoost model (with Scitkit-learn like API) is now supported inside jit functions: xgboost.XGBClassifier xgboost.XGBRegressor Visit <https://docs.bodo.ai/latest/source/ml.htmlfor more information about supported ML functions. NumPy coverage: Support for numpy.any and numpy.all for all array types Support for numpy.cbrt Support for numpy.linspace arguments endpoint , retstep , and dtype np.argmin with axis=1 Support for np.float32(str) Support for str.format , math.factorial , zlib.crc32","title":"November 2020"},{"location":"release_notes/November_2020/#November_2020","text":"This release includes many new features, bug fixes and performance improvements. Overall, 126 code patches were merged since the last release.","title":"Bodo 2020.11 Release (Date: 11/19/2020)"},{"location":"release_notes/November_2020/#new-features-and-improvements","text":"Bodo is updated to use Apache Arrow 2.0 (latest) Performance and memory optimizations Significant memory usage optimizations for several operations involving string arrays Up to 2x speedup for many string operations such as Series.str.replace/get/contains and groupby.sum() User-defined functions (UDFs) Support for returning datafarames from DataFrame.apply() and Series.apply() Support for returning nested arrays Caching: for Bodo functions that receive CSV and JSON file names as string arguments, the cache will now be reused when file name arguments differ but have the same dataset type (schema). Support for distributed deep learning with Tensorflow and PyTorch: https://docs.bodo.ai/latest/source/dl.html Pandas coverage: Support for tuple values in Series and DataFrame columns Improvements to error checking and handling Automatic unrolling of loops over dataframe columns when necessary for type stability Support integer column names for Dataframes Support for pd.Timedelta values Support for pd.tseries.offsets.DateOffset and pd.tseries.offsets.Monthend Support for Series.dt, Timestamp, and DateTimeIndex attributes ( is_month_start , is_month_end , is_quarter_start , is_quarter_end , is_year_start , is_year_end , week , weekofyear , weekday ) Support for Series.dt and Timestamp normalize method Support for Timestamp.components and Timestamp.strftime Support for Series.dt.ceil and Series.dt.round Support for pd.to_timedelta Support Series.replace for categorical arrays where value and to_replace are scalars or lists Support for comparison operators on Decimal types Support for Series.add() with String, datetime, and timedelta Support for Series.mul() with string and int literal Support for setting values in categorical arrays Initial support for pd.get_dummies() Support for Series.groupby() Scikit-learn: the following classes and functions are supported inside jit functions: sklearn.linear_model.LinearRegression sklearn.linear_model.LogisticRegression sklearn.linear_model.Ridge sklearn.linear_model.Lasso sklearn.svm.LinearSVC sklearn.naive_bayes.MultinomialNB sklearn.metrics.accuracy_score sklearn.metrics.mean_squared_error sklearn.metrics.mean_absolute_error XGBoost: Training XGBoost model (with Scitkit-learn like API) is now supported inside jit functions: xgboost.XGBClassifier xgboost.XGBRegressor Visit <https://docs.bodo.ai/latest/source/ml.htmlfor more information about supported ML functions. NumPy coverage: Support for numpy.any and numpy.all for all array types Support for numpy.cbrt Support for numpy.linspace arguments endpoint , retstep , and dtype np.argmin with axis=1 Support for np.float32(str) Support for str.format , math.factorial , zlib.crc32","title":"New Features and Improvements"},{"location":"release_notes/November_2021/","text":"Bodo 2021.11 Release (Date: 11/30/2021) \u00b6 This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 107 code patches were merged since the last release. New Features and Improvements \u00b6 Support for \"wide\" DataFrames with large number of columns: Bodo compiler is transitioning to a new internal dataframe compilation format that substantially decreases compliation time for dataframes with thousands of columns. All DataFrame APIs will transition to this new format over time. read_csv , read_parquet , bodo.gatherv and dataframe filtering are upgraded to support this new format in this release. Connectors: Significantly improved performance when reading Parquet from S3 (up to 10x faster read depending on the dataset). General support for predicate pushdown when reading from Parquet (filtering rows at the storage level). Improvements to BodoSQL's filter pushdown, such as higher compiler accuracy in detecting possible filters. Faster read_parquet compilation time by validating the schema only at runtime. Faster pd.read_csv() execution time with large numbers of columns. Bodo automatically maintains type information when passing DataFrames and Series between Bodo and regular Python. This avoids potential typing issues when parallel data chunks do not have enough non-null data for automatic type inference. Improved error messages and documentation. Pandas: Support for Array of dictionary outputs of DataFrame.apply() and Series.apply() Support for Array of dictionary inputs to pd.concat() Support for Series.astype(str) with Categorical type for non-string categories. Support for callable arguments to DataFrame.assign() Support for passing a list as skiprows of pd.read_csv() Support for low_memory argument in pd.read_csv() Support for using a string label for indexing Series with string Index (for non-parallel Series) Support for initializing a Series with a constant dictionary Support for subset argument to DataFrame.drop_duplicates Support for DataFrame.plot with arguments x , y , kind , figsize , xlabel , ylabel , title , legend , fontsize , xticks , yticks , and ax . DataFrame.plot behaves the same as Bodo's Matplotlib support. Support for DataFrame.groupby.head Numpy: Support for np.select ML: Support predict_proba and predict_log_proba for RandomForestClassifier , SGD Classifier and LogisticRegression Support predict_proba for XGBoostClassifier Support for sklearn.metrics.confusion_matrix BodoSQL 2021.11beta Release (Date: 11/30/2021) This release includes SQL bug fixes and support for Bodo's filter pushdown from BodoSQL. Most of the improvements to BodoSQL are integrating enhancements made to Bodo. Overall, 10 code patches were merged since the last release. New Features and Improvements \u00b6 Support for a new filepath API bodosql.TablePath . This API takes the path and file type and uses this to load/remove the data within the query. For example: bc = bodosql.BodoSQLContext(\"table1\": bodosql.TablePath(\"myfile.pq\", \"parquet\")) return bc.query(\"Select A from table1\") This is functionally equivalent to using the Pandas read_ functions inside a Bodo function, but it may have some additional performance optimizations. Currently only Parquet files are supported. Support for Bodo's filter pushdown when using the bodosql.TablePath API. Reduced compliation and execution time when using the FIRST_VALUE function repeatedly on the same exact window. SQL Coverage This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage. Support for omitting the second argument from the ROUND function (defaults to 0). Support for providing an integer as the second argument DATE_ADD and DATE_SUB . If you pass an integer, it is assigned days as its unit.","title":"November 2021"},{"location":"release_notes/November_2021/#November_2021","text":"This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 107 code patches were merged since the last release.","title":"Bodo 2021.11 Release (Date: 11/30/2021)"},{"location":"release_notes/November_2021/#new-features-and-improvements","text":"Support for \"wide\" DataFrames with large number of columns: Bodo compiler is transitioning to a new internal dataframe compilation format that substantially decreases compliation time for dataframes with thousands of columns. All DataFrame APIs will transition to this new format over time. read_csv , read_parquet , bodo.gatherv and dataframe filtering are upgraded to support this new format in this release. Connectors: Significantly improved performance when reading Parquet from S3 (up to 10x faster read depending on the dataset). General support for predicate pushdown when reading from Parquet (filtering rows at the storage level). Improvements to BodoSQL's filter pushdown, such as higher compiler accuracy in detecting possible filters. Faster read_parquet compilation time by validating the schema only at runtime. Faster pd.read_csv() execution time with large numbers of columns. Bodo automatically maintains type information when passing DataFrames and Series between Bodo and regular Python. This avoids potential typing issues when parallel data chunks do not have enough non-null data for automatic type inference. Improved error messages and documentation. Pandas: Support for Array of dictionary outputs of DataFrame.apply() and Series.apply() Support for Array of dictionary inputs to pd.concat() Support for Series.astype(str) with Categorical type for non-string categories. Support for callable arguments to DataFrame.assign() Support for passing a list as skiprows of pd.read_csv() Support for low_memory argument in pd.read_csv() Support for using a string label for indexing Series with string Index (for non-parallel Series) Support for initializing a Series with a constant dictionary Support for subset argument to DataFrame.drop_duplicates Support for DataFrame.plot with arguments x , y , kind , figsize , xlabel , ylabel , title , legend , fontsize , xticks , yticks , and ax . DataFrame.plot behaves the same as Bodo's Matplotlib support. Support for DataFrame.groupby.head Numpy: Support for np.select ML: Support predict_proba and predict_log_proba for RandomForestClassifier , SGD Classifier and LogisticRegression Support predict_proba for XGBoostClassifier Support for sklearn.metrics.confusion_matrix BodoSQL 2021.11beta Release (Date: 11/30/2021) This release includes SQL bug fixes and support for Bodo's filter pushdown from BodoSQL. Most of the improvements to BodoSQL are integrating enhancements made to Bodo. Overall, 10 code patches were merged since the last release.","title":"New Features and Improvements"},{"location":"release_notes/November_2021/#new-features-and-improvements_1","text":"Support for a new filepath API bodosql.TablePath . This API takes the path and file type and uses this to load/remove the data within the query. For example: bc = bodosql.BodoSQLContext(\"table1\": bodosql.TablePath(\"myfile.pq\", \"parquet\")) return bc.query(\"Select A from table1\") This is functionally equivalent to using the Pandas read_ functions inside a Bodo function, but it may have some additional performance optimizations. Currently only Parquet files are supported. Support for Bodo's filter pushdown when using the bodosql.TablePath API. Reduced compliation and execution time when using the FIRST_VALUE function repeatedly on the same exact window. SQL Coverage This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage. Support for omitting the second argument from the ROUND function (defaults to 0). Support for providing an integer as the second argument DATE_ADD and DATE_SUB . If you pass an integer, it is assigned days as its unit.","title":"New Features and Improvements"},{"location":"release_notes/October_2020/","text":"Bodo 2020.10 Release (Date: 10/20/2020) \u00b6 This release includes many new features, bug fixes and performance improvements. Overall, 117 code patches were merged since the last release. New Features and Improvements \u00b6 Initial support for Python classes using bodo.jitclass decorator. Scikit-learn: Initial support for these scikit-learn classes: : - `sklearn.linear_model.SGDClassifier` - `sklearn.linear_model.SGDRegressor` - `sklearn.cluster.KMeans` For more information please refer to the documentation [here](https://docs.bodo.ai/latest/source/sklearn.html) - Improved scaling of `RandomForestClassifier` training Memory management and memory consumption improvements Improvements for User-defined functions (UDFs): Compilation errors are now clearly shown for UDFs Support more complex UDFs (by running a full compiler pipeline) Support passing keyword arguments to UDF in DataFrame.apply() and Series.apply() Support much wider range of UDF types in groupby.agg Connectors: Improved connector error handling Improved performance of pd.read_csv (further improvements in next release) pd.read_parquet supports column containing all NA (null) values Caching: for Bodo functions that receive parquet file names as string arguments, the cache will now be reused when file name arguments differ but have the same parquet dataset type (schema). Significantly improved the performance of merge/join operations in some cases Support for loops over dataframe columns by automatic loop unrolling Support using global dataframe/array values inside jit functions Performance optimization for the series.str.split().explode() pattern Pandas coverage: Support setting df.columns and df.index Support setting values in Categorical arrays series.str.split : added support for regular expression and n parameter Series.replace support for more array types Support pd.series.dt.quarter Support series.str.slice_replace Support series.str.repeat Improved support for df.pivot_table and pd.crosstab Support for Series.notnull Support integer label indexing for Dataframes and Series with RangeIndex Support setting None and Optional values for most arrays NumPy coverage: Support for np.union1d np.where , np.unique , np.sort , np.repeat : support for Series and most array types Support np.argmax with axis=1 Support for np.min , np.max , min , max , np.sum , sum , np.prod on nullable arrays","title":"October 2020"},{"location":"release_notes/October_2020/#October_2020","text":"This release includes many new features, bug fixes and performance improvements. Overall, 117 code patches were merged since the last release.","title":"Bodo 2020.10 Release (Date: 10/20/2020)"},{"location":"release_notes/October_2020/#new-features-and-improvements","text":"Initial support for Python classes using bodo.jitclass decorator. Scikit-learn: Initial support for these scikit-learn classes: : - `sklearn.linear_model.SGDClassifier` - `sklearn.linear_model.SGDRegressor` - `sklearn.cluster.KMeans` For more information please refer to the documentation [here](https://docs.bodo.ai/latest/source/sklearn.html) - Improved scaling of `RandomForestClassifier` training Memory management and memory consumption improvements Improvements for User-defined functions (UDFs): Compilation errors are now clearly shown for UDFs Support more complex UDFs (by running a full compiler pipeline) Support passing keyword arguments to UDF in DataFrame.apply() and Series.apply() Support much wider range of UDF types in groupby.agg Connectors: Improved connector error handling Improved performance of pd.read_csv (further improvements in next release) pd.read_parquet supports column containing all NA (null) values Caching: for Bodo functions that receive parquet file names as string arguments, the cache will now be reused when file name arguments differ but have the same parquet dataset type (schema). Significantly improved the performance of merge/join operations in some cases Support for loops over dataframe columns by automatic loop unrolling Support using global dataframe/array values inside jit functions Performance optimization for the series.str.split().explode() pattern Pandas coverage: Support setting df.columns and df.index Support setting values in Categorical arrays series.str.split : added support for regular expression and n parameter Series.replace support for more array types Support pd.series.dt.quarter Support series.str.slice_replace Support series.str.repeat Improved support for df.pivot_table and pd.crosstab Support for Series.notnull Support integer label indexing for Dataframes and Series with RangeIndex Support setting None and Optional values for most arrays NumPy coverage: Support for np.union1d np.where , np.unique , np.sort , np.repeat : support for Series and most array types Support np.argmax with axis=1 Support for np.min , np.max , min , max , np.sum , sum , np.prod on nullable arrays","title":"New Features and Improvements"},{"location":"release_notes/October_2021/","text":"Bodo 2021.10 Release (Date: 10/28/2021) \u00b6 This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 71 code patches were merged since the last release. New Features and Improvements \u00b6 The Bodo Community Edition can now run on up to 8 cores. Bodo is updated to use Numba 0.54.1 (latest). Improved error messages and documentation. Connectors: pandas.read_csv : support for chunksize and nrows parameters Snowflake: Improved performance and scalability using the new parallel fetch functionality of Snowflake's Python connector, which retrieves data as batches of Arrow tables Support for removing unused columns from the SQL query. Support for filter pushdown of Pandas comparison operations into the SQL query. Reduced compilation time for DataFrame.describe Pandas: DataFrame.sort_values : supports passing na_position as a list with one value per column. BodoSQL 2021.10beta Release (Date: 10/28/2021) This release includes SQL bug fixes, increased SQL coverage and various usability improvements. Overall, 27 code patches were merged since the last release. New Features and Improvements \u00b6 Improved error messages with expanded documentation. Support for passing CategoricalArray and DateArray to BodoSQL. BodoSQL will automatically convert these arrays to supported types. SQL Coverage This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage. Support for TO_DATE function Support for string column casting inside DATE_ADD and DATE_SUB Support for nulls first and nulls last inside order by . Support for String columns in Window Aggregation Functions. Provided more efficient implementations for NVL and IFNULL when there is a column and a scalar.","title":"October 2021"},{"location":"release_notes/October_2021/#October_2021","text":"This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 71 code patches were merged since the last release.","title":"Bodo 2021.10 Release (Date: 10/28/2021)"},{"location":"release_notes/October_2021/#new-features-and-improvements","text":"The Bodo Community Edition can now run on up to 8 cores. Bodo is updated to use Numba 0.54.1 (latest). Improved error messages and documentation. Connectors: pandas.read_csv : support for chunksize and nrows parameters Snowflake: Improved performance and scalability using the new parallel fetch functionality of Snowflake's Python connector, which retrieves data as batches of Arrow tables Support for removing unused columns from the SQL query. Support for filter pushdown of Pandas comparison operations into the SQL query. Reduced compilation time for DataFrame.describe Pandas: DataFrame.sort_values : supports passing na_position as a list with one value per column. BodoSQL 2021.10beta Release (Date: 10/28/2021) This release includes SQL bug fixes, increased SQL coverage and various usability improvements. Overall, 27 code patches were merged since the last release.","title":"New Features and Improvements"},{"location":"release_notes/October_2021/#new-features-and-improvements_1","text":"Improved error messages with expanded documentation. Support for passing CategoricalArray and DateArray to BodoSQL. BodoSQL will automatically convert these arrays to supported types. SQL Coverage This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage. Support for TO_DATE function Support for string column casting inside DATE_ADD and DATE_SUB Support for nulls first and nulls last inside order by . Support for String columns in Window Aggregation Functions. Provided more efficient implementations for NVL and IFNULL when there is a column and a scalar.","title":"New Features and Improvements"},{"location":"release_notes/September_2020/","text":"Bodo 2020.09 Release (Date: 09/17/2020) \u00b6 This release includes many new features, bug fixes and performance improvements. Overall, 88 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is updated to use the latest versions of Numba, pandas and Arrow: Numba 0.51.2 pandas 1.1.2 Arrow 1.0.1 Major improvements in memory management. Bodo's memory consumption is reduced significantly by releasing memory as soon as possible in various operations such as Join, GroupBy, and Sort. Significant improvements in checking and handling various errors in I/O, providing clear error messages and graceful exits. Improvements in speed and scalability of read_parquet when reading from directories with large number of files. Distributed diagnostics is improved to provide clear messages on why a variable was assigned REP distribution. Improvements in caching support for I/O calls and groupby user-defined functions (UDFs). Support for more distributed getitem/setitem cases on arrays. Improvements on checking for unsupported functions and optional arguments. Significant performance improvements in groupby transformations (e.g. GroupBy.cumsum ). Enhanced support for DataFrame.select_dtypes . Support for axis=1 in DataFrame.var/std . Support for Series.autocorr . Support for Series.is_monotonic_increasing/is_monotonic_decreasing . Support pd.Series() constructor with a scalar data value. Support for dayofweek , is_leap_year and days_in_month in Timestamp and `Series.dt]{.title-ref}. Support for isocalendar in Series.dt and DatetimeIndex . Support for Series.cumsum/cummin/cummax . Support for Decimal values in nested data structures. Improvements in table join performance. Support for Series.drop_duplicates . Support for np.dot and @ operator on Series . Improvements in pd.concat support. Optimized Series.astype(str) for int64 values. Support for pd.Index constructor.","title":"September 2020"},{"location":"release_notes/September_2020/#September_2020","text":"This release includes many new features, bug fixes and performance improvements. Overall, 88 code patches were merged since the last release.","title":"Bodo 2020.09 Release (Date: 09/17/2020)"},{"location":"release_notes/September_2020/#new-features-and-improvements","text":"Bodo is updated to use the latest versions of Numba, pandas and Arrow: Numba 0.51.2 pandas 1.1.2 Arrow 1.0.1 Major improvements in memory management. Bodo's memory consumption is reduced significantly by releasing memory as soon as possible in various operations such as Join, GroupBy, and Sort. Significant improvements in checking and handling various errors in I/O, providing clear error messages and graceful exits. Improvements in speed and scalability of read_parquet when reading from directories with large number of files. Distributed diagnostics is improved to provide clear messages on why a variable was assigned REP distribution. Improvements in caching support for I/O calls and groupby user-defined functions (UDFs). Support for more distributed getitem/setitem cases on arrays. Improvements on checking for unsupported functions and optional arguments. Significant performance improvements in groupby transformations (e.g. GroupBy.cumsum ). Enhanced support for DataFrame.select_dtypes . Support for axis=1 in DataFrame.var/std . Support for Series.autocorr . Support for Series.is_monotonic_increasing/is_monotonic_decreasing . Support pd.Series() constructor with a scalar data value. Support for dayofweek , is_leap_year and days_in_month in Timestamp and `Series.dt]{.title-ref}. Support for isocalendar in Series.dt and DatetimeIndex . Support for Series.cumsum/cummin/cummax . Support for Decimal values in nested data structures. Improvements in table join performance. Support for Series.drop_duplicates . Support for np.dot and @ operator on Series . Improvements in pd.concat support. Optimized Series.astype(str) for int64 values. Support for pd.Index constructor.","title":"New Features and Improvements"},{"location":"release_notes/September_2021/","text":"Bodo 2021.9 Release (Date: 9/29/2021) \u00b6 This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 98 code patches were merged since the last release. New Features and Improvements \u00b6 Bodo is updated to use Numba 0.54 (latest) Performance improvements: Significantly improved the performance and scalability of parallel merge and join operations Improved the performance and scalability of groupby.nunique General performance improvements for operations involving data shuffling Optimized many compilation paths, especially those involving DataFrames. This will lead to shorter compilation times for many use cases. Optimizations in pd.read_sql to limit the data read when LIMIT is provided. Pandas: Support for Series.shift on timedelta64 data Support for pd.cut() and pd.qcut() Support for first , last , median , nunique , prod , and var in groupby.transform Support for multiplication with DateOffset Support for Series.round() on nullable integers Support for to_strip argument in series.str.strip/lstrip/rstrip Increased Binary Array/Series/DataFrame support. In particular: Support for first , last , shift , count , nunique , size , value_counts for Binary Series and DataFrames. Groupby support with binary keys/values. Support for sort_values with binary columns. Join with binary keys Most generic Series/DataFrame operations. Support for equi-join with additional non-equi-join conditions through our general merge condition syntax. Please refer to the documentation for more information. BodoSQL 2021.9beta Release (Date: 9/29/2021) This release adds SQL bug fixes and various usability improvements, including a reduced package size. BodoSQL users should also benefit from compilation time improvements due to improvements in the engine. Overall, 25 code patches were merged since the last release. New Features and Improvements \u00b6 Decreased package size and removed external dependencies. Improved error messages with shortened stack traces. SQL Coverage This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage. Support for UTC_TIMESTAMP function Support for UTC_DATE function Support for PIVOT Support for the following Window Functions: MAX MIN COUNT/COUNT(*) SUM AVG STDDEV STDDEV_POP VARIANCE VAR_POP LEAD LAG FIRST_VALUE LAST_VALUE NTH_VALUE NTILE ROW_NUMBER","title":"September 2021"},{"location":"release_notes/September_2021/#September_2021","text":"This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 98 code patches were merged since the last release.","title":"Bodo 2021.9 Release (Date: 9/29/2021)"},{"location":"release_notes/September_2021/#new-features-and-improvements","text":"Bodo is updated to use Numba 0.54 (latest) Performance improvements: Significantly improved the performance and scalability of parallel merge and join operations Improved the performance and scalability of groupby.nunique General performance improvements for operations involving data shuffling Optimized many compilation paths, especially those involving DataFrames. This will lead to shorter compilation times for many use cases. Optimizations in pd.read_sql to limit the data read when LIMIT is provided. Pandas: Support for Series.shift on timedelta64 data Support for pd.cut() and pd.qcut() Support for first , last , median , nunique , prod , and var in groupby.transform Support for multiplication with DateOffset Support for Series.round() on nullable integers Support for to_strip argument in series.str.strip/lstrip/rstrip Increased Binary Array/Series/DataFrame support. In particular: Support for first , last , shift , count , nunique , size , value_counts for Binary Series and DataFrames. Groupby support with binary keys/values. Support for sort_values with binary columns. Join with binary keys Most generic Series/DataFrame operations. Support for equi-join with additional non-equi-join conditions through our general merge condition syntax. Please refer to the documentation for more information. BodoSQL 2021.9beta Release (Date: 9/29/2021) This release adds SQL bug fixes and various usability improvements, including a reduced package size. BodoSQL users should also benefit from compilation time improvements due to improvements in the engine. Overall, 25 code patches were merged since the last release.","title":"New Features and Improvements"},{"location":"release_notes/September_2021/#new-features-and-improvements_1","text":"Decreased package size and removed external dependencies. Improved error messages with shortened stack traces. SQL Coverage This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage. Support for UTC_TIMESTAMP function Support for UTC_DATE function Support for PIVOT Support for the following Window Functions: MAX MIN COUNT/COUNT(*) SUM AVG STDDEV STDDEV_POP VARIANCE VAR_POP LEAD LAG FIRST_VALUE LAST_VALUE NTH_VALUE NTILE ROW_NUMBER","title":"New Features and Improvements"}]}