package com.bodo.iceberg;

import com.bodo.iceberg.catalog.CatalogCreator;
import java.net.URISyntaxException;
import java.sql.SQLException;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;
import org.apache.iceberg.*;
import org.apache.iceberg.catalog.Catalog;
import org.apache.iceberg.catalog.Namespace;
import org.apache.iceberg.catalog.TableIdentifier;
import org.apache.iceberg.types.TypeUtil;

public class BodoIcebergHandler {
  /**
   * Java Class used to map Bodo's required read and write operations to a corresponding Iceberg
   * table. This is meant to provide 1 instance per Table and Bodo is responsible for closing it.
   */
  private final Catalog catalog;

  // Map of transaction hashcode to transaction instance
  private final HashMap<Integer, Transaction> transactions;

  private BodoIcebergHandler(Catalog catalog) {
    this.catalog = catalog;
    this.transactions = new HashMap<>();
  }

  // Note: This API is exposed to Python.
  public BodoIcebergHandler(String connStr, String catalogType, String coreSitePath)
      throws URISyntaxException {
    this(CatalogCreator.create(connStr, catalogType, coreSitePath));
  }

  private static TableIdentifier genTableID(String dbName, String tableName) {
    // Snowflake uses dot separated strings for DB and schema names
    // Iceberg uses Namespaces with multiple levels to represent this
    Namespace dbNamespace = Namespace.of(dbName.split("\\."));
    return TableIdentifier.of(dbNamespace, tableName);
  }

  /**
   * Inner function to help load tables
   *
   * @return Iceberg table associated with ID
   */
  private Table loadTable(String dbName, String tableName) {
    return catalog.loadTable(genTableID(dbName, tableName));
  }

  /**
   * Inner function to help load tables
   *
   * @return Iceberg table associated with ID
   */
  private Table loadTable(String tableId) {
    return catalog.loadTable(TableIdentifier.parse(tableId));
  }

  /**
   * When creating a new table using createOrReplaceTable, we pass it a schema with valid field IDs
   * populated. However, as part of the transaction, Iceberg Java library creates a "fresh schema"
   * and re-assigns field IDs. They say it's for "consistency" reasons. However, this is problematic
   * for us since we only commit after writing the parquet files, which means that the field IDs we
   * wrote in the parquet fields' metadata could be incorrect. To work around this, we instead call
   * this function before writing any parquet files. This simulates and generate the field-ids in
   * the same way that createOrReplaceTable would. Using these field IDs is therefore more reliable.
   *
   * <p>Note: This API is exposed to Python.
   *
   * @param bodoSchema Original schema generated by Bodo with field IDs.
   * @return Same schema, except the field IDs would be as if generated by Iceberg Java Library in
   *     the final metadata during commit.
   */
  public static Schema getInitSchema(Schema bodoSchema) {
    // Taken from the 'newTableMetadata' function. This is important since it's
    // what the Catalog implementations call to generate schema during the
    // create/replace transaction.
    // Flow:
    // - catalog.newCreateTableTransaction
    // - buildTable (in Catalog.newCreateTableTransaction). This returns an instance
    // of BaseMetastoreCatalogTableBuilder.
    // - BaseMetastoreCatalogTableBuilder.create_transaction(). This calls
    // TableMetadata.newTableMetadata().
    // - This creates a 'freshSchema' using this code:
    AtomicInteger lastColumnId = new AtomicInteger(0);
    return TypeUtil.assignFreshIds(bodoSchema, lastColumnId::incrementAndGet);
  }

  /**
   * Get Information About Table
   *
   * <p>Note: This API is exposed to Python.
   *
   * @return Information about Table needed by Bodo
   */
  public TableInfo getTableInfo(String dbName, String tableName, boolean error)
      throws SQLException, URISyntaxException, InterruptedException {
    if (!catalog.tableExists(genTableID(dbName, tableName)) && !error) {
      return null;
    }

    // Note that repeated calls to loadTable are cheap due to CachingCatalog
    return new TableInfo(loadTable(dbName, tableName));
  }

  /**
   * Update properties of a transaction and remove (existing) table comments. Currently using a map
   * for possible generalization of other properties.
   *
   * @param txn Transaction ID
   * @param prop Map of key-value pairs of properties to insert into transaction
   */
  public void UpdateTxnProperties(Transaction txn, Map<String, String> prop) {
    UpdateProperties txnupd = txn.updateProperties();
    for (Map.Entry<String, String> entry : prop.entrySet()) {
      String key = entry.getKey();
      String value = entry.getValue();
      txnupd = txnupd.set(key, value);
    }
    txnupd.commit();
  }

  /**
   * Create a transaction to create a new table in the DB.
   *
   * <p>Note: This API is exposed to Python.
   *
   * @param schema Schema of the table
   * @param replace Whether to replace the table if it already exists
   * @return Transaction ID
   */
  public Integer startCreateOrReplaceTable(
      String dbName, String tableName, Schema schema, boolean replace, Map<String, String> prop)
      throws SQLException, URISyntaxException, InterruptedException {
    Map<String, String> properties = new HashMap<>();
    properties.put(TableProperties.FORMAT_VERSION, "2");
    // TODO: Support passing in new partition spec and sort order as well
    final Transaction txn;
    TableIdentifier id = genTableID(dbName, tableName);

    if (replace) {
      if (getTableInfo(dbName, tableName, false) == null) {
        // Temporarily create the table to avoid breaking the rest catalog.
        // TODO: REMOVE. The REST catalog runtime should use the information
        // from the active transaction.
        catalog.createTable(id, schema, PartitionSpec.unpartitioned(), properties);
      }
      txn =
          catalog.newReplaceTableTransaction(
              id, schema, PartitionSpec.unpartitioned(), properties, true);
    } else {
      // Create the table and then replace it,
      // this is so we can fetch credentials for the table in python, otherwise we get a table not
      // found error
      catalog.createTable(id, schema, PartitionSpec.unpartitioned(), properties);

      txn =
          catalog.newReplaceTableTransaction(
              id, schema, PartitionSpec.unpartitioned(), properties, false);
    }
    // Same as directly adding prop into properties dictionary above.
    UpdateTxnProperties(txn, prop);
    this.transactions.put(txn.hashCode(), txn);
    return txn.hashCode();
  }

  /**
   * Commit a new table in the DB.
   *
   * <p>Note: This API is exposed to Python.
   */
  public void commitCreateOrReplaceTable(int txnID, String fileInfoJson) {
    Transaction txn = this.transactions.get(txnID);
    List<DataFileInfo> fileInfos = DataFileInfo.fromJson(fileInfoJson);
    this.addData(txn.newAppend(), PartitionSpec.unpartitioned(), SortOrder.unsorted(), fileInfos);
    txn.commitTransaction();
  }

  /**
   * Start a transaction to append data to a pre-existing table
   *
   * <p>Note: This API is exposed to Python.
   */
  public Integer startAppendTable(String dbName, String tableName, Map<String, String> prop) {
    Transaction txn = loadTable(dbName, tableName).newTransaction();
    UpdateTxnProperties(txn, prop);
    this.transactions.put(txn.hashCode(), txn);
    return txn.hashCode();
  }

  /**
   * Commit appending rows into a pre-existing table.
   *
   * <p>Note: This API is exposed to Python.
   */
  public void commitAppendTable(int txnID, String fileInfoJson, int schemaID) {
    Transaction txn = this.transactions.get(txnID);
    Table table = txn.table();

    List<DataFileInfo> fileInfos = DataFileInfo.fromJson(fileInfoJson);
    this.addData(txn.table().newAppend(), table.spec(), table.sortOrder(), fileInfos);
    txn.commitTransaction();
  }

  /**
   * Commit a statistics file to the table.
   *
   * <p>Note: This API is exposed to Python.
   */
  public void commitStatisticsFile(String tableId, long snapshotID, String statisticsFileJson) {
    StatisticsFile statisticsFile = BodoStatisticFile.fromJson(statisticsFileJson);
    Table table = loadTable(tableId);
    table.refresh();
    Transaction txn = table.newTransaction();
    txn.updateStatistics().setStatistics(snapshotID, statisticsFile).commit();
    txn.commitTransaction();
  }

  /**
   * Merge Rows into Pre-existing Table by Copy-on-Write Rules
   *
   * <p>Note: This API is exposed to Python.
   */
  public void mergeCOWTable(
      String dbName,
      String tableName,
      List<String> oldFileNames,
      String newFileInfoJson,
      long snapshotID) {

    // Remove the Table instance associated with `id` from the cache
    // So that the next load gets the current instance from the underlying catalog
    TableIdentifier id = genTableID(dbName, tableName);
    catalog.invalidateTable(id);
    Table table = catalog.loadTable(id);
    if (table.currentSnapshot().snapshotId() != snapshotID)
      throw new IllegalStateException(
          "Iceberg Table has been updated since reading. Can not complete MERGE INTO");

    List<DataFileInfo> fileInfos = DataFileInfo.fromJson(newFileInfoJson);

    this.overwriteData(
        table.newTransaction(), table.spec(), table.sortOrder(), oldFileNames, fileInfos);
  }

  /** Insert data files into the table */
  public void addData(
      AppendFiles action, PartitionSpec spec, SortOrder order, List<DataFileInfo> fileInfos) {
    // Make sure to set the app-id field to "bodo" for easy identification
    action.set("app-id", "bodo");
    boolean isPartitionedPaths = spec.isPartitioned();

    for (DataFileInfo info : fileInfos) {
      DataFile dataFile = info.toDataFile(spec, order, isPartitionedPaths);
      action.appendFile(dataFile);
    }

    action.commit();
  }

  /** Overwrite Data Files with New Modified Versions */
  public void overwriteData(
      Transaction transaction,
      PartitionSpec spec,
      SortOrder order,
      List<String> oldFileNames,
      List<DataFileInfo> newFiles) {
    boolean isPartitionedPaths = spec.isPartitioned();

    // Data Files should be uniquely identified by path only. Other values should
    // not matter
    DeleteFiles delAction = transaction.newDelete();
    // Make sure to set the app-id field to "bodo" for easy identification
    delAction.set("app-id", "bodo");
    for (String oldFileName : oldFileNames) {
      delAction.deleteFile(oldFileName);
    }
    delAction.commit();

    AppendFiles action = transaction.newAppend();
    // Make sure to set the app-id field to "bodo" for easy identification
    action.set("app-id", "bodo");
    for (DataFileInfo newFile : newFiles) {
      action.appendFile(newFile.toDataFile(spec, order, isPartitionedPaths));
    }
    action.commit();

    transaction.commitTransaction();
  }
}
