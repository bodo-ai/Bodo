parameters:
- name: name
  type: string
  default: ''
- name: matrix
  type: object
  default: []
- name: CONDA_ENV
  type: string
  default: ''
- name: useNumbaDev
  type: boolean
  default: false
- name: caching_test
  type: boolean
  default: false
- name: BODO_TESTING_PIPELINE_HAS_MULTI_RANK_TEST
  type: boolean
  default: false

jobs:
- job: ${{ parameters.name }}
  timeoutInMinutes: 360
  variables:
    - group: AWS-427-S3-Access-Keys
    - group: SnowflakeCredentials
    - group: TabularCredentials
    - name: CONDA_ENV
      value: 'bodo'
  pool: ScalingVMSet
  strategy:
    matrix:
      ${{ insert }}: ${{ parameters.matrix }}

  steps:
  # TODO: Use Micromamba when it supports remote_max_retries
  - script: |
      set -exo pipefail
      curl -L -O "https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh"
      bash Mambaforge-$(uname)-$(uname -m).sh -b
    displayName: 'Install Mambaforge'

  - bash: echo "##vso[task.prependpath]$HOME/mambaforge/bin"
    displayName: Add Mamba to PATH

  - script: |
      conda config --set remote_max_retries 5
      conda config --set remote_backoff_factor 60
      # Ensure that conda/mamba install/update/remove commands
      # do not try to mutate the environment beyond what's required.
      conda config --add aggressive_update_packages nodefaults
    displayName: 'Set Conda Config'

  - script: |
      source activate base
      mamba install -y pip
      pip install pipx
      pipx run conda-lock install --dev -n ${{ parameters.CONDA_ENV }} $(System.DefaultWorkingDirectory)/buildscripts/envs/conda-lock.yml
    displayName: 'Create Mamba Env'
    retryCountOnTaskFailure: 5

  - script: |
      set -eo pipefail
      source activate ${{ parameters.CONDA_ENV }}
      set -x
      mamba install -y numba -c numba/label/dev -c conda-forge
    displayName: 'Install Numba Dev'
    condition: and(succeeded(), eq('${{ parameters.useNumbaDev }}', 'true'))

  - download: current
    artifact: Bodo-CondaPkg-Linux

  - script: |
      set -eo pipefail
      source activate ${{ parameters.CONDA_ENV }}
      set -x

      export BODO_VERSION=`python -m setuptools_scm`

      # -c $(Pipeline.Workspace)/Bodo-CondaPkg-Linux/ is used to install the package
      # from the locally built artifact in the previous step.
      # This works even if we build multiple versions of the package in the same pipeline.
      # --no-update-deps ensures that no dependencies are upgraded.
      # If the conda-lock file is out of date, this would cause this installation
      # to fail. To fix, we usually need to update the lock-file.
      mamba install -y bodo=$BODO_VERSION -c $(Pipeline.Workspace)/Bodo-CondaPkg-Linux/ -c conda-forge --no-update-deps
    displayName: 'Install Bodo'

  - script: |
      set -eo pipefail
      source activate ${{ parameters.CONDA_ENV }}
      set -x

      mkdir -p $HOME/bodo-inc
      cd $HOME/bodo-inc
      mkdir bodo
      unamestr=`uname`
      if [[ "$unamestr" == 'Linux' ]]; then
        cp -avr $(System.DefaultWorkingDirectory)/bodo/tests bodo
      else
        cp -r $(System.DefaultWorkingDirectory)/bodo/tests bodo
      fi
    displayName: 'Copy Test Data'

  - script: |
      buildscripts/setup_minio.sh
    displayName: 'Setup Minio'
    retryCountOnTaskFailure: 5

  - script: |
      set -eo pipefail
      source activate ${{ parameters.CONDA_ENV }}
      set -x

      wget -q -O - "https://adlsresources.blob.core.windows.net/adlsresources/hadoop-3.3.2.tar.gz" | sudo tar -xzf - -C /opt
      cd azurefs-sas-token-provider
      python setup.py develop
    displayName: "Setup Snowflake ADLS Testing (only Linux)"
    retryCountOnTaskFailure: 5

  - script: |
      set -eo pipefail
      source activate ${{ parameters.CONDA_ENV }}
      set -x

      cd iceberg
      pip install -v .
    displayName: "Build Iceberg Connector"
    retryCountOnTaskFailure: 5

  - script: |
      set -exo pipefail

      source activate base
      mamba install -y -c conda-forge unzip

      if [[ "$(uname)" == 'Linux' ]]; then
        wget https://download.oracle.com/otn_software/linux/instantclient/215000/instantclient-basic-linux.x64-21.5.0.0.0dbru.zip
        sudo $(which unzip) instantclient-basic-linux.x64-21.5.0.0.0dbru.zip -d /usr/local/lib
      else
        wget https://download.oracle.com/otn_software/mac/instantclient/198000/instantclient-basic-macos.x64-19.8.0.0.0dbru.zip
        sudo $(which unzip) instantclient-basic-macos.x64-19.8.0.0.0dbru.zip -d /usr/local/lib
      fi
    displayName: 'Setup Oracle Database Testing'
    retryCountOnTaskFailure: 5

  - script: |
      set -eo pipefail
      source activate ${{ parameters.CONDA_ENV }}
      set -x

      mkdir env
      mamba env export | tee env/environment.yml
      mamba list --explicit > env/package-list.txt
      pip list | tee env/requirements.txt
    continueOnError: true
    displayName: Export Environment Spec

  - publish: $(System.DefaultWorkingDirectory)/env
    artifact: TestEnv $(Agent.JobName)
    continueOnError: true

  - task: DownloadSecureFile@1
    name: testLicense
    inputs:
      secureFile: 'bodo.lic'
    displayName: 'Download Test License'

  - script: |
      set -eo pipefail
      source activate ${{ parameters.CONDA_ENV }}
      set -x

      sudo chmod a+r $(testLicense.secureFilePath)
      sudo cp $(testLicense.secureFilePath) $HOME/bodo-inc

      unamestr=`uname`
      if [[ "$unamestr" == 'Linux' ]]; then
        export LD_LIBRARY_PATH=/usr/local/lib/instantclient_21_5:$LD_LIBRARY_PATH
      else
        export LD_LIBRARY_PATH=/usr/local/lib/instantclient_19_8:$LD_LIBRARY_PATH
      fi

      if [[ "$unamestr" == 'Linux' ]]; then
        echo "Setting up Hadoop (and Arrow) environment variables"
        export HADOOP_HOME=/opt/hadoop-3.3.2
        export HADOOP_INSTALL=$HADOOP_HOME
        export HADOOP_MAPRED_HOME=$HADOOP_HOME
        export HADOOP_COMMON_HOME=$HADOOP_HOME
        export HADOOP_HDFS_HOME=$HADOOP_HOME
        export YARN_HOME=$HADOOP_HOME
        export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
        export HADOOP_OPTS='-Djava.library.path=$HADOOP_HOME/lib'
        export HADOOP_OPTIONAL_TOOLS=hadoop-azure
        export ARROW_LIBHDFS_DIR=$HADOOP_HOME/lib/native
        export CLASSPATH=`$HADOOP_HOME/bin/hdfs classpath --glob`
      else
        echo "Skipping hadoop/arrow env var setup"
      fi

      # For debugging purposes
      export BODO_SF_WRITE_DEBUG=1

      cd $HOME/bodo-inc
      cp `python -c "import bodo; print(bodo.__file__[:-11])"`pytest.ini .
      # For caching tests we need to update the location of the decryption file.
      # We set the absolute path as an environment variable.
      export BODO_TRACING_DECRYPTION_FILE_PATH=`echo "$(System.DefaultWorkingDirectory)/obfuscation/decompress_traces.py"`
      if [[ "${{ parameters.caching_test }}" == "True" ]]; then
        python -m bodo.runtests_caching "${{ parameters.name }}" "$NP" bodo/tests/caching_tests
      else
        python -m bodo.runtests "${{ parameters.name }}" "$NP" --pyargs bodo -s -v -m "$(PYTEST_MARKER)"
      fi
    env:
      SF_USERNAME: $(SNOWFLAKE_USER)
      SF_PASSWORD: $(SNOWFLAKE_PASSWORD)
      SF_USER2: $(SNOWFLAKE_USER2)
      SF_PASSWORD2: $(SNOWFLAKE_PASSWORD2)
      SF_AZURE_USER: $(SF_AZURE_USER)
      SF_AZURE_PASSWORD: $(SF_AZURE_PASSWORD)
      AZURE_STORAGE_ACCOUNT_NAME: $(AZURE_ICEBERG_STORAGE_ACCOUNT)
      AZURE_STORAGE_ACCOUNT_KEY: $(AZURE_ICEBERG_ACCESS_KEY)
      AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
      AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
      BODO_TESTING_PIPELINE_HAS_MULTI_RANK_TEST: ${{ parameters.BODO_TESTING_PIPELINE_HAS_MULTI_RANK_TEST }}
      TABULAR_CREDENTIAL: $(TABULAR_CREDENTIAL)
    displayName: 'Test Bodo'

  - script: |
      set -eo pipefail
      export IS_RELEASE=`git tag --points-at HEAD`
      if [[ -n "$IS_RELEASE" ]]; then
        source activate ${{ parameters.CONDA_ENV }}
        git clone https://github.com/Bodo-inc/Bodo-examples.git
        cd Bodo-examples/06-Compare-Bodo-with-Spark/tpch/
        # Invalid env variables for AWS credentials in the pipeline, s3 access is denied unless we run it by resetting them
        AWS_ACCESS_KEY_ID="" AWS_SECRET_ACCESS_KEY="" mpiexec -n 4 python bodo_queries.py --folder s3://bodo-example-data/tpch/SF1
      fi
    displayName: 'Run Bodo Examples for Major Release'
