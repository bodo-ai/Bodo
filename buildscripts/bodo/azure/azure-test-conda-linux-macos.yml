parameters:
- name: name
  type: string
  default: ''
- name: matrix
  type: object
  default: []
- name: test_type
  type: string
  default: COMPILER
  values:
    - COMPILER
    - SPAWN
    - CACHE
    - DF_LIB
- name: pool_name
  type: string
  default: 'ScalingVMSet'

jobs:
- job: ${{ parameters.name }}
  timeoutInMinutes: 360
  variables:
    - group: AWS-427-S3-Access-Keys
    - group: SnowflakeCredentials
    - group: AzureCredentials
    - group: SQLDBCredentials
  pool: ${{ parameters.pool_name }}
  strategy:
    matrix:
      ${{ insert }}: ${{ parameters.matrix }}

  steps:
  - bash: |
      set -exo pipefail
      curl -fsSL https://pixi.sh/install.sh | bash
      source ~/.bashrc
      echo "##vso[task.prependpath]$HOME/.pixi/bin"
    displayName: Install Pixi
  - bash: pixi install -v --locked -e azure
    retryCountOnTaskFailure: 3
    displayName: Install Test Environment

  - bash: |
      set -exo pipefail
      pixi global install unzip
      which unzip

      if [[ "$(uname)" == 'Linux' ]]; then
        if [[ "$(uname -m)" == "x86_64" ]]; then
          wget https://download.oracle.com/otn_software/linux/instantclient/215000/instantclient-basic-linux.x64-21.5.0.0.0dbru.zip
          sudo $(which unzip) instantclient-basic-linux.x64-21.5.0.0.0dbru.zip -d /usr/local/lib
        else
          wget https://download.oracle.com/otn_software/linux/instantclient/1925000/instantclient-basic-linux.arm64-19.25.0.0.0dbru.zip
          sudo $(which unzip) instantclient-basic-linux.arm64-19.25.0.0.0dbru.zip -d /usr/local/lib
        fi
      else
        wget https://download.oracle.com/otn_software/mac/instantclient/198000/instantclient-basic-macos.x64-19.8.0.0.0dbru.zip
        sudo $(which unzip) instantclient-basic-macos.x64-19.8.0.0.0dbru.zip -d /usr/local/lib
      fi
    displayName: 'Setup Oracle Database Testing'
    condition: ne(variables['Agent.OS'], 'Windows_NT')
    retryCountOnTaskFailure: 5

  - bash: |
      set -exo pipefail

      pixi run -e azure build-bodo
      pixi run -e azure build-iceberg
      pixi run -e azure -- sccache --show-stats
    env:
      AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
      AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
      DISABLE_CCACHE: 1  # Just use sccache directly on CI
    displayName: 'Build Bodo & Iceberg Connector'

  - bash: |
      pixi list
      pixi run pip list
    continueOnError: true
    displayName: Export Environment Spec

  - bash: |
      set -exo pipefail
      sudo apt-get update
      sudo apt-get install -y docker.io
      sudo service docker start
      sudo chmod 666 /var/run/docker.sock
      sudo usermod -aG docker $USER
    displayName: Install Docker
    retryCountOnTaskFailure: 5

  - bash: |
      set -exo pipefail

      pixi run -e azure pip install transformers
      # This isn't published on pypi so no good way to list it as a dependency
      pixi run -e azure pip install --no-deps 'git+https://github.com/apache/polaris.git@release/1.0.x#subdirectory=client/python'

      unamestr=`uname`
      if [[ "$unamestr" == 'Linux' ]]; then
        export LD_LIBRARY_PATH=/usr/local/lib/instantclient_21_5:$LD_LIBRARY_PATH
      else
        export LD_LIBRARY_PATH=/usr/local/lib/instantclient_19_8:$LD_LIBRARY_PATH
      fi

      if [[ "$unamestr" == 'Linux' ]]; then
        echo "Setting up Hadoop (and Arrow) environment variables"
        export HADOOP_HOME=/opt/hadoop-3.4.0
        export HADOOP_INSTALL=$HADOOP_HOME
        export HADOOP_MAPRED_HOME=$HADOOP_HOME
        export HADOOP_COMMON_HOME=$HADOOP_HOME
        export HADOOP_HDFS_HOME=$HADOOP_HOME
        export YARN_HOME=$HADOOP_HOME
        export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
        export HADOOP_OPTS='-Djava.library.path=$HADOOP_HOME/lib'
        export HADOOP_OPTIONAL_TOOLS=hadoop-azure
        export ARROW_LIBHDFS_DIR=$HADOOP_HOME/lib/native
        export CLASSPATH=`$HADOOP_HOME/bin/hdfs classpath --glob`
      elif [[ "$(Agent.OS)" == "Windows_NT" ]]; then
        # On Windows we set HADOOP_HOME to a dummy directory.
        # Spark needs to verify HADOOP_HOME exists at initialization even if it is not used.
        export HADOOP_HOME="$(System.DefaultWorkingDirectory)/buildscripts/local_utils/hadoop_dummy"
        export PATH=$HADOOP_HOME/bin:$PATH
        # Visual Studio 2010 DLLs are needed for winutils.exe
        curl -LO https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x64.exe
        ./vcredist_x64.exe -passive
      else
        echo "Skipping hadoop/arrow env var setup"
      fi

      cd "$(System.DefaultWorkingDirectory)"

      # For caching tests we need to update the location of the decryption file.
      # We set the absolute path as an environment variable.
      export BODO_TRACING_DECRYPTION_FILE_PATH=`echo "$(System.DefaultWorkingDirectory)/buildscripts/decompress_traces.py"`

      pixi_azure() {
        if [[ "$(uname -m)" == "aarch64" ]]; then
          # Preload libgomp on aarch64 to avoid dynamic loader issues in Sklearn tests:
          # https://stackoverflow.com/questions/61065541/importerror-usr-lib-aarch64-linux-gnu-libgomp-so-1-cannot-allocate-memory-in
          pixi run -e azure env LD_PRELOAD=libgomp.so.1 "$@"
        else
          pixi run -e azure "$@"
        fi
      }

      # Ignore streaming/caching/metadata tests in Dataframes/Spawn mode to avoid importing JIT during test collection.
      PYTEST_IGNORE="--ignore bodo/tests/test_streaming --ignore bodo/tests/caching_tests"
      if [[ "${{ parameters.test_type }}" == "CACHE" ]]; then
        pixi_azure python -m bodo.runtests_caching "${{ parameters.name }}" "$NP" bodo/tests/caching_tests
      elif [[ "${{ parameters.test_type }}" == "SPAWN" ]]; then
        # Disabling the DataFrame library for spawn tests since some of the tests
        # create Pandas manager states for testing that are not fully functional.
        export BODO_ENABLE_DATAFRAME_LIBRARY=0
        export BODO_TEST_SPAWN_MODE=1
        export BODO_NUM_WORKERS="$NP"
        pixi_azure pytest -s -v -Wignore -m "$(PYTEST_MARKER)" $PYTEST_IGNORE \
          bodo/tests --junitxml=pytest-report-spawn-mode.xml --test-run-title="${{ parameters.name }}"
      elif [[ "${{ parameters.test_type }}" == "DF_LIB" ]]; then
        export BODO_ENABLE_TEST_DATAFRAME_LIBRARY=1
        export BODO_NUM_WORKERS="$NP"
        pixi_azure pytest -s -v -Wignore -m "$(PYTEST_MARKER) and not jit_dependency" $PYTEST_IGNORE \
          bodo/tests --junitxml=pytest-report-df-library.xml --test-run-title="${{ parameters.name }}"
        # Run tests with JIT dependency separately to avoid JIT imports impacting other tests.
        pixi_azure pytest -s -v -Wignore -m "$(PYTEST_MARKER) and jit_dependency" $PYTEST_IGNORE \
          bodo/tests --junitxml=pytest-report-df-library-jit-dependency.xml --test-run-title="${{ parameters.name }} - JIT"
      else
        # Some tests assume a batch size of 4096 for testing specific behavior.
        export BODO_STREAMING_BATCH_SIZE=4096 
        pixi_azure python -m bodo.runtests "${{ parameters.name }}" "$NP" --pyargs bodo -s -v -m "$(PYTEST_MARKER)"
      fi
    env:
      SF_USERNAME: $(SNOWFLAKE_USER)
      SF_PASSWORD: $(SNOWFLAKE_PASSWORD)
      SF_USER2: $(SNOWFLAKE_USER2)
      SF_PASSWORD2: $(SNOWFLAKE_PASSWORD2)
      SF_AZURE_USER: $(SF_AZURE_USER)
      SF_AZURE_PASSWORD: $(SF_AZURE_PASSWORD)
      AZURE_STORAGE_ACCOUNT_NAME: $(AZURE_ICEBERG_STORAGE_ACCOUNT)
      AZURE_STORAGE_ACCOUNT_KEY: $(AZURE_ICEBERG_ACCESS_KEY)
      AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
      AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
      AZURE_CLIENT_ID: $(AZURE_CLIENT_ID)
      AZURE_CLIENT_SECRET: $(AZURE_CLIENT_SECRET)
      AZURE_TENANT_ID: "ac373ae0-dc77-4cbb-bbb7-deddcf6133b3"
      BODO_TEST_SQL_DB_CREDENTIAL: $(SQL_DB_CREDENTIAL)
      BODO_TEST_ORACLE_DB_CREDENTIAL: $(ORACLE_DB_CREDENTIAL)
      BODO_SPAWN_MODE: "0"
      BODO_BUFFER_POOL_REMOTE_MODE: "1"
      BODO_BUFFER_POOL_DEBUG_MODE: "1"
      # For debugging purposes
      BODO_SF_WRITE_DEBUG: "1"
    displayName: 'Test Bodo'
