parameters:
- name: name
  type: string
  default: ''
- name: matrix
  type: object
  default: []
- name: pool_name
  type: string
  default: 'ScalingVMSet'
- name: CACHE_TEST
  type: boolean
  default: false
- name: SPAWN_MODE_TEST
  type: boolean
  default: false

jobs:
- job: ${{ parameters.name }}
  variables:
    - group: AWS-427-S3-Access-Keys
    - group: SnowflakeCredentials
    - group: AzureCredentials
  timeoutInMinutes: 360
  pool: ${{ parameters.pool_name }}
  strategy:
    matrix:
      ${{ insert }}: ${{ parameters.matrix }}

  steps:
  - bash: |
      set -exo pipefail
      curl -fsSL https://pixi.sh/install.sh | bash
      source ~/.bashrc
      echo "##vso[task.prependpath]$HOME/.pixi/bin"
    displayName: Install Pixi
  - bash: pixi install -v --locked -e azure
    retryCountOnTaskFailure: 3
    displayName: Install Test Environment

  - bash: |
      set -exo pipefail

      unamestr=`uname`
      if [[ "$unamestr" == 'Linux' ]]; then
        export USE_BODO_ARROW_FORK=1
      fi

      pixi run -e azure build-bodo -Cbuild.verbose=true
      pixi run -e azure build-bodosql
      pixi run -e azure build-iceberg
      pixi run -e azure -- sccache --show-stats
    env:
      AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
      AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
      DISABLE_CCACHE: 1  # Just use sccache directly on CI
    displayName: 'Build Bodo, BodoSQL, and Iceberg Connector'

  - bash: |
      set -exo pipefail
      eval "$(pixi shell-hook --shell bash -e azure)"

      wget -q -O - "https://adlsresources.blob.core.windows.net/adlsresources/hadoop-3.3.2.tar.gz" | sudo tar -xzf - -C /opt
      cd azurefs-sas-token-provider
      pip install --no-deps --no-build-isolation -ve .
    displayName: "Setup Snowflake ADLS Testing (only Linux)"
    retryCountOnTaskFailure: 5
    condition: ne(variables['Agent.OS'], 'Windows_NT')

  - bash: |
      pixi list
      pixi run pip list
    continueOnError: true
    displayName: Export Environment Spec

  - bash: |
      set -exo pipefail
      sudo apt-get install -y docker.io
      sudo service docker start
      sudo chmod 666 /var/run/docker.sock
      sudo usermod -aG docker $USER
    displayName: Install Docker
    retryCountOnTaskFailure: 5

  - script: |
      set -exo pipefail

      pixi run -e azure python -c "import bodo; print(bodo.__version__)"
      pixi run -e azure python -c "import bodosql; print(bodosql.__version__)"

      # This isn't published on pypi so no good way to list it as a dependency
      pixi run -e azure pip install 'git+https://github.com/apache/polaris.git#subdirectory=regtests/client/python'

      unamestr=`uname`
      if [[ "$unamestr" == 'Linux' ]]; then
        echo "Setting up Hadoop (and Arrow) environment variables"
        export HADOOP_HOME=/opt/hadoop-3.3.2
        export HADOOP_INSTALL=$HADOOP_HOME
        export HADOOP_MAPRED_HOME=$HADOOP_HOME
        export HADOOP_COMMON_HOME=$HADOOP_HOME
        export HADOOP_HDFS_HOME=$HADOOP_HOME
        export YARN_HOME=$HADOOP_HOME
        export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
        export HADOOP_OPTS='-Djava.library.path=$HADOOP_HOME/lib'
        export HADOOP_OPTIONAL_TOOLS=hadoop-azure
        export ARROW_LIBHDFS_DIR=$HADOOP_HOME/lib/native
        export CLASSPATH=`$HADOOP_HOME/bin/hdfs classpath --glob`
      elif [[ "$(Agent.OS)" == "Windows_NT" ]]; then
        # On Windows we set HADOOP_HOME to a dummy directory.
        # Spark needs to verify HADOOP_HOME exists at initialization even if it is not used.
        export HADOOP_HOME="$(System.DefaultWorkingDirectory)/buildscripts/local_utils/hadoop_dummy"
        export PATH=$HADOOP_HOME/bin:$PATH
        # Visual Studio 2010 DLLs are needed for winutils.exe
        curl -LO https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x64.exe
        ./vcredist_x64.exe -passive
      else
        echo "Skipping hadoop/arrow env var setup"
      fi

      cd "$(System.DefaultWorkingDirectory)/BodoSQL"
      # For caching tests we need to update the location of the decryption file.
      # We set the absolute path as an environment variable.
      export BODO_TRACING_DECRYPTION_FILE_PATH=`echo "$(System.DefaultWorkingDirectory)/buildscripts/decompress_traces.py"`
      if [[ "${{ parameters.CACHE_TEST }}" == "True" ]]; then
        pixi run -e azure python -u -m bodosql.runtests_caching "${{ parameters.name }}" "$(NP)" bodosql/tests/caching_tests/
      elif [[ "${{ parameters.SPAWN_MODE_TEST }}" == "True" ]]; then
        export BODO_TEST_SPAWN_MODE=1
        export BODO_NUM_WORKERS="$NP"
        pixi run -e azure pytest -s -v -p no:faulthandler -Wignore -m "$(PYTEST_MARKER)" bodosql/tests --junitxml=pytest-report-sql-spawn-mode.xml --test-run-title="${{ parameters.name }}"
      else
        pixi run -e azure python -u -m bodosql.runtests "${{ parameters.name }}" "$(NP)" -s -v -p no:faulthandler -m "$(PYTEST_MARKER)" bodosql/tests/
      fi
    env:
      SF_USERNAME: $(SNOWFLAKE_USER)
      SF_PASSWORD: $(SNOWFLAKE_PASSWORD)
      SF_AZURE_USER: $(SF_AZURE_USER)
      SF_AZURE_PASSWORD: $(SF_AZURE_PASSWORD)
      AZURE_CLIENT_ID: $(AZURE_CLIENT_ID)
      AZURE_CLIENT_SECRET: $(AZURE_CLIENT_SECRET)
      AZURE_STORAGE_ACCOUNT_NAME: $(AZURE_ICEBERG_STORAGE_ACCOUNT)
      AZURE_STORAGE_ACCOUNT_KEY: $(AZURE_ICEBERG_ACCESS_KEY)
      AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
      AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
      AWS_REGION: us-east-1
      BODO_STREAMING_ENABLED: 1
      BODO_SPAWN_MODE: "0"
      BODO_TESTING_PIPELINE_HAS_MULTI_RANK_TEST: "1"
      BODO_BUFFER_POOL_REMOTE_MODE: "1"
      BODO_BUFFER_POOL_DEBUG_MODE: "1"
    displayName: 'Test BodoSQL'
