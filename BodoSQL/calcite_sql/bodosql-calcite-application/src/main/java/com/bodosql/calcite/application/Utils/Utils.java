package com.bodosql.calcite.application.Utils;

import com.bodosql.calcite.application.BodoSQLCodegenException;
import java.util.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.apache.calcite.rel.core.AggregateCall;
import org.apache.calcite.rex.RexCall;
import org.apache.calcite.rex.RexLiteral;
import org.apache.calcite.rex.RexOver;
import org.apache.calcite.sql.SqlKind;
import org.apache.calcite.sql.type.SqlTypeName;

/** Class filled with static utility functions. */
public class Utils {

  // Name of Dummy Colnames for Bodo Intermediate operations
  private static final String dummyColNameBase = "__bodo_dummy__";

  // two space indent
  private static final String bodoIndent = "  ";

  /** Function used to return the standard indent used within BodoSql */
  public static String getBodoIndent() {
    return bodoIndent;
  }

  /**
   * Function to return the baseDummyColumnName. This should be extended with a counter if an
   * operation requires multiple dummy columns. NOTE: We assume dummy columns do not persist between
   * operations.
   *
   * @return dummyColNameBase
   */
  public static String getDummyColNameBase() {
    return dummyColNameBase;
  }

  /**
   * Function to check whether given variable name is generated by planner
   *
   * @param varName variable name to be checked
   * @return true if it is a variable generated by planner, false otherwise
   */
  public static boolean isPlannerGenVar(String varName) {
    return varName.contains("$");
  }

  /**
   * Function to enclose string in quotes
   *
   * @param unquotedString string to be enclosed
   * @return single quoted string
   */
  public static String makeQuoted(String unquotedString) {
    return '"' + unquotedString + '"';
  }

  /**
   * Function to convert a Java Hashmap of names into a Python dictionary for use in a
   * DataFrame.rename(columns) calls.
   */
  public static String renameColumns(HashMap<String, String> colMap) {
    StringBuilder dictStr = new StringBuilder();
    dictStr.append("{");
    // Generate a sorted version of the map so the same code is always
    // generated on all nodes
    TreeMap<String, String> sortedMap = new TreeMap<>(colMap);
    for (String prv : sortedMap.keySet()) {
      dictStr.append(makeQuoted(prv));
      dictStr.append(": ");
      dictStr.append(makeQuoted(colMap.get(prv)));
      dictStr.append(", ");
    }
    dictStr.append("}");
    return dictStr.toString();
  }

  /**
   * Escapes " so Python interprets String correctly.
   *
   * @param inputStr String possibly containing "
   * @return String with quotes properly escaped.
   */
  public static String escapePythonQuotes(String inputStr) {
    return inputStr.replaceAll("(?<!\\\\)\"", "\\\\\"");
  }

  /**
   * Takes a set of columns, an if result, and an else result and generates code that returns the if
   * result if any column is NULL and otherwise the else case.
   *
   * @param inputVar the input table which the columns reference
   * @param colSet Set of columns names
   * @param ifCase Code to return if any column is null.
   * @param elseCase Code to return if no column is null.
   * @return A string representing the code generated to check if any of the columns is null.
   */
  public static String generateNullCheck(
      String inputVar, HashSet<String> colSet, String ifCase, String elseCase) {
    if (colSet.size() == 0) {
      return elseCase;
    }
    StringBuilder result = new StringBuilder();
    result.append("(").append(ifCase).append(" if ");
    result.append(checkNullColumns(inputVar, colSet));
    result.append(" else ").append(elseCase).append(")");
    return result.toString();
  }

  /**
   * Generates code for checking if any column in a set is null.
   *
   * @param inputVar Name of the input table, which the columns reference
   * @param colSet Set of column names
   * @return A string representing the code generated to check if any of the columns is null.
   */
  public static String checkNullColumns(String inputVar, HashSet<String> colSet) {
    if (colSet.size() == 0) {
      return "";
    }
    StringBuilder nullCheck = new StringBuilder();
    nullCheck.append("(");
    // Convert to a sorted set so the same code is generated
    // on very core.
    TreeSet<String> sortedColSet = new TreeSet<>(colSet);
    for (String col : sortedColSet) {
      nullCheck.append(String.format("pd.isna(%s) or ", inputVar + "[" + makeQuoted(col) + "]"));
    }
    // Remove the final OR
    nullCheck.delete(nullCheck.length() - 3, nullCheck.length());
    nullCheck.append(")");
    return nullCheck.toString();
  }

  /**
   * Generates code for checking that no column in a set is null.
   *
   * @param inputVar Name of the input table, which the columns reference
   * @param colSet Set of column names
   * @return A string representing the code generated that no column is null.
   */
  public static String checkNotNullColumns(String inputVar, HashSet<String> colSet) {
    if (colSet.size() == 0) {
      return "";
    }
    StringBuilder nullCheck = new StringBuilder();
    nullCheck.append("(");
    // Convert to a sorted set so the same code is generated
    // on very core.
    TreeSet<String> sortedColSet = new TreeSet<>(colSet);
    for (String col : sortedColSet) {
      nullCheck.append(String.format("pd.notna(%s) and ", inputVar + "[" + makeQuoted(col) + "]"));
    }
    // Remove the final AND
    nullCheck.delete(nullCheck.length() - 4, nullCheck.length());
    nullCheck.append(")");
    return nullCheck.toString();
  }

  /**
   * Function to convert a SQL type to a matching Pandas type.
   *
   * @param typeName SQL Type.
   * @param outputScalar Should the output generate a type for converting scalars.
   * @return The pandas type
   */
  public static String sqlTypenameToPandasTypename(SqlTypeName typeName, boolean outputScalar) {
    String dtype;
    switch (typeName) {
      case BOOLEAN:
        if (outputScalar) {
          dtype = "bodosql.libs.generated_lib.sql_null_checking_scalar_conv_bool";
        } else {
          dtype = makeQuoted("boolean");
        }
        break;
      case TINYINT:
        if (outputScalar) {
          dtype = "bodosql.libs.generated_lib.sql_null_checking_scalar_conv_int8";
        } else {
          dtype = "pd.Int8Dtype()";
        }
        break;
      case SMALLINT:
        if (outputScalar) {
          dtype = "bodosql.libs.generated_lib.sql_null_checking_scalar_conv_int16";
        } else {
          dtype = "pd.Int16Dtype()";
        }
        break;
      case INTEGER:
        if (outputScalar) {
          dtype = "bodosql.libs.generated_lib.sql_null_checking_scalar_conv_int32";
        } else {
          dtype = "pd.Int32Dtype()";
        }
        break;
      case BIGINT:
        if (outputScalar) {
          dtype = "bodosql.libs.generated_lib.sql_null_checking_scalar_conv_int64";
        } else {
          dtype = "pd.Int64Dtype()";
        }
        break;
      case FLOAT:
        dtype = "np.float32";
        break;
      case DOUBLE:
      case DECIMAL:
        dtype = "np.float64";
        break;
      case DATE:
      case TIMESTAMP:
        if (outputScalar) {
          // pd.to_datetime(None) returns None in standard python, but not in Bodo
          // This should likely be in the engine itself, to match pandas behavior
          // BE-2882
          dtype = "pd.to_datetime";
        } else {
          dtype = "np.dtype('datetime64[ns]')";
        }
        break;
      case VARCHAR:
      case CHAR:
        if (outputScalar) {
          dtype = "bodosql.libs.generated_lib.sql_null_checking_scalar_conv_str";
        } else {
          dtype = "str";
        }
        break;
      case INTERVAL_DAY_HOUR:
      case INTERVAL_DAY_MINUTE:
      case INTERVAL_DAY_SECOND:
      case INTERVAL_HOUR_MINUTE:
      case INTERVAL_HOUR_SECOND:
      case INTERVAL_MINUTE_SECOND:
      case INTERVAL_HOUR:
      case INTERVAL_MINUTE:
      case INTERVAL_SECOND:
      case INTERVAL_DAY:
        if (outputScalar) {
          // pd.to_timedelta(None) returns None in standard python, but not in Bodo
          // This should likely be in the engine itself, to match pandas behavior
          // BE-2882
          dtype = "pd.to_timedelta";
        } else {
          dtype = "np.dtype('timedelta64[ns]')";
        }
        break;
      case INTERVAL_YEAR:
      case INTERVAL_MONTH:
      case INTERVAL_YEAR_MONTH:
        // May later refactor this code to create DateOffsets, for now
        // causes an error
      default:
        throw new BodoSQLCodegenException(
            "Internal Error: Calcite Plan Produced an Unsupported Type: " + typeName.getName());
    }
    return dtype;
  }

  /**
   * In Bodo, when you get an scalar value from a column, the value may be of a non-nullable type
   * but Still have a null value. This code will wrap scalar a column reference, converting null
   * values with non-nullable types to null values with null types.
   *
   * @param colExpr The string value that represents the column expression
   * @return A string representing the column expression,
   */
  public static String nullcheckScalarColumnReference(String colExpr) {
    return "(None if pd.isna(" + colExpr + ") else " + colExpr + ")";
  }

  /**
   * Calcite optimizes a large number of windowed aggregation functions into case statements, which
   * check if the window size is valid. This checks if the supplied node is one of those case
   * statements.
   *
   * <p>The rough location in which this occurs within calcite is here:
   * https://github.com/apache/calcite/blob/master/core/src/main/java/org/apache/calcite/sql2rel/SqlToRelConverter.java#L2081
   * I am still trying to find the exact location where this translation into case statments occurs.
   *
   * @param node the case node to check
   * @return true if it is a wrapped windowed aggregation function, and False if it is not
   */
  public static boolean isWindowedAggFn(RexCall node) {
    // First, we expect exactly three operands in the case statement
    if (node.getOperands().size() != 3) {
      return false;
    }
    return isEmptyWindowCheck(node) || windowLen1Check(node);
  }

  /**
   * Calcite optimizes a large number of windowed aggregation functions into case statements, which
   * check if the window size is valid. This checks if the rexcall is a windowed aggregation
   * function checking that the size of the window is 0.
   *
   * @param node the rexCall on which to perform the check
   * @return Boolean determining if a rexcall is in fact a windowed aggregation with an empty window
   *     check
   */
  public static boolean isEmptyWindowCheck(RexCall node) {
    // For arg0 (when case), we expect a comparison to the size of the window
    boolean arg0IsWindowSizeComparison =
        node.getOperands().get(0) instanceof RexCall
            && ((RexCall) node.getOperands().get(0)).getOperator().getKind() == SqlKind.GREATER_THAN
            && ((RexCall) node.getOperands().get(0)).getOperands().get(0) instanceof RexOver;
    // For arg1 (then case), we expect a windowed aggregation function
    boolean arg1IsWindowed = node.getOperands().get(1) instanceof RexOver;
    // For the else case, we expect NULL
    boolean arg2Null = node.getOperands().get(2) instanceof RexLiteral;

    return arg0IsWindowSizeComparison && arg1IsWindowed && arg2Null;
  }

  /**
   * Calcite optimizes a large number of windowed aggregation functions into case statements, which
   * check if the window size is valid. This checks if the input rexcall is a windowed aggregation
   * function checking that the size of the window is 1.
   *
   * @param node the rexCall on which to perform the check
   * @return Boolean determining if a rexcall is in fact a windowed aggregation with a window size 1
   *     check
   */
  public static boolean windowLen1Check(RexCall node) {
    // For arg0 (when case), we expect a comparison to the size of the window
    boolean arg0IsWindowSizeComparison =
        node.getOperands().get(0) instanceof RexCall
            && ((RexCall) node.getOperands().get(0)).getOperator().getKind() == SqlKind.EQUALS
            && ((RexCall) node.getOperands().get(0)).getOperands().get(0) instanceof RexOver;
    // For arg1 (then case), we expect NULL
    boolean arg1IsWindowed = node.getOperands().get(1) instanceof RexLiteral;
    // For the else case, we expect a windowed aggregation function
    //    boolean arg2Null = node.getOperands().get(2) instanceof RexOver;

    return arg0IsWindowSizeComparison && arg1IsWindowed;
  }

  /**
   * Helper function, takes the existing column names and a hashset of columns to add, and returns a
   * new dataframe, consisting of both the new and old columns. Generally used immediately before
   * generating an apply.
   *
   * @param inputVar The name of the input dataframe, to which we add the new columns.
   * @param colNames The list of the columns already present in inputVar, which need to be present
   *     in the output dataframe
   * @param colsToAddList The List of pd.Series variables that must be added to new dataframe.
   * @return
   */
  public static String generateCombinedDf(
      String inputVar, List<String> colNames, List<String> colsToAddList) {
    // TODO filter out the columns that don't need to be kept
    StringBuilder newDf = new StringBuilder("pd.DataFrame({");
    for (String curCol : colNames) {
      newDf
          .append(makeQuoted(curCol))
          .append(":")
          .append(inputVar)
          .append("[")
          .append(makeQuoted(curCol))
          .append("], ");
    }
    for (String preGeneratedCol : colsToAddList) {
      newDf.append(makeQuoted(preGeneratedCol)).append(":").append(preGeneratedCol).append(", ");
    }
    newDf.append("})");
    return newDf.toString();
  }

  /**
   * Checks if a string is a legal name for a Python identifier
   *
   * @param name the string name that needs to be checked
   * @return Boolean for if the name matches the regex [A-Za-z_]\w*
   */
  public static boolean isValidPythonIdentifier(String name) {
    final Pattern p = Pattern.compile("[a-zA-Z_]\\w*");
    Matcher m = p.matcher(name);
    return m.matches();
  }

  public static String getInputColumn(List<String> inputColumnNames, AggregateCall a) {
    if (a.getArgList().isEmpty()) {
      // count(*) case
      // count(*) is turned into to count() by Calcite
      // in this case, we can use any column for aggregation, since inputColumnNames should
      // always contain at least one group by column, we use the first column for the
      // aggregation, and manually set the fieldname to *. However, count(*) includes
      // NULL values (whereas count does not).
      assert !inputColumnNames.isEmpty();
      return inputColumnNames.get(0);
    } else {
      return inputColumnNames.get(a.getArgList().get(0));
    }
  }

  /***
   * Searches the input expression for table references to oldTableName, and replaces them to reference the new Table
   *
   * @param expr The expression to replace table references
   * @param oldTableName The old table name, that the input expr uses for table references
   * @param newTableName The new table name, that the output expr will use for table references
   * @return
   */
  public static String renameTableRef(String expr, String oldTableName, String newTableName) {
    return expr.replaceAll(Pattern.quote(oldTableName + "["), newTableName + "[");
  }

  /***
   *  Renames a list of codeExpressions to use a new table reference
   *
   * @param codeExprs The list of expressions to replace table references
   * @param oldTableName The old table name, that the input expr uses for table references
   * @param newTableName The new table name, that the output expr will use for table references
   * @return
   */
  public static List<String> renameExprsList(
      List<String> codeExprs, String oldTableName, String newTableName) {
    List<String> outputExprs = new ArrayList<>();
    for (int i = 0; i < codeExprs.size(); i++) {
      outputExprs.add(renameTableRef(codeExprs.get(i), oldTableName, newTableName));
    }
    return outputExprs;
  }

  /***
   *  Renames a hashset of codeExpressions to use a new table reference
   *
   * @param codeExprs The hashset of expressions to replace table references
   * @param oldTableName The old table name, that the input expr uses for table references
   * @param newTableName The new table name, that the output expr will use for table references
   * @return
   */
  public static HashSet<String> renameExprsHashset(
      HashSet<String> codeExprs, String oldTableName, String newTableName) {
    HashSet<String> outputExprs = new HashSet<>();
    // In this case, we don't have to sort before iterating over the hashset, as the
    // output will still be sorted in the relevant check null function, meaning the
    // code generated on each rank will still be the same.
    for (String curExpr : codeExprs) {
      outputExprs.add(renameTableRef(curExpr, oldTableName, newTableName));
    }
    return outputExprs;
  }

  // TODO: use this interface in a later rewrite of generateDfApply
  interface OpsToLambdaStrFn {
    String makeLambdaStr(List<String> operandExpressions);
  }

  public static String generateDfApply(String inputVar, BodoCtx ctx, String lambdaFnStr) {
    // We assume, at this point, that the ctx.colsToAddList has been added to inputVar, and
    // the arguments/nullset have been renamed appropriately.
    // TODO: Do everything needed for df applies in this function, so it's more understandable.

    TreeSet<String> sortedParamSet = new TreeSet<>(ctx.getNamedParams());

    StringBuilder applyStr = new StringBuilder(inputVar + ".apply(lambda " + inputVar);

    for (String param : sortedParamSet) {
      applyStr.append(", ").append(param);
    }

    lambdaFnStr = generateNullCheck(inputVar, ctx.getNullSet(), "None", lambdaFnStr);
    applyStr.append(":").append(lambdaFnStr).append(", ");

    for (String param : sortedParamSet) {
      applyStr.append(param + "=" + param).append(", ");
    }
    applyStr.append("axis=1)");

    return applyStr.toString();
  }

  static HashSet<SqlTypeName> validDateCastTypes;

  static {
    validDateCastTypes = new HashSet<>();
    validDateCastTypes.addAll(SqlTypeName.STRING_TYPES);
    validDateCastTypes.addAll(SqlTypeName.DATETIME_TYPES);
  }

  public static Boolean valid_type_cast_to_date(SqlTypeName typ) {
    return validDateCastTypes.contains(typ);
  }

  public static void assertWithErrMsg(boolean test, String msg) {
    if (!test) {
      throw new RuntimeException(msg);
    }
  }
}
