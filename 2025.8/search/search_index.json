{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about_bodo/","title":"Bodo: Simple Python Acceleration and Scaling for Data and AI","text":"<p>Bodo Dataframe Library is a drop in replacement for pandas (<code>import bodo.pandas as pd</code>, a single line import change), which applies advanced High-Performance Computing (HPC) and database optimization techniques to significantly boost the performance of your existing Python code without code rewrites. Bodo also scales Python code from single laptops to large clusters and supercomputers automatically.</p> <p>In addition, Bodo's auto-parallelizing just-in-time (JIT) compiler transforms custom Python code using Pandas and Numpy into highly optimized, parallel binaries without requiring code rewrites.</p>"},{"location":"about_bodo/#technical-differentiation","title":"Technical Differentiation","text":"<p>Unlike traditional distributed computing frameworks, Bodo:</p> <ul> <li>Provides drop-in replacement for Pandas APIs.</li> <li>Automatically optimizes queries at database-grade levels and code at HPC compiler levels for maximum efficiency.</li> <li>Eliminates overheads common in driver-executor models by leveraging Message Passing Interface (MPI) for true distributed execution.</li> </ul>"},{"location":"about_bodo/#key-features","title":"Key Features","text":"<ul> <li>Automatic Optimization &amp; Parallelization: Converts standard Python programs using Pandas and NumPy into high-performance parallel binaries automatically.</li> <li>Linear Scalability: Effortlessly scales from single laptops to large clusters and supercomputers.</li> <li>Optimized I/O Operations: Advanced data access capabilities for Iceberg, Snowflake, Parquet, CSV, and JSON with automatic optimizations like filter pushdown and column pruning.</li> <li>Integrated SQL Engine: Built-in, high-performance SQL capabilities directly within Python workflows.</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#how-well-does-bodo-scale","title":"How well does Bodo scale?","text":"<p>Bodo scales linearly across cores and nodes by using HPC technologies such as MPI in the backend. Bodo can parallelize applications from local laptops all the way to large clusters (5000+ cores) effectively.</p>"},{"location":"faq/#what-data-formats-does-bodo-support","title":"What data formats does Bodo support?","text":"<p>Bodo supports various data storage formats such as Iceberg, Snowflake, Parquet, CSV, and JSON natively. See the File I/O docs for more details.</p>"},{"location":"faq/#what-are-the-hardware-requirements-to-run-bodo","title":"What are the hardware requirements to run Bodo?","text":"<p>Bodo is fully portable across various CPU architectures and environments. We currently distribute packages for Mac and Linux with x86 and ARM CPU architectures (Windows support is in progress). Bodo can run on any on-premises or cloud environment.</p>"},{"location":"faq/#how-is-bodo-different-from-dask-ray-or-spark","title":"How is Bodo different from Dask, Ray, or Spark?","text":"<p>Unlike traditional distributed computing frameworks, Bodo:</p> <ul> <li>Seamlessly supports native Python APIs like Pandas and NumPy using a compiler approach.</li> <li>Eliminates runtime overheads common in driver-executor models by leveraging Message Passing Interface (MPI) technology for true parallel execution.</li> </ul>"},{"location":"faq/#will-bodo-just-work-on-my-existing-code","title":"Will Bodo \u201cjust work\u201d on my existing code?","text":"<p>Pretty close. You only need to annotate your Python compute functions with <code>@bodo.jit</code>, make sure they are jittable, and Bodo will handle the parallelization and optimization automatically. The vast majority of your application logic will remain unchanged, as long as it uses common libraries like Pandas, NumPy and Scikit-Learn.</p>"},{"location":"faq/#what-types-of-workloads-are-ideal-for-bodo","title":"What types of workloads are ideal for Bodo?","text":"<p>Bodo excels at accelerating large-scale data processing, AI/ML workflows, and many other workloads requiring significant computation.</p>"},{"location":"faq/#can-i-use-bodo-in-jupyter-notebooks-or-other-ides","title":"Can I use Bodo in Jupyter Notebooks or other IDEs?","text":"<p>Bodo works in Jupyter, VS Code and other IDEs that support Python.</p>"},{"location":"faq/#does-bodo-support-cloud-environments","title":"Does Bodo support cloud environments?","text":"<p>Bodo can run on any cloud environment using virtual machines or Kubernetes clusters. Creating multi-node clusters just requires network configuration for cross-node communication. Bodo cloud service simplifies managing compute clusters and jobs on AWS and Azure.</p>"},{"location":"faq/#how-can-i-get-help-if-i-encounter-an-issue","title":"How can I get help if I encounter an issue?","text":"<p>For support, you can join the Bodo Community Slack, where users and contributors actively discuss and resolve issues. You can also refer to the documentation or raise an issue on GitHub if you\u2019ve encountered a bug or need technical guidance.</p>"},{"location":"faq/#are-there-any-usage-limits-or-restrictions","title":"Are there any usage limits or restrictions?","text":"<p>No, there are no usage limits. Bodo is open source and distributed under the Apache-2.0 License. You can use Bodo in personal, academic, and commercial projects.</p>"},{"location":"faq/#how-does-bodo-handle-security-and-privacy","title":"How does Bodo handle security and privacy?","text":"<p>Since Bodo runs in your environment, you have complete control over the data and compute resources, ensuring that sensitive information remains secure.</p>"},{"location":"faq/#how-does-bodo-handle-fault-tolerance-and-failures","title":"How does Bodo handle fault tolerance and failures?","text":"<p>The Bodo compiler creates binaries that run efficiently on bare metal, eliminating most software failures of other framework (e.g., scheduler errors, JVM errors, ...). To handle rare case of hardware failures, we recommend simply configuring job restarts in your environment. Bodo's high performance ensures fast restart and job completion even with failures.</p>"},{"location":"faq/#does-bodo-have-a-job-scheduler","title":"Does Bodo have a job scheduler?","text":"<p>Bodo provides parallel compute that can work with any job scheduler. Bodo does not bundle its own job scheduler.</p>"},{"location":"faq/#am-i-supposed-to-have-mpi-nodes-set-up-already","title":"Am I supposed to have MPI nodes set up already?","text":"<p>MPI setup is required for distributed execution, which involves passwordless SSH access across nodes. See cluster installation docs.</p>"},{"location":"faq/#how-do-i-know-if-my-function-is-actually-being-parallelized","title":"How do I know if my function is actually being parallelized?","text":"<p>You can see distributed diagnostics of the compiled function to understand parallelism decisions by the compiler. See parallism docs. In addition, the Bodo compiler throws a warning if it didn't find any parallelism opportunities in a function. You can also see your local machine's CPU usage using tools like <code>top</code> and <code>htop</code>.</p>"},{"location":"faq/#how-do-i-handle-unsupported-operations-or-libraries","title":"How do I handle unsupported operations or libraries?","text":"<p>You can use unsupported operations and libraries outside JIT functions, or use the <code>@bodo.wrap_python</code> decorator to use any regular Python function inside JIT functions. See wrap_python docs.</p>"},{"location":"faq/#can-bodo-read-from-databases-like-postgres-snowflake-or-bigquery","title":"Can Bodo read from databases like Postgres, Snowflake, or BigQuery?","text":"<p>Bodo can read from all databases that are readable in Python. The Snowflake connector is especially optimized for very high performance.</p>"},{"location":"faq/#does-bodo-work-inside-databricks-environment","title":"Does Bodo work inside Databricks environment?","text":"<p>Bodo currently does not work inside Databricks due to networking restrictions. We plan to investigate and provide a solution.</p>"},{"location":"faq/#can-i-create-custom-user-defined-functions-udfs-with-bodo","title":"Can I create custom user-defined functions (UDFs) with Bodo?","text":"<p>Yes, Bodo is particularly good at accelerating UDFs in Pandas APIs such as <code>DataFrame.apply</code> and <code>Series.map</code>.</p>"},{"location":"faq/#what-is-the-difference-between-the-open-source-compute-engine-and-the-bodo-cloud-platform","title":"What is the difference between the open source compute engine and the Bodo Cloud Platform?","text":"<p>The Bodo Cloud Platform simplifies managing compute clusters, notebooks and jobs that use the Bodo engine. Bodo Cloud Platform currently supports AWS and Azure.</p>"},{"location":"faq/#does-bodo-have-a-sql-interface","title":"Does Bodo have a SQL interface?","text":"<p>Yes, BodoSQL is a high performance SQL engine that provides vectorized streaming execution and support distributed clusters using MPI. BodoSQL compiles SQL queries into optimized binaries, which is particularly good for large batch jobs.</p>"},{"location":"faq/#when-is-bodo-not-appropriate-what-is-it-not-designed-for","title":"When is Bodo Not appropriate? What is it not designed for?","text":"<p>Bodo is designed for large-scale data processing and may not be appropriate for other use cases such as accelerating Python web frameworks (e.g. Django) and other non-compute Python applications.</p>"},{"location":"file_io/","title":"Scalable Data I/O","text":"<p>Efficient parallel data processing requires data I/O to be parallelized effectively as well. Bodo provides parallel file I/O for many different formats such as Parquet, CSV, JSON, Numpy binaries, HDF5 and SQL databases. This diagram demonstrates how chunks of data are partitioned among parallel execution engines by Bodo.</p> <p> </p> <p>Bodo automatically parallelizes I/O for any number of cores and cluster size without any additional API layers.</p>"},{"location":"file_io/#io_workflow","title":"I/O Workflow","text":"<p>Make sure I/O calls for large data are inside JIT functions to allow Bodo to parallelize I/O and divide the data across cores automatically (see below for supported formats and APIs).</p> <p>Warning</p> <p>Performing I/O in regular Python (outside JIT functions) causes data distribution overheads when data is passed to JIT functions. In addition, SPMD launch mode replicates data on all Python processes, which can result in out-of-memory errors if the data is large. For example, a 1 GB dataframe replicated on 1000 cores consumes 1 TB of memory.</p> <p>Bodo looks at the schema of the input dataset during compilation time to infer the datatype of the resulting dataframe. This requires the dataset path to be available to the compiler. The path should be either a constant string value, an argument to the JIT function, or a simple combination of the two. For example, the following code passes the dataset path as an argument, allowing Bodo to infer the data type of <code>df</code>:</p> <pre><code>import os\nimport pandas as pd\nimport bodo\n\ndata_path = os.environ[\"JOB_DATA_PATH\"]\n\n@bodo.jit\ndef f(path):\n    df = pd.read_parquet(path)\n    print(df.A.sum())\n\nf(data_path)\n</code></pre> <p>Concatenating arguments and constant values also works:</p> <pre><code>import os\nimport pandas as pd\nimport bodo\n\ndata_root = os.environ[\"JOB_DATA_ROOT\"]\n\n@bodo.jit\ndef f(root):\n    df = pd.read_parquet(root + \"/table1.pq\")\n    print(df.A.sum())\n\nf(data_root)\n</code></pre> <p>In the rare case that the path should be a dynamic value inside JIT functions, the data types have to be specified manually (see Specifying I/O Data Types Manually). This is error-prone and should be avoided as much as possible.</p>"},{"location":"file_io/#supported-data-formats","title":"Supported Data Formats","text":"<p>Currently, Bodo supports I/O for Parquet, CSV, SQL, JSON, HDF5, and Numpy binaries formats. It can read these formats from multiple filesystems, including S3, HDFS and Azure Data Lake (ADLS) (see File Systems below for more information). Many databases and data warehouses such as Snowflake are supported as well.</p> <p>Also see supported pandas APIs for supported arguments of I/O functions.</p>"},{"location":"file_io/#parquet-section","title":"Parquet","text":"<p>Parquet is a commonly used file format in analytics due to its efficient columnar storage. Bodo supports the standard pandas API for reading Parquet: <code>pd.read_parquet(path)</code>, where path can be a parquet file, a directory with multiple parquet files (all are part of the same dataframe), a glob pattern, list of files or list of glob patterns:</p> <pre><code>import pandas as pd\nimport bodo\n\n@bodo.jit\ndef write_pq(df):\n    df.to_parquet(\"s3://bucket-name/example.pq\")\n\n@bodo.jit\ndef read_pq():\n    df = pd.read_parquet(\"s3://bucket-name/example.pq\")\n    return df\n</code></pre> <p>Note</p> <p>Bodo reads datasets in parallel using multiple cores while ensuring that the number of rows read on all cores is roughly equal. The size and number of row groups can affect parallel read performance significantly. Currently, reading any number of rows in Bodo requires reading at least one row-group. To read even a single row from a parquet dataset, the entire row-group containing that row (and its corresponding metadata) needs to be read first, and then the required row is extracted from it. Therefore, for best parallel read performance, there must be sufficient row-groups to minimize the number of instances where multiple cores need to read from the same row group. This means there must be at least as many row groups as the number of cores, but ideally a lot more. At the same time, the size of the row-groups should not be too small since this can lead to overheads. For more details about parquet file format, refer to the format specification.</p> <p><code>to_parquet(name)</code> with distributed data writes to a folder called <code>name</code>. Each process writes one file into the folder, but if the data is not distributed, <code>to_parquet(name)</code> writes to a single file called <code>name</code>:</p> <pre><code>df = pd.DataFrame({\"A\": range(10)})\n\n# Only execute on a single core\n@bodo.jit(distributed=False)\ndef example1_pq(df):\n    df.to_parquet(\"example1.pq\")\n\n# Execute on all cores\n@bodo.jit\ndef example2_pq(df):\n    df.to_parquet(\"example2.pq\")\n\nexample1_pq(df)\nexample2_pq(df)\n</code></pre> <p>Run the code above with 4 processors:</p> <pre><code>BODO_NUM_WORKERS=4 python example_pq.py\n</code></pre> <p><code>example1_pq(df)</code> writes 1 single file, and <code>example2_pq(df)</code> writes a folder containing 4 parquet files:</p> <pre><code>.\n\u251c\u2500\u2500 example1.pq\n\u251c\u2500\u2500 example2.pq\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-00.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-01.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-02.parquet\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part-03.parquet\n</code></pre> <p>See <code>read_parquet()</code>, <code>to_parquet()</code> for supported arguments.</p>"},{"location":"file_io/#filter-pushdown-and-column-pruning","title":"Filter Pushdown and Column Pruning","text":"<p>Filter Pushdown and Column Pruning</p> <p>Bodo can detect filters used by the code and optimize the <code>read_parquet</code> call by pushing the filters down to the storage layer, so that only the rows required by the program are read. In addition, Bodo only reads the columns that are used in the program, and prunes the unused columns. These optimizations can significantly speed up I/O in many cases and can substantially reduce the program's memory footprint.</p> <p>For example, suppose we have a large dataset with many columns that spans many years, and we only need to read revenue data for a particular year:</p> <pre><code>@bodo.jit\ndef query():\n    df = pd.read_parquet(\"s3://my-bucket/data.pq\")\n    df = df[df[\"year\"] == 2021]\n    return df.groupby(\"customer_key\")[\"revenue\"].max()\n</code></pre> <p>When compiling the above, Bodo detects the <code>df[df[\"year\"] == 2021]</code> filter and optimizes the <code>read_parquet</code> call so that it only reads data for year 2021 from S3. This requires the dataframe filtering operation to be in the same JIT function as <code>read_parquet</code>, and the dataframe variable shouldn't be used before filtering. Bodo also makes sure only <code>customer_key</code> and <code>revenue</code> columns are read since other columns are not used in the programs.</p> <p>In general, if the dataset is hive-partitioned and partition columns appear in filter expressions, only the files that contain relevant data are read, and the rest are discarded based on their path. For example, if <code>year</code> is a partition column above and we have a dataset:</p> <pre><code>.\n\u2514\u2500\u2500 data.pq/\n    \u2502   ...\n    \u251c\u2500\u2500\u2500year=2020/\n    \u2502   \u251c\u2500\u2500 part-00.parquet\n    \u2502   \u2514\u2500\u2500 part-01.parquet\n    \u2514\u2500\u2500\u2500year=2021/\n        \u251c\u2500\u2500 part-02.parquet\n        \u2514\u2500\u2500 part-03.parquet\n</code></pre> <p>Bodo will only read the files in the <code>year=2021</code> directory.</p> <p>For non-partition columns, Bodo may discard files entirely just by looking at their parquet metadata (depending on the filters and statistics contained in the metadata) or filter the rows during read.</p> <p>Note</p> <p>Filter pushdown is often a very important optimization and critical for having manageable memory footprint in big data workloads. Make sure filtering happens in the same JIT function right after dataset read (or JIT functions for I/O are inlined, see inlining).</p>"},{"location":"file_io/#exploring-large-data-without-full-read","title":"Exploring Large Data Without Full Read","text":"<p>Exploring Large Data Without Full Read</p> <p>Exploring large datasets often requires seeing its shape and a sample of the data. Bodo is able to provide this information quickly without loading the full Parquet dataset, which means there is no need for a large cluster with a lot of memory. For example:</p> <pre><code>@bodo.jit\ndef head_only_read():\n    df = pd.read_parquet(\"s3://my-bucket/example.pq\")\n    print(df.shape)\n    print(df.head())\n</code></pre> <p>In this example, Bodo provides the shape information for the full dataset in <code>df.shape</code>, but only loads the first few rows that are necessary for <code>df.head()</code>.</p>"},{"location":"file_io/#csv-section","title":"CSV","text":"<p>CSV is a common text format for data exchange. Bodo supports most of the standard pandas API to read CSV files:</p> <pre><code>import pandas as pd\nimport bodo\n\n@bodo.jit\ndef write_csv(df):\n    df.to_csv(\"s3://my-bucket/example.csv\")\n\n@bodo.jit\ndef read_csv():\n    df = pd.read_csv(\"s3://my-bucket/example.csv\")\n    return df\n</code></pre> <p>Unlike <code>read_csv</code> in regular pandas, Bodo can read a directory that contains multiple partitioned CSV files as well. All files in the folder must have the same number and datatype of columns. They can have different number of rows.</p> <p>Usage:</p> <pre><code>@bodo.jit\ndef read_csv_folder():\n    df = pd.read_csv(\"s3://my-bucket/path/to/folder/foldername\")\n    doSomething(df)\n</code></pre> <p>Use <code>sep=\"\\n\"</code> to read text files line by line into a single-column dataframe (without creating separate columns, useful when text data is unstructured or there are too many columns to read efficiently):</p> <pre><code>@bodo.jit\ndef read_test():\n    df = pd.read_csv(\"example.csv\", sep=\"n\", names=[\"value\"], dtype={\"value\": \"str\"})\n    return df\n</code></pre> <p>Note</p> <p>Bodo uses nullable integer types of pandas to ensure type stability (see integer NA issue in pandas for more details). Therefore, data types must be specified explicitly for accurate performance comparisons of Bodo and pandas for <code>read_csv</code>.</p> <p><code>to_csv(name)</code> has different behaviors for different file systems:</p> <ol> <li> <p>POSIX file systems: always writes to a single file, regardless of the number of processes and whether the data is     distributed, but writing is still done in parallel when more than 1 processor is used:</p> <pre><code>df = pd.DataFrame({\"A\": np.arange(n)})\n\n# Only execute on a single core\n@bodo.jit(distributed=False)\ndef example1_csv(df):\n    df.to_csv(\"example1.csv\")\n\n# Execute on all cores\n@bodo.jit\ndef example2_csv(df):\n    df.to_csv(\"example2.csv\")\n\nexample1_csv(df)\nexample2_csv(df)\n</code></pre> <p>Run the code above with 4 processors:</p> <pre><code>BODO_NUM_WORKERS=4 python example_csv.py\n</code></pre> <p>each <code>example1_csv(df)</code> and <code>example2_csv(df)</code> writes to a single file:</p> <pre><code>.\n\u251c\u2500\u2500 example1.csv\n\u251c\u2500\u2500 example2.csv\n</code></pre> </li> <li> <p>S3 and HDFS: distributed data is written to a folder called <code>name</code>.     Each process writes one file into the folder, but if the data is not distributed,     <code>to_csv(name)</code> writes to a single file called <code>name</code>:</p> <pre><code>df = pd.DataFrame({\"A\": np.arange(n)})\n\n@bodo.jit(distributed=False)\ndef example1_csv(df):\n    df.to_csv(\"s3://bucket-name/example1.csv\")\n\n@bodo.jit\ndef example2_csv(df):\n    df.to_csv(\"s3://bucket-name/example2.csv\")\n\nexample1_csv(df)\nexample2_csv(df)\n</code></pre> <p>Run the code above with 4 processors:</p> <pre><code>BODO_NUM_WORKERS=4 python example_csv.py\n</code></pre> <p><code>example1_csv(df)</code> writes 1 single file, and <code>example2_csv(df)</code> writes a folder containing 4 csv files:</p> <pre><code>.\n\u251c\u2500\u2500 example1.csv\n\u251c\u2500\u2500 example2.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-00.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-01.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-02.csv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part-03.csv\n</code></pre> </li> </ol> <p>See <code>read_csv()</code>, <code>to_csv()</code> for supported arguments.</p>"},{"location":"file_io/#json-section","title":"JSON","text":"<p>For JSON, the syntax is also the same as pandas.</p> <p>Usage:</p> <pre><code>@bodo.jit\ndef example_write_json(df, fname):\n    df.to_json(fname)\n\n@bodo.jit\ndef example_read_json_lines_format():\n    df = pd.read_json(\"example.json\", orient = \"records\", lines = True)\n\n@bodo.jit\ndef example_read_json_multi_lines():\n    df = pd.read_json(\"example_file.json\", orient = \"records\", lines = False,\n        dtype={\"A\": float, \"B\": \"bool\", \"C\": int})\n</code></pre> <p>Note</p> <ul> <li>The <code>dtype</code> argument is required when reading a regular multi-line JSON     file.</li> <li>Bodo cannot read a directory containing multiple multi-line JSON     files</li> <li>Bodo's default values for <code>orient</code> and <code>lines</code> are <code>records</code> and <code>False</code> respectively.</li> </ul> <p><code>to_json(name)</code> has different behaviors for different file systems:</p> <ol> <li> <p>POSIX file systems: <code>to_json(name)</code> behavior depends on <code>orient</code> and <code>lines</code> arguments.</p> <ol> <li> <p><code>DataFrame.to_json(name, orient=\"records\", lines=True)</code>     (i.e. writing JSON Lines text file format) always writes to a single file,     regardless of the number of processes and whether the data is distributed,     but writing is still done in parallel when more than 1 processor is used:</p> <pre><code>df = pd.DataFrame({\"A\": np.arange(n)})\n\n# Only execute on a single core\n@bodo.jit(distributed=False)\ndef example1_json(df):\n    df.to_json(\"example1.json\", orient=\"records\", lines=True)\n\n# Execute on all cores\n@bodo.jit\ndef example2_json(df):\n    df.to_json(\"example2.json\", orient=\"records\", lines=True)\n\nexample1_json(df)\nexample2_jsons(df)\n</code></pre> <p>Run the code above with 4 processors:</p> <pre><code>BODO_NUM_WORKERS=4 python example_json.py\n</code></pre> <p>each <code>example1_json(df)</code> and <code>example2_json(df)</code> writes to a single file:</p> <pre><code>.\n\u251c\u2500\u2500 example1.json\n\u251c\u2500\u2500 example2.json\n</code></pre> </li> <li> <p>All other combinations of values for <code>orient</code> and <code>lines</code> have the same behavior as S3 and HDFS explained below.</p> </li> </ol> </li> <li> <p>S3 and HDFS : distributed data is written to a folder called <code>name</code>.     Each process writes one file into the folder, but if the data is not distributed,     <code>to_json(name)</code> writes to a file called <code>name</code>:</p> <pre><code>df = pd.DataFrame({\"A\": np.arange(n)})\n\n# Only execute on a single core\n@bodo.jit(distributed=False)\ndef example1_json(df):\n    df.to_json(\"s3://bucket-name/example1.json\")\n\n# Execute on all cores\n@bodo.jit\ndef example2_json(df):\n    df.to_json(\"s3://bucket-name/example2.json\")\n\nexample1_json(df)\nexample2_json(df)\n</code></pre> <p>Run the code above with 4 processors:</p> <pre><code>BODO_NUM_WORKERS=4 python example_json.py\n</code></pre> <p><code>example1_json(df)</code> writes 1 single file, and <code>example2_json(df)</code> writes a folder containing 4 json files:</p> <pre><code>.\n\u251c\u2500\u2500 example1.json\n\u251c\u2500\u2500 example2.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-00.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-01.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-02.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part-03.json\n</code></pre> <p>See <code>read_json()][pandas-f-in], [</code>to_json()` for supported arguments.</p> </li> </ol>"},{"location":"file_io/#sql-section","title":"SQL","text":"<p>See Databases for the list of supported Relational Database Management Systems (RDBMS) with Bodo.</p> <p>For SQL, the syntax is also the same as pandas. For reading:</p> <pre><code>@bodo.jit\ndef example_read_sql():\n    df = pd.read_sql(\"select * from employees\", \"mysql+pymysql://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;/&lt;db_name&gt;\")\n</code></pre> <p>See <code>read_sql()</code> for supported arguments.</p> <p>For writing:</p> <pre><code>@bodo.jit\ndef example_write_sql(df):\n    df.to_sql(\"table_name\", \"mysql+pymysql://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;/&lt;db_name&gt;\")\n</code></pre> <p>See <code>to_sql()</code> for supported arguments.</p> <p>Note</p> <p><code>sqlalchemy</code> must be installed in order to use <code>pandas.read_sql</code>.</p>"},{"location":"file_io/#filter-pushdown-and-column-pruning_1","title":"Filter Pushdown and Column Pruning","text":"<p>Filter Pushdown and Column Pruning</p> <p>Similar to Parquet read, Bodo JIT compiler is able to push down filters to the data source and prune unused columns automatically. For example, this program reads data from a very large Snowflake table, but only needs limited rows and columns:</p> <pre><code>@bodo.jit\ndef filter_ex(conn, int_val):\n    df = pd.read_sql(\"SELECT * FROM LINEITEM\", conn)\n    df = df[(df[\"l_orderkey\"] &gt; 10) &amp; (int_val &gt;= df[\"l_linenumber\"])]\n    result = df[\"l_suppkey\"]\n    print(result)\n\nfilter_ex(conn, 2)\n</code></pre> <p>Bodo optimizes the query passed to <code>read_sql</code> to push filters down and prune unused columns. In this case, Bodo will replace <code>SELECT * FROM LINEITEM</code> with the optimized version automatically:</p> <pre><code>SELECT \"L_SUPPKEY\" FROM (SELECT * FROM LINEITEM) as TEMP\nWHERE  ( ( l_orderkey &gt; 10 ) AND ( l_linenumber &lt;= 2 ) )\n</code></pre>"},{"location":"file_io/#deltalake-section","title":"Delta Lake","text":"<p>Reading parquet files from Delta Lake is supported locally, from S3, and from Azure ADLS.</p> <ul> <li>The Delta Lake binding python packaged needs to be installed using pip:<code>pip install deltalake</code>.</li> <li>For S3, the <code>AWS_DEFAULT_REGION</code> environment variable should be set to the region of the bucket hosting     the Delta Lake table.</li> <li>For ADLS, the <code>AZURE_STORAGE_ACCOUNT</code> and <code>AZURE_STORAGE_KEY</code> environment variables need to be set.</li> </ul> <p>Example code for reading:</p> <pre><code>@bodo.jit\ndef example_read_deltalake():\n    df = pd.read_parquet(\"path/to/deltalake\")\n</code></pre> <p>Note</p> <p>Writing is currently not supported.</p>"},{"location":"file_io/#numpy-binary-section","title":"Numpy binaries","text":"<p>Numpy's <code>fromfile</code> and <code>tofile</code> are supported as below:</p> <pre><code>@bodo.jit\ndef example_np_io():\n    A = np.fromfile(\"myfile.dat\", np.float64)\n    ...\n    A.tofile(\"newfile.dat\")\n</code></pre> <p>Bodo has the same behavior as Numpy for <code>numpy.ndarray.tofile()</code>, where we always write to a single file. However, writing distributed data to POSIX is done in parallel, but writing to S3 &amp; HDFS is done sequentially (due to file system limitations).</p>"},{"location":"file_io/#hdf5","title":"HDF5","text":"<p>HDF5 is a common format in scientific computing, especially for multi-dimensional numerical data. HDF5 can be very efficient at scale, since it has native parallel I/O support. For HDF5, the syntax is the same as the h5py package. For example:</p> <pre><code>@bodo.jit\ndef example_h5():\n    f = h5py.File(\"data.hdf5\", \"r\")\n    X = f[\"points\"][:]\n    Y = f[\"responses\"][:]\n</code></pre>"},{"location":"file_io/#File","title":"File Systems","text":""},{"location":"file_io/#S3","title":"Amazon S3","text":"<p>Reading and writing CSV, Parquet, JSON, and Numpy binary files from and to Amazon S3 is supported.</p> <p>The file path should start with <code>s3://</code>:</p> <pre><code>@bodo.jit\ndef example_s3_parquet():\n    df = pd.read_parquet(\"s3://bucket-name/file_name.parquet\")\n</code></pre> <p>These environment variables are used for File I/O with S3 credentials:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_DEFAULT_REGION</code>: default as <code>us-east-1</code></li> <li><code>AWS_S3_ENDPOINT</code>: specify custom host name, default as AWS endpoint(<code>s3.amazonaws.com</code>)</li> </ul> <p>Connecting to S3 endpoints through a proxy is supported. The proxy URI can be provided by setting one of the following environment variables (listed in order of precedence):</p> <ul> <li><code>http_proxy</code></li> <li><code>https_proxy</code></li> <li><code>HTTP_PROXY</code></li> <li><code>HTTPS_PROXY</code></li> </ul> <p>By default, Bodo uses Apache Arrow internally for read and write of data on S3. If additional <code>storage_options</code> are provided to <code>pd.read_parquet</code> that Arrow does not support, then S3FS will be used instead. It can be optionally installed via pip or conda:</p> <pre><code>pip install \"s3fs&gt;=2022.1.0\"\n</code></pre> <pre><code>conda install -c conda-forge \"s3fs&gt;=2022.1.0\"\n</code></pre>"},{"location":"file_io/#Azure","title":"Azure Filesystems (ABFS or ADLS)","text":"<p>Reading and writing Parquet files from and to Azure Filesystems is  supported.</p> <p>The file path should start with <code>abfs://</code> or <code>abfss://</code>:</p> <pre><code>@bodo.jit\ndef example_abfs_parquet():\n    df = pd.read_parquet(\"abfs://container-name/file_name.parquet\")\n</code></pre> <p>The following parameters are used for credentials:</p> <ul> <li>Required: The name of the storage account, either<ul> <li>Set the <code>AZURE_STORAGE_ACCOUNT_NAME</code> environment variable</li> <li>Specify the <code>account_name</code> parameter in the <code>storage_options</code> argument of <code>pd.read_parquet</code></li> <li>Use the long-form URL syntax: <code>abfs://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/file_name.parquet</code></li> </ul> </li> <li>Optional: The authentication key for the storage account, either<ul> <li>Set the <code>AZURE_STORAGE_ACCOUNT_KEY</code> environment variable</li> <li>Specify the <code>account_key</code> parameter in the <code>storage_options</code> argument of <code>pd.read_parquet</code></li> </ul> </li> <li>Optional A SAS token for the storage account, either<ul> <li>Set the <code>AZURE_STORAGE_SAS_TOKEN</code> environment variable</li> <li>Include in the long-form URL syntax: <code>abfs://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{PATH}/{SAS_TOKEN}/file_name.parquet</code></li> </ul> </li> </ul> <p>Bodo uses Apache Arrow internally for Azure IO.</p>"},{"location":"file_io/#GCS","title":"Google Cloud Storage (GCS)","text":"<p>Reading and writing CSV, JSON and Parquet files from and to Google Cloud Storage (GCS) is supported.</p> <p>The file path should start with <code>gs://</code> or <code>gcs://</code>:</p> <pre><code>@bodo.jit\ndef example_gcs_parquet():\n    df = pd.read_parquet(\"gcs://bucket-name/file_name.parquet\")\n</code></pre> <p>By default uses the process described in here to resolve credentials. If not running on Google Cloud Platform (GCP), this may require the environment variable <code>GOOGLE_APPLICATION_CREDENTIALS</code> to point to a JSON file containing credentials. Details for <code>GOOGLE_APPLICATION_CREDENTIALS</code> can be seen in the Google docs here. Locally, one may use the <code>gcloud</code> CLI tool to authenticate:</p> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"file_io/#hugging-face-datasets","title":"Hugging Face Datasets","text":"<p>Bodo supports reading CSV, JSON and Parquet data from Hugging Face datasets using the huggingface_hub library (which can be installed using pip or Conda). The data path should start with <code>hf://</code>. For example:</p> <pre><code>@bodo.jit\ndef example_hf_dataset():\n    return pd.read_parquet(\"hf://datasets/openai/gsm8k/main/train-00000-of-00001.parquet\")\n</code></pre>"},{"location":"file_io/#HDFS","title":"Hadoop Distributed File System (HDFS)","text":"<p>Reading and writing CSV, Parquet, JSON, and Numpy binary files from and to Hadoop Distributed File System (HDFS) is supported.</p> <p>The <code>openjdk</code> version 11 package must be available, and the file path should start with <code>hdfs://</code>:</p> <pre><code>@bodo.jit\ndef example_hdfs_parquet():\n    df = pd.read_parquet(\"hdfs://host:port/dir/file_name.pq\")\n</code></pre> <p>These environment variables are used for File I/O with HDFS:</p> <ul> <li><code>HADOOP_HOME</code>: the root of your installed Hadoop distribution. Often is <code>lib/native/libhdfs.so</code>.</li> <li><code>ARROW_LIBHDFS_DIR</code>: location of libhdfs. Often is <code>$HADOOP_HOME/lib/native</code>.</li> <li><code>CLASSPATH</code>: must contain the Hadoop jars. You can set these using:     <pre><code>export CLASSPATH=`$HADOOP_HOME/bin/hdfs classpath --glob`\n</code></pre></li> </ul> <p>Bodo uses Apache Arrow internally for read and write of data on HDFS. <code>$HADOOP_HOME/etc/hadoop/hdfs-site.xml</code> provides default behaviors for the HDFS client used by Bodo. Inconsistent configurations (e.g. <code>dfs.replication</code>) could potentially cause errors in Bodo programs.</p>"},{"location":"file_io/#setting-up-hdfs-credentials","title":"Setting up HDFS Credentials","text":"<p>There are 3 supported methods for authenticating to HDFS:</p> <ul> <li>Environment Variables</li> <li>core-site.xml</li> </ul>"},{"location":"file_io/#azure-identities","title":"Azure Identities","text":"<p>To authenticate with Azure Identites assign an identity to your Azure VM and ensure it has the  Storage Blob Data Contributor role for the storage account to access. This is only supported on Bodo Platform.</p>"},{"location":"file_io/#core-sitexml","title":"core-site.xml","text":"<p>Note</p> <p>core-site.xml authentication is not supported on Bodo Platform, instead use the environment variables.</p> <p>To authenticate with a core-site.xml file Hadoop Filesystem sources its credentials from the first available <code>core-site.xml</code> file on the <code>CLASSPATH</code>. When Hadoop is set up (including on Bodo Platform), this file is usually created at <code>$HADOOP_HOME/etc/hadoop/core-site.xml</code> automatically. You can edit this file and set credentials appropriately.</p> <p>You can also write the core-site configuration to <code>bodo.HDFS_CORE_SITE_LOC</code>, which is a temporary file Bodo adds to the <code>CLASSPATH</code> when it is initialized:</p> <pre><code>import bodo\n\n# Initialize the temporary directory where the core-site file\n# will be written\nbodo.HDFS_CORE_SITE_LOC_DIR.initialize()\n\n# Define the core-site for your regular ADLS/HDFS read/write\n# operations\nCORE_SITE_SPEC = \"\"\"\n&lt;configuration&gt;\n...\n&lt;/configuration&gt;\n\"\"\"\n\n# Write it to the temporary core-site file.\n# Do it on one rank on every node to avoid filesystem conflicts.\nif bodo.get_rank() in bodo.get_nodes_first_ranks():\n    with open(bodo.HDFS_CORE_SITE_LOC, 'w') as f:\n        f.write(CORE_SITE_SPEC)\n\n\n@bodo.jit\ndef etl_job():\n    df = pd.read_parquet(\"hdfs://.../file.pq\")\n    # ..\n    # .. Some ETL Processing\n    # ..\n    df.to_parquet(\"hdfs://.../out_file.pq\")\n    return\n\n\netl_job()\n</code></pre>"},{"location":"file_io/#db","title":"Databases","text":"<p>Currently, Bodo supports most RDBMS that work with SQLAlchemy, with a corresponding driver.</p>"},{"location":"file_io/#snowflake-section","title":"Snowflake","text":""},{"location":"file_io/#prerequisites","title":"Prerequisites","text":"<p>In order to be able to query Snowflake or write a dataframe to Snowflake from Bodo, installing the Snowflake connector is necessary (it is installed by default on Bodo Platform).</p> <p>If you have installed Bodo using pip, then you can install the Snowflake connector using pip as well:</p> <pre><code>pip install snowflake-connector-python\n</code></pre> <p>If you are using Bodo in a conda environment:</p> <pre><code>conda install -c conda-forge snowflake-connector-python\n</code></pre>"},{"location":"file_io/#reading-from-snowflake","title":"Reading from Snowflake","text":"<p>To read a dataframe from a Snowflake database, users can use <code>pd.read_sql</code> with their Snowflake username and password: <code>pd.read_sql(query, \"snowflake://&lt;username&gt;:&lt;password&gt;@url\")</code>.</p>"},{"location":"file_io/#usage","title":"Usage","text":"<p>Bodo requires the Snowflake connection string to be passed as an argument to the <code>pd.read_sql</code> function. The complete code looks as follows:</p> <p><pre><code>import bodo\nimport pandas as pd\n\n@bodo.jit\ndef read_snowflake(db_name, table_name):\n    df = pd.read_sql(\n            f\"SELECT * FROM {table_name}\",\n            f\"snowflake://user:password@url/{db_name}/schema?warehouse=warehouse_name\",\n        )\n    return df\ndf = read_snowflake(db_name, temp_table_name)\n</code></pre> - <code>_bodo_read_as_dict</code> is a Bodo specific argument which forces     the specified string columns to be read with dictionary-encoding. Bodo automatically loads string columns using dictionary encoding when it determines it would be beneficial based on a heuristic. Dictionary-encoding stores data in memory in an efficient manner and is most effective when the column has many repeated values. Read more about dictionary-encoded layout here.     Bodo will raise a warning if the specified columns are not present in the schema or if they are not of type string.</p> <pre><code>For example:\n```py\n@bodo.jit\ndef impl(query, conn):\n    df = pd.read_sql(query, conn, _bodo_read_as_dict=[\"A\", \"B\", \"C\"])\n    return df\n```\n</code></pre>"},{"location":"file_io/#snowflake-write","title":"Writing to Snowflake","text":"<p>To write a dataframe to Snowflake using Bodo, users can use <code>df.to_sql</code> with their Snowflake username and password: <code>df.to_sql(table_name, \"snowflake://&lt;username&gt;:&lt;password&gt;@url\")</code>.</p>"},{"location":"file_io/#snowflake-write-usage","title":"Usage","text":"<p>Bodo requires the Snowflake connection string to be passed as an argument to the <code>df.to_sql</code> function. The complete code looks as follows:</p> <pre><code>import bodo\nimport pandas as pd\n\n\n@bodo.jit(distributed=[\"df\"])\ndef write_to_snowflake(df, table_name):\n    df.to_sql(\n        table_name,\n        \"snowflake://user:password@url/db_name/schema?warehouse=warehouse_name&amp;role=role_name\",\n    )\n\nwrite_to_snowflake(df, table_name)\n</code></pre> <p>Note</p> <ul> <li>Writing Pandas Dataframe index to Snowflake is not supported. If <code>index</code> and/or <code>index_label</code>   are provided, they will be ignored.</li> <li><code>if_exists=append</code> is needed if you want to append to a table that already exists in Snowflake.</li> <li><code>chunksize</code> argument is supported and used for writing parquet files to Snowflake stage   in batches. It is the maximum number of rows to write to any of the intermediate parquet files.   When not provided, Bodo will estimate and use a reasonable chunk size.</li> <li><code>dtype</code> and <code>method</code> arguments are not supported and will be ignored if provided.</li> </ul>"},{"location":"file_io/#required-snowflake-permissions","title":"Required Snowflake Permissions","text":""},{"location":"file_io/#snowflake-write-perms-create","title":"Creating a new table","text":"<p>Creating a new table</p> <p>To create a new table, the role being used must have the <code>USAGE</code> permission at the database level. In addition, the following permissions are required at the Schema level:</p> <ul> <li><code>USAGE</code></li> <li><code>CREATE TABLE</code></li> <li><code>CREATE STAGE</code></li> </ul>"},{"location":"file_io/#replacing-an-existing-table","title":"Replacing an existing table","text":"<p>Replacing an existing table</p> <p>To replace an existing table (i.e. when using <code>if_exists='replace'</code>), the role must be an owner of the table, in addition to having the permissions listed in the create section (at the Database and Schema level).</p>"},{"location":"file_io/#appending-to-an-existing-table","title":"Appending to an existing table","text":"<p>Appending to an existing table</p> <p>To append to an existing table (i.e. when using <code>if_exists='append'</code>), the role must have the <code>INSERT</code> permission at the table level, in addition to the permissions listed in the create section (at the Database and Schema level).</p>"},{"location":"file_io/#verifying-your-roles-permissions-in-snowflake","title":"Verifying your role's permissions in Snowflake","text":"<p>Verifying your role's permissions in Snowflake</p> <p>You can check the permissions granted to your role in Snowflake using the <code>SHOW GRANTS</code> command, e.g.:</p> <pre><code>show grants to role test_role\n</code></pre> <p>Alternatively, you can check the permissions at the database/schema/table level and verify that your role has the required permissions, e.g.:</p> <pre><code>-- Check that your role has the USAGE permission on the database\nshow grants on database test_db\n-- Check that your role has the required permissions on the schema\nshow grants on schema test_schema\n-- Check that your role has the required permissions on the table\nshow grants on table test_schema.\"TEST_TABLE\"\n</code></pre> <p>You can also use the Snowsight UI to check these permissions by navigating to the <code>Privileges</code> section of the <code>Details</code> tab of the database/schema/table.</p>"},{"location":"file_io/#advanced-configuration-options","title":"Advanced Configuration Options","text":"<p>Bodo provides highly performant distributed Snowflake write capability. This is done by writing parquet files to a Snowflake stage and then using Snowflake's <code>COPY INTO</code> to load the data into the Snowflake table. Based on the type of your Snowflake account (i.e. whether it is an AWS or Azure or GCP based account) and your environment (i.e. whether certain packages and modules are installed), Bodo will use one of two strategies to write to Snowflake: Direct Upload (preferred) or Put Upload.</p> <ol> <li> <p>Direct Upload: In this strategy, Bodo creates a temporary stage     and writes parquet files to it directly. Once these files are     written, Bodo will automatically execute the <code>COPY INTO</code> command     to copy the data into Snowflake. This is supported on     S3 and ADLS based Snowflake stages (used by AWS and Azure based     Snowflake accounts, respectively). Note that Bodo will drop the     temporary stage once the data has been written. Temporary stages     are also automatically cleaned up by Snowflake after the session ends.</p> </li> <li> <p>Put Upload: In this strategy, Bodo creates a 'named' stage, writes     parquet files to a temporary directory locally and then uses the     Snowflake Python Connector to upload these files to this named stage     using the <code>PUT</code> command. Similar to the Direct Upload strategy,     once the files have been transferred, Bodo will automatically     execute the <code>COPY INTO</code> command to copy the data into Snowflake.     This is used for GCS based stages (used by GCP based Snowflake     accounts), or when the user environment doesn't have all the     required packages and modules to use the Direct Upload strategy.</p> <p>Similar to the Direct Upload strategy, Bodo will drop the named stage after the data has been written to the table.</p> <p>Note</p> <p>In some cases, e.g. during abnormal exits, Bodo may not be able to drop these stages, which may require manual cleanup by the user. All stages created by Bodo are of the form \"bodo_io_snowflake_{random-uuid}\". You can list all stages created by Bodo in Snowflake by executing the <code>SHOW STAGES</code> command:</p> <pre><code>show stages like 'bodo_io_snowflake_%';\n</code></pre> <p>and then delete them using the <code>DROP STAGE</code> command, e.g.:</p> <pre><code>drop stage \"bodo_io_snowflake_02ca9beb-eaf6-4957-a6ff-ff426441cd7a\";\n</code></pre> <p>These operations are not supported in Bodo and must be executed directly through the Snowflake console.</p> </li> </ol> <p>Direct Upload is the preferred strategy and used by default whenever possible. When this is not possible, Bodo will show a warning to the user. For optimal Snowflake ingestion performance, Bodo writes multiple small intermediate parquet files on each rank, instead of a single file like it does for regular parquet writes.</p> <p>Users can set the environment variable <code>BODO_SF_WRITE_DEBUG</code> (to any value), to get more details during the Snowflake write process. This includes printing the raw result of the <code>COPY INTO</code> command execution, and printing more detailed error messages in case of exceptions.</p> <p>Bodo exposes the following configuration variables for tuning the write process further (for advanced users only):</p> <ul> <li><code>SF_WRITE_UPLOAD_USING_PUT</code>: This configuration variable can be set to <code>True</code>   to force Bodo to use the Put Upload strategy. This is <code>False</code> by default   since Direct Upload is the preferred strategy.</li> <li><code>SF_WRITE_PARQUET_COMPRESSION</code>: This configuration variable can set to   specify the compression method used for writing the intermediate   Parquet files. Supported values include: <code>\"snappy\"</code>, <code>\"gzip\"</code>   and <code>\"zstd\"</code>. Bodo uses <code>\"snappy\"</code> by default since it provided   the best overall performance in our benchmarks, however this can vary   based on your data.</li> <li><code>SF_WRITE_PARQUET_CHUNK_SIZE</code>: This configuration variable can be   used to specify the chunk size to use when writing the dataframe to   the intermediate Parquet files. This is measured by the uncompressed   memory usage of the dataframe (in bytes). Note that when provided,   <code>df.to_sql</code>'s <code>chunksize</code> parameter (see description in note in   the Usage section) overrides this value.</li> </ul> <p>These can be set as follows: <pre><code>import bodo\nimport bodo.io.snowflake\n\nbodo.io.snowflake.SF_WRITE_UPLOAD_USING_PUT = False\nbodo.io.snowflake.SF_WRITE_PARQUET_COMPRESSION = \"gzip\"\n...\n</code></pre></p>"},{"location":"file_io/#mysql","title":"MySQL","text":""},{"location":"file_io/#prerequisites_1","title":"Prerequisites","text":"<p>In addition to <code>sqlalchemy</code>, installing <code>pymysql</code> is required. If you have installed Bodo using pip:</p> <pre><code>pip install pymysql\n</code></pre> <p>If you are using Bodo in a conda environment:</p> <pre><code>conda install pymysql -c conda-forge\n</code></pre>"},{"location":"file_io/#usage_1","title":"Usage","text":"<p>Reading result of a SQL query in a dataframe:</p> <pre><code>import bodo\nimport pandas as pd\n\n\n@bodo.jit(distributed=[\"df\"])\ndef read_mysql(table_name, conn):\n    df = pd.read_sql(\n            f\"SELECT * FROM {table_name}\",\n            conn\n        )\n    return df\n\n\ntable_name = \"test_table\"\nconn = f\"mysql+pymysql://{username}:{password}@{host}/{db_name}\"\ndf = read_mysql(table_name, conn)\n</code></pre> <p>Writing dataframe as a table in the database:</p> <pre><code>import bodo\nimport pandas as pd\n\n\n@bodo.jit(distributed=[\"df\"])\ndef write_mysql(df, table_name, conn):\n    df.to_sql(table, conn)\n\n\ntable_name = \"test_table\"\ndf = pd.DataFrame({\"A\": [1.12, 1.1] * 5, \"B\": [213, -7] * 5})\nconn = f\"mysql+pymysql://{username}:{password}@{host}/{db_name}\"\nwrite_mysql(df, table_name, conn)\n</code></pre>"},{"location":"file_io/#oracle-database","title":"Oracle Database","text":""},{"location":"file_io/#prerequisites_2","title":"Prerequisites","text":"<p>In addition to <code>sqlalchemy</code>, install <code>cx_oracle</code> and Oracle instant client driver. If you have installed Bodo using pip:</p> <pre><code>pip install cx-Oracle\n</code></pre> <p>If you are using Bodo in a conda environment:</p> <pre><code>conda install cx_oracle -c conda-forge\n</code></pre> <ul> <li>Then, Download \"Basic\" or \"Basic light\" package matching your operating system from here.</li> <li>Unzip package and add it to <code>LD_LIBRARY_PATH</code> environment variable on Linux. For MacOS, use <code>init_oracle_client()</code> in your application to pass the Oracle Client directory name. See Using cx_Oracle.init_oracle_client() to set the Oracle Client directory.</li> </ul> <p>Note</p> <p>For linux <code>libaio</code> package is required as well.</p> <ul> <li>pip: <code>pip install libaio</code></li> <li>conda: <code>conda install libaio -c conda-forge</code></li> </ul> <p>See cx_oracle for more information. Alternatively, Oracle instant driver can be automatically downloaded using <code>wget</code> or <code>curl</code> commands. Here's an example of automatic installation on a Linux OS machine.</p> <pre><code>conda install cx_oracle libaio -c conda-forge\nmkdir -p /opt/oracle\ncd /opt/oracle\nwget https://download.oracle.com/otn_software/linux/instantclient/215000/instantclient-basic-linux.x64-21.5.0.0.0dbru.zip\nunzip instantclient-basic-linux.x64-21.5.0.0.0dbru.zip\nexport LD_LIBRARY_PATH=/opt/oracle/instantclient_21_5:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"file_io/#usage_2","title":"Usage","text":"<p>Reading result of a SQL query in a dataframe:</p> <pre><code>import bodo\nimport pandas as pd\n\n\n@bodo.jit(distributed=[\"df\"])\ndef read_oracle(table_name, conn):\n    df = pd.read_sql(\n            f\"SELECT * FROM {table_name}\",\n            conn\n        )\n    return df\n\n\ntable_name = \"test_table\"\nconn = f\"oracle+cx_oracle://{username}:{password}@{host}/{db_name}\"\ndf = read_oracle(table_name, conn)\n</code></pre> <p>Writing dataframe as a table in the database:</p> <pre><code>import bodo\nimport pandas as pd\n\n\n@bodo.jit(distributed=[\"df\"])\ndef write_mysql(df, table_name, conn):\n    df.to_sql(table, conn)\n\n\ntable_name = \"test_table\"\ndf = pd.DataFrame({\"A\": [1.12, 1.1] * 5, \"B\": [213, -7] * 5})\nconn = f\"oracle+cx_oracle://{username}:{password}@{host}/{db_name}\"\nwrite_mysql(df, table_name, conn)\n</code></pre>"},{"location":"file_io/#postgresql","title":"PostgreSQL","text":""},{"location":"file_io/#prerequisites_3","title":"Prerequisites","text":"<p>In addition to <code>sqlalchemy</code>, install <code>psycopg2</code>.</p> <p>If you have installed Bodo using pip:</p> <pre><code>pip install psycopg2\n</code></pre> <p>If you are using Bodo in a conda environment:</p> <pre><code>conda install psycopg2 -c conda-forge\n</code></pre>"},{"location":"file_io/#usage_3","title":"Usage","text":"<p>Reading result of a SQL query in a dataframe:</p> <pre><code>import bodo\nimport pandas as pd\n\n\n@bodo.jit(distributed=[\"df\"])\ndef read_postgresql(table_name, conn):\n    df = pd.read_sql(\n            f\"SELECT * FROM {table_name}\",\n            conn\n        )\n    return df\n\n\ntable_name = \"test_table\"\nconn = f\"postgresql+psycopg2://{username}:{password}@{host}/{db_name}\"\ndf = read_postgresql(table_name, conn)\n</code></pre> <p>Writing dataframe as a table in the database:</p> <pre><code>import bodo\nimport pandas as pd\n\n\n@bodo.jit(distributed=[\"df\"])\ndef write_postgresql(df, table_name, conn):\n    df.to_sql(table, conn)\n\n\ntable_name = \"test_table\"\ndf = pd.DataFrame({\"A\": [1.12, 1.1] * 5, \"B\": [213, -7] * 5})\nconn = f\"postgresql+psycopg2://{username}:{password}@{host}/{db_name}\"\nwrite_postgresql(df, table_name, conn)\n</code></pre>"},{"location":"file_io/#non-constant-filepaths","title":"Specifying I/O Data Types Manually","text":"<p>In some rase use cases, the dataset path cannot be a constant value or a JIT function argument. In such cases, the path is determined dynamically, which does not allow automatic Bodo data type inference. Therefore, the user has to provide the data types manually. For example, <code>names</code> and <code>dtypes</code> keyword arguments of <code>pd.read_csv</code> and <code>pd.read_excel</code> allow the user to specify the data types:</p> <pre><code>@bodo.jit\ndef example_csv(fname1, fname2, flag):\n    if flag:\n        file_name = fname1\n    else:\n        file_name = fname2\n    return pd.read_csv(file_name, names = [\"A\", \"B\", \"C\"], dtype={\"A\": int, \"B\": float, \"C\": str})\n</code></pre> <p>For other pandas read functions, the existing APIs do not currently allow this information to be provided. Users can still provide typing information in the <code>bodo.jit</code> decorator, similar to Numba's typing syntax. For example:</p> <pre><code>@bodo.jit(locals={\"df\":{\"one\": bodo.float64[:],\n                  \"two\": bodo.string_array_type,\n                  \"three\": bodo.bool_[:],\n                  \"four\": bodo.float64[:],\n                  \"five\": bodo.string_array_type,\n                  }})\ndef example_df_schema(fname1, fname2, flag):\n    if flag:\n        file_name = fname1\n    else:\n        file_name = fname2\n    df = pd.read_parquet(file_name)\n    return df\n\n\n @bodo.jit(locals={\"X\": bodo.float64[:,:], \"Y\": bodo.float64[:]})\n def example_h5(fname1, fname2, flag):\n    if flag:\n        file_name = fname1\n    else:\n        file_name = fname2\n     f = h5py.File(file_name, \"r\")\n     X = f[\"points\"][:]\n     Y = f[\"responses\"][:]\n</code></pre> <p>For the complete list of supported types, please see the pandas dtype section. In the event that the dtypes are improperly specified, Bodo will throw a runtime error.</p> <p>Warning</p> <p>Providing data types manually is error-prone and should be avoided as much as possible.</p>"},{"location":"objmode/","title":"Using Regular Python inside JIT with @bodo.wrap_python","text":"<p>Regular Python functions and Bodo JIT functions can be used together in applications arbitrarily, but there are cases where regular Python code needs to be used inside JIT code. For example, you may want to use Bodo's parallel constructs with some code that does not have JIT support yet. The @bodo.wrap_python decorator allows calling regular Python functions without jitting them. The main requirement is that the user has to specify the data type of the function's output.</p> <p>For example, the following code calls a non-JIT function on rows of a distributed dataframe. The function decorated with <code>@bodo.wrap_python</code> runs as regular Python and its output data type is annotated as <code>float64</code>.</p> <pre><code>import pandas as pd\nimport numpy as np\nimport bodo\nimport scipy.special as sc\n\n\n@bodo.wrap_python(\"float64\")\ndef my_non_jit_function(r):\n    return np.log(r.A) + sc.entr(r.B)\n\n\n@bodo.jit\ndef objmode_example(n):\n    df = pd.DataFrame({\"A\": np.random.ranf(n), \"B\": np.arange(n)})\n    df[\"C\"] = df.apply(my_non_jit_function, axis=1)\n    print(df[\"C\"].sum())\n\n\nobjmode_example(10)\n</code></pre>"},{"location":"objmode/#output-type-specification","title":"Output Type Specification","text":"<p>There are various ways to specify the output data types in <code>wrap_python</code>. Basic data types such as <code>float64</code> and <code>int64</code> can be specified as string values (as in the previous example). For more complex data types like dataframes, <code>bodo.typeof()</code> can be used on sample data that has the same type as expected outputs. For example:</p> <pre><code>df_sample = pd.DataFrame({\"A\": [0], \"B\": [\"AB\"]}, index=[0])\ndf_type = bodo.typeof(df_sample)\n\n\n@bodo.wrap_python(df_type)\ndef g():\n    return pd.DataFrame({\"A\": [1, 2, 3], \"B\": [\"ab\", \"bc\", \"cd\"]}, index=[3, 2, 1])\n\n\n@bodo.jit\ndef f():\n    df = g()\n    return df\n</code></pre> <p>This is equivalent to creating the <code>DataFrameType</code> directly:</p> <pre><code>df_type = bodo.DataFrameType(\n    (bodo.int64[::1], bodo.string_array_type),\n    bodo.NumericIndexType(bodo.int64),\n    (\"A\", \"B\"),\n)\n</code></pre> <p>The data type can be registered in Bodo so it can be referenced using a string name later:</p> <pre><code>df_sample = pd.DataFrame({\"A\": [0], \"B\": [\"AB\"]}, index=[0])\nbodo.register_type(\"my_df_type\", bodo.typeof(df_sample))\n\n\n@bodo.wrap_python(\"my_df_type\")\ndef g():\n    return pd.DataFrame({\"A\": [1, 2, 3], \"B\": [\"ab\", \"bc\", \"cd\"]}, index=[3, 2, 1])\n</code></pre> <p>See pandas datatypes for more details on Bodo data types in general.</p>"},{"location":"objmode/#what-can-be-done-inside-bodowrap_python","title":"What Can Be Done Inside @bodo.wrap_python","text":"<p>The code inside <code>@bodo.wrap_python</code> runs in regular Python, which means <code>@bodo.wrap_python</code> does not include Bodo compiler's automatic parallel communication management. Therefore, the computation inside <code>@bodo.wrap_python</code> should be independent on different processors and not require communication. In general:</p> <ul> <li>Operations on scalars are safe</li> <li>Operations that compute on rows independently are safe</li> <li>Operations that compute across rows may not be safe</li> </ul> <p>The example below demonstrates a valid use of <code>@bodo.wrap_python</code>, since it uses <code>df.apply(axis=1)</code> which runs on different rows independently. </p> <pre><code>df_type = bodo.typeof(pd.DataFrame({\"A\": [1], \"B\": [1], \"C\": [1]}))\n\n@bodo.wrap_python(df_type)\ndef f(df):\n    return df.assign(C=df.apply(lambda r: r.A + r.B, axis=1))\n\n\n@bodo.jit\ndef valid_objmode():\n    df = pd.read_parquet(\"in_file.pq\")\n    df2 = f(df)\n    df2.to_parquet(\"out_file.pq\")\n\n\nvalid_objmode()\n</code></pre> <p>In contrast, the example below demonstrates an invalid use of <code>@bodo.wrap_python</code>. The reason is that groupby computation requires grouping together all rows with the same key across all chunks. However, on each processor, Bodo passes a chunk of <code>df</code> to <code>@bodo.wrap_python</code> which returns results from local groupby computation. Therefore, <code>df2</code> does not include valid global groupby output.</p> <pre><code>df_type = bodo.typeof(pd.DataFrame({\"A\": [1], \"B\": [1]}))\n\n\n@bodo.wrap_python(df_type)\ndef f(df):\n    return df.groupby(\"A\", as_index=False).sum()\n\n\n@bodo.jit\ndef invalid_objmode():\n    df = pd.read_parquet(\"in_file.pq\")\n    # Invalid use of wrap_python\n    df2 = f(df)\n    df2.to_parquet(\"out_file.pq\")\n\n\ninvalid_objmode()\n</code></pre>"},{"location":"api_docs/","title":"API Reference","text":"<ul> <li>Bodo DataFrame Library API</li> <li>Bodo JIT Compiler Python Support API Reference</li> <li>Bodo Parallel API Reference</li> <li>SQL Reference</li> <li>Bodo Platform SDK Reference</li> </ul>"},{"location":"api_docs/miscellaneous/","title":"Miscellaneous Supported Python API","text":"<p>In this page, we will discuss some useful Bodo features and concepts.</p>"},{"location":"api_docs/miscellaneous/#nullable-integers-in-pandas","title":"Nullable Integers in Pandas","text":"<p>DataFrame and Series objects with integer data need special care due to integer NA issues in Pandas. By default, Pandas dynamically converts integer columns to floating point when missing values (NAs) are needed, which can result in loss of precision as well as type instability.</p> <p></p> <p>Pandas introduced a new nullable integer datatype that can solve this issue, which is also supported by Bodo. For example, this code reads column A into a nullable integer array (the capital \"I\" denotes nullable integer type):</p> <pre><code>data = (\n    \"11,1.2\\n\"\n    \"-2,\\n\"\n    \",3.1\\n\"\n    \"4,-0.1\\n\"\n)\n\nwith open(\"data/data.csv\", \"w\") as f:\n    f.write(data)\n\n\n@bodo.jit(distributed=[\"df\"])\ndef f():\n    dtype = {\"A\": \"Int64\", \"B\": \"float64\"}\n    df = pd.read_csv(\"data/data.csv\", dtype = dtype, names = dtype.keys())\n    return df\n\nf()\n</code></pre> <p> </p> <p></p>"},{"location":"api_docs/miscellaneous/#checking-na-values","title":"Checking NA Values","text":"<p>When an operation iterates over the values in a Series or Array, type stability requires special handling for NAs using <code>pd.isna()</code>. For example, <code>Series.map()</code> applies an operation to each element in the series and failing to check for NAs can result in garbage values propagating.</p> <pre><code>S = pd.Series(pd.array([1, None, None, 3, 10], dtype=\"Int8\"))\n\n@bodo.jit\ndef map_copy(S):\n    return S.map(lambda a: a if not pd.isna(a) else None)\n\nprint(map_copy(S))\n</code></pre> <pre><code>0       1\n1     &lt;NA&gt;\n2     &lt;NA&gt;\n3       3\n4      10\ndtype: Int8\n</code></pre>"},{"location":"api_docs/miscellaneous/#boxingunboxing-overheads","title":"Boxing/Unboxing Overheads","text":"<p>Bodo uses efficient native data structures which can be different than Python. When Python values are passed to Bodo, they are unboxed to native representation. On the other hand, returning Bodo values requires boxing to Python objects. Boxing and unboxing can have significant overhead depending on size and type of data. For example, passing string column between Python/Bodo repeatedly can be expensive:</p> <p><pre><code>@bodo.jit(distributed=[\"df\"])\ndef gen_data():\n    df = pd.read_parquet(\"data/cycling_dataset.pq\")\n    df[\"hr\"] = df[\"hr\"].astype(str)\n    return df\n\n@bodo.jit(distributed=[\"df\", \"x\"])\ndef mean_power(df):\n    x = df.hr.str[1:]\n    return x\n\ndf = gen_data()\nres = mean_power(df)\nprint(res)\n</code></pre> Output:</p> <pre><code>0        1\n1        2\n2        2\n3        3\n4        3\n        ..\n3897    00\n3898    00\n3899    00\n3900    00\n3901    00\nName: hr, Length: 3902, dtype: object\n</code></pre> <p>One can try to keep data in Bodo functions as much as possible to avoid boxing/unboxing overheads:</p> <pre><code>@bodo.jit(distributed=[\"df\"])\ndef gen_data():\n    df = pd.read_parquet(\"data/cycling_dataset.pq\")\n    df[\"hr\"] = df[\"hr\"].astype(str)\n    return df\n\n@bodo.jit(distributed=[\"df\", \"x\"])\ndef mean_power(df):\n    x = df.hr.str[1:]\n    return x\n\n@bodo.jit\ndef f():\n    df = gen_data()\n    res = mean_power(df)\n    print(res)\n\nf()\n</code></pre> <pre><code>0        1\n1        2\n2        2\n3        3\n4        3\n        ..\n3897    00\n3898    00\n3899    00\n3900    00\n3901    00\nName: hr, Length: 3902, dtype: object\n</code></pre>"},{"location":"api_docs/miscellaneous/#iterating-over-columns","title":"Iterating Over Columns","text":"<p>Iterating over columns in a dataframe can cause type stability issues, since column types in each iteration can be different. Bodo supports this usage for many practical cases by automatically unrolling loops over dataframe columns when possible. For example, the example below computes the sum of all data frame columns:</p> <pre><code>@bodo.jit\ndef f():\n    n = 20\n    df = pd.DataFrame({\"A\": np.arange(n), \"B\": np.arange(n) ** 2, \"C\": np.ones(n)})\n    s = 0\n    for c in df.columns:\n     s += df[c].sum()\n    return s\n\nf()\n</code></pre> <pre><code>2680.0\n</code></pre> <p>For automatic unrolling, the loop needs to be a <code>for</code> loop over column names that can be determined by Bodo at compile time.</p>"},{"location":"api_docs/miscellaneous/#regular-expressions-using-re","title":"Regular Expressions using <code>re</code>","text":"<p>Bodo supports string processing using Pandas and the <code>re</code> standard package, offering significant flexibility for string processing applications. For example, <code>re</code> can be used in user-defined functions (UDFs) applied to Series and DataFrame values:</p> <pre><code>import re\n\n@bodo.jit\ndef f(S):\n    def g(a):\n        res = 0\n        if re.search(\".*AB.*\", a):\n            res = 3\n        if re.search(\".*23.*\", a):\n            res = 5\n        return res\n\n    return S.map(g)\n\nS = pd.Series([\"AABCDE\", \"BBABCE\", \"1234\"])\nf(S)\n</code></pre> <pre><code>0    3\n1    3\n2    5\ndtype: int64\n</code></pre> <p>Below is a reference list of supported functionality. Full functionality is documented in standard re documentation. All functions except finditer are supported. Note that currently, Bodo JIT uses Python's <code>re</code> package as backend and therefore the compute speed of these functions is similar to Python.</p>"},{"location":"api_docs/miscellaneous/#rea","title":"re.A","text":"<ul> <li><code>re.A</code></li> </ul>"},{"location":"api_docs/miscellaneous/#reascii","title":"re.ASCII","text":"<ul> <li><code>re.ASCII</code></li> </ul>"},{"location":"api_docs/miscellaneous/#redebug","title":"re.DEBUG","text":"<ul> <li><code>re.DEBUG</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rei","title":"re.I","text":"<ul> <li><code>re.I</code></li> </ul>"},{"location":"api_docs/miscellaneous/#reignorecase","title":"re.IGNORECASE","text":"<ul> <li><code>re.IGNORECASE</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rel","title":"re.L","text":"<ul> <li><code>re.L</code></li> </ul>"},{"location":"api_docs/miscellaneous/#relocale","title":"re.LOCALE","text":"<ul> <li><code>re.LOCALE</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rem","title":"re.M","text":"<ul> <li><code>re.M</code></li> </ul>"},{"location":"api_docs/miscellaneous/#remultiline","title":"re.MULTILINE","text":"<ul> <li><code>re.MULTILINE</code></li> </ul>"},{"location":"api_docs/miscellaneous/#res","title":"re.S","text":"<ul> <li><code>re.S</code></li> </ul>"},{"location":"api_docs/miscellaneous/#redotall","title":"re.DOTALL","text":"<ul> <li><code>re.DOTALL</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rex","title":"re.X","text":"<ul> <li><code>re.X</code></li> </ul>"},{"location":"api_docs/miscellaneous/#reverbose","title":"re.VERBOSE","text":"<ul> <li><code>re.VERBOSE</code></li> </ul>"},{"location":"api_docs/miscellaneous/#research","title":"re.search","text":"<ul> <li><code>re.search(pattern, string, flags=0)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematch","title":"re.match","text":"<ul> <li><code>re.match(pattern, string, flags=0)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#refullmatch","title":"re.fullmatch","text":"<ul> <li><code>re.fullmatch(pattern, string, flags=0)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#resplit","title":"re.split","text":"<ul> <li><code>re.split(pattern, string, maxsplit=0, flags=0)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#refindall","title":"re.findall","text":"<ul> <li> <p><code>re.findall(pattern, string, flags=0)</code></p> <p>The <code>pattern</code> argument should be a constant string for multi-group patterns (for Bodo to know the output will be a list of string tuples). An error is raised otherwise.</p> <p>Example Usage:</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(pat, in_str):\n...     return re.findall(pat, in_str)\n...\n&gt;&gt;&gt; f(r\"\\w+\", \"Words, words, words.\")\n['Words', 'words', 'words']\n</code></pre> <p>Constant multi-group pattern works:</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f2(in_str):\n...     return re.findall(r\"(\\w+).*(\\d+)\", in_str)\n...\n&gt;&gt;&gt; f2(\"Words, 123\")\n[('Words', '3')]\n</code></pre> <p>Non-constant multi-group pattern throws an error:</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(pat, in_str):\n...     return re.findall(pat, in_str)\n...\n&gt;&gt;&gt; f(r\"(\\w+).*(\\d+)\", \"Words, 123\")\nTraceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nFile \"/Users/user/dev/bodo/bodo/libs/re_ext.py\", line 338, in _pat_findall_impl\n    raise ValueError(\nValueError: pattern string should be constant for 'findall' with multiple groups\n</code></pre> </li> </ul>"},{"location":"api_docs/miscellaneous/#resub","title":"re.sub","text":"<ul> <li><code>re.sub(pattern, repl, string, count=0, flags=0)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#resubn","title":"re.subn","text":"<ul> <li><code>re.subn(pattern, repl, string, count=0, flags=0)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#reescape","title":"re.escape","text":"<ul> <li><code>re.escape(pattern)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repurge","title":"re.purge","text":"<ul> <li><code>re.purge</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repatternsearch","title":"re.Pattern.search","text":"<ul> <li><code>re.Pattern.search(string[, pos[, endpos]])</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repatternmatch","title":"re.Pattern.match","text":"<ul> <li><code>re.Pattern.match(string[, pos[, endpos]])</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repatternfullmatch","title":"re.Pattern.fullmatch","text":"<ul> <li><code>re.Pattern.fullmatch(string[, pos[, endpos]])</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repatternsplit","title":"re.Pattern.split","text":"<ul> <li><code>re.Pattern.split(string, maxsplit=0)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repatternfindall","title":"re.Pattern.findall","text":"<ul> <li><code>re.Pattern.findall(string[, pos[, endpos]])</code></li> </ul> <p>This has the same limitation as <code>re.findall</code>. </p>"},{"location":"api_docs/miscellaneous/#repatternsub","title":"re.Pattern.sub","text":"<ul> <li><code>re.Pattern.sub(repl, string, count=0)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repatternsubn","title":"re.Pattern.subn","text":"<ul> <li><code>re.Pattern.subn(repl, string, count=0)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repatternflags","title":"re.Pattern.flags","text":"<ul> <li><code>re.Pattern.flags</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repatterngroups","title":"re.Pattern.groups","text":"<ul> <li><code>re.Pattern.groups</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repatterngroupindex","title":"re.Pattern.groupindex","text":"<ul> <li><code>re.Pattern.groupindex</code></li> </ul>"},{"location":"api_docs/miscellaneous/#repatternpattern","title":"re.Pattern.pattern","text":"<ul> <li><code>re.Pattern.pattern</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchexpand","title":"re.Match.expand","text":"<ul> <li><code>re.Match.expand(template)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchgroup","title":"re.Match.group","text":"<ul> <li><code>re.Match.group([group1, ...])</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematch__getitem__","title":"re.Match.__getitem__","text":"<ul> <li><code>re.Match.__getitem__(g)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchgroups","title":"re.Match.groups","text":"<ul> <li><code>re.Match.groups(default=None)</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchgroupdict","title":"re.Match.groupdict","text":"<ul> <li><code>re.Match.groupdict(default=None)</code></li> </ul> <p>(does not support default=None for groups that did not participate     in the match)</p>"},{"location":"api_docs/miscellaneous/#rematchstart","title":"re.Match.start","text":"<ul> <li><code>re.Match.start([group])</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchend","title":"re.Match.end","text":"<ul> <li><code>re.Match.end([group])</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchspan","title":"re.Match.span","text":"<ul> <li><code>re.Match.span([group])</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchpos","title":"re.Match.pos","text":"<ul> <li><code>re.Match.pos</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchendpos","title":"re.Match.endpos","text":"<ul> <li><code>re.Match.endpos</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchlastindex","title":"re.Match.lastindex","text":"<ul> <li><code>re.Match.lastindex</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchlastgroup","title":"re.Match.lastgroup","text":"<ul> <li><code>re.Match.lastgroup</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchre","title":"re.Match.re","text":"<ul> <li><code>re.Match.re</code></li> </ul>"},{"location":"api_docs/miscellaneous/#rematchstring","title":"re.Match.string","text":"<ul> <li><code>re.Match.string</code></li> </ul>"},{"location":"api_docs/miscellaneous/#class-support-using-jitclass","title":"Class Support using <code>@jitclass</code>","text":"<p>Bodo supports Python classes using the <code>@bodo.jitclass</code> decorator. It requires type annotation of the fields, as well as distributed annotation where applicable. For example, the example class below holds a distributed dataframe and a name filed. Types can either be specified directly using the imports in the bodo package or can be inferred from existing types using <code>bodo.typeof</code>. The <code>init</code> function is required, and has to initialize the attributes. In addition, subclasses are not supported in <code>jitclass</code> yet.</p> <p>Warning</p> <p>Class support is currently experimental and therefore we recommend refactoring computation into regular JIT functions instead if possible.</p> <pre><code>@bodo.jitclass(\n    {\n        \"df\": bodo.typeof(pd.DataFrame({\"A\": [1], \"B\": [0.1]})),\n        \"name\": bodo.string_type,\n    },\n    distributed=[\"df\"],\n)\n\nclass MyClass:\n    def init(self, n, name):\n        self.df = pd.DataFrame({\"A\": np.arange(n), \"B\": np.ones(n)})\n        self.name = name\n\n    def sum(self):\n        return self.df.A.sum()\n\n    @property\n    def sum_vals(self):\n        return self.df.sum().sum()\n\n    def get_name(self):\n        return self.name\n\n    @staticmethod\n    def add_one(a):\n        return a + 1\n</code></pre> <p>This JIT class can be used in regular Python code, as well as other Bodo JIT code.</p> <pre><code># From a compiled function\n@bodo.jit\ndef f():\n    my_instance = MyClass(32, \"my_name_jit\")\n    print(my_instance.sum())\n    print(my_instance.sum_vals)\n    print(my_instance.get_name())\n\nf()\n</code></pre> <pre><code>496\n528.0\nmy_name_jit\n</code></pre> <pre><code># From regular Python\nmy_instance = MyClass(32, \"my_name_python\")\nprint(my_instance.sum())\nprint(my_instance.sum_vals)\nprint(my_instance.get_name())\nprint(MyClass.add_one(8))\n</code></pre> <pre><code>496\n528.0\nmy_name_python\n9\n</code></pre> <p>Bodo's <code>jitclass</code> is built on top of Numba's <code>jitclass</code> (see Numba jitclass for more details).</p>"},{"location":"api_docs/numpy/","title":"Numpy Operations","text":"<p>Below is the list of the data-parallel Numpy operators that Bodo can optimize and parallelize.</p>"},{"location":"api_docs/numpy/#numpy-element-wise-array-operations","title":"Numpy element-wise array operations","text":""},{"location":"api_docs/numpy/#unary-operators","title":"Unary operators","text":"<ul> <li><code>+</code> </li> <li><code>-</code></li> <li><code>~</code></li> </ul>"},{"location":"api_docs/numpy/#binary-operators","title":"Binary operators","text":"<ul> <li><code>+</code> </li> <li><code>-</code> </li> <li><code>*</code> </li> <li><code>/</code> </li> <li><code>/?</code></li> <li><code>%</code> </li> <li><code>|</code> </li> <li><code>&gt;&gt;</code> </li> <li><code>^</code> </li> <li><code>&lt;&lt;</code></li> <li><code>&amp;</code> </li> <li><code>**</code> </li> <li><code>//</code></li> </ul>"},{"location":"api_docs/numpy/#comparison-operators","title":"Comparison operators","text":"<ul> <li><code>==</code></li> <li><code>!=</code></li> <li><code>&lt;</code> </li> <li><code>&lt;=</code> </li> <li><code>&gt;</code> </li> <li><code>&gt;=</code></li> </ul>"},{"location":"api_docs/numpy/#data-parallel-math-operations","title":"Data-parallel math operations","text":"<ul> <li><code>numpy.add</code></li> <li><code>numpy.subtract</code></li> <li><code>numpy.multiply</code></li> <li><code>numpy.divide</code></li> <li><code>numpy.logaddexp</code></li> <li><code>numpy.logaddexp2</code></li> <li><code>numpy.true_divide</code></li> <li><code>numpy.floor_divide</code></li> <li><code>numpy.negative</code></li> <li><code>numpy.positive</code></li> <li><code>numpy.power</code></li> <li><code>numpy.remainder</code></li> <li><code>numpy.mod</code></li> <li><code>numpy.fmod</code></li> <li><code>numpy.abs</code></li> <li><code>numpy.absolute</code></li> <li><code>numpy.fabs</code></li> <li><code>numpy.rint</code></li> <li><code>numpy.sign</code></li> <li><code>numpy.conj</code></li> <li><code>numpy.exp</code></li> <li><code>numpy.exp2</code></li> <li><code>numpy.log</code></li> <li><code>numpy.log2</code></li> <li><code>numpy.log10</code></li> <li><code>numpy.expm1</code></li> <li><code>numpy.log1p</code></li> <li><code>numpy.sqrt</code></li> <li><code>numpy.square</code></li> <li><code>numpy.reciprocal</code></li> <li><code>numpy.gcd</code></li> <li><code>numpy.lcm</code></li> <li><code>numpy.conjugate</code></li> </ul>"},{"location":"api_docs/numpy/#trigonometric-functions","title":"Trigonometric functions","text":"<ul> <li><code>numpy.sin</code></li> <li><code>numpy.cos</code></li> <li><code>numpy.tan</code></li> <li><code>numpy.arcsin</code></li> <li><code>numpy.arccos</code></li> <li><code>numpy.arctan</code></li> <li><code>numpy.arctan2</code></li> <li><code>numpy.hypot</code></li> <li><code>numpy.sinh</code></li> <li><code>numpy.cosh</code></li> <li><code>numpy.tanh</code></li> <li><code>numpy.arcsinh</code></li> <li><code>numpy.arccosh</code></li> <li><code>numpy.arctanh</code></li> <li><code>numpy.deg2rad</code></li> <li><code>numpy.rad2deg</code></li> <li><code>numpy.degrees</code></li> <li><code>numpy.radians</code></li> </ul>"},{"location":"api_docs/numpy/#bit-manipulation-functions","title":"Bit manipulation functions","text":"<ul> <li><code>numpy.bitwise_and</code></li> <li><code>numpy.bitwise_or</code></li> <li><code>numpy.bitwise_xor</code></li> <li><code>numpy.bitwise_not</code></li> <li><code>numpy.invert</code></li> <li><code>numpy.left_shift</code></li> <li><code>numpy.right_shift</code></li> </ul>"},{"location":"api_docs/numpy/#comparison-functions","title":"Comparison functions","text":"<ul> <li><code>numpy.logical_and</code></li> <li><code>numpy.logical_or</code></li> <li><code>numpy.logical_xor</code></li> <li><code>numpy.logical_not</code></li> </ul>"},{"location":"api_docs/numpy/#floating-functions","title":"Floating functions","text":"<ul> <li><code>numpy.isfinite</code></li> <li><code>numpy.isinf</code></li> <li><code>numpy.signbit</code></li> <li><code>numpy.ldexp</code></li> <li><code>numpy.floor</code></li> <li><code>numpy.ceil</code></li> <li><code>numpy.trunc</code></li> </ul>"},{"location":"api_docs/numpy/#numpy-reduction-functions","title":"Numpy reduction functions","text":"<ul> <li><code>numpy.sum</code></li> <li><code>numpy.prod</code></li> <li><code>numpy.min</code></li> <li><code>numpy.max</code></li> <li><code>numpy.argmin</code></li> <li><code>numpy.argmax</code></li> <li><code>numpy.all</code></li> <li><code>numpy.any</code></li> </ul>"},{"location":"api_docs/numpy/#numpy-array-creation-functions","title":"Numpy array creation functions","text":"<ul> <li><code>numpy.empty</code></li> <li><code>numpy.identity</code></li> <li><code>numpy.zeros</code></li> <li><code>numpy.ones</code></li> <li><code>numpy.empty_like</code></li> <li><code>numpy.zeros_like</code></li> <li><code>numpy.ones_like</code></li> <li><code>numpy.full_like</code></li> <li><code>numpy.array</code></li> <li><code>numpy.asarray</code></li> <li><code>numpy.copy</code></li> <li><code>numpy.arange</code></li> <li><code>numpy.linspace</code></li> <li><code>numpy.repeat</code>  only scalar <code>num_repeats</code></li> </ul>"},{"location":"api_docs/numpy/#numpy-array-manipulation-functions","title":"Numpy array manipulation functions","text":"<ul> <li><code>numpy.shape</code></li> <li> <p><code>numpy.reshape</code> </p> <p><code>shape</code> values cannot be -1.</p> </li> <li> <p><code>numpy.sort</code></p> </li> <li><code>numpy.concatenate</code></li> <li><code>numpy.append</code></li> <li> <p><code>numpy.unique</code>  The output is assumed to be \"small\" relative to input and is replicated.                   Use <code>Series.drop_duplicates()</code> if the output should remain distributed.</p> </li> <li> <p><code>numpy.where</code> (1 and 3 arguments)</p> </li> <li><code>numpy.select</code>  The default value for numeric/boolean types is <code>0/False</code>. For all other                   types, the default is <code>pd.NA</code>. If any of the values in                   <code>choicelist</code> are nullable, or the default is <code>pd.NA</code> or <code>None</code>, the                   output will be a nullable pandas array instead of a numpy                   array.  </li> <li><code>numpy.nan_to_num</code> converts infinity/NaN values to regular floats.</li> <li><code>numpy.union1d</code></li> <li><code>numpy.intersect1d</code>  no distributed support yet</li> <li><code>numpy.setdiff1d</code>  no distributed support yet</li> <li><code>numpy.hstack</code>  concatenates elements on each rank without maintaining order</li> <li><code>numpy.tile</code>  Supported in 2 cases: the array is 2D and <code>reps</code> is in the form <code>(1, x)</code>, or                 the array is 1D and <code>reps</code> is in the form <code>(x, 1)</code>.   </li> <li><code>numpy.ndarray.T</code> distributed array transpose is supported for 2D arrays.</li> </ul>"},{"location":"api_docs/numpy/#numpy-mathematical-and-statistics-functions","title":"Numpy mathematical and statistics functions","text":"<ul> <li><code>numpy.cumsum</code></li> <li><code>numpy.diff</code></li> <li><code>numpy.percentile</code></li> <li><code>numpy.quantile</code></li> <li><code>numpy.median</code></li> <li><code>numpy.mean</code></li> <li><code>numpy.std</code></li> <li><code>numpy.interp</code> no distributed support yet.</li> <li><code>np.linalg.norm</code> parallelized only for 2D inputs with axis=1.</li> </ul>"},{"location":"api_docs/numpy/#random-number-generator-functions","title":"Random number generator functions","text":"<ul> <li><code>numpy.random.rand</code></li> <li><code>numpy.random.randn</code></li> <li><code>numpy.random.ranf</code></li> <li><code>numpy.random.random_sample</code></li> <li><code>numpy.random.sample</code></li> <li><code>numpy.random.random</code></li> <li><code>numpy.random.standard_normal</code></li> <li><code>numpy.random.multivariate_normal</code> (must provide size)</li> <li><code>numpy.random.chisquare</code></li> <li><code>numpy.random.weibull</code></li> <li><code>numpy.random.power</code></li> <li><code>numpy.random.geometric</code></li> <li><code>numpy.random.exponential</code></li> <li><code>numpy.random.poisson</code></li> <li><code>numpy.random.rayleigh</code></li> <li><code>numpy.random.normal</code></li> <li><code>numpy.random.uniform</code></li> <li><code>numpy.random.beta</code></li> <li><code>numpy.random.binomial</code></li> <li><code>numpy.random.f</code></li> <li><code>numpy.random.gamma</code></li> <li><code>numpy.random.lognormal</code></li> <li><code>numpy.random.laplace</code></li> <li><code>numpy.random.randint</code></li> <li><code>numpy.random.triangular</code></li> </ul>"},{"location":"api_docs/numpy/#numpydot-function","title":"<code>numpy.dot</code> function","text":"<ul> <li><code>numpy.dot</code> between a matrix and a vector or between two vectors.</li> </ul>"},{"location":"api_docs/numpy/#numpy-io","title":"Numpy I/O","text":"<ul> <li><code>numpy.ndarray.tofile</code> </li> <li><code>numpy.fromfile</code> supports reading binary files. <code>file</code>, <code>dtype</code>, <code>count</code>, and <code>offset</code> arguments are supported (<code>file</code> and <code>dtype</code> are required). <code>file</code> should be a string. <code>s3://</code> and <code>hdfs://</code> file paths are also supported.</li> </ul> <p>Our documentation on scalable I/O contains example usage and more system specific instructions.</p>"},{"location":"api_docs/numpy/#numpy-matrix-support","title":"Numpy matrix support","text":"<ul> <li><code>numpy.asmatrix</code> parallelized only for array or matrix input.</li> <li><code>*</code> left-hand side argument can be distributed but right-hand side argument is replicated.</li> </ul>"},{"location":"api_docs/numpy/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Numpy array comprehension : e.g. : A = np.array([i**2 for i in range(N)])</li> </ul> <p>Note</p> <p>Optional arguments are not supported unless if explicitly mentioned here. For operations on multi-dimensional arrays, automatic broadcast of dimensions of size 1 is not supported.</p>"},{"location":"api_docs/numpy/#numpy-dot-parallelization","title":"Numpy dot() Parallelization","text":"<p>The <code>np.dot</code> function has different distribution rules based on the number of dimensions and the distributions of its input arrays. The example below demonstrates two cases:</p> <pre><code>@bodo.jit\ndef example_dot(N, D):\n    X = np.random.ranf((N, D))\n    Y = np.random.ranf(N)\n    w = np.dot(Y, X)\n    z = np.dot(X, w)\n    return z.sum()\n\nexample_dot(1024, 10)\nexample_dot.distributed_diagnostics()\n</code></pre> <p>Here is the output of <code>distributed_diagnostics()</code>:</p> <pre><code>Data distributions:\n  $X.130               1D_Block\n  $Y.131               1D_Block\n  $b.2.158             REP\n\nParfor distributions:\n  0                    1D_Block\n  1                    1D_Block\n  3                    1D_Block\n\nDistributed listing for function example_dot, ../tmp/dist_rep.py (4)\n++++++++++++++++++++++++++++++++++| parfor_id/variable: distribution\n@bodo.jit                         |\ndef example_dot(N, D):            |\n    X = np.random.ranf((N, D))++++| #0: 1D_Block, $X.130: 1D_Block\n    Y = np.random.ranf(N)+++++++++| #1: 1D_Block, $Y.131: 1D_Block\n    w = np.dot(Y, X)++++++++++++++| $b.2.158: REP\n    z = np.dot(X, w)++++++++++++++| #3: 1D_Block\n    return z.sum()                |\n</code></pre> <p>The first <code>dot</code> has a 1D array with <code>1D_Block</code> distribution as first input <code>Y</code>, while the second input <code>X</code> is a 2D array with <code>1D_Block</code> distribution. Hence, <code>dot</code> is a sum reduction across distributed datasets and therefore, the output (<code>w</code>) is on the <code>reduce</code> side and is  assigned <code>REP</code> distribution.</p> <p>The second <code>dot</code> has a 2D array with <code>1D_Block</code> distribution (<code>X</code>) as first input, while the second input is a REP array (<code>w</code>). Hence, the computation is data-parallel across rows of <code>X</code>, which implies a <code>1D_Block</code> distribution for output (<code>z</code>).</p> <p>Variable <code>z</code> does not exist in the distribution report since the compiler optimizations were able to eliminate it. Its values are generated and consumed on-the-fly, without memory load/store overheads.</p>"},{"location":"api_docs/platform_sdk/","title":"Bodo Platform SDK Reference","text":""},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster","title":"<code>bodosdk.clients.cluster</code>","text":""},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient","title":"<code>ClusterClient</code>","text":"<p>               Bases: <code>IClusterClient</code></p> <p>A client for managing cluster operations in a Bodo workspace.</p> <p>Attributes:</p> Name Type Description <code>_deprecated_methods</code> <code>Dict</code> <p>A dictionary of deprecated methods.</p> <code>_images</code> <code>List[IBodoImage]</code> <p>A list of available Bodo images.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_client</code> <code>IBodoWorkspaceClient</code> <p>The workspace client used for operations.</p> required"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.Cluster","title":"<code>Cluster: Cluster</code>  <code>property</code>","text":"<p>Provides access to cluster operations.</p> <p>Returns:</p> Name Type Description <code>Cluster</code> <code>Cluster</code> <p>An instance of Cluster for cluster operations.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.ClusterList","title":"<code>ClusterList: ClusterList</code>  <code>property</code>","text":"<p>Provides access to listing clusters.</p> <p>Returns:</p> Name Type Description <code>ClusterList</code> <code>ClusterList</code> <p>An instance of ClusterListAPIModel for listing clusters.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.latest_bodo_version","title":"<code>latest_bodo_version: str</code>  <code>property</code>","text":"<p>Retrieves the latest Bodo version available.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The latest Bodo version.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.__init__","title":"<code>__init__(workspace_client)</code>","text":"<p>Initializes the ClusterClient with a given workspace client.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_client</code> <code>IBodoWorkspaceClient</code> <p>The workspace client to interact with the API.</p> required"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.connect","title":"<code>connect(catalog, cluster_id)</code>","text":"<p>Connect to a specific catalog and cluster.</p> <p>:param catalog: The name the catalog to connect to. :param cluster_id: The UUID of the cluster to connect to. :return: An instance of Connection representing the connection to the catalog and cluster.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.create","title":"<code>create(name, instance_type=None, workers_quantity=None, description=None, bodo_version=None, auto_stop=None, auto_pause=None, auto_upgrade=None, auto_az=None, use_spot_instance=None, aws_deployment_subnet_id=None, availability_zone=None, instance_role=None, custom_tags=None, memory_report_enabled=None)</code>","text":"<p>Creates a new cluster with the specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the cluster.</p> required <code>instance_type</code> <code>str</code> <p>The type of instance to use for the cluster nodes.</p> <code>None</code> <code>workers_quantity</code> <code>int</code> <p>The number of worker nodes in the cluster.</p> <code>None</code> <code>description</code> <code>str</code> <p>A description of the cluster.</p> <code>None</code> <code>bodo_version</code> <code>str</code> <p>The Bodo version to use for the cluster. If not provided, the latest version is used.</p> <code>None</code> <code>auto_stop</code> <code>int</code> <p>The auto-stop time in minutes for the cluster.</p> <code>None</code> <code>auto_pause</code> <code>int</code> <p>The auto-pause time in minutes for the cluster.</p> <code>None</code> <code>auto_upgrade</code> <code>bool</code> <p>Should the cluster be automatically upgraded to the latest Bodo version on restart.</p> <code>None</code> <code>auto_az</code> <code>bool</code> <p>Whether to automatically select the availability zone.</p> <code>None</code> <code>use_spot_instance</code> <code>bool</code> <p>Whether to use spot instances for the cluster.</p> <code>None</code> <code>aws_deployment_subnet_id</code> <code>str</code> <p>The AWS deployment subnet ID.</p> <code>None</code> <code>availability_zone</code> <code>str</code> <p>The availability zone for the cluster.</p> <code>None</code> <code>instance_role</code> <code>InstanceRole | Dict</code> <p>The instance role or a custom role configuration.</p> <code>None</code> <code>custom_tags</code> <code>Dict</code> <p>Custom tags to assign to the cluster resources.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Cluster</code> <code>Cluster</code> <p>The created Cluster object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.get","title":"<code>get(id)</code>","text":"<p>Retrieves a cluster by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the cluster to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Cluster</code> <code>Cluster</code> <p>The retrieved Cluster object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.get_bodo_versions","title":"<code>get_bodo_versions()</code>","text":"<p>Retrieves a list of available Bodo versions.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of available Bodo versions.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.get_images","title":"<code>get_images()</code>","text":"<p>Retrieves a list of available images.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of image IDs available for clusters.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.get_instance_types","title":"<code>get_instance_types()</code>","text":"<p>Retrieves list of all supported instance types</p> Return <p>List[InstanceType]</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.list","title":"<code>list(filters=None, order=None)</code>","text":"<p>Lists clusters based on the provided filters and order.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Dict | ClusterFilter</code> <p>The filters to apply to the cluster listing.</p> <code>None</code> <code>order</code> <code>Dict</code> <p>The order in which to list the clusters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClusterList</code> <code>ClusterList</code> <p>A list of clusters matching the criteria.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.pause","title":"<code>pause(id, wait=False)</code>","text":"<p>Pauses the specified cluster.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the cluster to pause.</p> required <p>Returns:</p> Name Type Description <code>Cluster</code> <code>Cluster</code> <p>The paused Cluster object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.remove","title":"<code>remove(id, wait=False)</code>","text":"<p>Removes the specified cluster.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the cluster to remove.</p> required <p>Returns:</p> Type Description <code>Cluster</code> <p>None</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.resume","title":"<code>resume(id, wait=False)</code>","text":"<p>Resumes the specified paused cluster.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the cluster to resume.</p> required <p>Returns:</p> Name Type Description <code>Cluster</code> <code>Cluster</code> <p>The resumed Cluster object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.scale","title":"<code>scale(id, new_size)</code>","text":"<p>Scales the specified cluster to the new size.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the cluster to scale.</p> required <code>new_size</code> <code>int</code> <p>The new size for the cluster in terms of the number of worker nodes.</p> required <p>Returns:</p> Name Type Description <code>Cluster</code> <code>Cluster</code> <p>The scaled Cluster object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.start","title":"<code>start(id, wait=False)</code>","text":"<p>Starts the specified stopped cluster.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the cluster to start.</p> required <p>Returns:</p> Name Type Description <code>Cluster</code> <code>Cluster</code> <p>The started Cluster object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.stop","title":"<code>stop(id, wait=False)</code>","text":"<p>Stops the specified cluster.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the cluster to stop.</p> required <p>Returns:</p> Name Type Description <code>Cluster</code> <code>Cluster</code> <p>The stopped Cluster object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.update","title":"<code>update(id, name=None, description=None, auto_stop=None, auto_pause=None, auto_upgrade=True, workers_quantity=None, instance_role=None, instance_type=None, bodo_version=None, auto_az=None, availability_zone=None, custom_tags=None)</code>","text":"<p>Updates the specified cluster with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the cluster to update.</p> required <code>name</code> <code>str</code> <p>The new name for the cluster.</p> <code>None</code> <code>description</code> <code>str</code> <p>A new description for the cluster.</p> <code>None</code> <code>auto_stop</code> <code>int</code> <p>The new auto-stop time in minutes.</p> <code>None</code> <code>auto_pause</code> <code>int</code> <p>The new auto-pause time in minutes.</p> <code>None</code> <code>auto_upgrade</code> <code>bool</code> <p>if cluster should be updated after each restart.</p> <code>True</code> <code>workers_quantity</code> <code>int</code> <p>The new number of worker nodes.</p> <code>None</code> <code>instance_role</code> <code>InstanceRole | Dict</code> <p>The new instance role or custom role configuration.</p> <code>None</code> <code>instance_type</code> <code>str</code> <p>The new instance type for the cluster nodes.</p> <code>None</code> <code>bodo_version</code> <code>str</code> <p>The new Bodo version for the cluster.</p> <code>None</code> <code>auto_az</code> <code>bool</code> <p>Whether to automatically select the availability zone.</p> <code>None</code> <code>availability_zone</code> <code>str</code> <p>The new availability zone for the cluster.</p> <code>None</code> <code>custom_tags</code> <code>Dict</code> <p>New custom tags for the cluster resources.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Cluster</code> <code>Cluster</code> <p>The updated Cluster object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.cluster.ClusterClient.wait_for_status","title":"<code>wait_for_status(id, statuses, timeout=300, tick=30)</code>","text":"<p>Waits for the specified cluster to reach any of the given statuses within the timeout period.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the cluster to monitor.</p> required <code>statuses</code> <code>List</code> <p>The list of statuses to wait for.</p> required <code>timeout</code> <code>int</code> <p>The timeout period in seconds.</p> <code>300</code> <code>tick</code> <code>int</code> <p>The interval in seconds between status checks.</p> <code>30</code> <p>Returns:</p> Name Type Description <code>Cluster</code> <code>Cluster</code> <p>The Cluster object if it reaches the desired status within the timeout period.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.instance_role","title":"<code>bodosdk.clients.instance_role</code>","text":""},{"location":"api_docs/platform_sdk/#bodosdk.clients.instance_role.InstanceRoleClient","title":"<code>InstanceRoleClient</code>","text":"<p>               Bases: <code>IInstanceRoleClient</code></p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.instance_role.InstanceRoleClient.InstanceRole","title":"<code>InstanceRole: InstanceRole</code>  <code>property</code>","text":"<p>Get the InstanceRole object.</p> <p>Returns:</p> Name Type Description <code>InstanceRole</code> <code>InstanceRole</code> <p>An instance of InstanceRole.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.instance_role.InstanceRoleClient.InstanceRoleList","title":"<code>InstanceRoleList: InstanceRoleList</code>  <code>property</code>","text":"<p>Get the InstanceRoleList object.</p> <p>Returns:</p> Name Type Description <code>InstanceRoleList</code> <code>InstanceRoleList</code> <p>An instance of InstanceRoleList.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.instance_role.InstanceRoleClient.__init__","title":"<code>__init__(workspace_client)</code>","text":"<p>Initializes the InstanceRoleClient with a given workspace client.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_client</code> <code>IBodoWorkspaceClient</code> <p>The workspace client to interact with the API.</p> required"},{"location":"api_docs/platform_sdk/#bodosdk.clients.instance_role.InstanceRoleClient.create","title":"<code>create(role_arn=None, identity=None, description=None, name=None)</code>","text":"<p>Create a new instance role.</p> <p>Parameters:</p> Name Type Description Default <code>role_arn</code> <code>str</code> <p>The ARN of the role (for aws).</p> <code>None</code> <code>identity</code> <code>str</code> <p>The identity of the role (for azure).</p> <code>None</code> <code>description</code> <code>str</code> <p>A description of the role.</p> <code>None</code> <code>name</code> <code>str</code> <p>The name of the role. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InstanceRole</code> <code>InstanceRole</code> <p>The created instance role after saving.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.instance_role.InstanceRoleClient.delete","title":"<code>delete(id)</code>","text":"<p>Delete an instance role by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The UUID of the instance role to delete.</p> required"},{"location":"api_docs/platform_sdk/#bodosdk.clients.instance_role.InstanceRoleClient.get","title":"<code>get(id)</code>","text":"<p>Get an instance role by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The UUID of the instance role.</p> required <p>Returns:</p> Name Type Description <code>InstanceRole</code> <code>InstanceRole</code> <p>An instance of InstanceRole.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.instance_role.InstanceRoleClient.list","title":"<code>list(filters=None, order=None)</code>","text":"<p>List all instance roles with optional filters and order.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Optional[Union[Dict, InstanceRoleFilter]]</code> <p>A dictionary or InstanceRoleFilter</p> <code>None</code> <code>order</code> <code>Optional[Dict]</code> <p>A dictionary to specify the order of the results.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InstanceRoleList</code> <code>InstanceRoleList</code> <p>An instance of InstanceRoleList containing the filtered and ordered instance roles.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job","title":"<code>bodosdk.clients.job</code>","text":""},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient","title":"<code>JobClient</code>","text":"<p>               Bases: <code>IJobClient</code></p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient.JobRun","title":"<code>JobRun: bodosdk.models.job.JobRun</code>  <code>property</code>","text":"<p>Get the JobRun object.</p> <p>Returns:</p> Name Type Description <code>JobRun</code> <code>JobRun</code> <p>An instance of JobRun.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient.JobRunList","title":"<code>JobRunList: bodosdk.models.job.JobRunList</code>  <code>property</code>","text":"<p>Get the JobRunList object.</p> <p>Returns:</p> Name Type Description <code>JobRunList</code> <code>JobRunList</code> <p>An instance of JobRunList.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient.__init__","title":"<code>__init__(workspace_client)</code>","text":"<p>Initializes the JobClient with a given workspace client.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_client</code> <code>IBodoWorkspaceClient</code> <p>The workspace client to interact with the API.</p> required"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient.cancel_job","title":"<code>cancel_job(id)</code>","text":"<p>Cancel job by id.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Job id.</p> required <p>Returns:</p> Name Type Description <code>JobRun</code> <code>JobRun</code> <p>Job object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient.cancel_jobs","title":"<code>cancel_jobs(filters=None)</code>","text":"<p>Cancel jobs with the given filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Optional[Union[Dict, JobFilter]]</code> <p>Filters to apply on the list.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>JobRunList</code> <code>JobRunList</code> <p>JobRunList object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient.get","title":"<code>get(id)</code>","text":"<p>Get job by id.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Job id.</p> required <p>Returns:</p> Name Type Description <code>JobRun</code> <code>JobRun</code> <p>Job object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient.list","title":"<code>list(filters=None, order=None)</code>","text":"<p>List jobs with the given filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Optional[Union[Dict, JobFilter]]</code> <p>Filters to apply on the list.</p> <code>None</code> <code>order</code> <code>Optional[Dict]</code> <p>Order to apply on the list.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>JobRunList</code> <code>JobRunList</code> <p>JobRunList object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient.run","title":"<code>run(template_id=None, cluster=None, code_type=None, source=None, exec_file=None, exec_text=None, args=None, env_vars=None, timeout=None, num_retries=None, delay_between_retries=None, retry_on_timeout=None, name=None, catalog=None, store_result=None)</code>","text":"<p>Run a job with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>template_id</code> <code>str</code> <p>Job template id.</p> <code>None</code> <code>cluster</code> <code>Union[dict, ICluster]</code> <p>Cluster object or cluster config.</p> <code>None</code> <code>code_type</code> <code>str</code> <p>Code type.</p> <code>None</code> <code>source</code> <code>Union[dict, IS3Source, IGitRepoSource, IWorkspaceSource, ITextSource]</code> <p>Source object.</p> <code>None</code> <code>exec_file</code> <code>str</code> <p>Exec file path.</p> <code>None</code> <code>exec_text</code> <code>str</code> <p>Exec text.</p> <code>None</code> <code>args</code> <code>Union[Sequence[Any], Dict, str]</code> <p>Arguments.</p> <code>None</code> <code>env_vars</code> <code>dict</code> <p>Environment variables.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout.</p> <code>None</code> <code>num_retries</code> <code>int</code> <p>Number of retries.</p> <code>None</code> <code>delay_between_retries</code> <code>int</code> <p>Delay between retries.</p> <code>None</code> <code>retry_on_timeout</code> <code>bool</code> <p>Retry on timeout.</p> <code>None</code> <code>name</code> <code>str</code> <p>Job name.</p> <code>None</code> <code>catalog</code> <code>str</code> <p>Catalog, applicable only for SQL jobs.</p> <code>None</code> <code>store_result</code> <code>bool</code> <p>Whether to store the result.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>JobRun</code> <code>JobRun</code> <p>Job object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient.run_sql_query","title":"<code>run_sql_query(template_id=None, catalog=None, sql_query=None, cluster=None, name=None, args=None, timeout=None, num_retries=None, delay_between_retries=None, retry_on_timeout=None, store_result=True)</code>","text":"<p>Run a SQL job with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>template_id</code> <code>str</code> <p>Job template id.</p> <code>None</code> <code>catalog</code> <code>str</code> <p>Catalog.</p> <code>None</code> <code>sql_query</code> <code>str</code> <p>SQL query.</p> <code>None</code> <code>cluster</code> <code>Union[dict, ICluster]</code> <p>Cluster object or cluster config.</p> <code>None</code> <code>name</code> <code>str</code> <p>Job name.</p> <code>None</code> <code>args</code> <code>Union[Sequence[Any], Dict]</code> <p>Arguments.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout.</p> <code>None</code> <code>num_retries</code> <code>int</code> <p>Number of retries.</p> <code>None</code> <code>delay_between_retries</code> <code>int</code> <p>Delay between retries.</p> <code>None</code> <code>retry_on_timeout</code> <code>bool</code> <p>Retry on timeout.</p> <code>None</code> <code>store_result</code> <code>bool</code> <p>Whether to store the result.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>JobRun</code> <code>JobRun</code> <p>Job object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job.JobClient.wait_for_status","title":"<code>wait_for_status(id, statuses, timeout=3600, tick=30)</code>","text":"<p>Wait for job to reach one of the given statuses.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Job id.</p> required <code>statuses</code> <code>List[str]</code> <p>List of statuses to wait for.</p> required <code>timeout</code> <code>int</code> <p>Timeout.</p> <code>3600</code> <code>tick</code> <code>int</code> <p>Tick.</p> <code>30</code> <p>Returns:</p> Name Type Description <code>JobRun</code> <code>JobRun</code> <p>Job object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job_tpl","title":"<code>bodosdk.clients.job_tpl</code>","text":""},{"location":"api_docs/platform_sdk/#bodosdk.clients.job_tpl.JobTemplateClient","title":"<code>JobTemplateClient</code>","text":"<p>               Bases: <code>IJobTemplateClient</code></p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job_tpl.JobTemplateClient.create","title":"<code>create(name=None, description=None, cluster=None, code_type=None, source=None, exec_file=None, exec_text=None, args=None, env_vars=None, timeout=None, num_retries=None, delay_between_retries=None, retry_on_timeout=None, catalog=None, store_result=False)</code>","text":"<p>Create a new job template with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the job template.</p> <code>None</code> <code>description</code> <code>str</code> <p>Description of the job template.</p> <code>None</code> <code>cluster</code> <code>Union[dict, ICluster]</code> <p>Cluster object or cluster config.</p> <code>None</code> <code>code_type</code> <code>str</code> <p>Code type.</p> <code>None</code> <code>source</code> <code>Union[dict, IS3Source, IGitRepoSource, IWorkspaceSource, ITextSource]</code> <p>Source object.</p> <code>None</code> <code>exec_file</code> <code>str</code> <p>Exec file path.</p> <code>None</code> <code>exec_text</code> <code>str</code> <p>Exec text.</p> <code>None</code> <code>args</code> <code>Union[dict, str]</code> <p>Arguments.</p> <code>None</code> <code>env_vars</code> <code>dict</code> <p>Environment variables.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout.</p> <code>None</code> <code>num_retries</code> <code>int</code> <p>Number of retries.</p> <code>None</code> <code>delay_between_retries</code> <code>int</code> <p>Delay between retries.</p> <code>None</code> <code>retry_on_timeout</code> <code>bool</code> <p>Retry on timeout.</p> <code>None</code> <code>catalog</code> <code>str</code> <p>Catalog, applicable only for SQL code type.</p> <code>None</code> <code>store_result</code> <code>bool</code> <p>Whether to store the result.</p> <code>False</code>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job_tpl.JobTemplateClient.get","title":"<code>get(id)</code>","text":"<p>Get job template by id.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Job template id.</p> required <p>Returns:</p> Name Type Description <code>JobTemplate</code> <code>JobTemplate</code> <p>Job template object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job_tpl.JobTemplateClient.list","title":"<code>list(filters=None)</code>","text":"<p>List job templates with the given filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Dict</code> <p>Filters to apply on the list.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>JobTemplateList</code> <code>JobTemplateList</code> <p>JobTemplateList object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.job_tpl.JobTemplateClient.remove","title":"<code>remove(id)</code>","text":"<p>Delete job template by id.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Job template id.</p> required"},{"location":"api_docs/platform_sdk/#bodosdk.clients.organization","title":"<code>bodosdk.clients.organization</code>","text":""},{"location":"api_docs/platform_sdk/#bodosdk.clients.organization.BodoOrganizationClient","title":"<code>BodoOrganizationClient</code>","text":"<p>               Bases: <code>IBodoOrganizationClient</code></p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.organization.BodoOrganizationClient.create_aws_cloud_config","title":"<code>create_aws_cloud_config(name, tf_backend_region, role_arn=None, tf_bucket_name=None, account_id=None, access_key_id=None, secret_access_key=None, custom_tags=None)</code>","text":"<p>Create a new AWS cloud config in the organization with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the cloud config.</p> required <code>tf_backend_region</code> <code>str</code> <p>Terraform backend region.</p> required <code>role_arn</code> <code>Optional[str]</code> <p>Role ARN.</p> <code>None</code> <code>tf_bucket_name</code> <code>Optional[str]</code> <p>Terraform bucket name.</p> <code>None</code> <code>account_id</code> <code>Optional[str]</code> <p>Account id.</p> <code>None</code> <code>access_key_id</code> <code>Optional[str]</code> <p>Access key id.</p> <code>None</code> <code>secret_access_key</code> <code>Optional[str]</code> <p>Secret access key.</p> <code>None</code> <code>custom_tags</code> <code>Optional[dict]</code> <p>Custom tags for the cloud config.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CloudConfig</code> <code>CloudConfig</code> <p>CloudConfig object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.organization.BodoOrganizationClient.create_azure_cloud_config","title":"<code>create_azure_cloud_config(name, tf_backend_region, tenant_id, subscription_id, resource_group, custom_tags=None)</code>","text":"<p>Create a new Azure cloud config in the organization with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the cloud config.</p> required <code>tf_backend_region</code> <code>str</code> <p>Terraform backend region.</p> required <code>tenant_id</code> <code>str</code> <p>Tenant id.</p> required <code>subscription_id</code> <code>str</code> <p>Subscription id.</p> required <code>resource_group</code> <code>str</code> <p>Resource group.</p> required <code>custom_tags</code> <code>Optional[dict]</code> <p>Custom tags for the cloud config.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CloudConfig</code> <code>CloudConfig</code> <p>CloudConfig object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.organization.BodoOrganizationClient.create_workspace","title":"<code>create_workspace(name, region, cloud_config_id, vpc_id=None, public_subnets_ids=None, private_subnets_ids=None, custom_tags=None, kms_key_arn=None)</code>","text":"<p>Create a new workspace in the organization with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the workspace.</p> required <code>region</code> <code>str</code> <p>Region of the workspace.</p> required <code>storage_endpoint_enabled</code> <code>bool</code> <p>Enable storage endpoint for the workspace.</p> required <code>cloud_config_id</code> <code>str</code> <p>Cloud config id for the workspace.</p> required <code>vpc_id</code> <code>Optional[str]</code> <p>VPC id for the workspace.</p> <code>None</code> <code>public_subnets_ids</code> <code>Optional(List[str]</code> <p>List of public subnet ids.</p> <code>None</code> <code>private_subnets_ids</code> <code>Optional(List[str]</code> <p>List of private subnet ids.</p> <code>None</code> <code>custom_tags</code> <code>Optional(dict</code> <p>Custom tags for the workspace.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Workspace</code> <code>Workspace</code> <p>Workspace object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.organization.BodoOrganizationClient.delete_workspace","title":"<code>delete_workspace(id)</code>","text":"<p>Delete workspace by id.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Workspace id.</p> required <p>Returns:</p> Name Type Description <code>Workspace</code> <code>Workspace</code> <p>Workspace object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.organization.BodoOrganizationClient.get_cloud_config","title":"<code>get_cloud_config(id)</code>","text":"<p>Get cloud config by id.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Cloud config id.</p> required <p>Returns:</p> Name Type Description <code>CloudConfig</code> <code>CloudConfig</code> <p>CloudConfig object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.organization.BodoOrganizationClient.get_workspace","title":"<code>get_workspace(id)</code>","text":"<p>Get workspace by id.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Workspace id.</p> required <p>Returns:</p> Name Type Description <code>Workspace</code> <code>Workspace</code> <p>Workspace object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.organization.BodoOrganizationClient.list_cloud_configs","title":"<code>list_cloud_configs(filters=None)</code>","text":"<p>List cloud configs in the organization.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Optional[Union[dict]]</code> <p>Filters to apply on the list.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CloudConfigList</code> <code>CloudConfigList</code> <p>CloudConfigList object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.organization.BodoOrganizationClient.list_workspaces","title":"<code>list_workspaces(filters=None)</code>","text":"<p>List workspaces in the organization.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Optional[Union[WorkspaceFilter, dict]]</code> <p>Filters to apply on the list.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>WorkspaceList</code> <code>WorkspaceList</code> <p>WorkspaceList object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.workspace","title":"<code>bodosdk.clients.workspace</code>","text":""},{"location":"api_docs/platform_sdk/#bodosdk.clients.workspace.BodoWorkspaceClient","title":"<code>BodoWorkspaceClient</code>","text":"<p>               Bases: <code>IBodoWorkspaceClient</code></p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.workspace.BodoWorkspaceClient.workspace_data","title":"<code>workspace_data: IWorkspace</code>  <code>property</code>","text":"<p>Get workspace data.</p> <p>Returns:</p> Name Type Description <code>Workspace</code> <code>IWorkspace</code> <p>Workspace object.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.workspace.BodoWorkspaceClient.workspace_id","title":"<code>workspace_id: str</code>  <code>property</code>","text":"<p>Get workspace id.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Workspace id.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.clients.workspace.BodoWorkspaceClient.__init__","title":"<code>__init__(client_id=None, secret_key=None, api_url='https://api.bodo.ai/api', auth_url='https://auth.bodo.ai', print_logs=False)</code>","text":"<p>Initialize BodoWorkspaceClient.</p> <p>Parameters:</p> Name Type Description Default <code>client_id</code> <code>str</code> <p>Client id.</p> <code>None</code> <code>secret_key</code> <code>str</code> <p>Secret key.</p> <code>None</code> <code>api_url</code> <code>str</code> <p>API url.</p> <code>'https://api.bodo.ai/api'</code> <code>auth_url</code> <code>str</code> <p>Auth url.</p> <code>'https://auth.bodo.ai'</code> <code>print_logs</code> <code>bool</code> <p>Print logs</p> <code>False</code> <p>Raises:</p> Type Description <code>APIKeysMissing</code> <p>If client_id or secret_key is not passed and environment variables are not set</p>"},{"location":"api_docs/platform_sdk/#bodosdk.models.catalog","title":"<code>bodosdk.models.catalog</code>","text":""},{"location":"api_docs/platform_sdk/#bodosdk.models.catalog.CatalogList","title":"<code>CatalogList</code>","text":"<p>               Bases: <code>ICatalogList</code>, <code>SDKBaseModel</code></p>"},{"location":"api_docs/platform_sdk/#bodosdk.models.catalog.CatalogList.Config","title":"<code>Config</code>","text":"<p>Configuration for Pydantic models. https://docs.pydantic.dev/latest/api/config/</p>"},{"location":"api_docs/platform_sdk/#bodosdk.models.cloud_config","title":"<code>bodosdk.models.cloud_config</code>","text":""},{"location":"api_docs/platform_sdk/#bodosdk.models.cloud_config.CloudConfig","title":"<code>CloudConfig</code>","text":"<p>               Bases: <code>SDKBaseModel</code>, <code>ICloudConfig</code></p>"},{"location":"api_docs/platform_sdk/#bodosdk.models.cloud_config.CloudConfig.__call__","title":"<code>__call__(**data)</code>","text":"<p>Creates a new CloudConfig with the same CloudConfig client and provided data.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <p>Arbitrary keyword arguments representing CloudConfig properties.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CloudConfig</code> <p>A new instance of CloudConfig.</p>"},{"location":"api_docs/platform_sdk/#bodosdk.models.cloud_config.CloudConfig.__init__","title":"<code>__init__(org_client=None, **data)</code>","text":"<p>Initializes a new CloudConfig model.</p> <p>Parameters:</p> Name Type Description Default <code>org_client</code> <code>IBodoOrganizationClient</code> <p>An optional client for interacting with the CloudConfig API.</p> <code>None</code> <code>**data</code> <p>Arbitrary keyword arguments representing CloudConfig properties.</p> <code>{}</code>"},{"location":"api_docs/platform_sdk/#bodosdk.models.cloud_config.CloudConfigList","title":"<code>CloudConfigList</code>","text":"<p>               Bases: <code>ICloudConfigList</code>, <code>SDKBaseModel</code></p>"},{"location":"api_docs/platform_sdk/#bodosdk.models.cloud_config.CloudConfigList.Config","title":"<code>Config</code>","text":"<p>Configuration for Pydantic models. https://docs.pydantic.dev/latest/api/config/</p>"},{"location":"api_docs/udfs/","title":"User-Defined Functions (UDFs)","text":"<p>While Pandas and other APIs can be extremely expressive, many data science and data engineering use cases require additional functionality beyond what is directly offered. In these situations, many programmers create User Defined Functions, or UDFs, which are Python functions designed to compute on each row or groups of rows depending on the context.</p>"},{"location":"api_docs/udfs/#using-udfs-with-bodo","title":"Using UDFs with Bodo","text":"<p>Bodo users can construct UDFs either by defining a separate JIT function or by creating a function within a JIT function (either via a lambda or closure). For example, here are two ways to construct a UDF that advances each element of a Timestamp Series to the last day of the current month.</p> <pre><code>import pandas as pd\nimport bodo\n\n@bodo.jit\ndef jit_udf(x):\n    return x + pd.tseries.offsets.MonthEnd(n=0, normalize=True)\n\n@bodo.jit\ndef jit_example(S):\n    return S.map(jit_udf)\n\n@bodo.jit\ndef lambda_example(S):\n    return S.map(lambda x: x + pd.tseries.offsets.MonthEnd(n=0, normalize=True))\n\nS = pd.Series(pd.date_range(start='1/1/2021', periods=100))\npd.testing.assert_series_equal(jit_example(S), lambda_example(S))\n</code></pre> <p>UDFs can be used to compute one value per row or group (map functions) or compute an aggregation (agg functions). Bodo provides APIs for both, which are summarized below. Please refer to supported Pandas API for more information.</p>"},{"location":"api_docs/udfs/#map-functions","title":"Map Functions","text":"<ul> <li><code>Series.map</code></li> <li><code>Series.apply</code></li> <li><code>Series.pipe</code></li> <li><code>DataFrame.map</code></li> <li><code>DataFrame.apply</code></li> <li><code>DataFrame.pipe</code></li> <li><code>GroupBy.apply</code></li> <li><code>GroupBy.pipe</code></li> <li><code>GroupBy.transform</code></li> </ul>"},{"location":"api_docs/udfs/#agg-functions","title":"Agg Functions","text":"<ul> <li><code>GroupBy.agg</code></li> <li><code>GroupBy.aggregate</code></li> </ul>"},{"location":"api_docs/udfs/#udf-performance","title":"UDF Performance","text":"<p>Bodo offers support for UDFs without the significant runtime penalty generally incurred in Pandas. An example of this is shown in the quick started guide.</p> <p>Bodo achieves a drastic performance advantage on UDFs because UDFs can be optimized by similar to any other JIT code. In contrast, library based solutions are typically limited in their ability to optimize UDFs.</p>"},{"location":"api_docs/udfs/#additional-arguments","title":"Additional Arguments","text":"<p>We recommend passing additional variables to UDFs explicitly, instead of directly using variables local to the function defining the UDF. The latter is called the \\\"captured\\\" variables case, which is often error-prone and may result in compilation errors.</p> <p>For example, consider a UDF that appends a variable suffix to each string in a Series of strings. The proper way to write this function is to use the <code>args</code> argument to <code>Series.apply()</code>.</p> <pre><code>import pandas as pd\nimport bodo\n\n@bodo.jit\ndef add_suffix(S, suffix):\n    return S.apply(lambda x, suf: x + suf, args=(suffix,))\n\nS = pd.Series([\"abc\", \"edf\", \"32\", \"Vew3\", \"er3r2\"] * 10)\nsuffix = \"_\"\nadd_suffix(S, suffix)\n</code></pre> <p>Alternatively, arguments can be passed by keyword.</p> <pre><code>@bodo.jit\ndef add_suffix(S, suffix):\n    return S.apply(lambda x, suf: x + suf, suf=suffix)\n</code></pre> <p>Note</p> <p>Not all APIs support additional arguments. Please refer to supported Pandas API for more information on intended API usage.</p>"},{"location":"api_docs/udfs/#apply-with-pandas-methods-and-numpy-ufuncs","title":"Apply with Pandas Methods and Numpy ufuncs","text":"<p>In addition to UDFs, the <code>apply</code> API can also be used to call Pandas methods and Numpy ufuncs. To execute a Pandas method, you can provide the method name as a string.</p> <pre><code>import pandas as pd\nimport bodo\n\n@bodo.jit\ndef ex(S):\n    return S.apply(\"nunique\")\n\nS = pd.Series(list(np.arange(100) + list(np.arange(100))))\nex(S)\n</code></pre> <p>Numpy ufuncs can either be provided with a string matching the name or with the function itself.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport bodo\n\n@bodo.jit\ndef ex_str(S):\n    return S.apply(\"sin\")\n\ndef ex_func(S):\n    return S.apply(np.sin)\n\nS = pd.Series(list(np.arange(100) + list(np.arange(100))))\npd.testing.assert_series_equal(ex_str(S), ex_func(S))\n</code></pre> <p>Note</p> <p>Numpy ufuncs are not currently supported with DataFrames.</p>"},{"location":"api_docs/udfs/#type-stability-restrictions","title":"Type Stability Restrictions","text":"<p>Bodo's type stability requirements can encounter some limitations when either using <code>DataFrame.apply</code> with different column types or when returning a DataFrame.</p>"},{"location":"api_docs/udfs/#differently-typed-columns","title":"Differently Typed Columns","text":"<p><code>DataFrame.apply</code> maps user provided UDFs to each row of the DataFrame. In the situation where a DataFrame has columns of different types, the Series passed to the UDF will contain values with different types. Bodo internally represents these as a Heterogeneous Series. This representation has limitations in the Series operations it supports. Please refer to the supported operations for heterogeneous series for more information.</p>"},{"location":"api_docs/udfs/#returning-a-dataframe","title":"Returning a DataFrame","text":"<p>In Pandas, <code>Series.apply</code> or <code>DataFrame.apply</code> there are multiple ways to return a DataFrame instead of a Series. However, for type stability reasons, Bodo can only infer a DataFrame when returning a Series whose size can be inferred at compile time for each row.</p> <p>Note</p> <p>If you provide an Index, then all Index values must be compile time constants.</p> <p>Here is an example using<code>Series.apply</code> to return a DataFrame.</p> <pre><code>import pandas as pd\nimport bodo\n\n@bodo.jit\ndef series_ex(S):\n    return S.apply(lambda x: pd.Series((1, x)))\n\nS = pd.Series(list(np.arange(100) + list(np.arange(100))))\nseries_ex(S)\n</code></pre> <p>If using a UDF that returns a DataFrame in Pandas through another means, this behavior will not match in Bodo and may result in a compilation error. Please convert your solution to one of the supported methods if possible.</p>"},{"location":"api_docs/udfs/#using-the-engine-keyword-argument-in-pandas","title":"Using the Engine Keyword Argument in Pandas","text":"<p>Starting in Pandas 3.0, users will be able to pass a Bodo jit decorator as the <code>engine</code> argument to <code>DataFrame.apply</code>, which will automatically apply the decorator when applying the UDF. For example:</p> <pre><code>import pandas as pd\nimport bodo\n\ndef update_score(S, answer, num_points):\n    if S.guess == answer:\n        return S.score + num_points\n    else:\n        return S.score\n\ndf = pd.DataFrame(\n    {\n        \"guess\": [\"A\", \"B\", \"C\", \"D\", \"A\"],\n        \"score\": [0, 3, 4, 2, 1]\n    }\n)\n\ndf[\"updated_score\"] = df.apply(update_score, axis=1, args=(\"A\", 3), engine=bodo.jit)\n</code></pre> <p>Note that the same restrictions related to type stability discussed in the previous sections apply here as well. The example above is equivalent to: <pre><code>import pandas as pd\nimport bodo\n\ndef update_score(S, answer, num_points):\n    if S.guess == answer:\n        return S.score + num_points\n    else:\n        return S.score\n\n@bodo.jit\ndef apply_update_scores(df, answer, num_points):\n    return df.apply(update_score, axis=1, args=(answer, num_points))\n\ndf = pd.DataFrame(\n    {\n        \"guess\": [\"A\", \"B\", \"C\", \"D\", \"A\"],\n        \"score\": [0, 3, 4, 2, 1]\n    }\n)\n\ndf[\"updated_score\"] = apply_update_scores(df, \"A\", 3)\n</code></pre></p>"},{"location":"api_docs/bodo_parallel_apis/","title":"Bodo Parallel APIs","text":"<p>This section lists advanced parallel APIs provided by Bodo for finer-grained control over data distribution and processes.</p> <ul> <li><code>bodo.allgatherv</code> </li> <li><code>bodo.barrier</code> </li> <li><code>bodo.gatherv</code> </li> <li><code>bodo.get_rank</code> </li> <li><code>bodo.get_size</code> </li> <li><code>bodo.random_shuffle</code></li> <li><code>bodo.rebalance</code> </li> <li><code>bodo.scatterv</code> </li> </ul>"},{"location":"api_docs/bodo_parallel_apis/allgatherv/","title":"<code>bodo.allgatherv</code>","text":"<p><code>bodo.allgatherv(data, warn_if_rep=True)</code>  Gather data from all ranks and send to all, effectively replicating the data.</p>"},{"location":"api_docs/bodo_parallel_apis/allgatherv/#arguments","title":"Arguments","text":"<ul> <li><code>data</code>: data to gather.</li> <li><code>warn_if_rep</code>: prints a BodoWarning if data to gather is replicated. </li> </ul>"},{"location":"api_docs/bodo_parallel_apis/allgatherv/#example-usage","title":"Example Usage","text":"<pre><code>import bodo\nimport pandas as pd\n\n@bodo.jit\ndef mean_power():\n    df = pd.read_parquet(\"data/cycling_dataset.pq\")\n    df = bodo.allgatherv(df)\n    print(df)\n    return df\n\ndf = mean_power()\n</code></pre> <p>Save code in <code>test_allgatherv.py</code> file and run with 4 processes.</p> <pre><code>BODO_NUM_WORKERS=4 python test_allgatherv.py\n</code></pre> <p>Output:</p> <pre><code>[stdout:0]\n      Unnamed: 0    altitude  cadence  ...  power  speed                time\n0              0  185.800003       51  ...     45  3.459 2016-10-20 22:01:26\n1              1  185.800003       68  ...      0  3.710 2016-10-20 22:01:27\n2              2  186.399994       38  ...     42  3.874 2016-10-20 22:01:28\n3              3  186.800003       38  ...      5  4.135 2016-10-20 22:01:29\n4              4  186.600006       38  ...      1  4.250 2016-10-20 22:01:30\n...          ...         ...      ...  ...    ...    ...                 ...\n3897        1127  178.199997        0  ...      0  3.497 2016-10-20 23:14:31\n3898        1128  178.199997        0  ...      0  3.289 2016-10-20 23:14:32\n3899        1129  178.199997        0  ...      0  2.969 2016-10-20 23:14:33\n3900        1130  178.399994        0  ...      0  2.969 2016-10-20 23:14:34\n3901        1131  178.399994        0  ...      0  2.853 2016-10-20 23:14:35\n\n[3902 rows x 10 columns]\n[stdout:1]\n      Unnamed: 0    altitude  cadence  ...  power  speed                time\n0              0  185.800003       51  ...     45  3.459 2016-10-20 22:01:26\n1              1  185.800003       68  ...      0  3.710 2016-10-20 22:01:27\n2              2  186.399994       38  ...     42  3.874 2016-10-20 22:01:28\n3              3  186.800003       38  ...      5  4.135 2016-10-20 22:01:29\n4              4  186.600006       38  ...      1  4.250 2016-10-20 22:01:30\n...          ...         ...      ...  ...    ...    ...                 ...\n3897        1127  178.199997        0  ...      0  3.497 2016-10-20 23:14:31\n3898        1128  178.199997        0  ...      0  3.289 2016-10-20 23:14:32\n3899        1129  178.199997        0  ...      0  2.969 2016-10-20 23:14:33\n3900        1130  178.399994        0  ...      0  2.969 2016-10-20 23:14:34\n3901        1131  178.399994        0  ...      0  2.853 2016-10-20 23:14:35\n\n[3902 rows x 10 columns]\n[stdout:2]\n      Unnamed: 0    altitude  cadence  ...  power  speed                time\n0              0  185.800003       51  ...     45  3.459 2016-10-20 22:01:26\n1              1  185.800003       68  ...      0  3.710 2016-10-20 22:01:27\n2              2  186.399994       38  ...     42  3.874 2016-10-20 22:01:28\n3              3  186.800003       38  ...      5  4.135 2016-10-20 22:01:29\n4              4  186.600006       38  ...      1  4.250 2016-10-20 22:01:30\n...          ...         ...      ...  ...    ...    ...                 ...\n3897        1127  178.199997        0  ...      0  3.497 2016-10-20 23:14:31\n3898        1128  178.199997        0  ...      0  3.289 2016-10-20 23:14:32\n3899        1129  178.199997        0  ...      0  2.969 2016-10-20 23:14:33\n3900        1130  178.399994        0  ...      0  2.969 2016-10-20 23:14:34\n3901        1131  178.399994        0  ...      0  2.853 2016-10-20 23:14:35\n\n[3902 rows x 10 columns]\n[stdout:3]\n      Unnamed: 0    altitude  cadence  ...  power  speed                time\n0              0  185.800003       51  ...     45  3.459 2016-10-20 22:01:26\n1              1  185.800003       68  ...      0  3.710 2016-10-20 22:01:27\n2              2  186.399994       38  ...     42  3.874 2016-10-20 22:01:28\n3              3  186.800003       38  ...      5  4.135 2016-10-20 22:01:29\n4              4  186.600006       38  ...      1  4.250 2016-10-20 22:01:30\n...          ...         ...      ...  ...    ...    ...                 ...\n3897        1127  178.199997        0  ...      0  3.497 2016-10-20 23:14:31\n3898        1128  178.199997        0  ...      0  3.289 2016-10-20 23:14:32\n3899        1129  178.199997        0  ...      0  2.969 2016-10-20 23:14:33\n3900        1130  178.399994        0  ...      0  2.969 2016-10-20 23:14:34\n3901        1131  178.399994        0  ...      0  2.853 2016-10-20 23:14:35\n\n[3902 rows x 10 columns]\n</code></pre>"},{"location":"api_docs/bodo_parallel_apis/barrier/","title":"bodo.barrier","text":"<p><code>bodo.barrier()</code></p> <p>Synchronize all processes. Block process from proceeding until all processes reach this point.</p>"},{"location":"api_docs/bodo_parallel_apis/barrier/#example-usage","title":"Example Usage","text":"<p>A typical example is to make sure all processes see side effects simultaneously. For example, a process can delete files from storage while others wait before writing to file. The following example uses SPMD launch mode:</p> <pre><code>import shutil, os\nimport numpy as np\n\n# remove file if exists\nif bodo.get_rank() == 0:\n    if os.path.exists(\"data/data.pq\"):\n        shutil.rmtree(\"data/data.pq\")\n\n# make sure all processes are synchronized\n# (e.g. all processes need to see effect of rank 0's work)\nbodo.barrier()\n\n\n@bodo.jit\ndef f(n):\n    df = pd.DataFrame({\"A\": np.arange(n)})\n    df.to_parquet(\"data/data.pq\")\n\n\nf(10)\n</code></pre> <p>The following figure illustrates what happens when processes call <code>bodo.barrier()</code>. When barrier is called, a process pauses and waits until all other processes have reached the barrier:</p> <p></p> <p>Danger</p> <p>The example above shows that it is possible to have each process follow a different control flow, but all processes must always call the same Bodo functions in the same order.</p>"},{"location":"api_docs/bodo_parallel_apis/gatherv/","title":"bodo.gatherv","text":"<p><code>bodo.gatherv(data, allgather=False, warn_if_rep=True, root=0)</code>   Collect distributed data manually by gathering them into a single rank. </p>"},{"location":"api_docs/bodo_parallel_apis/gatherv/#arguments","title":"Arguments","text":"<ul> <li><code>data</code>: data to gather.</li> <li><code>root</code>: specify rank to collect the data. Default: rank <code>0</code>.</li> <li><code>warn_if_rep</code>: prints a BodoWarning if data to gather is replicated. </li> <li><code>allgather</code>: send gathered data to all ranks. Default: <code>False</code>. Same behavior as <code>bodo.allgatherv</code>.</li> </ul>"},{"location":"api_docs/bodo_parallel_apis/gatherv/#example-usage","title":"Example Usage","text":"<p><pre><code>import bodo\nimport pandas as pd\n\n@bodo.jit\ndef mean_power():\n    df = pd.read_parquet(\"data/cycling_dataset.pq\")\n    df = bodo.gatherv(df, root=1)\n    print(df)\n\nmean_power()\n</code></pre> Save code in <code>test_gatherv.py</code> file and run with 4 processes.</p> <pre><code>BODO_NUM_WORKERS=4 python test_gatherv.py\n</code></pre> <p>Output:</p> <pre><code>[stdout:1]\n      Unnamed: 0    altitude  cadence  ...  power  speed                time\n0              0  185.800003       51  ...     45  3.459 2016-10-20 22:01:26\n1              1  185.800003       68  ...      0  3.710 2016-10-20 22:01:27\n2              2  186.399994       38  ...     42  3.874 2016-10-20 22:01:28\n3              3  186.800003       38  ...      5  4.135 2016-10-20 22:01:29\n4              4  186.600006       38  ...      1  4.250 2016-10-20 22:01:30\n...          ...         ...      ...  ...    ...    ...                 ...\n3897        1127  178.199997        0  ...      0  3.497 2016-10-20 23:14:31\n3898        1128  178.199997        0  ...      0  3.289 2016-10-20 23:14:32\n3899        1129  178.199997        0  ...      0  2.969 2016-10-20 23:14:33\n3900        1130  178.399994        0  ...      0  2.969 2016-10-20 23:14:34\n3901        1131  178.399994        0  ...      0  2.853 2016-10-20 23:14:35\n\n[3902 rows x 10 columns]\n[stdout:0]\nEmpty DataFrame\nColumns: [Unnamed: 0, altitude, cadence, distance, hr, latitude, longitude, power, speed, time]\nIndex: []\n\n[0 rows x 10 columns]\n[stdout:2]\nEmpty DataFrame\nColumns: [Unnamed: 0, altitude, cadence, distance, hr, latitude, longitude, power, speed, time]\nIndex: []\n\n[0 rows x 10 columns]\n[stdout:3]\nEmpty DataFrame\nColumns: [Unnamed: 0, altitude, cadence, distance, hr, latitude, longitude, power, speed, time]\nIndex: []\n\n[0 rows x 10 columns]\n</code></pre>"},{"location":"api_docs/bodo_parallel_apis/get_rank_/","title":"bodo.get_rank","text":"<p><code>bodo.get_rank()</code></p> <p>Get the process number from Bodo (called <code>rank</code> in MPI terminology).</p>"},{"location":"api_docs/bodo_parallel_apis/get_rank_/#example-usage","title":"Example Usage","text":"<p>Save following code in <code>get_rank.py</code> file and run with multiple cores.</p> <pre><code>import bodo\n\n@bodo.jit\ndef run_in_parallel():\n    # some work only on rank 0\n    if bodo.get_rank() == 0:\n        print(\"rank 0 done\")\n\n    # some work on every process\n    print(\"rank\", bodo.get_rank(), \"here\")\nrun_in_parallel()\n</code></pre> <pre><code>BODO_NUM_WORKERS=4 python get_rank.py\n</code></pre> <p>Output</p> <pre><code>rank 0 done\nrank 0 here\nrank 1 here\nrank 2 here\nrank 3 here\n</code></pre>"},{"location":"api_docs/bodo_parallel_apis/get_size_/","title":"bodo.get_size","text":"<p><code>bodo.get_size()</code>  Get the total number of processes.</p>"},{"location":"api_docs/bodo_parallel_apis/get_size_/#example-usage","title":"Example Usage","text":"<p>Save following code in <code>get_rank.py</code> file and run with multiple cores.</p> <pre><code>import bodo\n\n@bodo.jit\ndef run_in_parallel():\n    # some work only on rank 0\n    if bodo.get_rank() == 0:\n        print(\"rank 0 done\")\n\n    # some work on every process\n    print(\"rank\", bodo.get_rank(), \"here\")\n    print(\"total ranks:\", bodo.get_size())\nrun_in_parallel()\n</code></pre> <pre><code>BODO_NUM_WORKERS=4 python get_rank.py\n</code></pre> <p>Output</p> <pre><code>rank 0 done\nrank 0 here\ntotal ranks: 4\nrank 1 here\ntotal ranks: 4\nrank 2 here\ntotal ranks: 4\nrank 3 here\ntotal ranks: 4\n</code></pre>"},{"location":"api_docs/bodo_parallel_apis/random_shuffle/","title":"bodo.random_shuffle","text":"<p><code>bodo.random_shuffle(data, seed=None, dests=None, parallel=False)</code> Manually shuffle data evenly across selected ranks.</p>"},{"location":"api_docs/bodo_parallel_apis/random_shuffle/#arguments","title":"Arguments","text":"<ul> <li><code>data</code>: data to shuffle.</li> <li><code>seed</code>: number to initialze random number generator.</li> <li><code>dests</code>: selected ranks to distribute shuffled data to. By default, distribution includes all ranks.</li> <li><code>parallel</code>: flag to indicate whether data is distributed. Default: <code>False</code>. Inside JIT default value depends on Bodo's distribution analysis algorithm for the data passed (For more information, see Data Distribution section below).</li> </ul>"},{"location":"api_docs/bodo_parallel_apis/random_shuffle/#example-usage","title":"Example Usage","text":"<p>Note that this example uses SPMD launch mode.</p> <pre><code>import bodo\nimport pandas as pd\n\n@bodo.jit(spawn=False)\ndef test_random_shuffle():\n    df = pd.DataFrame({\"A\": range(100)})\n    return df\n\ndf = test_random_shuffle()\nprint(df.head())\ndf = bodo.random_shuffle(res, parallel=True)\nprint(df.head())\n</code></pre> <p>Save code in <code>test_random_shuffle.py</code> file and run with <code>mpiexec</code>.</p> <pre><code>mpiexec -n 4 python test_random_shuffle.py\n</code></pre> <p>Output:</p> <pre><code>[stdout:1]\n    A\n0  25\n1  26\n2  27\n3  28\n4  29\n    A\n19  19\n10  10\n17  42\n9    9\n17  17\n[stdout:3]\n    A\n0  75\n1  76\n2  77\n3  78\n4  79\n    A\n6   31\n0   25\n24  49\n22  22\n5   30\n[stdout:2]\n    A\n0  50\n1  51\n2  52\n3  53\n4  54\n    A\n11  36\n24  24\n15  65\n14  14\n10  35\n[stdout:0]\n    A\n0  0\n1  1\n2  2\n3  3\n4  4\n    A\n4   29\n18  18\n8   58\n15  15\n3   28\n</code></pre>"},{"location":"api_docs/bodo_parallel_apis/rebalance/","title":"bodo.rebalance","text":"<p><code>bodo.rebalance(data, dests=None, random=False, random_seed=None, parallel=False)</code>  Manually redistribute data evenly across [selected] ranks.</p>"},{"location":"api_docs/bodo_parallel_apis/rebalance/#arguments","title":"Arguments","text":"<ul> <li><code>data</code>: data to rebalance.</li> <li><code>dests</code>: selected ranks to distribute data to. By default, distribution includes all ranks.</li> <li><code>random</code>: flag to randomize order of the rows of the data. Default: <code>False</code>.</li> <li><code>random_seed</code>: number to initialize random number generator.</li> <li><code>parallel</code>: flag to indicate whether data is distributed. Default: <code>False</code>. Inside JIT default value depends on Bodo's distribution analysis algorithm for the data passed (For more information, see Data Distribution section below).</li> </ul>"},{"location":"api_docs/bodo_parallel_apis/rebalance/#example-usage","title":"Example Usage","text":"<ul> <li> <p>Example with just the <code>parallel</code> flag set to <code>True</code>:</p> <pre><code>import bodo\nimport pandas as pd\n\n@bodo.jit\ndef mean_power():\n    df = pd.read_parquet(\"data/cycling_dataset.pq\")\n    df = df.sort_values(\"power\")[df[\"power\"] &gt; 400]\n    print(df.shape)\n    df = bodo.rebalance(df, parallel=True)\n    print(\"After rebalance: \", df.shape)\n\nmean_power()\n</code></pre> <p>Save code in <code>test_rebalance.py</code> file and run with 4 processes.</p> <pre><code>BODO_NUM_WORKERS=4 python test_rebalance.py\n</code></pre> <pre><code>[stdout:0]\n(5, 10)\nAfter rebalance: (33, 10)\n[stdout:1]\n(18, 10)\nAfter rebalance: (33, 10)\n[stdout:2]\n(82, 10)\nAfter rebalance: (33, 10)\n[stdout:3]\n(26, 10)\nAfter rebalance: (32, 10)\n</code></pre> </li> <li> <p>Example to distribute the data from all ranks to subset of ranks using <code>dests</code> argument.</p> <p>Note</p> <p>The following example uses SPMD launch mode.</p> <p><pre><code>import bodo\nimport pandas as pd\n\n@bodo.jit(spawn=False)\ndef mean_power():\n    df = pd.read_parquet(\"data/cycling_dataset.pq\")\n    df = df.sort_values(\"power\")[df[\"power\"] &gt; 400]\n    return df\n\ndf = mean_power()\nprint(df.shape)\ndf = bodo.rebalance(df, dests=[1,3], parallel=True)\nprint(\"After rebalance: \", df.shape)\n</code></pre> Save code in <code>test_rebalance.py</code> file and run with 4 processes.</p> <pre><code>mpiexec -n 4 python test_rebalance.py\n</code></pre> <p>Output:</p> <pre><code>[stdout:0]\n(5, 10)\nAfter rebalance: (0, 10)\n[stdout:1]\n(18, 10)\nAfter rebalance: (66, 10)\n[stdout:2]\n(82, 10)\nAfter rebalance: (0, 10)\n[stdout:3]\n(26, 10)\nAfter rebalance: (65, 10)\n</code></pre> </li> </ul>"},{"location":"api_docs/bodo_parallel_apis/scatterv/","title":"bodo.scatterv","text":"<p><code>bodo.scatterv(data, warn_if_dist=True)</code>   Distribute data manually by scattering data from one process to all processes.</p>"},{"location":"api_docs/bodo_parallel_apis/scatterv/#arguments","title":"Arguments","text":"<ul> <li><code>data</code>: data to distribute.</li> <li><code>warn_if_dist</code>: flag to print a BodoWarning if <code>data</code> is already distributed.</li> </ul> <p>Note</p> <p>Currently, <code>bodo.scatterv</code> only supports scattering from rank 0.</p> <p>Note</p> <p>The following examples use SPMD launch mode.</p>"},{"location":"api_docs/bodo_parallel_apis/scatterv/#example-usage","title":"Example Usage","text":"<ul> <li>When used outside of JIT code, we recommend that the argument be set to <code>None</code> for all ranks except rank 0.    For example:</li> </ul> <pre><code>import bodo\nimport pandas as pd\n\n\n@bodo.jit(spawn=False, distributed=[\"df\"])\ndef mean_power(df):\n    x = df.power.mean()\n    return x\n\ndf = None\n# only rank 0 reads the data\nif bodo.get_rank() == 0:\n    df = pd.read_parquet(\"data/cycling_dataset.pq\")\n\ndf = bodo.scatterv(df)\nres = mean_power(df)\nprint(res)\n</code></pre> <p>Save the code in <code>test_scatterv.py</code> file and run with <code>mpiexec</code>.</p> <pre><code>mpiexec -n 4 python test_scatterv.py\n</code></pre> <p>Output: </p> <pre><code>[stdout:0] 102.07842132239877\n[stdout:1] 102.07842132239877\n[stdout:2] 102.07842132239877\n[stdout:3] 102.07842132239877\n</code></pre> <p>Note</p> <p><code>data/cycling_dataset.pq</code> is located in the Bodo tutorial repo.</p> <ul> <li>This is not a strict requirement. However, since this might be bad practice in certain situations,    Bodo will throw a warning if the data is not None on other ranks.</li> </ul> <pre><code>import bodo\nimport pandas as pd\n\ndf = pd.read_parquet(\"data/cycling_dataset.pq\")\ndf = bodo.scatterv(df)\nres = mean_power(df)\nprint(res)\n</code></pre> <p>Save code in <code>test_scatterv.py</code> file and run with <code>mpiexec</code>.</p> <pre><code>mpiexec -n 4 python test_scatterv.py\n</code></pre> <p>Output:</p> <pre><code>BodoWarning: bodo.scatterv(): A non-None value for 'data' was found on a rank other than the root. This data won't be sent to any other ranks and will be overwritten with data from rank 0.\n\n[stdout:0] 102.07842132239877\n[stdout:1] 102.07842132239877\n[stdout:2] 102.07842132239877\n[stdout:3] 102.07842132239877\n</code></pre> <ul> <li>When using <code>scatterv</code> inside of JIT code, the argument must have the same type on each rank due to Bodo's typing constraints.   All inputs except for rank 0 are ignored.</li> </ul> <pre><code>import bodo\nimport pandas as pd\n\n@bodo.jit(spawn=False)\ndef impl():\n    if bodo.get_rank() == 0:\n        df = pd.DataFrame({\"A\": [1,2,3,4,5,6,7,8]})\n    else:\n        df = pd.DataFrame({\"A\": [-1]*8})\n    return bodo.scatterv(df)\nprint(impl())\n</code></pre> <p>Save code in <code>test_scatterv.py</code> file and run with <code>mpiexec</code>.</p> <pre><code>mpiexec -n 8 python test_scatterv.py\n</code></pre> <p>Output:</p> <pre><code>[stdout:6]\n      A\n6     7\n[stdout:0]\n      A\n0     1\n[stdout:1]\n      A\n1     2\n[stdout:4]\n      A\n4     5\n[stdout:7]\n      A\n7     8\n[stdout:3]\n      A\n3     4\n[stdout:2]\n      A\n2     3\n[stdout:5]\n      A\n5     6\n</code></pre> <p>Note</p> <p><code>scatterv</code>, <code>gatherv</code>, <code>allgatherv</code>, <code>rebalance</code>, and <code>random_shuffle</code> work with all distributable data types. This includes:</p> <ul> <li>All supported numpy array types.</li> <li>All supported pandas array types (with the exception of Interval Arrays).</li> <li>All supported pandas Series types.</li> <li>All supported DataFrame types.</li> <li>All supported Index types (with the exception of Interval Index).</li> <li>Tuples of the above types.</li> </ul>"},{"location":"api_docs/dataframe_lib/","title":"Bodo DataFrame Library API","text":"<p>The Bodo DataFrame Library is designed to accelerate and scale Pandas workflows with just a one-line change \u2014 simply replace:</p> <pre><code>import pandas as pd\n</code></pre> <p>with</p> <pre><code>import bodo.pandas as pd\n</code></pre> <p>and your existing code can immediately take advantage of high-performance, scalable execution.</p> <p>Key features include:</p> <ul> <li> <p>Full Pandas compatibility with a transparent fallback mechanism to native Pandas, ensuring that your workflows continue uninterrupted even if a feature is not yet supported.</p> </li> <li> <p>Advanced query optimization such as  filter pushdown, column pruning and join reordering behind the scenes.</p> </li> <li> <p>Scalable MPI-based execution, leveraging High-Performance Computing (HPC) techniques for efficient parallelism; whether you're working on a laptop or running jobs across a large cloud cluster.</p> </li> <li> <p>Vectorized execution with streaming and spill-to-disk capabilities, making it possible to process datasets larger than memory reliably.</p> </li> </ul> <p>Warning</p> <p>Bodo DataFrame Library is under active development and is currently considered experimental. Some features and APIs may not yet be fully supported. We welcome your feedback \u2014 please join our community Slack or open an issue on our GitHub if you encounter any problems!</p>"},{"location":"api_docs/dataframe_lib/#lazy-evaluation-and-fallback-to-pandas","title":"Lazy Evaluation and Fallback to Pandas","text":"<p>Bodo DataFrame Library operates with lazy evaluation to allow query optimization, meaning operations are recorded into a query plan rather than executed immediately. Execution is automatically triggered only when results are actually needed, such as when displaying a DataFrame <code>df</code> with <code>print(df)</code>.</p> <p>If the user code encounters an unsupported Pandas API or an unsupported parameter, Bodo DataFrame library gracefully falls back to native Pandas. When this happens, the current query plan of the DataFrame is immediately executed, the resulting data is collected onto a single core and converted to a Pandas DataFrame, and further operations proceed using Pandas.</p> <p>Warning</p> <p>Fallback to Pandas may lead to degraded performance and increase the risk of out-of-memory (OOM) errors, especially for large datasets.</p> <ul> <li>General Functions</li> <li>Dataframe API</li> <li>Input/Output</li> <li>Series API</li> <li>GroupBy</li> </ul>"},{"location":"api_docs/dataframe_lib/io/","title":"Input/Output","text":""},{"location":"api_docs/dataframe_lib/io/#bodopandasread_parquet","title":"bodo.pandas.read_parquet","text":"<pre><code>bodo.pandas.read_parquet(\n    path,\n    engine=\"auto\",\n    columns=None,\n    storage_options=None,\n    use_nullable_dtypes=lib.no_default,\n    dtype_backend=lib.no_default,\n    filesystem=None,\n    filters=None,\n    **kwargs,\n) -&gt; BodoDataFrame\n</code></pre> <p>Creates a BodoDataFrame object for reading from parquet file(s) lazily.</p> <p>Parameters</p> <p>path : str, list[str]: Location of the parquet file(s) to read. Refer to <code>pandas.read_parquet</code> for more details. The type of this argument differs from Pandas.</p> <p>All other parameters will trigger a fallback to <code>pandas.read_parquet</code> if a non-default value is provided.</p> <p>Returns</p> <p>BodoDataFrame</p> <p>Example</p> <pre><code>import bodo.pandas as bd\n\noriginal_df = bd.DataFrame(\n    {\"foo\": range(15), \"bar\": range(15, 30)}\n   )\n\noriginal_df.to_parquet(\"example.pq\")\n\nrestored_df = bd.read_parquet(\"example.pq\")\nprint(type(restored_df))\nprint(restored_df.head())\n</code></pre> <p>Output:</p> <pre><code>&lt;class 'bodo.pandas.frame.BodoDataFrame'&gt;\n   foo  bar\n0    0   15\n1    1   16\n2    2   17\n3    3   18\n4    4   19\n</code></pre>"},{"location":"api_docs/dataframe_lib/io/#bodopandasread_iceberg","title":"bodo.pandas.read_iceberg","text":"<pre><code>bodo.pandas.read_iceberg(\n    table_identifier: str,\n    catalog_name: str | None = None,\n    catalog_properties: dict[str, Any] | None = None,\n    row_filter: str | None = None,\n    selected_fields: tuple[str] | None = None,\n    case_sensitive: bool = True,\n    snapshot_id: int | None = None,\n    limit: int | None = None,\n    scan_properties: dict[str, Any] | None = None,\n    location: str | None = None,\n) -&gt; BodoDataFrame\n</code></pre> <p>Creates a BodoDataFrame object for reading from an Iceberg table lazily.</p> <p>Refer to <code>pandas.read_iceberg</code> for more details.</p> <p>Warning</p> <p>This function is experimental in Pandas and may change in future releases.</p> <p>Parameters</p> <p>table_identifier: str: Identifier of the Iceberg table to read. This should be in the format <code>schema.table</code></p> <p>catalog_name: str, optional: Name of the catalog to use. If not provided, the default catalog will be used. See PyIceberg's documentation for more details.</p> <p>catalog_properties: dict[str, Any], optional: Properties for the catalog connection.</p> <p>row_filter: str, optional: expression to filter rows.</p> <p>selected_fields: tuple[str], optional: Fields to select from the table, if not provided, all fields will be selected.</p> <p>snapshot_id: int, optional: ID of the snapshot to read from. If not provided, the latest snapshot will be used.</p> <p>limit: int, optional: Maximum number of rows to read. If not provided, all rows will be read.</p> <p>location: str, optional: Location of the table (if supported by the catalog). If this is passed a path and catalog_name and catalog_properties are None, it will use a filesystem catalog with the provided location. If the location is an S3 Tables ARN it will use the S3TablesCatalog.</p> <p>Non-default values for case_sensitive and scan_properties will trigger a fallback to <code>pandas.read_iceberg</code>.</p> <p>Returns</p> <p>BodoDataFrame</p> <p>Examples</p> <p>Simple read of a table stored without a catalog on the filesystem: <pre><code>import bodo.pandas as bd\n\ndf = bd.read_iceberg(\"my_table\", location=\"s3://path/to/iceberg/warehouse\")\n</code></pre></p> <p>Read a table using a predefined PyIceberg catalog. <pre><code>import bodo.pandas as bd\n\ndf = bd.read_iceberg(\n    table_identifier=\"my_schema.my_table\",\n    catalog_name=\"my_catalog\",\n    row_filter=\"col1 &gt; 10\",\n    selected_fields=(\"col1\", \"col2\"),\n    snapshot_id=123456789,\n    limit=1000\n)\n</code></pre></p> <p>Read a table using a new PyIceberg catalog with custom properties. <pre><code>import bodo.pandas as bd\nimport pyiceberg.catalog\n\ndf = bd.read_iceberg(\n    table_identifier=\"my_schema.my_table\",\n    catalog_properties={\n        pyiceberg.catalog.PY_CATALOG_IMPL: \"bodo.io.iceberg.catalog.dir.DirCatalog\",\n        pyiceberg.catalog.WAREHOUSE_LOCATION: path_to_warehouse_dir,\n    }\n)\n</code></pre></p> <p>Read a table from an S3 Tables Bucket using the location parameter.</p> <pre><code>import bodo.pandas as bd\n\ndf = bd.read_iceberg(\n    table_identifier=\"my_table\",\n    location=\"arn:aws:s3tables:&lt;region&gt;:&lt;account_number&gt;:my-bucket/my-table\"\n)\n</code></pre>"},{"location":"api_docs/dataframe_lib/io/#bodopandasread_iceberg_table","title":"bodo.pandas.read_iceberg_table","text":"<pre><code>bodo.pandas.read_iceberg_table(\n    table: pyiceberg.table.Table,\n) -&gt; BodoDataFrame\n</code></pre> <p>Creates a BodoDataFrame object for reading from an Iceberg table lazily.</p> <p>Warning</p> <p>This function is not part of the Pandas API and is specific to Bodo.</p> <p>Parameters</p> <p>table_identifier: pyiceberg.table.Table: PyIceberg Table object to read with Bodo.</p> <p>Returns</p> <p>BodoDataFrame</p> <p>Examples</p> <p>Simple read of a local table stored in a sql catalog: <pre><code>from pyiceberg.catalog import load_catalog\n\nwarehouse_path = \"/tmp/warehouse\"\ncatalog = load_catalog(\n    \"default\",\n    **{\n        'type': 'sql',\n        \"uri\": f\"sqlite:///{warehouse_path}/pyiceberg_catalog.db\",\n        \"warehouse\": f\"file://{warehouse_path}\",\n    },\n)\ntable = catalog.load_table(\"my_schema.my_table\")\ndf = bodo.pandas.read_iceberg_table(table)\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/dataframe/","title":"DataFrame API","text":"<p>The Bodo DataFrame Library supports Pandas DataFrame methods and accessors that are listed below. They can be accessed through <code>BodoDataFrame</code> and follow the same behavior as their Pandas equivalents. For details on usage, we link to either the corresponding Pandas documentation or relevant sections of the Bodo documentation, depending on the context.</p> <p>Note</p> <p>If the user code encounters an unsupported Pandas API or an unsupported parameter, Bodo  DataFrame Library gracefully falls back to native Pandas. See overview of   the Bodo DataFrame Library for more info.</p>"},{"location":"api_docs/dataframe_lib/dataframe/#function-application-groupby-window","title":"Function application, GroupBy &amp; window","text":"<ul> <li><code>bodo.pandas.BodoDataFrame.apply</code></li> <li><code>bodo.pandas.BodoDataFrame.groupby</code></li> <li><code>bodo.pandas.BodoDataFrame.map_partitions</code></li> </ul>"},{"location":"api_docs/dataframe_lib/dataframe/#reindexing-selection-label-manipulation","title":"Reindexing / selection / label manipulation","text":"<ul> <li><code>bodo.pandas.BodoDataFrame.head</code></li> <li><code>bodo.pandas.BodoDataFrame.reset_index</code></li> </ul>"},{"location":"api_docs/dataframe_lib/dataframe/#reshaping-sorting-transposing","title":"Reshaping, sorting, transposing","text":"<ul> <li><code>bodo.pandas.BodoDataFrame.sort_values</code></li> </ul>"},{"location":"api_docs/dataframe_lib/dataframe/#serialization-io-conversion","title":"Serialization / IO / conversion","text":"<ul> <li><code>bodo.pandas.BodoDataFrame.to_iceberg</code></li> <li><code>bodo.pandas.BodoDataFrame.to_parquet</code></li> <li><code>bodo.pandas.BodoDataFrame.to_s3_vectors</code></li> </ul>"},{"location":"api_docs/dataframe_lib/dataframe/apply/","title":"bodo.pandas.BodoDataFrame.apply","text":"<pre><code>BodoDataFrame.apply(\n        func,\n        axis=0,\n        raw=False,\n        result_type=None,\n        args=(),\n        by_row=\"compat\",\n        engine=\"bodo\",\n        engine_kwargs=None,\n        **kwargs,\n    ) -&gt; BodoSeries\n</code></pre> <p>Apply a function along an axis of the BodoDataFrame.</p> <p>Currently only supports applying a function that returns a scalar value for each row (i.e. <code>axis=1</code>). All other uses will fall back to Pandas. See <code>pandas.DataFrame.apply</code> for more details.</p> <p>By default, bodo.jit will be applied to <code>func</code>.  If this JIT compilation fails for any reason, the mapping function will be run as a normal Python function.  If the compilation succeeds, the JIT compiled function will be used for apply and the overheads associated with running Python code from within the execution pipeline are avoided.</p> <p>Note</p> <p>Calling <code>BodoDataFrame.apply</code> will immediately execute a plan if this JIT compilation fails, generating a small sample of the BodoDataFrame and calling <code>pandas.DataFrame.apply</code> on the sample to infer output types before proceeding with lazy evaluation.</p> <p>Note</p> <p>Functions passed to <code>func</code> (whether explicitly wrapper with a JIT decorator or not) may not use Numba's <code>with objmode</code> context.  Doing so will result in a runtime exception.</p> <p>Parameters</p> <p>func : function: Function to apply to each row.</p> <p>axis : {0 or 1}, default 0: The axis to apply the function over. <code>axis=0</code> will fall back to <code>pandas.DataFrame.apply</code>.</p> <p>args : tuple: Additional positional arguments to pass to func.</p> <p>engine : {'bodo', 'python', 'numba'}, default 'bodo': The engine to use to compute the UDF. By default, <code>engine=\"bodo\"</code> will apply bodo.jit to <code>func</code> with fallback to Python described above. Use engine='python' to avoid any jit compilation. <code>engine='numba'</code> will trigger a fall back to <code>pandas.DataFrame.apply</code>.</p> <p>**kwargs: Additional keyword arguments to pass as keyword arguments to func.</p> <p>All other parameters will trigger a fallback to <code>pandas.DataFrame.apply</code> if a non-default value is provided.</p> <p>Returns</p> <p>BodoSeries: The result of applying func to each row in the BodoDataFrame.</p> <p>Example</p> <pre><code>import bodo.pandas as bd\n\nbdf = bd.DataFrame(\n        {\n            \"a\": bd.array([1, 2, 3] * 4, \"Int64\"),\n            \"b\": bd.array([4, 5, 6] * 4, \"Int64\"),\n            \"c\": [\"a\", \"b\", \"c\"] * 4,\n        },\n    )\n\nout_bodo = bdf.apply(lambda x: x[\"a\"] + 1, axis=1)\n\nprint(type(out_bodo))\nprint(out_bodo)\n</code></pre> <p>Output: <pre><code>&lt;class 'bodo.pandas.series.BodoSeries'&gt;\n0     2\n1     3\n2     4\n3     2\n4     3\n5     4\n6     2\n7     3\n8     4\n9     2\n10    3\n11    4\ndtype: int64[pyarrow]\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/dataframe/groupby/","title":"bodo.pandas.BodoDataFrame.groupby","text":"<pre><code>BodoDataFrame.groupby(\n    by=None,\n    axis=lib.no_default,\n    level=None,\n    as_index=True,\n    sort=False,\n    group_keys=True,\n    observed=lib.no_default,\n    dropna=True\n) -&gt; DataFrameGroupBy\n</code></pre> <p>Creates a DataFrameGroupBy object representing the data in the input DataFrame grouped by a column or list of columns. The object can then be used to apply functions over groups.</p> <p>Parameters</p> <p>by : str | List[str]: The column or list of columns to use when creating groups.</p> <p>as_index : bool, default True: Whether the grouped labels will appears as an index in the final output. If as_index is False, then the grouped labels will appear as regular columns.</p> <p>dropna: bool, default True If True, rows where the group label contains a missing value will be dropped from the final output.</p> <p>All other parameters will trigger a fallback to <code>pandas.DataFrame.groupby</code> if a non-default value is provided.</p> <p>Returns</p> <p>DataFrameGroupBy</p> <p>Examples</p> <p><pre><code>import bodo.pandas as bd\n\nbdf1 = bd.DataFrame({\n    \"A\": [\"foo\", \"foo\", \"bar\", \"bar\"],\n    \"B\": [1, 1, 1, None],\n    \"C\": [1, 2, 3, 4]\n})\n\nbdf2 = bdf1.groupby([\"A\", \"B\"]).sum()\nprint(bdf2)\n</code></pre> Output: <pre><code>         C\nA   B\nbar 1.0  3\nfoo 1.0  3\n</code></pre></p> <p><pre><code>bdf3 = bdf1.groupby([\"A\", \"B\"], as_index=False, dropna=False).sum()\nprint(bdf2)\n</code></pre> Output: <pre><code>     A     B  C\n0  bar  &lt;NA&gt;  4\n1  foo   1.0  3\n2  bar   1.0  3\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/dataframe/head/","title":"bodo.pandas.BodoDataFrame.head","text":"<pre><code>BodoDataFrame.head(n=5) -&gt; BodoDataFrame\n</code></pre> <p>Returns the first n rows of the BodoDataFrame.</p> <p>Parameters</p> <p>n : int, default 5: Number of rows to select.</p> <p>Returns</p> <p>BodoDataFrame</p> <p>Example</p> <pre><code>import bodo.pandas as bd\n\noriginal_df = bd.DataFrame(\n    {\"foo\": range(15), \"bar\": range(15, 30)}\n   )\n\noriginal_df.to_parquet(\"example.pq\")\n\nrestored_df = bd.read_parquet(\"example.pq\")\nrestored_df_head = restored_df.head(2)\nprint(type(restored_df_head))\nprint(restored_df_head)\n</code></pre> <p>Output: <pre><code>&lt;class 'bodo.pandas.frame.BodoDataFrame'&gt;\n   foo  bar\n0    0   15\n1    1   16\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/dataframe/map_partitions/","title":"bodo.pandas.BodoDataFrame.map_partitions","text":"<pre><code>BodoDataFrame.map_partitions(func, *args, **kwargs) -&gt; BodoSeries | BodoDataFrame\n</code></pre> <p>Apply a function to groups of rows in a DataFrame and return a DataFrame or Series of the same size.</p> <p>If the input DataFrame is lazy (i.e. its plan has not been evaluated yet) and func returns a Series, then the output will be lazy as well. When the lazy output is evaluated, func will take batches of rows from the input DataFrame. In the cases where func returns a DataFrame or the input DataFrame is not lazy, each worker will call func on their entire local chunk of the input DataFrame.</p> <p>Parameters</p> <p>func : Callable: A function that takes in a DataFrame and returns a DataFrame or Series (with the same number of rows). Currently, functions that return a DataFrame will trigger execution even if the input DataFrame has a lazy plan.</p> <p>*args: Additional positional arguments to pass to func.</p> <p>**kwargs: Additional keyword arguments to pass as keyword arguments to func.</p> <p>Returns</p> <p>BodoSeries or BodoDataFrame:  The result of applying func to the BodoDataFrame.</p> <p>Example</p> <pre><code>import bodo.pandas as bd\n\nbdf = bd.DataFrame(\n    {\"foo\": range(15), \"bar\": range(15, 30)}\n   )\n\nbdf_mapped = bdf.map_partitions(lambda df_: df_.foo + df_.bar)\nprint(bdf_mapped)\n</code></pre> <p>Output: <pre><code>0     15\n1     17\n2     19\n3     21\n4     23\n5     25\n6     27\n7     29\n8     31\n9     33\n10    35\n11    37\n12    39\n13    41\n14    43\ndtype: int64[pyarrow]\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/dataframe/set_columns/","title":"Setting DataFrame Columns","text":"<p>Bodo DataFrames support setting columns lazily when the value is a Series created from the same DataFrame or a constant value. Other cases will fallback to Pandas.</p> <p>Examples</p> <pre><code>import bodo.pandas as bd\n\nbdf = bd.DataFrame(\n        {\n            \"A\": bd.array([1, 2, 3, 7] * 3, \"Int64\"),\n            \"B\": [\"A1\", \"B1 \", \"C1\", \"Abc\"] * 3,\n            \"C\": bd.array([4, 5, 6, -1] * 3, \"Int64\"),\n        }\n    )\n\nbdf[\"D\"] = bdf[\"B\"].str.lower()\nprint(type(bdf))\nprint(bdf.D)\n</code></pre> <p>Output: <pre><code>&lt;class 'bodo.pandas.frame.BodoDataFrame'&gt;\n0      a1\n1     b1\n2      c1\n3     abc\n4      a1\n5     b1\n6      c1\n7     abc\n8      a1\n9     b1\n10     c1\n11    abc\nName: D, dtype: string\n</code></pre></p> <pre><code>import bodo.pandas as bd\n\nbdf = bd.DataFrame(\n        {\n            \"A\": bd.array([1, 2, 3, 7] * 3, \"Int64\"),\n            \"B\": [\"A1\", \"B1 \", \"C1\", \"Abc\"] * 3,\n            \"C\": bd.array([4, 5, 6, -1] * 3, \"Int64\"),\n        }\n    )\n\nbdf[\"D\"] = 11\nprint(type(bdf))\nprint(bdf.D)\n</code></pre> <p>Output: <pre><code>&lt;class 'bodo.pandas.frame.BodoDataFrame'&gt;\n0     11\n1     11\n2     11\n3     11\n4     11\n5     11\n6     11\n7     11\n8     11\n9     11\n10    11\n11    11\nName: D, dtype: int64[pyarrow]\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/dataframe/sort_values/","title":"bodo.pandas.BodoDataFrame.sort_values","text":"<p><pre><code>BodoDataFrame.sort_values(by, *, axis=0, ascending=True, inplace=False, kind=\"quicksort\", na_position=\"last\", ignore_index=False, key=None)\n</code></pre> Sorts the elements of the BodoDataFrame and returns a new sorted BodoDataFrame.</p> <p>Parameters</p> <p>by: str or list of str: Name or list of column names to sort by.</p> <p>ascending : bool or list of bool, default True: Sort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by.</p> <p>na_position: str {'first', 'last'} or list of str, default 'last': Puts NaNs at the beginning if first; last puts NaNs at the end. Specify list for multiple NaN orders by key.  If this is a list of strings, must match the length of the by.</p> <p>All other parameters will trigger a fallback to <code>pandas.DataFrame.sort_values</code> if a non-default value is provided.</p> <p>Returns</p> <p>BodoDataFrame</p> <p>Example</p> <pre><code>import bodo.pandas as bd\n\nbdf = bd.DataFrame(\n    {\n        \"A\": bd.array([1, 2, 3, 7] * 3, \"Int64\"),\n        \"B\": [\"A1\", \"B1\", \"C1\", \"Abc\"] * 3,\n        \"C\": bd.array([6, 5, 4] * 4, \"Int64\"),\n    }\n)\n\nbdf_sorted = bdf.sort_values(by=[\"A\", \"C\"], ascending=[False, True])\nprint(bdf_sorted)\n</code></pre> <p>Output: <pre><code>    A    B  C\n0   7  Abc  4\n1   7  Abc  5\n2   7  Abc  6\n3   3   C1  4\n4   3   C1  5\n5   3   C1  6\n6   2   B1  4\n7   2   B1  5\n8   2   B1  6\n9   1   A1  4\n10  1   A1  5\n11  1   A1  6\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/dataframe/to_iceberg/","title":"bodo.pandas.BodoDataFrame.to_iceberg","text":"<p><pre><code>BodoDataFrame.to_iceberg(\n        table_identifier,\n        catalog_name=None,\n        *,\n        catalog_properties=None,\n        location=None,\n        append=False,\n        partition_spec=None,\n        sort_order=None,\n        properties=None,\n        snapshot_properties=None\n)\n</code></pre> Write a DataFrame as an Iceberg dataset.</p> <p>Refer to <code>pandas.DataFrame.to_iceberg</code> for more details.</p> <p>Warning</p> <p>This function is experimental in Pandas and may change in future releases.</p> <p>Note</p> <p>This function assumes that the Iceberg namespace is already created in the catalog.</p> <p>Parameters</p> <p>table_identifier: str: Table identifier to write</p> <p>catalog_name: str, optional: Name of the catalog to use. If not provided, the default catalog will be used. See PyIceberg's documentation for more details.</p> <p>catalog_properties: dict[str, Any], optional: Properties for the catalog connection.</p> <p>location: str, optional: Location of the table (if supported by the catalog). If this is passed a path and catalog_name and catalog_properties are None, it will use a filesystem catalog with the provided location. If the location is an S3 Tables ARN it will use the S3TablesCatalog.</p> <p>append: bool: Append or overwrite if the table exists</p> <p>partition_spec: PartitionSpec, optional: PyIceberg partition spec for the table (only used if creating a new table). See PyIceberg's documentation for more details.</p> <p>sort_order: SortOrder, optional: PyIceberg sort order for the table (only used if creating a new table). See PyIceberg's documentation for more details.</p> <p>properties: dict[str, Any], optional: Properties to add to the new table.</p> <p>snapshot_properties: dict[str, Any], optional: Properties to add to the new table snapshot.</p> <p>Example</p> <p>Simple write of a table on the filesystem without a catalog: <pre><code>import bodo.pandas as bd\nfrom pyiceberg.transforms import IdentityTransform\nfrom pyiceberg.partitioning import PartitionField, PartitionSpec\nfrom pyiceberg.table.sorting import SortField, SortOrder\n\nbdf = bd.DataFrame(\n        {\n            \"one\": [-1.0, 1.3, 2.5, 3.0, 4.0, 6.0, 10.0],\n            \"two\": [\"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\", \"foo\"],\n            \"three\": [True, False, True, True, True, False, False],\n            \"four\": [-1.0, 5.1, 2.5, 3.0, 4.0, 6.0, 11.0],\n            \"five\": [\"foo\", \"bar\", \"baz\", None, \"bar\", \"baz\", \"foo\"],\n        }\n    )\n\npart_spec = PartitionSpec(PartitionField(2, 1001, IdentityTransform(), \"id_part\"))\nsort_order = SortOrder(SortField(source_id=4, transform=IdentityTransform()))\nbdf.to_iceberg(\"test_table\", location=\"./iceberg_warehouse\", partition_spec=part_spec, sort_order=sort_order)\n\nout_df = bd.read_iceberg(\"test_table\", location=\"./iceberg_warehouse\")\n# Only reads Parquet files of partition \"foo\" from storage\nprint(out_df[out_df[\"two\"] == \"foo\"])\n</code></pre></p> <p>Output: <pre><code>    one  two  three  four  five\n0  -1.0  foo   True  -1.0   foo\n1   3.0  foo   True   3.0  &lt;NA&gt;\n2  10.0  foo  False  11.0   foo\n</code></pre></p> <p>Write a DataFrame to an Iceberg table in S3 Tables using the location parameter:</p> <pre><code>df.to_iceberg(\n    table_identifier=\"my_table\",\n    location=\"arn:aws:s3tables:&lt;region&gt;:&lt;account_number&gt;:my-bucket/my-table\"\n)\n</code></pre>"},{"location":"api_docs/dataframe_lib/dataframe/to_parquet/","title":"bodo.pandas.BodoDataFrame.to_parquet","text":"<p><pre><code>BodoDataFrame.to_parquet(path=None, engine=\"auto\", compression=\"snappy\", index=None, partition_cols=None, storage_options=None, row_group_size=-1, **kwargs)\n</code></pre> Write a DataFrame as a Parquet dataset.</p> <p>Parameters</p> <p>path: str: Output path to write. It can be a local path (e.g. <code>output.parquet</code>), AWS S3 (<code>s3://...</code>), Azure ALDS (<code>abfs://...</code>, <code>abfss://...</code>), or GCP GCS (<code>gcs://...</code>, <code>gs://</code>).</p> <p>compression : str, default 'snappy': File compression to use. Can be None, 'snappy', 'gzip', or 'brotli'.</p> <p>row_group_size : int: Row group size in output Parquet files. -1 allows the backend to choose.</p> <p>All other parameters will trigger a fallback to <code>pandas.DataFrame.to_parquet</code>.</p> <p>Example</p> <pre><code>import bodo.pandas as bd\n\nbdf = bd.DataFrame(\n    {\n        \"A\": bd.array([1, 2, 3, 7] * 3, \"Int64\"),\n        \"B\": [\"A1\", \"B1\", \"C1\", \"Abc\"] * 3,\n        \"C\": bd.array([6, 5, 4] * 4, \"Int64\"),\n    }\n)\n\nbdf.to_parquet(\"output.parquet\")\nprint(bd.read_parquet(\"output.parquet\"))\n</code></pre> <p>Output: <pre><code>    A    B  C\n0   1   A1  6\n1   2   B1  5\n2   3   C1  4\n3   7  Abc  6\n4   1   A1  5\n5   2   B1  4\n6   3   C1  6\n7   7  Abc  5\n8   1   A1  4\n9   2   B1  6\n10  3   C1  5\n11  7  Abc  4\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/dataframe/to_s3_vectors/","title":"bodo.pandas.BodoDataFrame.to_s3_vectors","text":"<p><pre><code>BodoDataFrame.to_s3_vectors(vector_bucket_name, index_name)\n</code></pre> Write a DataFrame to an S3 Vectors index. The DataFrame should have \"key\", \"data\" and \"metadata\" columns. \"key\" column data should be strings, and \"data\" column should be float32 embeddings with the same length as expected by the vector index in each row. \"metadata\" should be key-value pairs. See S3 documentation for more details.</p> <p>Parameters</p> <p>vector_bucket_name: str: S3 Vectors bucket name to use.</p> <p>index_name : str: S3 Vectors index name to use.</p> <p>region : str, optional: Region of S3 Vector bucket.</p> <p>Example</p> <pre><code>import pandas as pd\nimport bodo.pandas as bd\nimport boto3\nimport json\n\nbedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-east-2\")\ntexts = [\n    \"Star Wars: A farm boy joins rebels to fight an evil empire in space\", \n    \"Jurassic Park: Scientists create dinosaurs in a theme park that goes wrong\",\n    \"Finding Nemo: A father fish searches the ocean to find his lost son\"\n]\n\nembeddings = []\nfor text in texts:\n    response = bedrock.invoke_model(\n        modelId=\"amazon.titan-embed-text-v2:0\",\n        body=json.dumps({\"inputText\": text})\n    )\n    response_body = json.loads(response[\"body\"].read())\n    embeddings.append(response_body[\"embedding\"])\n\ndf = pd.DataFrame({\"key\": [\"Star Wars\", \"Jurassic Park\", \"Finding Nemo\"],\n                   \"data\": embeddings,\n                   \"texts\": texts})\ndf[\"metadata\"] = [\n    {\"source_text\": texts[0], \"genre\": \"scifi\"},\n    {\"source_text\": texts[1], \"genre\": \"scifi\"},\n    {\"source_text\": texts[2], \"genre\": \"family\"}\n]\n\n\nbdf = bd.from_pandas(df)\nbdf.to_s3_vectors(\n    vector_bucket_name=\"my-test-vector\",\n    index_name=\"my-test-ind\",\n    region=\"us-east-2\",\n)\n</code></pre>"},{"location":"api_docs/dataframe_lib/general_functions/","title":"General Functions","text":""},{"location":"api_docs/dataframe_lib/general_functions/#conversion-from-pandas","title":"Conversion from Pandas","text":"<ul> <li><code>bodo.pandas.from_pandas</code></li> </ul>"},{"location":"api_docs/dataframe_lib/general_functions/#top-level-dealing-with-datetimelike-data","title":"Top-level dealing with datetimelike data","text":"<p>Note</p> <p><code>to_datetime</code> currently supports only BodoSeries and BodoDataFrame inputs. Passing arguments of other types will trigger a fallback to Pandas.</p> <ul> <li><code>bodo.pandas.to_datetime</code></li> </ul>"},{"location":"api_docs/dataframe_lib/general_functions/#top-level-missing-data","title":"Top-level missing data","text":"<p>Note</p> <p><code>isna</code>, <code>isnull</code>, <code>notna</code>, and <code>notnull</code> currently support only BodoSeries and scalar inputs (e.g., integers, strings). Passing other types will trigger a fallback to Pandas.</p> <ul> <li><code>bodo.pandas.isna</code></li> <li><code>bodo.pandas.isnull</code></li> <li><code>bodo.pandas.notna</code></li> <li><code>bodo.pandas.notnull</code></li> </ul>"},{"location":"api_docs/dataframe_lib/general_functions/from_pandas/","title":"bodo.pandas.from_pandas","text":"<pre><code>bodo.pandas.from_pandas(df: pandas.DataFrame) -&gt; BodoDataFrame\n</code></pre> <p>Converts a Pandas DataFrame into an equivalent BodoDataFrame.</p> <p>Parameters</p> <p>df : pandas.DataFrame: The Pandas DataFrame to use as data source.</p> <p>Returns</p> <p>BodoDataFrame</p> <p>Example</p> <pre><code>import pandas as pd\nimport bodo.pandas as bodo_pd\n\ndf = pd.DataFrame(\n        {\n            \"a\": [1, 2, 3, 7] * 3,\n            \"b\": [4, 5, 6, 8] * 3,\n            \"c\": [\"a\", \"b\", None, \"abc\"] * 3,\n        },\n    )\n\nbdf = bodo_pd.from_pandas(df)\nprint(type(bdf))\nprint(bdf)\n</code></pre> <p>Output: <pre><code>&lt;class 'bodo.pandas.frame.BodoDataFrame'&gt;\n    a  b     c\n0   1  4     a\n1   2  5     b\n2   3  6  &lt;NA&gt;\n3   7  8   abc\n4   1  4     a\n5   2  5     b\n6   3  6  &lt;NA&gt;\n7   7  8   abc\n8   1  4     a\n9   2  5     b\n10  3  6  &lt;NA&gt;\n11  7  8   abc\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/groupby/","title":"GroupBy","text":"<p>The DataFrame Library supports grouping BodoDataFrames on columns and aggregating the grouped data via the <code>bodo.pandas.DataFrameGroupBy</code> and <code>bodo.pandas.SeriesGroupBy</code> classes. An instance of one of these classes will be returned when using the <code>BodoDataFrame.groupby()</code> method.</p> <p>Note</p> <p>Currently, the DataFrame Library supports a subset of aggregation functions with the default parameters (listed below). Future releases will add more functionality including transformation and filtering. For now, if an unsupported method or property of a <code>bodo.pandas.SeriesGroupBy</code> or <code>bodo.pandas.DataFrameGroupBy</code> is encountered, the user's code will gracefully fall back to Pandas.</p>"},{"location":"api_docs/dataframe_lib/groupby/#function-application","title":"Function Application","text":"<ul> <li><code>DataFrameGroupBy.agg</code></li> <li><code>SeriesGroupBy.agg</code></li> </ul>"},{"location":"api_docs/dataframe_lib/groupby/#dataframegroupby-computations-descriptive-stats","title":"DataFrameGroupBy Computations / Descriptive Stats","text":"<ul> <li><code>DataFrameGroupBy.sum</code></li> <li><code>DataFrameGroupBy.count</code></li> <li><code>DataFrameGroupBy.min</code></li> <li><code>DataFrameGroupBy.max</code></li> <li><code>DataFrameGroupBy.median</code></li> <li><code>DataFrameGroupBy.mean</code></li> <li><code>DataFrameGroupBy.std</code></li> <li><code>DataFrameGroupBy.var</code></li> <li><code>DataFrameGroupBy.skew</code></li> <li><code>DataFrameGroupBy.nunique</code></li> <li><code>DataFrameGroupBy.size</code></li> </ul>"},{"location":"api_docs/dataframe_lib/groupby/#seriesgroupby-computations-descriptive-stats","title":"SeriesGroupby Computations / Descriptive Stats","text":"<ul> <li><code>SeriesGroupBy.sum</code></li> <li><code>SeriesGroupBy.count</code></li> <li><code>SeriesGroupBy.min</code></li> <li><code>SeriesGroupBy.max</code></li> <li><code>SeriesGroupBy.median</code></li> <li><code>SeriesGroupBy.mean</code></li> <li><code>SeriesGroupBy.std</code></li> <li><code>SeriesGroupBy.var</code></li> <li><code>SeriesGroupBy.skew</code></li> <li><code>SeriesGroupBy.nunique</code></li> <li><code>SeriesGroupBy.size</code></li> </ul>"},{"location":"api_docs/dataframe_lib/groupby/frame_agg/","title":"DataFrameGroupBy.agg","text":"<pre><code>DataFrameGroupBy.agg(func=None, engine=None, engine_kwargs=None, **kwargs) -&gt; BodoDataFrame\n</code></pre> <p>Apply one or more aggregate functions to groups of data in a BodoDataFrame. This method is the same as <code>DataFrameGroupBy.aggregate</code>.</p> <p>Parameters</p> <p>func : function, str, list, dict or None: Function(s) to use for aggregating the data. Acceptable combinations are:</p> <ul> <li>A supported function e.g. <code>sum</code></li> <li>The name of a supported aggregation function e.g. <code>\"sum\"</code></li> <li>A list of functions, which will be applied to each selected column e.g. <code>[\"sum\"</code>, <code>\"count\"]</code></li> <li>A dictionary mapping column name to aggregate function e.g. <code>{\"col_1\": \"sum\", \"col_2\": \"mean\"}</code></li> <li>None along with key word arguments specifying Named Aggregates.</li> </ul> <p>Refer to our documentation for aggregate functions that are currently supported. Any other combination of arguments or user defined functions will either fallback to Pandas <code>DataFrameGroupBy.agg</code> or raise a descriptive error.</p> <p>**kwargs Key word arguments are used to create Named Aggregations and should be in the form <code>new_name=pd.NamedAgg(column_name, function)</code> or simply <code>new_name=(column_name, function)</code>.</p> <p>Note</p> <p>The <code>engine</code> and <code>engine_kwargs</code> parameters are not supported, and will trigger a fallback to Pandas if specified.</p> <p>Returns</p> <p>BodoDataFrame</p> <p>Examples</p> <p><pre><code>import bodo.pandas as bd\n\nbdf1 = bd.DataFrame({\n    \"A\": [\"foo\", \"foo\", \"bar\", \"bar\"],\n    \"C\": [1, 2, 3, 4],\n    \"D\": [\"A\", \"A\", \"C\", \"D\"]\n})\n\nbdf2 = bdf1.groupby(\"A\").agg(\"sum\")\n\nprint(bdf2)\n</code></pre> Output: <pre><code>     C   D\nA\nbar  7  CD\nfoo  3  AA\n</code></pre></p> <p><pre><code>bdf3 = bdf1.groupby(\"A\").agg([\"sum\", \"count\"])\n\nprint(bdf3)\n</code></pre> Output: <pre><code>      C         D\n    sum count sum count\nA\nbar   7     2  CD     2\nfoo   3     2  AA     2\n</code></pre></p> <p><pre><code>bdf4 = bdf1.groupby(\"A\").agg({\"C\": \"mean\", \"D\": \"nunique\"})\n\nprint(bdf4)\n</code></pre> Output: <pre><code>       C  D\nA\nbar  3.5  2\nfoo  1.5  1\n</code></pre></p> <p><pre><code>bdf5 = bdf1.groupby(\"A\").agg(mean_C=bd.NamedAgg(\"C\", \"mean\"), sum_D=bd.NamedAgg(\"D\", \"sum\"))\n\nprint(bdf5)\n</code></pre> Output: <pre><code>     mean_C sum_D\nA\nbar     3.5    CD\nfoo     1.5    AA\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/groupby/series_agg/","title":"SeriesGroupBy.agg","text":"<pre><code>SeriesGroupBy.agg(func=None, engine=None, engine_kwargs=None, **kwargs) -&gt; BodoDataFrame | BodoSeries\n</code></pre> <p>Apply one or more aggregate functions to groups of data in a single column from a BodoDataFrame. This method is the same as <code>SeriesGroupBy.aggregate</code>.</p> <p>Parameters</p> <p>func : function, str, list, dict or None: Function(s) to use for aggregating the data. Acceptable combinations are:</p> <ul> <li>A supported function e.g. <code>sum</code></li> <li>The name of a supported aggregation function e.g. <code>\"sum\"</code></li> <li>A list of functions, which will be applied to each selected column e.g. <code>[\"sum\"</code>, <code>\"count\"]</code></li> <li>None along with key word arguments specifying the supported functions to apply.</li> </ul> <p>While providing a dictionary argument for func is supported, this use has been deprecated in Pandas and will raise an error in newer versions. Refer to our documentation for aggregate functions that are currently supported. Any other combination of arguments or user defined functions will either fallback to Pandas <code>SeriesGroupBy.agg</code> or raise a descriptive error.</p> <p>**kwargs Key word arguments are used to create Named Aggregations and should be in the form <code>new_name=\"function\"</code>.</p> <p>Note</p> <p>The <code>engine</code> and <code>engine_kwargs</code> parameters are not supported, and will trigger a fallback to Pandas if specified.</p> <p>Returns</p> <p>BodoDataFrame or BodoSeries, depending on the value of func.</p> <p>Examples</p> <p><pre><code>import bodo.pandas as bd\n\nbdf1 = bd.DataFrame({\n    \"A\": [\"foo\", \"foo\", \"bar\", \"bar\"],\n    \"C\": [1, 2, 3, 4],\n    \"D\": [\"A\", \"A\", \"C\", \"D\"]\n})\n\nbdf2 = bdf1.groupby(\"A\")[\"C\"].agg(\"sum\")\n\nprint(bdf2)\n</code></pre> Output: <pre><code>A\nbar    7\nfoo    3\nName: C, dtype: int64[pyarrow]\n</code></pre></p> <p><pre><code>bdf3 = bdf1.groupby(\"A\")[\"C\"].agg([\"sum\", \"count\"])\n\nprint(bdf3)\n</code></pre> Output: <pre><code>     sum  count\nA\nbar    7      2\nfoo    3      2\n</code></pre></p> <p><pre><code>bdf4 = bdf1.groupby(\"A\")[\"C\"].agg(sum_C=\"sum\", mean_C=\"mean\")\n\nprint(bdf4)\n</code></pre> Output: <pre><code>     sum_C  mean_C\nA\nbar      7     3.5\nfoo      3     1.5\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/series/","title":"Series API","text":"<p>The Bodo DataFrame Library supports Pandas Series methods and accessors that are listed below. They can be accessed through <code>BodoSeries</code> and follow the same behavior as their Pandas equivalents. For details on usage, we link to the corresponding Pandas documentation.</p> <p>Note</p> <p>If the user code encounters an unsupported Pandas API or an unsupported parameter, Bodo  DataFrame Library gracefully falls back to native Pandas. See overview of   the Bodo DataFrame Library for more info.</p>"},{"location":"api_docs/dataframe_lib/series/#ai-functions","title":"AI Functions","text":"<ul> <li><code>bodo.pandas.BodoSeries.ai.tokenize</code></li> <li><code>bodo.pandas.BodoSeries.ai.embed</code></li> <li><code>bodo.pandas.BodoSeries.ai.llm_generate</code></li> <li><code>bodo.pandas.BodoSeries.ai.query_s3_vectors</code></li> </ul>"},{"location":"api_docs/dataframe_lib/series/#binary-operator-functions","title":"Binary operator functions","text":"<ul> <li><code>bodo.pandas.BodoSeries.add</code></li> <li><code>bodo.pandas.BodoSeries.sub</code></li> <li><code>bodo.pandas.BodoSeries.radd</code></li> <li><code>bodo.pandas.BodoSeries.rsub</code></li> </ul>"},{"location":"api_docs/dataframe_lib/series/#computations-descriptive-stats","title":"Computations / descriptive stats","text":"<ul> <li><code>bodo.pandas.BodoSeries.abs</code></li> <li><code>bodo.pandas.BodoSeries.clip</code></li> <li><code>bodo.pandas.BodoSeries.describe</code></li> </ul> <p>Note</p> <p>Unlike pandas\u2019, the quantile in Bodo DataFrames is an approximate quantile based on the KLL sketch algorithm with probabilistic error guarantees, since computing exact quantiles across large datasets is extremely expensive.</p> <ul> <li><code>bodo.pandas.BodoSeries.quantile</code></li> <li><code>bodo.pandas.BodoSeries.round</code></li> </ul> <p>Note</p> <p>For the following reduction methods, only default parameters are currently supported.</p> <ul> <li><code>bodo.pandas.BodoSeries.count</code></li> <li><code>bodo.pandas.BodoSeries.max</code></li> <li><code>bodo.pandas.BodoSeries.mean</code></li> <li><code>bodo.pandas.BodoSeries.min</code></li> <li><code>bodo.pandas.BodoSeries.product</code></li> <li><code>bodo.pandas.BodoSeries.sum</code></li> <li><code>bodo.pandas.BodoSeries.std</code></li> </ul>"},{"location":"api_docs/dataframe_lib/series/#datetimelike-properties","title":"Datetimelike properties","text":"<p>Note</p> <p>Input must be a Series of <code>datetime-like</code> data.</p>"},{"location":"api_docs/dataframe_lib/series/#datetime-properties","title":"Datetime properties","text":"<p>Note</p> <p>For missing datetime values (<code>NaT</code>), Bodo's datetime predicate accessors (e.g., <code>.is_month_end</code>, <code>.is_leap_year</code>) return <code>&lt;NA&gt;</code> to preserve nullability, whereas Pandas returns <code>False</code>.</p> <ul> <li><code>bodo.pandas.BodoSeries.dt.year</code></li> <li><code>bodo.pandas.BodoSeries.dt.month</code></li> <li><code>bodo.pandas.BodoSeries.dt.day</code></li> <li><code>bodo.pandas.BodoSeries.dt.days</code></li> <li><code>bodo.pandas.BodoSeries.dt.hour</code></li> <li><code>bodo.pandas.BodoSeries.dt.minute</code></li> <li><code>bodo.pandas.BodoSeries.dt.second</code></li> <li><code>bodo.pandas.BodoSeries.dt.seconds</code></li> <li><code>bodo.pandas.BodoSeries.dt.microsecond</code></li> <li><code>bodo.pandas.BodoSeries.dt.microseconds</code></li> <li><code>bodo.pandas.BodoSeries.dt.nanosecond</code></li> <li><code>bodo.pandas.BodoSeries.dt.nanoseconds</code></li> <li><code>bodo.pandas.BodoSeries.dt.dayofweek</code></li> <li><code>bodo.pandas.BodoSeries.dt.day_of_week</code></li> <li><code>bodo.pandas.BodoSeries.dt.weekday</code></li> <li><code>bodo.pandas.BodoSeries.dt.dayofyear</code></li> <li><code>bodo.pandas.BodoSeries.dt.day_of_year</code></li> <li><code>bodo.pandas.BodoSeries.dt.daysinmonth</code></li> <li><code>bodo.pandas.BodoSeries.dt.days_in_month</code></li> <li><code>bodo.pandas.BodoSeries.dt.date</code></li> <li><code>bodo.pandas.BodoSeries.dt.time</code></li> <li><code>bodo.pandas.BodoSeries.dt.quarter</code></li> <li><code>bodo.pandas.BodoSeries.dt.is_month_start</code></li> <li><code>bodo.pandas.BodoSeries.dt.is_month_end</code></li> <li><code>bodo.pandas.BodoSeries.dt.is_quarter_start</code></li> <li><code>bodo.pandas.BodoSeries.dt.is_quarter_end</code></li> <li><code>bodo.pandas.BodoSeries.dt.is_year_start</code></li> <li><code>bodo.pandas.BodoSeries.dt.is_year_end</code></li> <li><code>bodo.pandas.BodoSeries.dt.is_leap_year</code></li> <li><code>bodo.pandas.BodoSeries.dt.components</code></li> </ul>"},{"location":"api_docs/dataframe_lib/series/#datetime-methods","title":"Datetime methods","text":"<p>Note</p> <p>Locale format must be strict: The locale parameter in <code>month_name</code> and <code>day_name</code> must  follow the exact system locale naming convention (e.g., \"pt_BR.UTF-8\" or \"en_US.utf-8\").  Variants like \"pt_BR.utf8\" may not be recognized and trigger an error.</p> <ul> <li><code>bodo.pandas.BodoSeries.dt.normalize</code></li> <li><code>bodo.pandas.BodoSeries.dt.floor</code></li> <li><code>bodo.pandas.BodoSeries.dt.ceil</code></li> <li><code>bodo.pandas.BodoSeries.dt.month_name</code></li> <li><code>bodo.pandas.BodoSeries.dt.day_name</code></li> <li><code>bodo.pandas.BodoSeries.dt.round</code></li> <li><code>bodo.pandas.BodoSeries.dt.total_seconds</code></li> <li><code>bodo.pandas.BodoSeries.dt.isocalendar</code></li> </ul> <p>Note</p> <p>Bodo currently only supports \"NaT\" for the ambiguous parameter in <code>tz_localize</code>.  \"raise\", \"infer\", or boolean arrays are not supported and will trigger a fallback to  Pandas. Similarly, for the nonexistent parameter,  \"raise\" is not supported and will trigger a fallback. Due to these limitations, the default behavior in Bodo is <code>ambiguous=\"NaT\"</code> and <code>nonexistent=\"NaT\"</code>.</p> <ul> <li><code>bodo.pandas.BodoSeries.dt.tz_localize</code></li> </ul>"},{"location":"api_docs/dataframe_lib/series/#function-application","title":"Function application","text":"<ul> <li><code>bodo.pandas.BodoSeries.agg</code></li> <li><code>bodo.pandas.BodoSeries.aggregate</code></li> <li><code>bodo.pandas.BodoSeries.map</code></li> <li><code>bodo.pandas.BodoSeries.map_partitions</code></li> </ul>"},{"location":"api_docs/dataframe_lib/series/#missing-data-handling","title":"Missing data handling","text":"<ul> <li><code>bodo.pandas.BodoSeries.isna</code></li> <li><code>bodo.pandas.BodoSeries.isnull</code></li> <li><code>bodo.pandas.BodoSeries.notna</code></li> <li><code>bodo.pandas.BodoSeries.notnull</code></li> <li><code>bodo.pandas.BodoSeries.replace</code></li> </ul>"},{"location":"api_docs/dataframe_lib/series/#reindexing-selection-label-manipulation","title":"Reindexing / Selection / Label manipulation","text":"<ul> <li><code>bodo.pandas.BodoSeries.head</code></li> <li><code>bodo.pandas.BodoSeries.isin</code></li> <li><code>bodo.pandas.BodoSeries.reset_index</code></li> </ul>"},{"location":"api_docs/dataframe_lib/series/#reshaping-sorting","title":"Reshaping, sorting","text":"<ul> <li><code>bodo.pandas.BodoSeries.sort_values</code></li> </ul>"},{"location":"api_docs/dataframe_lib/series/#string-handling","title":"String handling","text":"<ul> <li><code>bodo.pandas.BodoSeries.str.capitalize</code></li> <li><code>bodo.pandas.BodoSeries.str.casefold</code></li> </ul> <p>Note</p> <p><code>cat</code> falls back to Pandas when the others parameter is not specified (i.e., <code>others=None</code>).</p> <ul> <li><code>bodo.pandas.BodoSeries.str.cat</code></li> <li><code>bodo.pandas.BodoSeries.str.center</code></li> <li><code>bodo.pandas.BodoSeries.str.contains</code></li> <li><code>bodo.pandas.BodoSeries.str.count</code></li> <li><code>bodo.pandas.BodoSeries.str.decode</code></li> <li><code>bodo.pandas.BodoSeries.str.encode</code></li> <li><code>bodo.pandas.BodoSeries.str.endswith</code></li> <li><code>bodo.pandas.BodoSeries.str.extract</code></li> <li><code>bodo.pandas.BodoSeries.str.find</code></li> <li><code>bodo.pandas.BodoSeries.str.findall</code></li> <li><code>bodo.pandas.BodoSeries.str.fullmatch</code></li> <li><code>bodo.pandas.BodoSeries.str.get</code></li> <li><code>bodo.pandas.BodoSeries.str.index</code></li> <li><code>bodo.pandas.BodoSeries.str.isalnum</code></li> <li><code>bodo.pandas.BodoSeries.str.isalpha</code></li> <li><code>bodo.pandas.BodoSeries.str.isdecimal</code></li> <li><code>bodo.pandas.BodoSeries.str.isdigit</code></li> <li><code>bodo.pandas.BodoSeries.str.islower</code></li> <li><code>bodo.pandas.BodoSeries.str.isnumeric</code></li> <li><code>bodo.pandas.BodoSeries.str.isspace</code></li> <li><code>bodo.pandas.BodoSeries.str.istitle</code></li> <li><code>bodo.pandas.BodoSeries.str.isupper</code></li> <li><code>bodo.pandas.BodoSeries.str.join</code></li> <li><code>bodo.pandas.BodoSeries.str.len</code></li> <li><code>bodo.pandas.BodoSeries.str.ljust</code></li> <li><code>bodo.pandas.BodoSeries.str.lower</code></li> <li><code>bodo.pandas.BodoSeries.str.lstrip</code></li> <li><code>bodo.pandas.BodoSeries.str.match</code></li> <li><code>bodo.pandas.BodoSeries.str.normalize</code></li> <li><code>bodo.pandas.BodoSeries.str.pad</code></li> <li><code>bodo.pandas.BodoSeries.str.partition</code></li> <li><code>bodo.pandas.BodoSeries.str.removeprefix</code></li> <li><code>bodo.pandas.BodoSeries.str.removesuffix</code></li> <li><code>bodo.pandas.BodoSeries.str.repeat</code></li> <li><code>bodo.pandas.BodoSeries.str.replace</code></li> <li><code>bodo.pandas.BodoSeries.str.rfind</code></li> <li><code>bodo.pandas.BodoSeries.str.rindex</code></li> <li><code>bodo.pandas.BodoSeries.str.rjust</code></li> <li><code>bodo.pandas.BodoSeries.str.rpartition</code></li> <li><code>bodo.pandas.BodoSeries.str.rsplit</code></li> <li><code>bodo.pandas.BodoSeries.str.rstrip</code></li> <li><code>bodo.pandas.BodoSeries.str.slice</code></li> <li><code>bodo.pandas.BodoSeries.str.slice_replace</code></li> <li><code>bodo.pandas.BodoSeries.str.split</code></li> <li><code>bodo.pandas.BodoSeries.str.startswith</code></li> <li><code>bodo.pandas.BodoSeries.str.strip</code></li> <li><code>bodo.pandas.BodoSeries.str.swapcase</code></li> <li><code>bodo.pandas.BodoSeries.str.title</code></li> <li><code>bodo.pandas.BodoSeries.str.translate</code></li> <li><code>bodo.pandas.BodoSeries.str.upper</code></li> <li><code>bodo.pandas.BodoSeries.str.wrap</code></li> <li><code>bodo.pandas.BodoSeries.str.zfill</code></li> </ul>"},{"location":"api_docs/dataframe_lib/series/head/","title":"bodo.pandas.BodoSeries.head","text":"<pre><code>BodoSeries.head(n=5) -&gt; BodoSeries\n</code></pre> <p>Returns the first n rows of the BodoSeries.</p> <p>Parameters</p> <p>n : int, default 5: Number of elements to select.</p> <p>Returns</p> <p>BodoSeries</p> <p>Example</p> <pre><code>import bodo.pandas as bd\n\nbdf = bd.DataFrame(\n        {\n            \"A\": bd.array([1, 2, 3, 7] * 3, \"Int64\"),\n        }\n    )\n\nbodo_ser_head = bdf.A.head(3)\nprint(type(bodo_ser_head))\nprint(bodo_ser_head)\n</code></pre> <p>Output:</p> <pre><code>&lt;class 'pandas.core.series.Series'&gt;\n0    1\n1    2\n2    3\nName: A, dtype: Int64\n</code></pre>"},{"location":"api_docs/dataframe_lib/series/map/","title":"bodo.pandas.BodoSeries.map","text":"<p><pre><code>BodoSeries.map(arg, na_action=None, engine=\"bodo\") -&gt; BodoSeries\n</code></pre> Map values of a BodoSeries according to a mapping.</p> <p>If <code>arg</code> is a function, bodo.jit will be applied to <code>arg</code>.  If this JIT compilation fails for any reason, the mapping function will be run as a normal Python function.  If the compilation succeeds, the JIT compiled function will be used for the map and the overheads associated with running Python code from within the execution pipeline are avoided.</p> <p>Note</p> <p>Calling <code>BodoSeries.map</code> will immediately execute a plan if this JIT compilation fails to generate a small sample of the BodoSeries and then call <code>pandas.Series.map</code> on the sample to infer output types before proceeding with lazy evaluation.</p> <p>Note</p> <p>Functions passed to <code>arg</code> (whether explicitly wrapper with a JIT decorator or not) may not use Numba's <code>with objmode</code> context.  Doing so will result in a runtime exception.</p> <p>Parameters</p> <p>arg : function, collections.abc.Mapping subclass or Series: Mapping correspondence.  function may be a Python function or a dispatcher generated through numba.jit or bodo.jit.</p> <p>na_actions : {None, \u2018ignore\u2019}, default None: If 'ignore' then NaN values will be propagated without passing them to the mapping correspondence.</p> <p>engine : {'bodo', 'python'}, default 'bodo':  The engine to use to compute the UDF. By default, engine='bodo' will apply bodo.jit to <code>arg</code> with fallback to Python as described above. Use engine='python' to avoid any jit compilation.</p> <p>Returns</p> <p>BodoSeries</p> <p>Example</p> <pre><code>import bodo.pandas as bd\n\nbdf = bd.DataFrame(\n    {\n        \"A\": bd.array([1, 2, 3, 7] * 3, \"Int64\"),\n        \"B\": [\"A1\", \"B1\", \"C1\", \"Abc\"] * 3,\n        \"C\": bd.array([4, 5, 6, -1] * 3, \"Int64\"),\n    }\n)\n\nbodo_ser = bdf.A.map(lambda x: x ** 2)\nprint(type(bodo_ser))\nprint(bodo_ser)\n</code></pre> <p>Output: <pre><code>&lt;class 'bodo.pandas.series.BodoSeries'&gt;\n0      1\n1      4\n2      9\n3     49\n4      1\n5      4\n6      9\n7     49\n8      1\n9      4\n10     9\n11    49\nName: A, dtype: int64[pyarrow]\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/series/map_partitions/","title":"bodo.pandas.BodoSeries.map_partitions","text":"<pre><code>BodoSeries.map_partitions(func, *args, **kwargs) -&gt; BodoSeries | BodoDataFrame\n</code></pre> <p>Apply a function to groups of rows in a Series and return a Series or Series of the same size.</p> <p>If the input Series is lazy (i.e. its plan has not been evaluated yet) and func returns a Series, then the output will be lazy as well. When the lazy output is evaluated, func will take batches of rows from the input Series. In the cases where func returns a Series or the input Series is not lazy, each worker will call func on their entire local chunk of the input Series.</p> <p>Parameters</p> <p>func : Callable: A function that takes in a Series and returns a Series or DataFrame (with the same number of rows). Currently, functions that return a DataFrame will trigger execution even if the input Series has a lazy plan.</p> <p>*args: Additional positional arguments to pass to func.</p> <p>**kwargs: Additional keyword arguments to pass as keyword arguments to func.</p> <p>Returns</p> <p>BodoSeries or BodoDataFrame:  The result of applying func to the BodoSeries.</p> <p>Example</p> <pre><code>import bodo.pandas as bd\n\nbs = bd.Series(\n    range(15)\n   )\n\nbs_mapped = bs.map_partitions(lambda ser: ser + 15)\nprint(bs_mapped)\n</code></pre> <p>Output: <pre><code>0     15\n1     17\n2     19\n3     21\n4     23\n5     25\n6     27\n7     29\n8     31\n9     33\n10    35\n11    37\n12    39\n13    41\n14    43\ndtype: int64[pyarrow]\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/series/map_with_state/","title":"bodo.pandas.BodoSeries.map_with_state","text":"<p><pre><code>BodoSeries.map_with_state(init_state_fn, row_fn, na_action=None, output_type=None) -&gt; BodoSeries\n</code></pre> Map values of a BodoSeries according to a mapping with a one-time initialization routine whose result is passed to the row mapping function.</p> <p>Sometimes, there is initialization code that is so expensive to run that one would like to minimize overheads by running it just once per worker.  Other APIs such as map or map_partitions are not suitable for this purpose because those would require per row initialization or per partition initialization.</p> <p>Note</p> <p>Calling <code>BodoSeries.map_with_state</code> will immediately execute a plan and will perform an initialization of the state followed by running row_fn on a small number of rows in order to determine the output type of the series.  This plan execution and initialization can be avoided if the output_type is manually specified.</p> <p>Parameters</p> <p>init_state_fn : function: Initialization function.  Run only once per worker.</p> <p>row_fn : function: Mapping correspondence.  The first argument to row_fn when called is the previously initialized state variable.</p> <p>na_actions : {None, \u2018ignore\u2019}, default None: If 'ignore' then NaN values will be propagated without passing them to the mapping correspondence.</p> <p>output_type: {None, Pandas.series}, default None: If none, then plan is executed and sample of rows passed to row_fn after calling init_state_fn to determine the output type.  This parameter can be a Pandas series with the output dtype set in which case the plan, row_fn, and init_state_fn are not immediately executed.</p> <p>Returns</p> <p>BodoSeries</p> <p>Example</p> <pre><code>import bodo.pandas as pd\n\ndef init_state():\n    return {1:7}\n\ndef per_row(state, row):\n    return \"bodo\" + str(row + state[1])\n\na = pd.Series(list(range(20)))\nb = a.map_with_state(init_state, per_row, output_type=pd.Series(dtype=\"string[pyarrow]\"))\nprint(b)\n</code></pre> <p>Output: <pre><code>0      bodo7\n1      bodo8\n2      bodo9\n3     bodo10\n4     bodo11\n5     bodo12\n6     bodo13\n7     bodo14\n8     bodo15\n9     bodo16\n10    bodo17\n11    bodo18\n12    bodo19\n13    bodo20\n14    bodo21\n15    bodo22\n16    bodo23\n17    bodo24\n18    bodo25\n19    bodo26\ndtype: large_string[pyarrow]\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/series/sort_values/","title":"bodo.pandas.BodoSeries.sort_values","text":"<p><pre><code>BodoSeries.sort_values(\n        self,\n        *,\n        axis: Axis = 0,\n        ascending: bool = True,\n        inplace: bool = False,\n        kind: SortKind = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n        key: ValueKeyFunc | None = None,\n    ) -&gt; BodoSeries\n</code></pre> Sorts the elements of the BodoSeries and returns a new sorted BodoSeries.</p> <p>Parameters</p> <p>ascending : bool, default True: If True, sort values in ascending order, otherwise descending.</p> <p>na_position: str, default 'last': Argument \u2018first\u2019 puts NaNs at the beginning, \u2018last\u2019 puts NaNs at the end.</p> <p>All other parameters will trigger a fallback to <code>pandas.Series.sort_values</code> if a non-default value is provided.</p> <p>Returns</p> <p>BodoSeries</p> <p>Example</p> <pre><code>import bodo.pandas as bd\n\nbdf = bd.DataFrame(\n    {\n        \"A\": bd.array([1, 2, 3, 7] * 3, \"Int64\"),\n        \"B\": [\"A1\", \"B1\", \"C1\", \"Abc\"] * 3,\n        \"C\": bd.array([4, 5, 6, -1] * 3, \"Int64\"),\n    }\n)\n\nsa = bdf[\"A\"]\nsa_sorted = sa.sort_values(ascending=False)\nprint(sa_sorted)\n</code></pre> <p>Output: <pre><code>0     7\n1     7\n2     7\n3     3\n4     3\n5     3\n6     2\n7     2\n8     2\n9     1\n10    1\n11    1\nName: A, dtype: int64[pyarrow]\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/series/ai/embed/","title":"bodo.pandas.BodoSeries.ai.embed","text":"<p><pre><code>BodoSeries.ai.embed(\n        api_key: str,\n        model: str | None = None,\n        base_url: str | None = None,\n        **embed_kwargs) -&gt; BodoSeries\n</code></pre> Embed a series of strings using an LLM endpoint.</p> <p>Parameters</p> <ul> <li>api_key: str: The API key for authentication with the LLM endpoint</li> <li>model: str | None: The model to use for embedding. If None is provided, the default model from the endpoint will be used.</li> <li>base_url: str: The URL of the OpenAI compatible LLM endpoint.</li> <li>**embed_kwargs: dict: Additional keyword arguments for the LLM embedding API.</li> </ul> <p>Returns</p> <ul> <li>BodoSeries: A series containing the embedded vectors as lists of doubles.</li> </ul> <p>Example</p> <pre><code>import bodo.pandas as pd\n\n# Example series\na = pd.Series([\"bodo.ai will improve your workflows.\", \"This is a professional sentence.\"])\n# Define the LLM base_url and API key\nbase_url = \"https://api.example.com/v1\"\napi_key = \"your_api_key_here\"\n# Embed the series using the LLM\nb = a.ai.embed(api_key=api_key, model=\"text-embedding-3-small\", base_url=base_url)\nprint(b)\n</code></pre> <p>Output: <pre><code>0    [0.123, 0.456, 0.789, ...]\n1    [0.234, 0.567, 0.890, ...]\ndtype: list&lt;item: float64&gt;[pyarrow]\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/series/ai/llm_generate/","title":"bodo.pandas.BodoSeries.ai.llm_generate","text":"<pre><code>BodoSeries.ai.llm_generate(\n        api_key: str,\n        model: str | None = None,\n        base_url: str | None = None,\n        **generation_kwargs) -&gt; BodoSeries\n</code></pre> <p>Each element in the series is passed to the LLM endpoint for generation.</p> <p>Parameters</p> <ul> <li>api_key: str: The API key for authentication with the LLM endpoint</li> <li>model: str | None: The model to use for embedding. If None is provided, the default model from the endpoint will be used.</li> <li>base_url: str: The URL of the OpenAI compatible LLM endpoint.</li> <li>**generation_kwargs: dict: Additional keyword arguments for the LLM generation API.</li> </ul> <p>Returns</p> <ul> <li>BodoSeries: A series containing the generated text from the LLM.</li> </ul> <p>Example</p> <pre><code>import bodo.pandas as pd\n\n# Example series\na = pd.Series([\"What is the capital of France?\", \"Who wrote 'To Kill a Mockingbird'?\", \"What is the largest mammal?\"])\n# Define the LLM base_url and API key\nbase_url = \"https://api.example.com/v1\"\napi_key = \"your_api_key_here\"\n# Generate responses using the LLM\nb = a.ai.llm_generate(api_key=api_key, model=\"gpt-3.5-turbo\", base_url=base_url, max_tokens=50)\nprint(b)\n</code></pre> <p>Output: <pre><code>0    The capital of France is Paris.\n1    'To Kill a Mockingbird' was written by Harper Lee.\n2    The largest mammal is the blue whale.\ndtype: string[pyarrow]\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/series/ai/query_s3_vectors/","title":"bodo.pandas.BodoSeries.ai.query_s3_vectors","text":"<p><pre><code>BodoSeries.query_s3_vectors(\n        self,\n        vector_bucket_name: str,\n        index_name: str,\n        topk: int,\n        region: str = None,\n        filter: dict = None,\n        return_distance: bool = False,\n        return_metadata: bool = False,\n    ) -&gt; BodoDataFrame\n</code></pre> Query S3 vector index and return data of matching vectors in vector index. See S3 documentation for more details.</p> <p>Parameters</p> <p>vector_bucket_name: str: S3 Vectors bucket name to use.</p> <p>index_name : str: S3 Vectors index name to use.</p> <p>topk : int: Number of results to return.</p> <p>region : str, optional: Region of S3 Vector bucket.</p> <p>filter : dict, optional: Metadata filter to apply during the query.</p> <p>return_distance : bool, optional: Whether to include the computed distance in the response.</p> <p>return_metadata : bool, optional: Whether to include the metadata in the response.</p> <p>Returns</p> <p>BodoDataFrame</p> <p>Example</p> <pre><code>import pandas as pd\nimport bodo.pandas as bd\nimport boto3 \nimport json \n\nbedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-east-2\")\n\ninput_text = \"adventures in space\"\n\nresponse = bedrock.invoke_model(\n    modelId=\"amazon.titan-embed-text-v2:0\",\n    body=json.dumps({\"inputText\": input_text})\n)\n\nmodel_response = json.loads(response[\"body\"].read())\nembedding = model_response[\"embedding\"]\n\ndf = pd.DataFrame({\"data\": [embedding]*10})\nbdf = bd.from_pandas(df)\n\nout = bdf.data.ai.query_s3_vectors(\n    vector_bucket_name=\"my-test-vector\",\n    index_name=\"my-test-ind\",\n    region=\"us-east-2\",\n    topk=3,\n    filter={\"genre\": \"scifi\"},\n    return_distance=True,\n    return_metadata=True,\n)\nprint(out)\n</code></pre> <p>Output: <pre><code>                            keys              distances                                           metadata\n0  ['Star Wars' 'Jurassic Park']  [0.7918925 0.8599859]  [\"{'source_text': 'Star Wars: A farm boy joins...\n1  ['Star Wars' 'Jurassic Park']  [0.7918925 0.8599859]  [\"{'genre': 'scifi', 'source_text': 'Star Wars...\n2  ['Star Wars' 'Jurassic Park']  [0.7918925 0.8599859]  [\"{'genre': 'scifi', 'source_text': 'Star Wars...\n3  ['Star Wars' 'Jurassic Park']  [0.7918925 0.8599859]  [\"{'source_text': 'Star Wars: A farm boy joins...\n4  ['Star Wars' 'Jurassic Park']  [0.7918925 0.8599859]  [\"{'source_text': 'Star Wars: A farm boy joins...\n5  ['Star Wars' 'Jurassic Park']  [0.7918925 0.8599859]  [\"{'genre': 'scifi', 'source_text': 'Star Wars...\n6  ['Star Wars' 'Jurassic Park']  [0.7918925 0.8599859]  [\"{'genre': 'scifi', 'source_text': 'Star Wars...\n7  ['Star Wars' 'Jurassic Park']  [0.7918925 0.8599859]  [\"{'source_text': 'Star Wars: A farm boy joins...\n8  ['Star Wars' 'Jurassic Park']  [0.7918925 0.8599859]  [\"{'genre': 'scifi', 'source_text': 'Star Wars...\n9  ['Star Wars' 'Jurassic Park']  [0.7918925 0.8599859]  [\"{'genre': 'scifi', 'source_text': 'Star Wars...\n</code></pre></p>"},{"location":"api_docs/dataframe_lib/series/ai/tokenize/","title":"bodo.pandas.BodoSeries.ai.tokenize","text":"<p><pre><code>BodoSeries.ai.tokenize(tokenizer) -&gt; BodoSeries\n</code></pre> Tokenize a series of string dtype into a series of lists of int64.</p> <p>Parameters</p> <p>tokenizer: function: A function returning a Transformers.PreTrainedTokenizer.</p> <p>Returns</p> <p>BodoSeries</p> <p>Example</p> <pre><code>import bodo.pandas as pd\nfrom transformers import AutoTokenizer\n\na = pd.Series([\"bodo.ai will improve your workflows.\", \"This is a professional sentence.\", \"I am the third entry in this series.\", \"May the fourth be with you.\"])\ndef ret_tokenizer():\n    # Load a pretrained tokenizer (e.g., BERT)\n    return AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nb = a.ai.tokenize(ret_tokenizer)\nprint(b)\n</code></pre> <p>Output: <pre><code>0    [  101 28137  1012  9932  2097  5335  2115  21...\n1            [ 101 2023 2003 1037 2658 6251 1012  102]\n2    [ 101 1045 2572 1996 2353 4443 1999 2023 2186 ...\n3       [ 101 2089 1996 2959 2022 2007 2017 1012  102]\ndtype: list&lt;item: int64&gt;[pyarrow]\n</code></pre></p>"},{"location":"api_docs/ml/","title":"Machine Learning","text":"<p>Bodo natively supports use of scikit-learn and XGBoost libraries with large-scale distributed data inside <code>bodo.jit</code> decorated functions.</p> <ul> <li>Scikit-Learn</li> <li>XGBoost</li> </ul>"},{"location":"api_docs/ml/xgboost/","title":"XGBoost","text":"<p>This page lists the XGBoost (using the Scikit-Learn-like API) classes and functions that Bodo supports natively inside JIT functions.</p>"},{"location":"api_docs/ml/xgboost/#installing-xgboost","title":"Installing XGBoost","text":"<p>You will need to build XGBoost with MPI support from source. XGBoost version must be <code>&lt;= 1.5.1</code>. Refer to XGBoost instructions about building requirements for more details. Then, build XGBoost with MPI support from source and install it in your Bodo environment as follows:</p> <pre><code>git clone --recursive https://github.com/dmlc/xgboost --branch v1.5.1\ncd xgboost\nmkdir build\ncd build\ncmake -DRABIT_BUILD_MPI=ON ..\nmake -j4\ncd ../python-package\npython setup.py install\n</code></pre>"},{"location":"api_docs/ml/xgboost/#xgboostxgbclassifier","title":"<code>xgboost.XGBClassifier</code>","text":"<p>This class provides implementation of the scikit-learn API for XGBoost classification with distributed large-scale learning.</p>"},{"location":"api_docs/ml/xgboost/#methods","title":"Methods","text":""},{"location":"api_docs/ml/xgboost/#xgboostxgbclassifierfit","title":"<code>xgboost.XGBClassifier.fit</code>","text":"<ul> <li> <p><code>xgboost.XGBClassifier.fit(X, y, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)</code></p> <p>Supported Arguments</p> argument datatypes <code>X</code> NumPy Array or Pandas Dataframes <code>y</code> NumPy Array or Pandas Dataframes </li> </ul>"},{"location":"api_docs/ml/xgboost/#xgboostxgbclassifierpredict","title":"<code>xgboost.XGBClassifier.predict</code>","text":"<ul> <li> <p><code>xgboost.XGBClassifier.predict(X, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)</code></p> <p>Supported Arguments</p> argument datatypes <code>X</code> NumPy Array or Pandas Dataframes </li> </ul>"},{"location":"api_docs/ml/xgboost/#xgboostxgbclassifierpredict_proba","title":"<code>xgboost.XGBClassifier.predict_proba</code>","text":"<ul> <li> <p><code>xgboost.XGBClassifier.predict_proba(X, ntree_limit=None, validate_features=True, base_margin=None)</code></p> <p>Supported Arguments</p> argument datatypes <code>X</code> NumPy Array or Pandas Dataframes </li> </ul>"},{"location":"api_docs/ml/xgboost/#attributes","title":"Attributes","text":""},{"location":"api_docs/ml/xgboost/#xgboostxgbclassifierfeature_importances_","title":"<code>xgboost.XGBClassifier.feature_importances_</code>","text":"<ul> <li><code>xgboost.XGBClassifier.feature_importances_</code></li> </ul>"},{"location":"api_docs/ml/xgboost/#example-usage","title":"Example Usage:","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import xgboost as xgb\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_xgbc():\n...   X = np.random.rand(5, 10)\n...   y = np.random.randint(0, 2, 5)\n...   clf = xgb.XGBClassifier(\n...   booster=\"gbtree\",\n...   random_state=0,\n...   tree_method=\"hist\",\n...   )\n...   clf.fit(X, y)\n...   print(clf.predict([[1, 2, 3, 4, 5, 6]]))\n...   print(clf.feature_importances_)\n...\n&gt;&gt;&gt; test_xgbc(X, y)\n[1]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n</code></pre>"},{"location":"api_docs/ml/xgboost/#xgboostxgbregressor","title":"<code>xgboost.XGBRegressor</code>","text":"<p>This class provides implementation of the scikit-learn API for XGBoost regression with distributed large-scale learning.</p>"},{"location":"api_docs/ml/xgboost/#methods_1","title":"Methods","text":""},{"location":"api_docs/ml/xgboost/#xgboostxgbregressorfit","title":"<code>xgboost.XGBRegressor.fit</code>","text":"<ul> <li><code>xgboost.XGBRegressor.fit(X, y, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)</code></li> </ul> <p>Supported Arguments</p> <pre><code>| argument                    | datatypes                               |\n|-----------------------------|-----------------------------------------|\n|``X``                        | NumPy Array                             |\n|``y``                        | NumPy Array                             |\n</code></pre>"},{"location":"api_docs/ml/xgboost/#xgboostxgbregressorpredict","title":"<code>xgboost.XGBRegressor.predict</code>","text":"<ul> <li><code>xgboost.XGBRegressor.predict(X, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)</code></li> </ul> <p>Supported Arguments</p> <pre><code>| argument                    | datatypes                               |\n|-----------------------------|-----------------------------------------|\n|``X``                        | NumPy Array                             |\n</code></pre>"},{"location":"api_docs/ml/xgboost/#attributes_1","title":"Attributes","text":""},{"location":"api_docs/ml/xgboost/#xgboostxgbregressorfeature_importances_","title":"<code>xgboost.XGBRegressor.feature_importances_</code>","text":"<ul> <li><code>xgboost.XGBRegressor.feature_importances_</code></li> </ul>"},{"location":"api_docs/ml/xgboost/#example-usage_1","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import xgboost as xgb\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_xgbc():\n...   X = np.random.rand(5, 10)\n...   y = np.random.rand(5)\n...   clf = xgb.XGBRegressor()\n...   clf.fit(X, y)\n...   print(clf.predict([[1, 2, 3, 4, 5, 6]]))\n...   print(clf.feature_importances_)\n...\n&gt;&gt;&gt; test_xgbc(X, y)\n[0.84368145]\n[5.7460850e-01 1.2052832e-04 0.0000000e+00 4.2441860e-01 1.5441242e-04\n 6.9795933e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n</code></pre>"},{"location":"api_docs/ml/sklearn/","title":"Scikit-learn","text":"<p>Bodo supports <code>scikit-learn</code> versions <code>1.4.*</code>. To install scikit-learn in your Bodo environment:</p> <p><pre><code>pip install scikit-learn=='1.4.*'\n</code></pre> or <pre><code>conda install scikit-learn='1.4.*' -c conda-forge\n</code></pre></p> <ul> <li>sklearn.cluster</li> <li>sklearn.ensemble</li> <li>sklearn.feature_extraction</li> <li>sklearn.linear_model</li> <li>sklearn.metrics</li> <li>sklearn.model_selection</li> <li>sklearn.naive_bayes</li> <li>sklearn.preprocessing</li> <li>sklearn.svm</li> </ul>"},{"location":"api_docs/ml/sklearn/cluster/","title":"sklearn.cluster: Clustering","text":""},{"location":"api_docs/ml/sklearn/cluster/#sklearnclusterkmeans","title":"sklearn.cluster.KMeans","text":"<p><code>sklearn.cluster.KMeans</code> This class provides K-Means clustering model.</p> <p>Important</p> <p>Currently, this model works by gathering all the data in a single node and  then generating K-Means model. Make sure you have enough memory on the first  node in your hostfile.</p>"},{"location":"api_docs/ml/sklearn/cluster/#methods","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/cluster/#sklearnclusterkmeansfit","title":"sklearn.cluster.KMeans.fit","text":"<ul> <li> <p><code>sklearn.cluster.KMeans.fit(X, y=None, sample_weight=None)</code></p> <p>Supported Arguments  * <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix. * <code>sample_weight</code>: Numeric NumPy Array</p> <p>Note</p> <p>Bodo ignores <code>y</code>, which is consistent with scikit-learn.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/cluster/#sklearnclusterkmeanspredict","title":"sklearn.cluster.KMeans.predict","text":"<ul> <li> <p><code>sklearn.cluster.KMeans.predict(X, sample_weight=None)</code></p> <p>Supported Arguments  - <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix. - <code>sample_weight</code>: Numeric NumPy Array</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/cluster/#sklearnclusterkmeansscore","title":"sklearn.cluster.KMeans.score","text":"<ul> <li> <p><code>sklearn.cluster.KMeans.score(X, y=None, sample_weight=None)</code></p> <p>Supported Arguments  - <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix. - <code>sample_weight</code>: Numeric NumPy Array</p> <p>Note</p> <p>Bodo ignores y, which is consistent with scikit-learn.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/cluster/#sklearnclusterkmeanstransform","title":"sklearn.cluster.KMeans.transform","text":"<ul> <li> <p><code>sklearn.cluster.KMeans.transform(X)</code></p> <p>Supported Arguments    - <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/cluster/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.cluster import KMeans\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_kmeans(X):\n...   kmeans = KMeans(n_clusters=2)\n...   kmeans.fit(X)\n...   ans = kmeans.predict([[0, 0], [12, 3]])\n...   print(ans)\n...\n&gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n&gt;&gt;&gt; test_kmeans(X)\n[1 0]\n</code></pre>"},{"location":"api_docs/ml/sklearn/ensemble/","title":"sklearn.ensemble","text":""},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifier","title":"sklearn.ensemble.RandomForestClassifier","text":"<p><code>sklearn.ensemble.RandomForestClassifier</code></p> <p>This class provides Random Forest Classifier, an ensemble learning model, for distributed large-scale learning.</p> <p>Important</p> <p><code>random_state</code> value is ignored when running on a multi-node cluster.</p>"},{"location":"api_docs/ml/sklearn/ensemble/#methods","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifierfit","title":"sklearn.ensemble.RandomForestClassifier.fit","text":"<ul> <li> <p><code>sklearn.ensemble.RandomForestClassifier.fit(X, y, sample_weight=None)</code></p> <p>Supported Arguments   -   <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix. -   <code>y</code>: NumPy Array -   <code>sample_weight</code>: Numeric NumPy Array (only if data is not     distributed)</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifierpredict","title":"sklearn.ensemble.RandomForestClassifier.predict","text":"<ul> <li> <p><code>sklearn.ensemble.RandomForestClassifier.predict(X)</code></p> <p>Supported Arguments   -   <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifierpredict_log_proba","title":"sklearn.ensemble.RandomForestClassifier.predict_log_proba","text":"<ul> <li> <p><code>sklearn.ensemble.RandomForestClassifier.predict_log_proba(X)</code></p> <p>Supported Arguments   -   <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifierpredict_proba","title":"sklearn.ensemble.RandomForestClassifier.predict_proba","text":"<ul> <li> <p><code>sklearn.ensemble.RandomForestClassifier.predict_proba(X)</code></p> <p>Supported Arguments   -   <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestclassifierscore","title":"sklearn.ensemble.RandomForestClassifier.score","text":"<ul> <li> <p><code>sklearn.ensemble.RandomForestClassifier.score(X, y, sample_weight=None)</code></p> <p>Supported Arguments   -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array -   <code>sample_weight</code>: Numeric NumPy Array</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/ensemble/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier\n&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_random_forest_classifier(X, y):\n...   clf = RandomForestClassifier(max_depth=2)\n...   clf.fit(X, y)\n...   ans = clf.predict(np.array([[0, 0, 0, 0]]))\n...   print(ans)\n...\n&gt;&gt;&gt; test_random_forest_classifier(X, y)\n[1]\n</code></pre>"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestregressor","title":"sklearn.ensemble.RandomForestRegressor","text":"<p><code>sklearn.ensemble.RandomForestRegressor</code></p> <p>This class provides Random Forest Regressor, an ensemble learning model, for distributed large-scale learning.</p> <p>Important</p> <p><code>random_state</code> value is ignored when running on a multi-node cluster.</p>"},{"location":"api_docs/ml/sklearn/ensemble/#methods_1","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestregressorfit","title":"sklearn.ensemble.RandomForestRegressor.fit","text":"<ul> <li> <p><code>sklearn.ensemble.RandomForestRegressor.fit(X, y, sample_weight=None)</code></p> <p>Supported Arguments   -   <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix. -   <code>y</code>: NumPy Array -   <code>sample_weight</code>: Numeric NumPy Array (only if data is not     distributed)</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestregressorpredict","title":"sklearn.ensemble.RandomForestRegressor.predict","text":"<ul> <li> <p><code>sklearn.ensemble.RandomForestRegressor.predict(X)</code></p> <p>Supported Arguments   -   <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/ensemble/#sklearnensemblerandomforestregressorscore","title":"sklearn.ensemble.RandomForestRegressor.score","text":"<ul> <li> <p><code>sklearn.ensemble.RandomForestRegressor.score(X, y, sample_weight=None)</code></p> <p>Supported Arguments   -   <code>X</code>: NumPy Array, Pandas Dataframes, or CSR sparse matrix. -   <code>y</code>: NumPy Array -   <code>sample_weight</code>: Numeric NumPy Array</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/ensemble/#example-usage_1","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_features=4, n_informative=2,\n... random_state=0, shuffle=False)\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_random_forest_regressor(X, y):\n...   regr = RandomForestRegressor(max_depth=2)\n...   regr.fit(X, y)\n...   ans = regr.predict(np.array([[0, 0, 0, 0]]))\n...   print(ans)\n...\n&gt;&gt;&gt; test_random_forest_regressor(X, y)\n[-6.7933243]\n</code></pre>"},{"location":"api_docs/ml/sklearn/feature_extraction/","title":"sklearn.feature_extraction","text":""},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extractiontextcountvectorizer","title":"sklearn.feature_extraction.text.CountVectorizer","text":"<p><code>sklearn.feature_extraction.text.CountVectorizer</code></p> <p>This class provides CountVectorizer support to convert a collection of text documents to a matrix of token counts.</p> <p>Note</p> <p>Arguments <code>max_df</code> and <code>min_df</code> are not supported yet.</p>"},{"location":"api_docs/ml/sklearn/feature_extraction/#methods","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extractiontextcountvectorizerfit_transform","title":"sklearn.feature_extraction.text.CountVectorizer.fit_transform","text":"<ul> <li> <p><code>sklearn.feature_extraction.text.CountVectorizer.fit_transform ( raw_documents, y=None )</code></p> <p>Supported Arguments   -   <code>raw_documents</code>: iterables ( list, tuple, or NumPy Array, or Pandas Series that contains string)</p> <p>Note</p> <p>Bodo ignores <code>y</code>, which is consistent with scikit-learn.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extractiontextcountvectorizerget_feature_names_out","title":"sklearn.feature_extraction.text.CountVectorizer.get_feature_names_out","text":"<ul> <li><code>sklearn.feature_extraction.text.CountVectorizer. get_feature_names_out()</code></li> </ul>"},{"location":"api_docs/ml/sklearn/feature_extraction/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer\n&gt;&gt;&gt; corpus = [\n... 'This is the first document.',\n... 'This document is the second document.',\n... 'And this is the third one.',\n... 'Is this the first document?',\n... ]\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_count_vectorizer(corpus):\n&gt;&gt;&gt;   vectorizer = CountVectorizer()\n&gt;&gt;&gt;   X = vectorizer.fit_transform(corpus)\n&gt;&gt;&gt;   print(vectorizer.get_feature_names_out())\n...\n&gt;&gt;&gt; test_count_vectorizer(corpus)\n['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n</code></pre>"},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extractiontexthashingvectorizer","title":"sklearn.feature_extraction.text.HashingVectorizer","text":"<p><code>sklearn.feature_extraction.text.HashingVectorizer</code></p> <p>This class provides <code>HashingVectorizer</code> support to convert a collection of text documents to a matrix of token occurrences.</p>"},{"location":"api_docs/ml/sklearn/feature_extraction/#methods_1","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/feature_extraction/#sklearnfeature_extractiontexthashingvectorizerfit_transform","title":"sklearn.feature_extraction.text.HashingVectorizer.fit_transform","text":"<ul> <li> <p><code>sklearn.feature_extraction.text.HashingVectorizer.fit_transform(X, y=None)</code></p> <p>Supported Arguments   - <code>X</code>: iterables ( list, tuple, or NumPy Array, or Pandas        Series that contains string)</p> <p>Note</p> <p>Bodo ignores <code>y</code>, which is consistent with scikit-learn.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/feature_extraction/#example-usage_1","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.feature_extraction.text import HashingVectorizer\n&gt;&gt;&gt; corpus = [\n... 'This is the first document.',\n... 'This document is the second document.',\n... 'And this is the third one.',\n... 'Is this the first document?',\n... ]\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_hashing_vectorizer(corpus):\n&gt;&gt;&gt;   vectorizer = HashingVectorizer(n_features=2**4)\n&gt;&gt;&gt;   X = vectorizer.fit_transform(corpus)\n&gt;&gt;&gt;   print(X.shape)\n...\n&gt;&gt;&gt; test_hashing_vectorizer(corpus)\n(4, 16)\n</code></pre>"},{"location":"api_docs/ml/sklearn/linear_model/","title":"sklearn.linear_model","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellasso","title":"sklearn.linear_model.Lasso","text":"<p><code>sklearn.linear_model.Lasso</code></p> <p>This class provides Lasso regression support.</p>"},{"location":"api_docs/ml/sklearn/linear_model/#methods","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellassofit","title":"sklearn.linear_model.Lasso.fit","text":"<ul> <li> <p><code>sklearn.linear_model.Lasso.fit(X, y, sample_weight=None, check_input=True)</code></p> <p>Supported Arguments   -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array. -   <code>sample_weight</code>: Numeric NumPy Array (only if data is not     distributed)</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellassopredict","title":"sklearn.linear_model.Lasso.predict","text":"<ul> <li> <p><code>sklearn.linear_model.Lasso.predict(X)</code></p> <p>Supported Arguments   -   <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellassoscore","title":"sklearn.linear_model.Lasso.score","text":"<ul> <li> <p><code>sklearn.linear_model.Lasso.score(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array or Pandas Dataframes. -   <code>sample_weight</code>: Numeric NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.linear_model import Lasso\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(\n... n_samples=10,\n... n_features=10,\n... n_informative=5,\n... )\n&gt;&gt;&gt; @bodo.jit\n... def test_lasso(X, y):\n...   scaler = StandardScaler()\n...   scaler.fit(X)\n...   X = scaler.transform(X)\n...   reg = Lasso(alpha=0.1)\n...   reg.fit(X, y)\n...   ans = reg.predict(X)\n...   print(ans)\n...   print(\"score: \", reg.score(X, y))\n...\n&gt;&gt;&gt; test_lasso(X, y)\n[-108.40717491  -92.14977392  -54.82835898  -52.81762142  291.33173703\n60.60660979  128.64172956   30.42129155  110.20607814   58.05321319]\nscore:  0.9999971902794988\n</code></pre>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellinearregression","title":"sklearn.linear_model.LinearRegression","text":"<p><code>sklearn.linear_model.LinearRegression</code></p> <p>This class provides linear regression support.</p> <p>Note</p> <p>Multilabel targets are not currently supported.</p>"},{"location":"api_docs/ml/sklearn/linear_model/#methods_1","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellinearregressionfit","title":"sklearn.linear_model.LinearRegression.fit","text":"<ul> <li> <p><code>sklearn.linear_model.LinearRegression.fit(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array. -   <code>sample_weight</code>: Numeric NumPy Array (only if data is not     distributed)</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellinearregressionpredict","title":"sklearn.linear_model.LinearRegression.predict","text":"<ul> <li> <p><code>sklearn.linear_model.LinearRegression.predict(X)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellinearregressionscore","title":"sklearn.linear_model.LinearRegression.score","text":"<ul> <li> <p><code>sklearn.linear_model.LinearRegression.score(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array or Pandas Dataframes. -   <code>sample_weight</code>: Numeric NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#attributes","title":"Attributes","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellinearregressioncoef_","title":"sklearn.linear_model.LinearRegression.coef_","text":"<ul> <li><code>sklearn.linear_model.LinearRegression.&lt;apiname&gt;coef\\_&lt;/apiname&gt;</code></li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage_1","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n&gt;&gt;&gt; y = np.dot(X, np.array([1, 2])) + 3\n&gt;&gt;&gt; @bodo.jit\n... def test_linear_reg(X, y):\n...   reg = LinearRegression()\n...   reg.fit(X, y)\n...   print(\"score: \", reg.score(X, y))\n...   print(\"coef_: \", reg.coef_)\n...   ans = reg.predict(np.array([[3, 5]]))\n...   print(ans)\n...\n&gt;&gt;&gt; test_linear_reg(X, y)\nscore:  1.0\ncoef_:  [1. 2.]\n[16.]\n</code></pre>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregression","title":"sklearn.linear_model.LogisticRegression","text":"<p><code>sklearn.linear_model.LogisticRegression</code> This class provides logistic regression classifier.</p> <p>Note</p> <p>Bodo uses Stochastic Gradient Descent (SGD) to train linear models across multiple nodes in a distributed fashion. This produces models that have similar accuracy compared to their corresponding sequential version in most cases. To achieve that, it is highly recommended to scale your data using <code>StandardScaler</code> before training and/or testing the model. See scikit-learn for more tips on how to tune model parameters for SGD here.</p>"},{"location":"api_docs/ml/sklearn/linear_model/#methods_2","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressionfit","title":"sklearn.linear_model.LogisticRegression.fit","text":"<ul> <li> <p><code>sklearn.linear_model.LogisticRegression.fit(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array. -   <code>sample_weight</code>: Numeric NumPy Array (only if data is not      distributed)</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressionpredict","title":"sklearn.linear_model.LogisticRegression.predict","text":"<ul> <li> <p><code>sklearn.linear_model.LogisticRegression.predict(X)</code></p> <p>Supported Arguments  -  <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressionpredict_log_proba","title":"sklearn.linear_model.LogisticRegression.predict_log_proba","text":"<ul> <li> <p><code>sklearn.linear_model.LogisticRegression.predict_log_proba(X)</code></p> <p>Supported Arguments  -  <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressionpredict_proba","title":"sklearn.linear_model.LogisticRegression.predict_proba","text":"<ul> <li> <p><code>sklearn.linear_model.LogisticRegression.predict_proba(X)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressionscore","title":"sklearn.linear_model.LogisticRegression.score","text":"<ul> <li> <p><code>sklearn.linear_model.LogisticRegression.score(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array or Pandas Dataframes. -   <code>sample_weight</code>: Numeric NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#attributes_1","title":"Attributes","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modellogisticregressioncoef_","title":"sklearn.linear_model.LogisticRegression.coef_","text":"<ul> <li><code>sklearn.linear_model.LogisticRegression.&lt;apiname&gt;coef\\_&lt;/apiname&gt;</code></li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage_2","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n&gt;&gt;&gt; X, y = make_classification(\n... n_samples=1000,\n... n_features=10,\n... n_informative=5,\n... n_redundant=0,\n... random_state=0,\n... shuffle=0,\n... n_classes=2,\n... n_clusters_per_class=1\n... )\n&gt;&gt;&gt; @bodo.jit\n... def test_logistic(X, y):\n...   clf = LogisticRegression()\n...   clf.fit(X, y)\n...   ans = clf.predict(X)\n...   print(\"score: \", clf.score(X, y))\n...\n&gt;&gt;&gt; test_logistic(X, y)\nscore:  0.997\n</code></pre>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelridge","title":"sklearn.linear_model.Ridge","text":"<p><code>sklearn.linear_model.Ridge</code></p> <p>This class provides ridge regression support.</p>"},{"location":"api_docs/ml/sklearn/linear_model/#methods_3","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelridgefit","title":"sklearn.linear_model.Ridge.fit","text":"<ul> <li> <p><code>sklearn.linear_model.Ridge.fit(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array. -   <code>sample_weight</code>: Numeric NumPy Array (only if data is not  distributed)</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelridgepredict","title":"sklearn.linear_model.Ridge.predict","text":"<ul> <li> <p><code>sklearn.linear_model.Ridge.predict(X)</code></p> <p>Supported Arguments  -  <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelridgescore","title":"sklearn.linear_model.Ridge.score","text":"<ul> <li> <p><code>sklearn.linear_model.Ridge.score(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array or Pandas Dataframes. -   <code>sample_weight</code>: Numeric NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#attributes_2","title":"Attributes","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelridgecoef_","title":"sklearn.linear_model.Ridge.coef_","text":"<ul> <li><code>sklearn.linear_model.Ridge.&lt;apiname&gt;coef\\_&lt;/apiname&gt;</code></li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage_3","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(\n... n_samples=1000,\n... n_features=10,\n... n_informative=5,\n... )\n&gt;&gt;&gt; @bodo.jit\n... def test_ridge(X, y):\n...   reg = Ridge(alpha=1.0)\n...   reg.fit(X, y)\n...   print(\"score: \", reg.score(X, y))\n...   print(\"coef_: \", reg.coef_)\n...\n&gt;&gt;&gt; test_ridge(X, y)\nscore:  0.999998857191076\ncoef_:  [ 1.07963671e-03  2.35051611e+01  9.46672751e+01  8.01581769e-03\n3.66612234e+01  5.82527987e-03  2.60885671e+01 -3.49454103e-03\n8.39573884e+01 -7.52605483e-03]\n</code></pre>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifier","title":"sklearn.linear_model.SGDClassifier","text":"<p><code>sklearn.linear_model.SGDClassifier</code></p> <p>This class provides linear classification models with SGD optimization which allows distributed large-scale learning.</p> <ul> <li>Supported loss functions <code>hinge</code> and <code>log</code>.</li> <li><code>SGDClassifier(loss='hinge')</code> is equivalent to SVM linear classifer.</li> <li><code>SGDClassifier(loss='log')</code> is equivalent to logistic regression classifer.</li> <li><code>early_stopping</code> is not supported yet.</li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#methods_4","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifierfit","title":"sklearn.linear_model.SGDClassifier.fit","text":"<ul> <li> <p><code>sklearn.linear_model.SGDClassifier.fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array. -   <code>sample_weight</code>: Numeric NumPy Array (only if data is not  distributed)</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifierpredict","title":"sklearn.linear_model.SGDClassifier.predict","text":"<ul> <li> <p><code>sklearn.linear_model.SGDClassifier.predict(X)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifierpredict_log_proba","title":"sklearn.linear_model.SGDClassifier.predict_log_proba","text":"<ul> <li> <p><code>sklearn.linear_model.SGDClassifier.predict_log_proba(X)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifierpredict_proba","title":"sklearn.linear_model.SGDClassifier.predict_proba","text":"<ul> <li> <p><code>sklearn.linear_model.SGDClassifier.predict_proba(X)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifierscore","title":"sklearn.linear_model.SGDClassifier.score","text":"<ul> <li> <p><code>sklearn.linear_model.SGDClassifier.score(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array or Pandas Dataframes. -   <code>sample_weight</code>: Numeric NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#attributes_3","title":"Attributes","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdclassifiercoef_","title":"sklearn.linear_model.SGDClassifier.coef_","text":"<ul> <li><code>sklearn.linear_model.SGDClassifier.&lt;apiname&gt;coef\\_&lt;apiname&gt;</code></li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage_4","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n&gt;&gt;&gt; y = np.array([1, 1, 2, 2])\n&gt;&gt;&gt; @bodo.jit\n... def test_sgdclassifier(X, y):\n...   scaler = StandardScaler()\n...   scaler.fit(X)\n...   X = scaler.transform(X)\n...   clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n...   clf.fit(X, y)\n...   ans = clf.predict(np.array([[-0.8, -1]]))\n...   print(ans)\n...   print(\"coef_: \", clf.coef_)\n...\n&gt;&gt;&gt; test_sgdclassifier(X, y)\n[1]\ncoef_:  [[6.18236102 9.77517107]]\n</code></pre>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdregressor","title":"sklearn.linear_model.SGDRegressor","text":"<p><code>sklearn.linear_model.SGDRegressor</code></p> <p>This class provides linear regression models with SGD optimization which allows distributed large-scale learning.</p> <ul> <li>Supported loss function is <code>squared_error</code>. </li> <li> <p><code>early_stopping</code> is not supported yet.</p> </li> <li> <p><code>SGDRegressor(loss='squared_error', penalty='None')</code> is equivalent to linear regression.</p> </li> <li> <p><code>SGDRegressor(loss='squared_error', penalty='l2')</code> is equivalent to Ridge regression.</p> </li> <li> <p><code>SGDRegressor(loss='squared_error', penalty='l1')</code> is equivalent to Lasso regression.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#methods_5","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdregressorfit","title":"sklearn.linear_model.SGDRegressor.fit","text":"<ul> <li> <p><code>sklearn.linear_model.SGDRegressor.fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array. -   <code>sample_weight</code>: Numeric NumPy Array (only if data is not                      distributed)</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdregressorpredict","title":"sklearn.linear_model.SGDRegressor.predict","text":"<ul> <li><code>sklearn.linear_model.SGDRegressor.predict(X)</code></li> </ul> <p>Supported Arguments</p> <pre><code>-   `X`: NumPy Array or Pandas Dataframes.\n</code></pre>"},{"location":"api_docs/ml/sklearn/linear_model/#sklearnlinear_modelsgdregressorscore","title":"sklearn.linear_model.SGDRegressor.score","text":"<ul> <li> <p><code>sklearn.linear_model.SGDRegressor.score(X, y, sample_weight=None)</code></p> <p>Supported Arguments  - <code>X</code>: NumPy Array or Pandas Dataframes. - <code>y</code>: NumPy Array or Pandas Dataframes. - <code>sample_weight</code>: Numeric NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/linear_model/#example-usage_5","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.linear_model import SGDRegressor\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(\n... n_samples=1000,\n... n_features=10,\n... n_informative=5,\n... )\n&gt;&gt;&gt; @bodo.jit\n... def test_sgd_reg(X, y):\n...   scaler = StandardScaler()\n...   scaler.fit(X)\n...   X = scaler.transform(X)\n...   reg = SGDRegressor()\n...   reg.fit(X, y)\n...   print(\"score: \", reg.score(X, y))\n...\n&gt;&gt;&gt; test_sgd_reg(X, y)\n0.9999999836265652\n</code></pre>"},{"location":"api_docs/ml/sklearn/metrics/","title":"sklearn.metrics","text":""},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsaccuracy_score","title":"sklearn.metrics.accuracy_score","text":"<ul> <li> <p><code>sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>y_true</code>: 1d array-like. -   <code>y_pred</code>: 1d array-like. -   <code>normalize</code>: bool. -   <code>sample_weight</code>: 1d numeric array-like or None.</p> <p>Note</p> <p><code>y_true</code>, <code>y_pred</code>, and <code>sample_weight</code> (if provided) must be of same length.</p> <p>Example Usage</p> <pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.metrics import accuracy_score\n&gt;&gt;&gt; y_pred = np.array([0, 2, 1, 3])\n&gt;&gt;&gt; y_true = np.array([0, 1, 2, 3])\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_accuracy_score(y_true, y_pred):\n...   print(accuracy_score(y_true, y_pred))\n&gt;&gt;&gt; test_accuracy_score(y_true, y_pred)\n0.5\n</code></pre> </li> </ul>"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsconfusion_matrix","title":"sklearn.metrics.confusion_matrix","text":"<ul> <li> <p><code>sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)</code></p> <p>Supported Arguments  -   <code>y_true</code>: 1d array-like. -   <code>y_pred</code>: 1d array-like. -   <code>labels</code>: 1d array-like. -   <code>sample_weight</code>: 1d numeric array-like or <code>None</code>. -   <code>normalize</code>: Must be one of <code>'true'</code>, <code>'pred'</code>, <code>'all'</code>, or <code>None</code></p> <p>Note</p> <p><code>y_true</code>, <code>y_pred</code>, and <code>sample_weight</code> (if provided) must be of same length.</p> <p>Example Usage</p> <pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.metrics import confusion_matrix\n&gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]\n&gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_confusion_matirx(y_true, y_pred):\n...   print(confusion_matrix(y_true, y_pred))\n&gt;&gt;&gt; test_confusion_matrix(y_true, y_pred)\n[[2 0 0]\n[0 0 1]\n[1 0 2]]\n</code></pre> </li> </ul>"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsf1_score","title":"sklearn.metrics.f1_score","text":"<ul> <li> <p><code>sklearn.metrics.f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')</code></p> <p>Supported Arguments  -   <code>y_true</code>: 1d array-like. -   <code>y_pred</code>: 1d array-like. -   <code>average</code>: Must be one of <code>'micro'</code>, <code>'macro'</code>, <code>'samples'</code>,     <code>'weighted'</code>, <code>'binary'</code>, or None.</p> <p>Note</p> <p><code>y_true</code> and <code>y_pred</code> must be of same length.</p> <p>Example Usage</p> <pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.metrics import f1_score\n&gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]\n&gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_f1_score(y_true, y_pred):\n...   print(f1_score(y_true, y_pred, average='macro'))\n&gt;&gt;&gt; test_f1_score(y_true, y_pred)\n0.26666666666666666\n</code></pre> </li> </ul>"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsmean_absolute_error","title":"sklearn.metrics.mean_absolute_error","text":"<ul> <li> <p><code>sklearn.metrics.mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')</code></p> <p>Supported Arguments  -   <code>y_true</code>: NumPy array. -   <code>y_pred</code>: NumPy array. -   <code>sample_weight</code>: Numeric NumPy array or None. -   <code>multioutput</code>: Must be one of <code>'raw_values'</code>,     <code>'uniform_average'</code>, or array-like.</p> <p>Note</p> <p><code>y_true</code>, <code>y_pred</code>, and <code>sample_weight</code> (if provided) must be of same length.</p> <p>Example Usage <pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.metrics import mean_absolute_error\n&gt;&gt;&gt; y_true = np.array([[0.5, 1], [-1, 1], [7, -6]])\n&gt;&gt;&gt; y_pred = np.array([[0, 2], [-1, 2], [8, -5]])\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_mean_absolute_error(y_true, y_pred):\n...   print(mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7]))\n&gt;&gt;&gt; test_mean_absolute_error(y_true, y_pred)\n0.85\n</code></pre></p> </li> </ul>"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsmean_squared_error","title":"sklearn.metrics.mean_squared_error","text":"<ul> <li> <p><code>sklearn.metrics.mean_squared_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average', squared=True)</code></p> <p>Supported Arguments  -   <code>y_true</code>: NumPy array. -   <code>y_pred</code>: NumPy array. -   <code>sample_weight</code>: Numeric NumPy array or None. -   <code>multioutput</code>: Must be one of <code>'raw_values'</code>,     <code>'uniform_average'</code>, or array-like.</p> <p>Note</p> <p><code>y_true</code>, <code>y_pred</code>, and <code>sample_weight</code> (if provided) must be of same length.</p> <p>Example Usage</p> <pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n&gt;&gt;&gt; y_true = np.array([[0.5, 1], [-1, 1], [7, -6]])\n&gt;&gt;&gt; y_pred = np.array([[0, 2], [-1, 2], [8, -5]])\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_mean_squared_error(y_true, y_pred):\n...   print(mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7]))\n&gt;&gt;&gt; test_mean_squared_error(y_true, y_pred)\n0.825\n</code></pre> </li> </ul>"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsprecision_score","title":"sklearn.metrics.precision_score","text":"<ul> <li> <p><code>sklearn.metrics.precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')</code></p> <p>Supported Arguments  -   <code>y_true</code>: 1d array-like. -   <code>y_pred</code>: 1d array-like. -   <code>average</code>: Must be one of <code>'micro'</code>, <code>'macro'</code>, <code>'samples'</code>,     <code>'weighted'</code>, <code>'binary'</code>, or <code>None</code>.</p> <p>Note</p> <p><code>y_true</code> and <code>y_pred</code> must be of same length.</p> <p>Example Usage</p> <pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.metrics import precision_score\n&gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]\n&gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_precision_score(y_true, y_pred):\n...   print(precision_score(y_true, y_pred, average='macro'))\n&gt;&gt;&gt; test_precision_score(y_true, y_pred)\n0.2222222222222222\n</code></pre> </li> </ul>"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsr2_score","title":"sklearn.metrics.r2_score","text":"<ul> <li> <p><code>sklearn.metrics.r2_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')</code></p> <p>Supported Arguments  -   <code>y_true</code>: NumPy array. -   <code>y_pred</code>: NumPy array. -   <code>sample_weight</code>: Numeric NumPy array or <code>None</code>. -   <code>multioutput</code>: Must be one of <code>'raw_values'</code>,     <code>'uniform_average'</code>, <code>'variance_weighted'</code>, <code>None</code>, or      array-like.</p> <p>Note</p> <p><code>y_true</code>, <code>y_pred</code>, and <code>sample_weight</code> (if provided) must be of same length.</p> <p>Example Usage</p> <pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.metrics import r2_score\n&gt;&gt;&gt; y_true = np.array([[0.5, 1], [-1, 1], [7, -6]])\n&gt;&gt;&gt; y_pred = np.array([[0, 2], [-1, 2], [8, -5]])\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_r2_score(y_true, y_pred):\n...   print(r2_score(y_true, y_pred, multioutput=[0.3, 0.7]))\n&gt;&gt;&gt; test_r2_score(y_true, y_pred)\n0.9253456221198156\n</code></pre> </li> </ul>"},{"location":"api_docs/ml/sklearn/metrics/#sklearnmetricsrecall_score","title":"sklearn.metrics.recall_score","text":"<ul> <li> <p><code>sklearn.metrics.recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')</code></p> <p>Supported Arguments  -   <code>y_true</code>: 1d array-like. -   <code>y_pred</code>: 1d array-like. -   <code>average</code>: Must be one of <code>'micro'</code>, <code>'macro'</code>, <code>'samples'</code>,     <code>'weighted'</code>, <code>'binary'</code>, or <code>None</code>.</p> <p>Note</p> <p><code>y_true</code> and <code>y_pred</code> must be of same length.</p> <p>Example Usage</p> <pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; from sklearn.metrics import recall_score\n&gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]\n&gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_recall_score(y_true, y_pred):\n...   print(recall_score(y_true, y_pred, average='macro'))\n&gt;&gt;&gt; test_recall_score(y_true, y_pred)\n0.3333333333333333\n</code></pre> </li> </ul>"},{"location":"api_docs/ml/sklearn/model_selection/","title":"sklearn.model_selection","text":""},{"location":"api_docs/ml/sklearn/model_selection/#sklearnmodel_selectiontrain_test_split","title":"sklearn.model_selection.train_test_split","text":"<p><code>sklearn.model_selection.train_test_split(X, y, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)</code></p> <p>Supported Arguments</p> <ul> <li><code>X</code>: NumPy array or Pandas Dataframes.</li> <li><code>y</code>: NumPy array or Pandas Dataframes.</li> <li><code>train_size</code>: float between 0.0 and 1.0 or <code>None</code> only.</li> <li><code>test_size</code>: float between 0.0 and 1.0 or <code>None</code> only.</li> <li><code>random_state</code>: int, RandomState, or None.</li> <li><code>shuffle</code>: bool.</li> </ul> <p>Example Usage</p> <pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def test_split():\n...   X, y = np.arange(10).reshape(5, 2), np.arange(5)\n...   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=42)\n...   print(X_train)\n...   print(y_train)\nX_train:  [[4 5]\n[6 7]\n[8 9]]\ny_train:  [2 3 4]\nX_test:  [[2 3]\n[0 1]]\ny_test:  [1 0]\n</code></pre>"},{"location":"api_docs/ml/sklearn/naive_bayes/","title":"sklearn.naive_bayes","text":""},{"location":"api_docs/ml/sklearn/naive_bayes/#sklearnnaive_bayesmultinomialnb","title":"sklearn.naive_bayes.MultinomialNB","text":"<ul> <li><code>sklearn.naive_bayes.MultinomialNB</code></li> </ul> <p>This class provides Naive Bayes classifier for multinomial models with distributed large-scale learning.</p>"},{"location":"api_docs/ml/sklearn/naive_bayes/#methods","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/naive_bayes/#sklearnnaive_bayesmultinomialnbfit","title":"sklearn.naive_bayes.MultinomialNB.fit","text":"<ul> <li> <p><code>sklearn.naive_bayes.MultinomialNB.fit(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/naive_bayes/#sklearnnaive_bayesmultinomialnbpredict","title":"sklearn.naive_bayes.MultinomialNB.predict","text":"<ul> <li> <p><code>sklearn.naive_bayes.MultinomialNB.predict(X)</code></p> <p>Supported Arguments  -  <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/naive_bayes/#sklearnnaive_bayesmultinomialnbscore","title":"sklearn.naive_bayes.MultinomialNB.score","text":"<ul> <li> <p><code>sklearn.naive_bayes.MultinomialNB.score(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array or Pandas Dataframes. -   <code>sample_weight</code>: Numeric NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/naive_bayes/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.naive_bayes import MultinomialNB\n&gt;&gt;&gt; rng = np.random.RandomState(1)\n&gt;&gt;&gt; X = rng.randint(5, size=(6, 100))\n&gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5, 6])\n&gt;&gt;&gt; X_test = rng.randint(5, size=(1, 100))\n&gt;&gt;&gt; @bodo.jit\n... def test_mnb(X, y, X_test):\n...   clf = MultinomialNB()\n...   clf.fit(X, y)\n...   ans = clf.predict(X_test)\n...   print(ans)\n...\n&gt;&gt;&gt; test_mnb(X, y, X_test)\n[5]\n</code></pre>"},{"location":"api_docs/ml/sklearn/preprocessing/","title":"sklearn.preprocessing","text":""},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessinglabelencoder","title":"sklearn.preprocessing.LabelEncoder","text":"<ul> <li><code>sklearn.preprocessing.LabelEncoder</code></li> </ul> <p>This class provides LabelEncoder support to encode target labels <code>y</code> with values between 0 and n-classes-1.</p>"},{"location":"api_docs/ml/sklearn/preprocessing/#methods","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessinglabelencoderfit","title":"sklearn.preprocessing.LabelEncoder.fit","text":"<ul> <li> <p><code>sklearn.preprocessing.LabelEncoder.fit(y)</code></p> <p>Supported Arguments  -   <code>y</code>: 1d array-like.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessinglabelencoderfit_transform","title":"sklearn.preprocessing.LabelEncoder.fit_transform","text":"<ul> <li> <p><code>sklearn.preprocessing.LabelEncoder.fit_transform(y)</code></p> <p>Supported Arguments  -   <code>y</code>: 1d array-like.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessinglabelencodertransform","title":"sklearn.preprocessing.LabelEncoder.transform","text":"<ul> <li> <p><code>sklearn.preprocessing.LabelEncoder.transform(y)</code></p> <p>Supported Arguments  -   <code>y</code>: 1d array-like.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.preprocessing import LabelEncoder\n&gt;&gt;&gt; @bodo.jit\n... def test_le():\n...   le = LabelEncoder()\n...   le.fit([1, 2, 2, 6])\n...   print(le.transform([1, 1, 2, 6]))\n...\n&gt;&gt;&gt; test_le()\n[0 0 1 2]\n</code></pre>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingminmaxscaler","title":"sklearn.preprocessing.MinMaxScaler","text":"<p><code>sklearn.preprocessing.MinMaxScaler</code></p> <p>This class provides MinMax Scaler support to scale your data based on the range of its features.</p>"},{"location":"api_docs/ml/sklearn/preprocessing/#methods_1","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingminmaxscalerfit","title":"sklearn.preprocessing.MinMaxScaler.fit","text":"<ul> <li> <p><code>sklearn.preprocessing.MinMaxScaler.fit(X, y=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingminmaxscalerinverse_transform","title":"sklearn.preprocessing.MinMaxScaler.inverse_transform","text":"<ul> <li> <p><code>sklearn.preprocessing.MinMaxScaler.inverse_transform(X)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingminmaxscalertransform","title":"sklearn.preprocessing.MinMaxScaler.transform","text":"<ul> <li> <p><code>sklearn.preprocessing.MinMaxScaler.transform(X)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#example-usage_1","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n&gt;&gt;&gt; data = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]])\n&gt;&gt;&gt; @bodo.jit\n... def test_minmax(data):\n...   scaler = MinMaxScaler()\n...   scaler.fit(data)\n...   print(scaler.transform(data))\n...\n&gt;&gt;&gt; test_minmax(data)\n[[0.   0.  ]\n [0.25 0.25]\n [0.5  0.5 ]\n [1.   1.  ]]\n</code></pre>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingstandardscaler","title":"sklearn.preprocessing.StandardScaler","text":"<p><code>sklearn.preprocessing.StandardScaler</code></p> <p>This class provides Standard Scaler support to center your data and to scale it to achieve unit variance.</p>"},{"location":"api_docs/ml/sklearn/preprocessing/#methods_2","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingstandardscalerfit","title":"sklearn.preprocessing.StandardScaler.fit","text":"<ul> <li> <p><code>sklearn.preprocessing.StandardScaler.fit(X, y=None, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array. -   <code>sample_weight</code>: Numeric NumPy Array (only if data is not         distributed)</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingstandardscalerinverse_transform","title":"sklearn.preprocessing.StandardScaler.inverse_transform","text":"<ul> <li> <p><code>sklearn.preprocessing.StandardScaler.inverse_transform(X, copy=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>copy</code>: bool or None.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingstandardscalertransform","title":"sklearn.preprocessing.StandardScaler.transform","text":"<ul> <li> <p><code>sklearn.preprocessing.StandardScaler.transform(X, copy=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>copy</code>: bool or None.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#example-usage_2","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.svm import LinearSVC\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n&gt;&gt;&gt; @bodo.jit\n... def test_linearsvc(X, y):\n...   scaler = StandardScaler()\n...   scaler.fit(X)\n...   X = scaler.transform(X)\n...   clf = LinearSVC()\n...   clf.fit(X, y)\n...   ans = clf.predict(np.array([[0, 0, 0, 0]]))\n...   print(ans)\n...\n&gt;&gt;&gt; test_linearsvc(X, y)\n[1]\n</code></pre>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingrobustscaler","title":"sklearn.preprocessing.RobustScaler","text":"<p><code>sklearn.preprocessing.RobustScaler</code></p> <p>This class provides Robust Scaler support to scale your data while being robust to outliers.</p>"},{"location":"api_docs/ml/sklearn/preprocessing/#methods_3","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingrobustscalerfit","title":"sklearn.preprocessing.RobustScaler.fit","text":"<ul> <li> <p><code>sklearn.preprocessing.RobustScaler.fit(X, y=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy array or Pandas DataFrame. Sparse matrices are not yet supported.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingrobustscalerinverse_transform","title":"sklearn.preprocessing.RobustScaler.inverse_transform","text":"<ul> <li> <p><code>sklearn.preprocessing.RobustScaler.inverse_transform(X)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy array or Pandas DataFrame. Sparse matrices are not yet supported.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#sklearnpreprocessingrobustscalertransform","title":"sklearn.preprocessing.RobustScaler.transform","text":"<ul> <li> <p><code>sklearn.preprocessing.RobustScaler.transform(X)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy array or Pandas DataFrame. Sparse matrices are not yet supported.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/preprocessing/#example-usage_3","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.preprocessing import RobustScaler\n&gt;&gt;&gt; data = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18], [-100, 3], [0, 500]])\n&gt;&gt;&gt; @bodo.jit(distributed=[\"data\"])\n... def test_robust(data):\n...   scaler = RobustScaler()\n...   scaler.fit(data)\n...   print(scaler.transform(data))\n...\n&gt;&gt;&gt; test_robust(data)\n[[  -0.85714286   -0.48979592]\n [  -0.28571429   -0.16326531]\n [   0.28571429    0.16326531]\n [   1.42857143    0.81632653]\n [-114.           -0.40816327]\n [   0.28571429   40.16326531]]\n</code></pre>"},{"location":"api_docs/ml/sklearn/svm/","title":"sklearn.svm","text":""},{"location":"api_docs/ml/sklearn/svm/#sklearnsvmlinearsvc","title":"sklearn.svm.LinearSVC","text":"<p><code>sklearn.svm.LinearSVC</code></p> <p>This class provides Linear Support Vector Classification.</p>"},{"location":"api_docs/ml/sklearn/svm/#methods","title":"Methods","text":""},{"location":"api_docs/ml/sklearn/svm/#sklearnsvmlinearsvcfit","title":"sklearn.svm.LinearSVC.fit","text":"<ul> <li> <p><code>sklearn.svm.LinearSVC.fit(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array. -   <code>sample_weight</code>: Numeric NumPy Array (only if data is not  distributed)</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/svm/#sklearnsvmlinearsvcpredict","title":"sklearn.svm.LinearSVC.predict","text":"<ul> <li> <p><code>sklearn.svm.LinearSVC.predict(X)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/svm/#sklearnsvmlinearsvcscore","title":"sklearn.svm.LinearSVC.score","text":"<ul> <li> <p><code>sklearn.svm.LinearSVC.score(X, y, sample_weight=None)</code></p> <p>Supported Arguments  -   <code>X</code>: NumPy Array or Pandas Dataframes. -   <code>y</code>: NumPy Array or Pandas Dataframes. -   <code>sample_weight</code>: Numeric NumPy Array or Pandas Dataframes.</p> </li> </ul>"},{"location":"api_docs/ml/sklearn/svm/#example-usage","title":"Example Usage:","text":"<pre><code>&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.svm import LinearSVC\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n&gt;&gt;&gt; @bodo.jit\n... def test_linearsvc(X, y):\n...   scaler = StandardScaler()\n...   scaler.fit(X)\n...   X = scaler.transform(X)\n...   clf = LinearSVC()\n...   clf.fit(X, y)\n...   ans = clf.predict(np.array([[0, 0, 0, 0]]))\n...   print(ans)\n...\n&gt;&gt;&gt; test_linearsvc(X, y)\n[1]\n</code></pre>"},{"location":"api_docs/pandas/","title":"Pandas","text":"<ul> <li>General Functions</li> <li>Dataframe API</li> <li>Groupby</li> <li>Series API</li> <li>Window</li> <li>Date Offsets</li> <li>Input/Output</li> <li>Index Objects</li> <li>TimeDelta</li> <li>Timestamp</li> </ul>"},{"location":"api_docs/pandas/dataframe/","title":"DataFrame","text":"<p>Bodo provides extensive DataFrame support. This section covers the DataFrame API.</p>"},{"location":"api_docs/pandas/dataframe/#creation","title":"Creation","text":"Function Description <code>pd.DataFrame</code> Create a DataFrame"},{"location":"api_docs/pandas/dataframe/#attributes-and-underlying-data","title":"Attributes and underlying data","text":"Function Description <code>pd.DataFrame.columns</code> The column labels of the DataFrame <code>pd.DataFrame.dtypes</code> Return the dtypes in the DataFrame <code>pd.DataFrame.empty</code> Indicator whether DataFrame is empty <code>pd.DataFrame.index</code> The index (row labels) of the DataFrame <code>pd.DataFrame.ndim</code> Number of axes / array dimensions <code>pd.DataFrame.select_dtypes</code> Return a subset of the DataFrame's columns based on the column dtypes <code>pd.DataFrame.filter</code> Subset the DataFrame rows or columns according to the specified index labels <code>pd.DataFrame.shape</code> Return a tuple representing the dimensionality of the DataFrame <code>pd.DataFrame.size</code> Number of elements in the DataFrame <code>pd.DataFrame.to_numpy</code> Return a Numpy representation of the DataFrame <code>pd.DataFrame.values</code> Return a Numpy representation of the DataFrame"},{"location":"api_docs/pandas/dataframe/#conversion","title":"Conversion","text":"Function Description <code>pd.DataFrame.astype</code> Cast a pandas object to a specified dtype <code>pd.DataFrame.copy</code> Make a copy of the DataFrame <code>pd.DataFrame.isna</code> Detect missing values <code>pd.DataFrame.isnull</code> Detect missing values <code>pd.DataFrame.notna</code> Detect existing (non-missing) values <code>pd.DataFrame.notnull</code> Detect existing (non-missing) values <code>pd.DataFrame.info</code> Print a concise summary of a DataFrame <code>pd.DataFrame.infer_objects</code> Attempt to infer better dtypes for object columns"},{"location":"api_docs/pandas/dataframe/#indexing-iteration","title":"Indexing, iteration","text":"Function Description <code>pd.DataFrame.head</code> Return the first <code>n</code> rows <code>pd.DataFrame.iat</code> Access a single value for a row/column pair by integer position <code>pd.DataFrame.iloc</code> Purely integer-location based indexing for selection by position <code>pd.DataFrame.insert</code> Insert column into DataFrame at specified location <code>pd.DataFrame.isin</code> Determine if values are contained in a Series or DataFrame <code>pd.DataFrame.itertuples</code> Iterate over DataFrame rows as namedtuples <code>pd.DataFrame.query</code> Query the columns of a DataFrame with a boolean expression <code>pd.DataFrame.tail</code> Return the last <code>n</code> rows <code>pd.DataFrame.where</code> Replace values where the condition is False <code>pd.DataFrame.mask</code> Replace values where the condition is True"},{"location":"api_docs/pandas/dataframe/#function-application-groupby-window","title":"Function Application, GroupBy &amp; Window","text":"Function Description <code>pd.DataFrame.apply</code> Apply a function along an axis of the DataFrame <code>pd.DataFrame.groupby</code> Group DataFrame using a mapper or by a Series of columns <code>pd.DataFrame.rolling</code> Provide rolling window calculations"},{"location":"api_docs/pandas/dataframe/#computations-descriptive-stats","title":"Computations / Descriptive Stats","text":"Function Description <code>pd.DataFrame.abs</code> Return a DataFrame with absolute numeric value of each element <code>pd.DataFrame.corr</code> Compute pairwise correlation of columns, excluding NA/null values <code>pd.DataFrame.count</code> Count non-NA cells for each column or row <code>pd.DataFrame.cov</code> Compute pairwise covariance of columns, excluding NA/null values <code>pd.DataFrame.cumprod</code> Return cumulative product over a DataFrame or Series axis <code>pd.DataFrame.cumsum</code> Return cumulative sum over a DataFrame or Series axis <code>pd.DataFrame.describe</code> Generate descriptive statistics <code>pd.DataFrame.diff</code> First discrete difference of element <code>pd.DataFrame.max</code> Return the maximum of the values for the requested axis <code>pd.DataFrame.mean</code> Return the mean of the values for the requested axis <code>pd.DataFrame.median</code> Return the median of the values for the requested axis <code>pd.DataFrame.min</code> Return the minimum of the values for the requested axis <code>pd.DataFrame.nunique</code> Count distinct observations over requested axis <code>pd.DataFrame.pct_change</code> Percentage change between the current and a prior element <code>pd.DataFrame.pipe</code> Apply func(self, *args, **kwargs) <code>pd.DataFrame.prod</code> Return the product of the values for the requested axis <code>pd.DataFrame.product</code> Return the product of the values for the requested axis <code>pd.DataFrame.quantile</code> Return values at the given quantile over requested axis <code>pd.DataFrame.rank</code> Compute numerical data ranks (1 through n) along axis <code>pd.DataFrame.std</code> Return sample standard deviation over requested axis <code>pd.DataFrame.sum</code> Return the sum of the values for the requested axis <code>pd.DataFrame.var</code> Return unbiased variance over requested axis <code>pd.DataFrame.memory_usage</code> Return the memory usage of each column in bytes"},{"location":"api_docs/pandas/dataframe/#reindexing-selection-label-manipulation","title":"Reindexing / Selection / Label manipulation","text":"Function Description <code>pd.DataFrame.drop</code> Drop specified labels from rows or columns <code>pd.DataFrame.drop_duplicates</code> Return DataFrame with duplicate rows removed <code>pd.DataFrame.duplicated</code> Return boolean Series denoting duplicate rows <code>pd.DataFrame.first</code> Select initial periods of time series data based on a date offset <code>pd.DataFrame.idxmax</code> Return the row label of the maximum value <code>pd.DataFrame.idxmin</code> Return the row label of the minimum value <code>pd.DataFrame.last</code> Select final periods of time series data based on a date offset <code>pd.DataFrame.rename</code> Alter axes labels <code>pd.DataFrame.reset_index</code> Reset the index of the DataFrame <code>pd.DataFrame.set_index</code> Set the DataFrame index using existing columns <code>pd.DataFrame.take</code> Return the elements in the given positional indices along an axis"},{"location":"api_docs/pandas/dataframe/#missing-data-handling","title":"Missing data handling","text":"Function Description <code>pd.DataFrame.dropna</code> Remove missing values <code>pd.DataFrame.fillna</code> Fill NA/NaN values using the specified method <code>pd.DataFrame.replace</code> Replace values given in to_replace with value"},{"location":"api_docs/pandas/dataframe/#reshaping-sorting-transposing","title":"Reshaping, sorting, transposing","text":"Function Description <code>pd.DataFrame.explode</code> Transform each element of a list-like to a row, replicating index values <code>pd.DataFrame.melt</code> Unpivot a DataFrame from wide to long format <code>pd.DataFrame.pivot</code> Return reshaped DataFrame organized by given index / column values <code>pd.DataFrame.pivot_table</code> Create a spreadsheet-style pivot table as a DataFrame <code>pd.DataFrame.sample</code> Return a random sample of items from an axis of object <code>pd.DataFrame.sort_index</code> Sort object by labels (along an axis) <code>pd.DataFrame.sort_values</code> Sort by the values along either axis <code>pd.DataFrame.to_string</code> Render a DataFrame to a console-friendly tabular output"},{"location":"api_docs/pandas/dataframe/#combining-joining-merging","title":"Combining / joining / merging","text":"Function Description <code>pd.DataFrame.assign</code> Assign new columns to a DataFrame <code>pd.DataFrame.join</code> Join columns with other DataFrame either on index or on a key column <code>pd.DataFrame.merge</code> Merge DataFrame or named Series objects with a database-style join"},{"location":"api_docs/pandas/dataframe/#time-series-related","title":"Time series-related","text":"Function Description <code>pd.DataFrame.shift</code> Shift index by desired number of periods with an optional time freq"},{"location":"api_docs/pandas/dataframe/#serialization-io-conversion","title":"Serialization, IO, Conversion","text":"Function Description <code>pd.DataFrame.to_csv</code> Write object to a comma-separated values (csv) file <code>pd.DataFrame.to_json</code> Convert the object to a JSON string <code>pd.DataFrame.to_parquet</code> Write a DataFrame to the binary parquet format <code>pd.DataFrame.to_sql</code> Write records stored in a DataFrame to a SQL database"},{"location":"api_docs/pandas/dataframe/#plotting","title":"Plotting","text":"Function Description <code>pd.DataFrame.plot</code> Plot data"},{"location":"api_docs/pandas/dataframe/abs/","title":"<code>pd.DataFrame.abs</code>","text":"<p><code>pandas.DataFrame.abs()</code></p> <p>Note</p> <p>Only supported for dataframes containing numerical data and Timedeltas</p>"},{"location":"api_docs/pandas/dataframe/abs/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,-2], \"B\": [3.1,-4.2], \"C\": [pd.Timedelta(10, unit=\"D\"), pd.Timedelta(-10, unit=\"D\")]})\n...   return df.abs()\n&gt;&gt;&gt; f()\n   A    B       C\n0  1  3.1 10 days\n1  2  4.2 10 days\n</code></pre>"},{"location":"api_docs/pandas/dataframe/apply/","title":"<code>pd.DataFrame.apply</code>","text":"<p><code>pandas.DataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), _bodo_inline=False, \\**kwargs)</code></p>"},{"location":"api_docs/pandas/dataframe/apply/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>func</code>: function (e.g. lambda) (axis must = 1), jit function (axis must = 1), String which refers to a supported DataFrame method<ul> <li>Must be constant at Compile Time</li> </ul> </li> <li><code>axis</code>: Integer (0, 1), String (only if the method takes axis as an argument )<ul> <li>Must be constant at Compile Time</li> </ul> </li> <li><code>_bodo_inline</code>: boolean<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/apply/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.apply(lambda x: x[\"A\"] * (x[\"B\"] + x[\"C\"]))\n&gt;&gt;&gt; f()\n0    11\n1    26\n2    45\ndtype: int64\n</code></pre> <p>Note</p> <p>Supports extra <code>_bodo_inline</code> boolean argument to manually control bodo's inlining behavior. Inlining user-defined functions (UDFs) can potentially improve performance at the expense of extra compilation time. Bodo uses heuristics to make a decision automatically if <code>_bodo_inline</code> is not provided.</p>"},{"location":"api_docs/pandas/dataframe/assign/","title":"<code>pd.DataFrame.assign</code>","text":"<p><code>pandas.DataFrame.assign(\\**kwargs)</code></p>"},{"location":"api_docs/pandas/dataframe/assign/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6]})\n...   df2 = df.assign(C = 2 * df[\"B\"], D = lambda x: x.C -1)\n...   return df2\n&gt;&gt;&gt; f()\n   A  B   C   D\n0  1  4   8  -8\n1  2  5  10 -10\n2  3  6  12 -12\n</code></pre> <p>Note</p> <p>arguments can be JIT functions, lambda functions, or values that can be used to initialize a Pandas Series.</p>"},{"location":"api_docs/pandas/dataframe/astype/","title":"<code>pd.DataFrame.astype</code>","text":"<p><code>pandas.DataFrame.astype(dtype, copy=True, errors='raise')</code></p>"},{"location":"api_docs/pandas/dataframe/astype/#supported-arguments","title":"Supported Arguments","text":"<ul> <li> <p><code>dtype</code>: dict of string column names keys, and Strings/types values. String (string must be parsable by <code>np.dtype</code>), Valid type (see types), The following functions: float, int, bool, str</p> <ul> <li>Must be constant at Compile Time</li> </ul> </li> <li> <p><code>copy</code>: boolean</p> </li> </ul>"},{"location":"api_docs/pandas/dataframe/astype/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [3.1,4.2,5.3]})\n...   return df.astype({\"A\": float, \"B\": \"datetime64[ns]\"})\n&gt;&gt;&gt; f()\n     A                             B\n0  1.0 1970-01-01 00:00:00.000000003\n1  2.0 1970-01-01 00:00:00.000000004\n2  3.0 1970-01-01 00:00:00.000000005\n</code></pre>"},{"location":"api_docs/pandas/dataframe/columns/","title":"<code>pd.DataFrame.columns</code>","text":"<p><code>pandas.DataFrame.columns</code></p>"},{"location":"api_docs/pandas/dataframe/columns/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [\"X\", \"Y\", \"Z\"], \"C\": [pd.Timedelta(10, unit=\"D\"), pd.Timedelta(10, unit=\"H\"), pd.Timedelta(10, unit=\"S\")]})\n...   return df.columns\n&gt;&gt;&gt; f()\nIndex(['A', 'B', 'C'], dtype='object')\n</code></pre>"},{"location":"api_docs/pandas/dataframe/copy/","title":"<code>pd.DataFrame.copy</code>","text":"<p><code>pandas.DataFrame.copy(deep=True)</code></p>"},{"location":"api_docs/pandas/dataframe/copy/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>copy</code>: boolean</li> </ul>"},{"location":"api_docs/pandas/dataframe/copy/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3]})\n...   shallow_df = df.copy(deep=False)\n...   deep_df = df.copy()\n...   shallow_df[\"A\"][0] = -1\n...   formated_out = \"\\n\".join([df.to_string(), shallow_df.to_string(), deep_df.to_string()])\n...   return formated_out\n&gt;&gt;&gt; f()\n   A\n0  -1\n1  2\n2  3\n  A\n0  -1\n1  2\n2  3\n  A\n0  1\n1  2\n2  3\n</code></pre>"},{"location":"api_docs/pandas/dataframe/corr/","title":"<code>pd.DataFrame.corr</code>","text":"<p><code>pandas.DataFrame.corr(method='pearson', min_periods=1)</code></p>"},{"location":"api_docs/pandas/dataframe/corr/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>min_periods</code>: Integer</li> </ul>"},{"location":"api_docs/pandas/dataframe/corr/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [.9, .8, .7, .4], \"B\": [-.8, -.9, -.8, -.4], \"c\": [.7, .7, .7, .4]})\n...   return df.corr()\n&gt;&gt;&gt; f()\n          A         B        c\nA  1.000000 -0.904656  0.92582\nB -0.904656  1.000000 -0.97714\nc  0.925820 -0.977140  1.00000\n</code></pre>"},{"location":"api_docs/pandas/dataframe/count/","title":"<code>pd.DataFrame.count</code>","text":"<p><code>pandas.DataFrame.count(axis=0, level=None, numeric_only=False)</code></p>"},{"location":"api_docs/pandas/dataframe/count/#supported-arguments-none","title":"Supported Arguments : None","text":""},{"location":"api_docs/pandas/dataframe/count/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1, None, 3], \"B\": [None, 2, None]})\n...   return df.count()\n&gt;&gt;&gt; f()\nA    2\nB    1\n</code></pre>"},{"location":"api_docs/pandas/dataframe/cov/","title":"<code>pd.DataFrame.cov</code>","text":"<p><code>pandas.DataFrame.cov(min_periods=None, ddof=1)</code></p>"},{"location":"api_docs/pandas/dataframe/cov/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>min_periods</code>: Integer</li> </ul>"},{"location":"api_docs/pandas/dataframe/cov/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [0.695, 0.478, 0.628], \"B\": [-0.695, -0.478, -0.628], \"C\": [0.07, -0.68, 0.193]})\n...   return df.cov()\n&gt;&gt;&gt; f()\n          A         B         C\nA  0.012346 -0.012346  0.047577\nB -0.012346  0.012346 -0.047577\nC  0.047577 -0.047577  0.223293\n</code></pre>"},{"location":"api_docs/pandas/dataframe/cumprod/","title":"<code>pd.DataFrame.cumprod</code>","text":"<p><code>pandas.DataFrame.cumprod(axis=None, skipna=True)</code></p>"},{"location":"api_docs/pandas/dataframe/cumprod/#supported-arguments-none","title":"Supported Arguments : None","text":""},{"location":"api_docs/pandas/dataframe/cumprod/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [.1,np.nan,12.3],})\n...   return df.cumprod()\n&gt;&gt;&gt; f()\n   A    B\n0  1  0.1\n1  2  NaN\n2  6  NaN\n</code></pre> <p>Note</p> <p>Not supported for dataframe with nullable integer.</p>"},{"location":"api_docs/pandas/dataframe/cumsum/","title":"<code>pd.DataFrame.cumsum</code>","text":"<p><code>pandas.DataFrame.cumsum(axis=None, skipna=True)</code></p>"},{"location":"api_docs/pandas/dataframe/cumsum/#supported-arguments-none","title":"Supported Arguments : None","text":""},{"location":"api_docs/pandas/dataframe/cumsum/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [.1,np.nan,12.3],})\n...   return df.cumsum()\n&gt;&gt;&gt; f()\n   A    B\n0  1  0.1\n1  3  NaN\n2  6  NaN\n</code></pre> <p>Note</p> <p>Not supported for dataframe with nullable integer.</p>"},{"location":"api_docs/pandas/dataframe/dataframe/","title":"pd.DataFrame","text":"<p><code>pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=None)</code></p>"},{"location":"api_docs/pandas/dataframe/dataframe/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>data</code>: constant key dictionary, 2D Numpy array<ul> <li><code>columns</code> argument is required when using a 2D Numpy array</li> </ul> </li> <li><code>index</code>: List, Tuple, Pandas index types, Pandas array types, Pandas series types, Numpy array types</li> <li><code>columns</code>: Constant list of String, Constant tuple of String<ul> <li>Must be constant at Compile Time</li> </ul> </li> <li><code>dtype</code>: All values supported with <code>dataframe.astype</code> (see below)</li> <li><code>copy</code>: boolean<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/describe/","title":"<code>pd.DataFrame.describe</code>","text":"<p><code>pandas.DataFrame.describe(percentiles=None, include=None, exclude=None, datetime_is_numeric=False)</code></p>"},{"location":"api_docs/pandas/dataframe/describe/#supported-arguments-none","title":"Supported Arguments : None","text":""},{"location":"api_docs/pandas/dataframe/describe/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [pd.Timestamp(2000, 10, 2), pd.Timestamp(2001, 9, 5), pd.Timestamp(2002, 3, 11)]})\n...   return df.describe()\n&gt;&gt;&gt; f()\n        A                    B\ncount  3.0                    3\nmean   2.0  2001-07-16 16:00:00\nmin    1.0  2000-10-02 00:00:00\n25%    1.5  2001-03-20 00:00:00\n50%    2.0  2001-09-05 00:00:00\n75%    2.5  2001-12-07 12:00:00\nmax    3.0  2002-03-11 00:00:00\nstd    1.0                  NaN\n</code></pre> <p>Note</p> <p>Only supported for dataframes containing numeric data, and datetime data. Datetime_is_numeric defaults to True in JIT code.</p>"},{"location":"api_docs/pandas/dataframe/df_index/","title":"<code>pd.DataFrame.index</code>","text":"<p><code>pandas.DataFrame.index</code></p>"},{"location":"api_docs/pandas/dataframe/df_index/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3]}, index=[\"x\", \"y\", \"z\"])\n...   return df.index\n&gt;&gt;&gt; f()\nIndex(['x', 'y', 'z'], dtype='object')\n</code></pre>"},{"location":"api_docs/pandas/dataframe/diff/","title":"<code>pd.DataFrame.diff</code>","text":"<p><code>pandas.DataFrame.diff(periods=1, axis=0)</code></p>"},{"location":"api_docs/pandas/dataframe/diff/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>periods</code>: Integer</li> </ul>"},{"location":"api_docs/pandas/dataframe/diff/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [pd.Timestamp(2000, 10, 2), pd.Timestamp(2001, 9, 5), pd.Timestamp(2002, 3, 11)]})\n...   return df.diff(1)\n&gt;&gt;&gt; f()\n     A        B\n0  NaN      NaT\n1  1.0 338 days\n2  1.0 187 days\n</code></pre> <p>Note</p> <p>Only supported for dataframes containing float, non-null int, and datetime64ns values</p>"},{"location":"api_docs/pandas/dataframe/drop/","title":"<code>pd.DataFrame.drop</code>","text":"<p><code>pandas.DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')</code></p> <ul> <li>Only dropping columns supported, either using <code>columns</code> argument or setting <code>axis=1</code> and using the <code>labels</code> argument</li> <li><code>labels</code> and <code>columns</code> require constant string, or constant list/tuple of string values</li> <li><code>inplace</code> supported with a constant boolean value</li> <li>All other arguments are unsupported</li> </ul>"},{"location":"api_docs/pandas/dataframe/drop/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   df.drop(columns = [\"B\", \"C\"], inplace=True)\n...   return df\n&gt;&gt;&gt; f()\n   A\n0  1\n1  2\n2  3\n</code></pre>"},{"location":"api_docs/pandas/dataframe/drop_duplicates/","title":"<code>pd.DataFrame.drop_duplicates</code>","text":"<p><code>pandas.DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)</code></p>"},{"location":"api_docs/pandas/dataframe/drop_duplicates/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>subset</code>: Constant list/tuple of String column names, Constant list/tuple of Integer column names, Constant String column names, Constant Integer column names</li> </ul>"},{"location":"api_docs/pandas/dataframe/drop_duplicates/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,1,3,4], \"B\": [1,1,3,3], \"C\": [7,8,9,10]})\n...   return df.drop_duplicates(subset = [\"A\", \"B\"])\n&gt;&gt;&gt; f()\n   A  B   C\n0  1  1   7\n2  3  3   9\n3  4  3  10\n</code></pre>"},{"location":"api_docs/pandas/dataframe/dropna/","title":"<code>pd.DataFrame.dropna</code>","text":"<p><code>pandas.DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)</code></p>"},{"location":"api_docs/pandas/dataframe/dropna/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>how</code>: Constant String: either \"all\" or \"any\"</li> <li><code>thresh</code>: Integer</li> <li><code>subset</code>: Constant list/tuple of String column names, Constant list/tuple of Integer column names, Constant String column names, Constant Integer column names</li> </ul>"},{"location":"api_docs/pandas/dataframe/dropna/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3,None], \"B\": [4, 5,None, None], \"C\": [6, None, None, None]})\n...   df_1 = df.dropna(how=\"all\", subset=[\"B\", \"C\"])\n...   df_2 = df.dropna(thresh=3)\n...   formated_out = \"\\n\".join([df_1.to_string(), df_2.to_string()])\n...   return formated_out\n&gt;&gt;&gt; f()\n   A  B     C\n0  1  4     6\n1  2  5  &lt;NA&gt;\n   A  B  C\n0  1  4  6\n</code></pre>"},{"location":"api_docs/pandas/dataframe/dtypes/","title":"<code>pd.DataFrame.dtypes</code>","text":"<p><code>pandas.DataFrame.dtypes</code></p>"},{"location":"api_docs/pandas/dataframe/dtypes/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [\"X\", \"Y\", \"Z\"], \"C\": [pd.Timedelta(10, unit=\"D\"), pd.Timedelta(10, unit=\"H\"), pd.Timedelta(10, unit=\"S\")]})\n...   return df.dtypes\n&gt;&gt;&gt; f()\nA              int64\nB             string\nC    timedelta64[ns]\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/dataframe/duplicated/","title":"<code>pd.DataFrame.duplicated</code>","text":"<p><code>pandas.DataFrame.duplicated(subset=None, keep='first')</code></p>"},{"location":"api_docs/pandas/dataframe/duplicated/#supported-arguments-none","title":"Supported Arguments : None","text":""},{"location":"api_docs/pandas/dataframe/duplicated/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,1,3,4], \"B\": [1,1,3,3]})\n...   return df.duplicated()\n&gt;&gt;&gt; f()\n0    False\n1     True\n2    False\n3    False\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/dataframe/empty/","title":"<code>pd.DataFrame.empty</code>","text":"<p><code>pandas.DataFrame.empty</code></p>"},{"location":"api_docs/pandas/dataframe/empty/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df1 = pd.DataFrame({\"A\": [1,2,3]})\n...   df2 = pd.DataFrame()\n...   return df1.empty, df2.empty\n&gt;&gt;&gt; f()\n(False, True)\n</code></pre>"},{"location":"api_docs/pandas/dataframe/explode/","title":"<code>pd.DataFrame.explode</code>","text":"<p><code>pandas.DataFrame.explode(column, ignore_index=False)</code></p>"},{"location":"api_docs/pandas/dataframe/explode/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>column</code>: Constant Column label or list of labels</li> </ul>"},{"location":"api_docs/pandas/dataframe/explode/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df, cols):\n...   return df.explode(cols)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": [[0, 1, 2], [5], [], [3, 4]], \"B\": [1, 7, 2, 4], \"C\": [[1, 2, 3], np.nan, [], [1, 2]]})\n&gt;&gt;&gt; f(df, [\"A\", \"C\"])\n      A  B     C\n0     0  1     1\n0     1  1     2\n0     2  1     3\n1     5  7  &lt;NA&gt;\n2  &lt;NA&gt;  2  &lt;NA&gt;\n3     3  4     1\n3     4  4     2\n</code></pre>"},{"location":"api_docs/pandas/dataframe/fillna/","title":"<code>pd.DataFrame.fillna</code>","text":"<p><code>pandas.DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)</code></p>"},{"location":"api_docs/pandas/dataframe/fillna/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>value</code>: various scalars<ul> <li>Must be of the same type as the filled column</li> </ul> </li> <li><code>inplace</code>: Constant boolean<ul> <li><code>inplace</code> is not supported alongside method</li> </ul> </li> <li><code>method</code>: One of <code>bfill</code>, <code>backfill</code>, <code>ffill</code> , or <code>pad</code><ul> <li>Must be constant at Compile Time</li> <li><code>inplace</code> is not supported alongside method</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/fillna/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3,None], \"B\": [4, 5,None, None], \"C\": [6, None, None, None]})\n...   return df.fillna(-1)\n&gt;&gt;&gt; f()\n</code></pre>"},{"location":"api_docs/pandas/dataframe/filter/","title":"<code>pd.DataFrame.filter</code>","text":"<p><code>pandas.DataFrame.filter(items=None, like=None, regex=None, axis=None)</code></p>"},{"location":"api_docs/pandas/dataframe/filter/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>items</code>: Constant list of String</li> <li><code>like</code>: Constant string</li> <li><code>regex</code>: Constant String</li> <li><code>axis</code> (only supports the \"column\" axis): Constant String, Constant integer</li> </ul>"},{"location":"api_docs/pandas/dataframe/filter/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"ababab\": [1], \"hello world\": [2], \"A\": [3]})\n...   filtered_df_1 = pd.DataFrame({\"ababab\": [1], \"hello world\": [2], \"A\": [3]}).filter(items = [\"A\"])\n...   filtered_df_2 = pd.DataFrame({\"ababab\": [1], \"hello world\": [2], \"A\": [3]}).filter(like =\"hello\", axis = \"columns\")\n...   filtered_df_3 = pd.DataFrame({\"ababab\": [1], \"hello world\": [2], \"A\": [3]}).filter(regex=\"(ab){3}\", axis = 1)\n...   formated_out = \"\\n\".join([filtered_df_1.to_string(), filtered_df_2.to_string(), filtered_df_3.to_string()])\n...   return formated_out\n&gt;&gt;&gt; f()\n   A\n0  3\n  hello world\n0            2\n  ababab\n0       1\n</code></pre>"},{"location":"api_docs/pandas/dataframe/first/","title":"<code>pd.DataFrame.first</code>","text":"<p><code>pandas.DataFrame.first(offset)</code></p>"},{"location":"api_docs/pandas/dataframe/first/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>offset</code>: String or Offset type<ul> <li>String argument must be a valid frequency alias.</li> </ul> </li> </ul> <p>Note</p> <p>DataFrame must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted.</p>"},{"location":"api_docs/pandas/dataframe/first/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df, offset):\n...     return df.first(offset)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": np.arange(100), \"B\": np.arange(100, 200)}, index=pd.date_range(start='1/1/2022', end='12/31/2024', periods=100))\n&gt;&gt;&gt; f(df, \"2M\")\n                             A    B\n2022-01-01 00:00:00.000000000  0  100\n2022-01-12 01:27:16.363636363  1  101\n2022-01-23 02:54:32.727272727  2  102\n2022-02-03 04:21:49.090909091  3  103\n2022-02-14 05:49:05.454545454  4  104\n2022-02-25 07:16:21.818181818  5  105\n</code></pre>"},{"location":"api_docs/pandas/dataframe/groupby/","title":"<code>pd.DataFrame.groupby</code>","text":"<p><code>pandas.DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True)</code></p>"},{"location":"api_docs/pandas/dataframe/groupby/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>by</code>: String column label,  List/Tuple of column labels<ul> <li>Must be constant at Compile Time</li> </ul> </li> <li><code>as_index</code>: boolean<ul> <li>Must be constant at Compile Time</li> </ul> </li> <li><code>dropna</code>: boolean<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul> <p>Note</p> <p><code>sort=False</code> and <code>observed=True</code> are set by default. These are the only support values for sort and observed. For more information on using groupby, see the groupby section.</p>"},{"location":"api_docs/pandas/dataframe/groupby/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,1,2,2], \"B\": [-2,-2,2,2]})\n...   return df.groupby(\"A\").sum()\n&gt;&gt;&gt; f()\n   B\nA\n1 -4\n2  4\n</code></pre>"},{"location":"api_docs/pandas/dataframe/head/","title":"<code>pd.DataFrame.head</code>","text":"<p><code>pandas.DataFrame.head(n=5)</code></p>"},{"location":"api_docs/pandas/dataframe/head/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>head</code>: integer</li> </ul>"},{"location":"api_docs/pandas/dataframe/head/#example-usage","title":"Example Usage","text":"<pre><code>    &gt;&gt;&gt; @bodo.jit\n    ... def f():\n    ...   return pd.DataFrame({\"A\": np.arange(1000)}).head(3)\n       A\n    0  0\n    1  1\n    2  2\n</code></pre>"},{"location":"api_docs/pandas/dataframe/iat/","title":"<code>pd.DataFrame.iat</code>","text":"<p><code>pandas.DataFrame.iat</code></p> <p>Note</p> <p>We only support indexing using <code>iat</code> using a pair of integers. We require that the second int (the column integer) is a compile time constant</p>"},{"location":"api_docs/pandas/dataframe/iat/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   df.iat[0, 0] = df.iat[2,2]\n...   return df\n&gt;&gt;&gt; f()\n   A  B  C\n0  9  4  7\n1  2  5  8\n2  3  6  9\n</code></pre>"},{"location":"api_docs/pandas/dataframe/idxmax/","title":"<code>pd.DataFrame.idxmax</code>","text":"<p><code>pandas.DataFrame.idxmax(axis=0, skipna=True)</code></p>"},{"location":"api_docs/pandas/dataframe/idxmax/#supported-arguments-none","title":"Supported Arguments : None","text":""},{"location":"api_docs/pandas/dataframe/idxmax/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.idxmax()\n&gt;&gt;&gt; f()\nA    2\nB    2\nC    2\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/dataframe/idxmin/","title":"<code>pd.DataFrame.idxmin</code>","text":"<p><code>pandas.DataFrame.idxmin(axis=0, skipna=True)</code></p>"},{"location":"api_docs/pandas/dataframe/idxmin/#supported-arguments-none","title":"Supported Arguments : None","text":""},{"location":"api_docs/pandas/dataframe/idxmin/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.idxmax()\n&gt;&gt;&gt; f()\nA    0\nB    0\nC    20\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/dataframe/iloc/","title":"<code>pd.DataFrame.iloc</code>","text":"<p><code>pandas.DataFrame.iloc</code></p>"},{"location":"api_docs/pandas/dataframe/iloc/#getitem","title":"getitem","text":"<ul> <li><code>df.iloc</code> supports single integer indexing (returns row as series) <code>df.iloc[0]</code></li> <li><code>df.iloc</code> supports single list/array/series of integers/bool <code>df.iloc[[0,1,2]]</code></li> <li>for tuples indexing <code>df.iloc[row_idx, col_idx]</code> we allow:<ul> <li><code>row_idx</code> to be int list/array/series of integers/bool slice</li> <li><code>col_idx</code> to be constant int, constant list of integers, or constant slice</li> </ul> </li> <li>e.g.: <code>df.iloc[[0,1,2], :]</code></li> </ul>"},{"location":"api_docs/pandas/dataframe/iloc/#setitem","title":"setitem","text":"<ul> <li><code>df.iloc</code> only supports scalar setitem</li> <li><code>df.iloc</code> only supports tuple indexing <code>df.iloc[row_idx, col_idx]</code></li> <li><code>row_idx</code> can be anything supported for series setitem:<ul> <li>int</li> <li>list/array/series of integers/bool</li> <li>slice</li> </ul> </li> <li><code>col_idx</code> can be: constant int, constant list/tuple of integers</li> </ul>"},{"location":"api_docs/pandas/dataframe/iloc/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   df.iloc[0, 0] = df.iloc[2,2]\n...   df.iloc[1, [1,2]] = df.iloc[0, 1]\n...   df[\"D\"] = df.iloc[0]\n...   return df\n&gt;&gt;&gt; f()\n   A  B  C  D\n0  9  4  7  7\n1  2  4  4  4\n2  3  6  9  9\n</code></pre>"},{"location":"api_docs/pandas/dataframe/infer_objects/","title":"<code>pd.DataFrame.infer_objects</code>","text":"<p><code>pandas.DataFrame.infer_objects()</code></p>"},{"location":"api_docs/pandas/dataframe/infer_objects/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3]})\n...   return df.infer_objects()\n   A\n0  1\n1  2\n2  3\n</code></pre> <p>Note</p> <p>Bodo does not internally use the object dtype, so types are never inferred. As a result, this API just produces a deep copy, consistent with Pandas.</p>"},{"location":"api_docs/pandas/dataframe/info/","title":"<code>pd.DataFrame.info</code>","text":"<p><code>pandas.DataFrame.info(verbose=None, buf=None, max_cols=None, memory_usage=None, show_counts=None, null_counts=None)</code></p>"},{"location":"api_docs/pandas/dataframe/info/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/dataframe/info/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [\"X\", \"Y\", \"Z\"], \"C\": [pd.Timedelta(10, unit=\"D\"), pd.Timedelta(10, unit=\"H\"), pd.Timedelta(10, unit=\"S\")]})\n...   return df.info()\n&gt;&gt;&gt; f()\n&lt;class 'DataFrameType'&gt;\nRangeIndexType(none): 3 entries, 0 to 2\nData columns (total 3 columns):\n#   Column  Non-Null Count  Dtype\n\n0  A       3 non-null      int64\n1  B       3 non-null      unicode_type\n2  C       3 non-null      timedelta64[ns]\ndtypes: int64(1), timedelta64[ns](1), unicode_type(1)\nmemory usage: 108.0 bytes\n</code></pre> <p>Note</p> <p>The exact output string may vary slightly from Pandas.</p>"},{"location":"api_docs/pandas/dataframe/insert/","title":"<code>pd.DataFrame.insert</code>","text":"<p><code>pandas.DataFrame.insert(loc, column, value, allow_duplicates=False)</code></p>"},{"location":"api_docs/pandas/dataframe/insert/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>loc</code>: constant integer</li> <li><code>column</code>: constant string</li> <li><code>value</code>: scalar, list/tuple, Pandas/Numpy array, Pandas index types, series</li> <li><code>allow_duplicates</code>: constant boolean</li> </ul>"},{"location":"api_docs/pandas/dataframe/insert/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   df.insert(3, \"D\", [-1,-2,-3])\n...   return df\n&gt;&gt;&gt; f()\n  A  B  C  D\n0  1  4  7 -1\n1  2  5  8 -2\n2  3  6  9 -3\n</code></pre>"},{"location":"api_docs/pandas/dataframe/isin/","title":"<code>pd.DataFrame.isin</code>","text":"<p><code>pandas.DataFrame.isin(values)</code></p>"},{"location":"api_docs/pandas/dataframe/isin/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>values</code>: DataFrame (must have same indices) + iterable type, Numpy array types, Pandas array types, List/Tuple, Pandas Index Types (excluding interval Index and MultiIndex)</li> </ul>"},{"location":"api_docs/pandas/dataframe/isin/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   isin_1 = df.isin([1,5,9])\n...   isin_2 = df.isin(pd.DataFrame({\"A\": [4,5,6], \"C\": [7,8,9]}))\n...   formated_out = \"\\n\".join([isin_1.to_string(), isin_2.to_string()])\n...   return formated_out\n&gt;&gt;&gt; f()\n      A      B      C\n0  True   False  False\n1  False  True   False\n2  False  False  True\n      A      B     C\n0  False  False  True\n1  False  False  True\n2  False  False  True\n</code></pre> <p>Note</p> <p><code>DataFrame.isin</code> ignores DataFrame indices. For example:</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.isin(pd.DataFrame({\"A\": [1,2,3]}, index=[\"A\", \"B\", \"C\"]))\n&gt;&gt;&gt; f()\n        A      B      C\n        0  True  False  False\n        1  True  False  False\n        2  True  False  False\n\n&gt;&gt;&gt; def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.isin(pd.DataFrame({\"A\": [1,2,3]}, index=[\"A\", \"B\", \"C\"]))\n&gt;&gt;&gt; f()\n        A      B      C\n        0  False  False  False\n        1  False  False  False\n        2  False  False  False\n</code></pre>"},{"location":"api_docs/pandas/dataframe/isna/","title":"<code>pd.DataFrame.isna</code>","text":"<p><code>pandas.DataFrame.isna()</code></p>"},{"location":"api_docs/pandas/dataframe/isna/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,None,3]})\n...   return df.isna()\n&gt;&gt;&gt; f()\n       A\n0  False\n1   True\n2  False\n</code></pre>"},{"location":"api_docs/pandas/dataframe/isnull/","title":"<code>pd.DataFrame.isnull</code>","text":"<p><code>pandas.DataFrame.isnull()</code></p>"},{"location":"api_docs/pandas/dataframe/isnull/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,None,3]})\n...   return df.isnull()\n&gt;&gt;&gt; f()\n       A\n0  False\n1   True\n2  False\n</code></pre>"},{"location":"api_docs/pandas/dataframe/itertuples/","title":"<code>pd.DataFrame.itertuples</code>","text":"<p><code>pandas.DataFrame.itertuples(index=True, name='Pandas')</code></p>"},{"location":"api_docs/pandas/dataframe/itertuples/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/dataframe/itertuples/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   for x in pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]}).itertuples():\n...      print(x)\n...      print(x[0])\n...      print(x[2:])\n&gt;&gt;&gt; f()\nPandas(Index=0, A=1, B=4, C=7)\n0\n(4, 7)\nPandas(Index=1, A=2, B=5, C=8)\n1\n(5, 8)\nPandas(Index=2, A=3, B=6, C=9)\n2\n(6, 9)\n</code></pre>"},{"location":"api_docs/pandas/dataframe/join/","title":"<code>pd.DataFrame.join</code>","text":"<p><code>pandas.DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)</code></p>"},{"location":"api_docs/pandas/dataframe/join/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>other</code>: DataFrame</li> <li><code>on</code>: constant string column name, constant list/tuple of column names</li> </ul>"},{"location":"api_docs/pandas/dataframe/join/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,1,3], \"B\": [4,5,6]})\n...   return df.join(on = \"A\", other=pd.DataFrame({\"C\": [-1,-2,-3], \"D\": [4,5,6]}))\n&gt;&gt;&gt; f()\n   A  B     C     D\n0  1  4    -2     5\n1  1  5    -2     5\n2  3  6  &lt;NA&gt;  &lt;NA&gt;\n</code></pre> <p>Note</p> <p>Joined dataframes cannot have common columns. The output dataframe is not sorted by default for better parallel performance</p>"},{"location":"api_docs/pandas/dataframe/last/","title":"<code>pd.DataFrame.last</code>","text":"<p><code>pandas.DataFrame.last(offset)</code></p>"},{"location":"api_docs/pandas/dataframe/last/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>offset</code>: String or Offset type<ul> <li>String argument must be a valid frequency alias</li> </ul> </li> </ul> <p>Note</p> <p>DataFrame must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted.</p>"},{"location":"api_docs/pandas/dataframe/last/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df, offset):\n...     return df.last(offset)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": np.arange(100), \"B\": np.arange(100, 200)}, index=pd.date_range(start='1/1/2022', end='12/31/2024', periods=100))\n&gt;&gt;&gt; f(df, \"2M\")\n                              A    B\n2024-11-05 16:43:38.181818176  94  194\n2024-11-16 18:10:54.545454544  95  195\n2024-11-27 19:38:10.909090912  96  196\n2024-12-08 21:05:27.272727264  97  197\n2024-12-19 22:32:43.636363632  98  198\n2024-12-31 00:00:00.000000000  99  199\n</code></pre>"},{"location":"api_docs/pandas/dataframe/mask/","title":"<code>pd.DataFrame.mask</code>","text":"<p><code>pandas.DataFrame.mask(cond, other=np.nan, inplace=False, axis=1, level=None, errors='raise', try_cast=NoDefault.no_default)</code></p>"},{"location":"api_docs/pandas/dataframe/mask/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>cond</code>: Boolean DataFrame,Boolean Series,Boolean Array</li> <li>If 1-dimensional array or Series is provided, equivalent to Pandas <code>df.mask</code> with <code>axis=1</code>.</li> <li><code>other</code>: Scalar, DataFrame, Series, 1 or 2-D Array</li> <li><code>None</code>, - Data types in <code>other</code> must match corresponding entries in DataFrame.</li> <li><code>None</code> or omitting argument defaults to the respective <code>NA</code> value for each type.</li> </ul> <p>Note</p> <p>DataFrame can contain categorical data if <code>other</code> is a scalar.</p>"},{"location":"api_docs/pandas/dataframe/mask/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df, cond, other):\n...   return df.mask(cond, other)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4.3, 2.4, 1.2]})\n&gt;&gt;&gt; cond = df &gt; 2\n&gt;&gt;&gt; other = df + 100\n&gt;&gt;&gt; f(df, cond, other)\n    A      B\n0    1  104.3\n1    2  102.4\n2  103    1.2\n</code></pre>"},{"location":"api_docs/pandas/dataframe/max/","title":"<code>pd.DataFrame.max</code>","text":"<p><code>pandas.DataFrame.max(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/dataframe/max/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>axis</code>: Integer (0 or 1)<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/max/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.max(axis=1)\n&gt;&gt;&gt; f()\n0    7\n1    8\n2    9\n</code></pre> <p>Note</p> <p>Only supported for dataframes containing float, non-null int, and datetime64ns values.</p>"},{"location":"api_docs/pandas/dataframe/mean/","title":"<code>pd.DataFrame.mean</code>","text":"<p><code>pandas.DataFrame.mean(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/dataframe/mean/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>axis</code>: Integer (0 or 1)<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/mean/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.mean(axis=1)\n&gt;&gt;&gt; f()\n0    4.0\n1    5.0\n2    6.0\n</code></pre> <p>Note</p> <p>Only supported for dataframes containing float, non-null int, and datetime64ns values.</p>"},{"location":"api_docs/pandas/dataframe/median/","title":"<code>pd.DataFrame.median</code>","text":"<p><code>pandas.DataFrame.median(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/dataframe/median/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>axis</code>: Integer (0 or 1)<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/median/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.median(axis=1)\n&gt;&gt;&gt; f()\n0    4.0\n1    5.0\n2    6.0\n</code></pre> <p>Note</p> <p>Only supported for dataframes containing float, non-null int, and datetime64ns values.</p>"},{"location":"api_docs/pandas/dataframe/melt/","title":"<code>pd.DataFrame.melt</code>","text":"<p><code>pandas.DataFrame.melt(id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None)</code></p>"},{"location":"api_docs/pandas/dataframe/melt/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>id_vars</code>: Constant Column label or list of labels</li> <li><code>value_vars</code>: Constant Column label or list of labels</li> </ul>"},{"location":"api_docs/pandas/dataframe/melt/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df, id_vars, value_vars):\n...   return df.melt(id_vars, value_vars)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": [\"a\", \"b\", \"c\"], 'B': [1, 3, 5], 'C': [2, 4, 6]})\n&gt;&gt;&gt; f(df, [\"A\"], [\"B\", \"C\"])\n    A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n3  a        C      2\n4  b        C      4\n5  c        C      6\n</code></pre> <p>Note</p> <p>To offer increased performance, row ordering and corresponding Index value may not match Pandas when run on multiple cores.</p>"},{"location":"api_docs/pandas/dataframe/memory_usage/","title":"<code>pd.DataFrame.memory_usage</code>","text":"<p><code>pandas.DataFrame.memory_usage(index=True, deep=False)</code></p>"},{"location":"api_docs/pandas/dataframe/memory_usage/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>index</code>: boolean</li> </ul>"},{"location":"api_docs/pandas/dataframe/memory_usage/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": np.array([1,2,3], dtype=np.int64), \"B\": np.array([1,2,3], dtype=np.int32), \"C\": [\"1\", \"2\", \"3456689\"]})\n...   return df.memory_usage()\n&gt;&gt;&gt; f()\nIndex    24\nA        24\nB        12\nC        42\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/dataframe/merge/","title":"<code>pd.DataFrame.merge</code>","text":"<p><code>pandas.DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)</code></p> <p>Note</p> <p>See <code>pd.merge</code> for full list of supported arguments, and more examples.</p>"},{"location":"api_docs/pandas/dataframe/merge/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,1,3], \"B\": [4,5,6]})\n...   return df.merge(pd.DataFrame({\"C\": [-1,-2,-3], \"D\": [4,4,6]}), left_on = \"B\", right_on = \"D\")\n&gt;&gt;&gt; f()\n   A  B  C  D\n0  1  4 -1  4\n1  1  4 -2  4\n2  3  6 -3  6\n</code></pre>"},{"location":"api_docs/pandas/dataframe/min/","title":"<code>pd.DataFrame.min</code>","text":"<p><code>pandas.DataFrame.min(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/dataframe/min/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>axis</code>: Integer (0 or 1)<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/min/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.min(axis=1)\n&gt;&gt;&gt; f()\n0    1\n1    2\n2    3\n</code></pre> <p>Note</p> <p>Only supported for dataframes containing float, non-null int, and datetime64ns values.</p>"},{"location":"api_docs/pandas/dataframe/ndim/","title":"<code>pd.DataFrame.ndim</code>","text":"<p><code>pandas.DataFrame.ndim</code></p>"},{"location":"api_docs/pandas/dataframe/ndim/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [\"X\", \"Y\", \"Z\"], \"C\": [pd.Timedelta(10, unit=\"D\"), pd.Timedelta(10, unit=\"H\"), pd.Timedelta(10, unit=\"S\")]})\n...   return df.ndim\n&gt;&gt;&gt; f()\n2\n</code></pre>"},{"location":"api_docs/pandas/dataframe/notna/","title":"<code>pd.DataFrame.notna</code>","text":"<p><code>pandas.DataFrame.notna()</code></p>"},{"location":"api_docs/pandas/dataframe/notna/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,None,3]})\n...   return df.notna()\n&gt;&gt;&gt; f()\n       A\n0   True\n1  False\n2   True\n</code></pre>"},{"location":"api_docs/pandas/dataframe/notnull/","title":"<code>pd.DataFrame.notnull</code>","text":"<p><code>pandas.DataFrame.notnull()</code></p>"},{"location":"api_docs/pandas/dataframe/notnull/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,None,3]})\n...   return df.notnull()\n&gt;&gt;&gt; f()\n       A\n0   True\n1  False\n2   True\n</code></pre>"},{"location":"api_docs/pandas/dataframe/nunique/","title":"<code>pd.DataFrame.nunique</code>","text":"<p><code>pandas.DataFrame.nunique(axis=0, dropna=True)</code></p>"},{"location":"api_docs/pandas/dataframe/nunique/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>dropna</code>: boolean</li> </ul>"},{"location":"api_docs/pandas/dataframe/nunique/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [1,1,1], \"C\": [4, None, 6]})\n...   return df.nunique()\n&gt;&gt;&gt; f()\nA    3\nB    1\nC    2\n</code></pre>"},{"location":"api_docs/pandas/dataframe/pct_change/","title":"<code>pd.DataFrame.pct_change</code>","text":"<p><code>pandas.DataFrame.pct_change(periods=1, fill_method='pad', limit=None, freq=None)</code></p>"},{"location":"api_docs/pandas/dataframe/pct_change/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>periods</code>: Integer</li> </ul>"},{"location":"api_docs/pandas/dataframe/pct_change/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [10,100,1000,10000]})\n...   return df.pct_change()\n&gt;&gt;&gt; f()\n    A\n0  NaN\n1  9.0\n2  9.0\n3  9.0\n</code></pre>"},{"location":"api_docs/pandas/dataframe/pipe/","title":"<code>pd.DataFrame.pipe</code>","text":"<ul> <li>pandas.DataFrame.pipe(func, *args, **kwargs)</li> </ul>"},{"location":"api_docs/pandas/dataframe/pipe/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>func</code>: JIT function or callable defined within a JIT function.<ul> <li>Additional arguments for <code>func</code> can be passed as additional arguments.</li> </ul> </li> </ul> <p>Note</p> <p><code>func</code> cannot be a tuple</p>"},{"location":"api_docs/pandas/dataframe/pipe/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   def g(df, axis):\n...       return df.max(axis)\n...   df = pd.DataFrame({\"A\": [10,100,1000,10000]})\n...   return df.pipe(g, axis=0)\n...\n&gt;&gt;&gt; f()\nA    10000\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/dataframe/pivot/","title":"<code>pd.DataFrame.pivot</code>","text":"<p><code>pandas.DataFrame.pivot(values=None, index=None, columns=None)</code></p>"},{"location":"api_docs/pandas/dataframe/pivot/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>values</code>: Constant Column Label or list of labels</li> <li><code>index</code>: Constant Column Label or list of labels</li> <li><code>columns</code>: Constant Column Label</li> </ul> <p>Note</p> <p>The the number of columns and names of the output DataFrame won't be known   at compile time. To update typing information on DataFrame you should pass it back to Python.</p>"},{"location":"api_docs/pandas/dataframe/pivot/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [\"X\",\"X\",\"X\",\"X\",\"Y\",\"Y\"], \"B\": [1,2,3,4,5,6], \"C\": [10,11,12,20,21,22]})\n...   pivoted_tbl = df.pivot(columns=\"A\", index=\"B\", values=\"C\")\n...   return pivoted_tbl\n&gt;&gt;&gt; f()\nA     X     Y\nB\n1  10.0   NaN\n2  11.0   NaN\n3  12.0   NaN\n4  20.0   NaN\n5   NaN  21.0\n6   NaN  22.0\n</code></pre>"},{"location":"api_docs/pandas/dataframe/pivot_table/","title":"<code>pd.DataFrame.pivot_table</code>","text":"<p><code>pandas.DataFrame.pivot_table(values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True)</code></p>"},{"location":"api_docs/pandas/dataframe/pivot_table/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>values</code>: Constant Column Label or list of labels</li> <li><code>index</code>: Constant Column Label or list of labels</li> <li><code>columns</code>: Constant Column Label</li> <li><code>aggfunc</code>: String Constant</li> </ul> <p>Note</p> <p>This code takes two different paths depending on if pivot values are annotated. When   pivot values are annotated then output columns are set to the annotated values.   For example, <code>@bodo.jit(pivots={'pt': ['small', 'large']})</code>   declares the output pivot table <code>pt</code> will have columns called <code>small</code> and <code>large</code>.</p> <p>If pivot values are not annotated, then the number of columns and names of the output DataFrame won't be known   at compile time. To update typing information on DataFrame you should pass it back to Python.</p>"},{"location":"api_docs/pandas/dataframe/pivot_table/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit(pivots={'pivoted_tbl': ['X', 'Y']})\n... def f():\n...   df = pd.DataFrame({\"A\": [\"X\",\"X\",\"X\",\"X\",\"Y\",\"Y\"], \"B\": [1,2,3,4,5,6], \"C\": [10,11,12,20,21,22]})\n...   pivoted_tbl = df.pivot_table(columns=\"A\", index=\"B\", values=\"C\", aggfunc=\"mean\")\n...   return pivoted_tbl\n&gt;&gt;&gt; f()\n      X     Y\nB\n1  10.0   NaN\n2  11.0   NaN\n3  12.0   NaN\n4  20.0   NaN\n5   NaN  21.0\n6   NaN  22.0\n</code></pre>"},{"location":"api_docs/pandas/dataframe/plot/","title":"<code>pd.DataFrame.plot</code>","text":"<p><code>pandas.DataFrame.plot(x=None, y=None, kind=\"line\", figsize=None, xlabel=None, ylabel=None, title=None, legend=True, fontsize=None, xticks=None, yticks=None, ax=None)</code></p>"},{"location":"api_docs/pandas/dataframe/plot/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>x</code>: Constant String column name, Constant integer</li> <li><code>y</code>: Constant String column name, Constant integer</li> <li><code>kind</code>: constant String (\"line\" or \"scatter\")</li> <li><code>figsize</code>: constant numeric tuple (width, height)</li> <li><code>xlabel</code>: constant String</li> <li><code>ylabel</code>: constant String</li> <li><code>title</code>: constant String</li> <li><code>legend</code>: boolean</li> <li><code>fontsize</code>: integer</li> <li><code>xticks</code>: Constant Tuple</li> <li><code>yticks</code>: Constant Tuple</li> <li><code>ax</code>: Matplotlib Axes Object</li> </ul>"},{"location":"api_docs/pandas/dataframe/prod/","title":"<code>pd.DataFrame.prod</code>","text":"<p><code>pandas.DataFrame.prod(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/dataframe/prod/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>axis</code>: Integer (0 or 1)<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/prod/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.prod(axis=1)\n&gt;&gt;&gt; f()\nA      6\nB    120\nC    504\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/dataframe/product/","title":"<code>pd.DataFrame.product</code>","text":"<p><code>pandas.DataFrame.product(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/dataframe/product/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>axis</code>: Integer (0 or 1)<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/product/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.product(axis=1)\n&gt;&gt;&gt; f()\nA      6\nB    120\nC    504\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/dataframe/quantile/","title":"<code>pd.DataFrame.quantile</code>","text":"<p><code>pandas.DataFrame.quantile(q=0.5, axis=0, numeric_only=True, interpolation='linear')</code></p>"},{"location":"api_docs/pandas/dataframe/quantile/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>q</code>: Float or Int<ul> <li>must be 0&lt;= q &lt;= 1</li> </ul> </li> <li><code>axis</code>: Integer (0 or 1)<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/quantile/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.quantile()\n&gt;&gt;&gt; f()\nA    2.0\nB    5.0\nC    8.0\ndtype: float64\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/dataframe/query/","title":"<code>pd.DataFrame.query</code>","text":"<ul> <li>pandas.DataFrame.query(expr, inplace=False, **kwargs)</li> </ul>"},{"location":"api_docs/pandas/dataframe/query/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>expr</code>:  Constant String</li> </ul>"},{"location":"api_docs/pandas/dataframe/query/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(a):\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.query('A &gt; @a')\n&gt;&gt;&gt; f(1)\n   A  B  C\n1  2  5  8\n2  3  6  9\n</code></pre> <p>Note</p> <ul> <li>The output of the query must evaluate to a 1d boolean array.</li> <li>Cannot refer to the index by name in the query string.</li> <li>Query must be one line.</li> <li>If using environment variables, they should be passed as arguments to the function.</li> </ul>"},{"location":"api_docs/pandas/dataframe/rank/","title":"<code>pd.DataFrame.rank</code>","text":"<ul> <li><code>pandas.DataFrame.rank(axis=0, method='average', numeric_only=NoDefault.no_default, na_option='keep', ascending=True, pct=False)</code></li> </ul>"},{"location":"api_docs/pandas/dataframe/rank/#supported-arguments","title":"Supported Arguments","text":"<p>+-----------------------------+------------------------------------------------------------+ | argument                    | datatypes                                                  | | <code>method</code>                    | -   String in {'average', 'min', 'max', 'first', 'dense'}  | +-----------------------------+------------------------------------------------------------+ | <code>na_option</code>                 | -   String in {'keep', 'top', 'bottom'}                    | +-----------------------------+------------------------------------------------------------+ | <code>ascending</code>                 | -   Boolean                                                | +-----------------------------+------------------------------------------------------------+ | <code>pct</code>                       | -   Boolean                                                | +-----------------------------+------------------------------------------------------------+</p> <p>Note</p> <ul> <li>Using <code>method='first'</code>  with <code>ascending=False</code> is currently unsupported.</li> </ul>"},{"location":"api_docs/pandas/dataframe/rank/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.rank(method='dense', na_option='keep', pct=True)\n&gt;&gt;&gt; df = pd.DataFrame('A': [np.nan, 4, 2, 4, 8, np.nan])\n&gt;&gt;&gt; f(df)\n    A    B\n0  NaN  0.5\n1  1.0  1.0\n2  0.5  1.0\n3  1.0  NaN\n</code></pre>"},{"location":"api_docs/pandas/dataframe/rename/","title":"<code>pd.DataFrame.rename</code>","text":"<p><code>pandas.DataFrame.rename(mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors='ignore')</code></p>"},{"location":"api_docs/pandas/dataframe/rename/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>mapper</code>: must be constant dictionary.<ul> <li>Can only be used alongside axis=1</li> </ul> </li> <li><code>columns</code>: must be constant dictionary</li> <li><code>axis</code>: Integer<ul> <li>Can only be used alongside mapper argument</li> </ul> </li> <li><code>copy</code>: boolean</li> <li><code>inplace</code>:  must be constant boolean</li> </ul>"},{"location":"api_docs/pandas/dataframe/rename/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.rename(columns={\"A\": \"X\", \"B\":\"Y\", \"C\":\"Z\"})\n&gt;&gt;&gt; f()\n   X  Y  Z\n0  1  4  7\n1  2  5  8\n2  3  6  9\n</code></pre>"},{"location":"api_docs/pandas/dataframe/replace/","title":"<code>pd.DataFrame.replace</code>","text":"<p><code>pandas.DataFrame.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad')</code></p>"},{"location":"api_docs/pandas/dataframe/replace/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>to_replace</code>: various scalars<ul> <li>Required argument</li> </ul> </li> <li><code>value</code>: various scalars<ul> <li>Must be of the same type as to_replace</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/replace/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.replace(1, -1)\n&gt;&gt;&gt; f()\n   A  B  C\n0 -1  4  7\n1  2  5  8\n2  3  6  9\n</code></pre>"},{"location":"api_docs/pandas/dataframe/reset_index/","title":"<code>pd.DataFrame.reset_index</code>","text":"<p><code>pandas.DataFrame.reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill='')</code></p>"},{"location":"api_docs/pandas/dataframe/reset_index/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>level</code>: Integer<ul> <li>If specified, must drop all levels.</li> </ul> </li> <li><code>drop</code>: Constant boolean</li> <li><code>inplace</code>: Constant boolean</li> </ul>"},{"location":"api_docs/pandas/dataframe/reset_index/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]}, index = [\"X\", \"Y\", \"Z\"])\n...   return df.reset_index()\n&gt;&gt;&gt; f()\n  index  A  B  C\n0     X  1  4  7\n1     Y  2  5  8\n2     Z  3  6  9\n</code></pre>"},{"location":"api_docs/pandas/dataframe/rolling/","title":"<code>pd.DataFrame.rolling</code>","text":"<p><code>pandas.DataFrame.rolling(window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None, method='single')</code></p>"},{"location":"api_docs/pandas/dataframe/rolling/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>window</code>: Integer, String (must be parsable as a time offset),<code>datetime.timedelta</code> ,pd.Timedelta`, List/Tuple of column labels</li> <li><code>min_periods</code>: Integer</li> <li><code>center</code>: boolean</li> <li><code>on</code>: Scalar column label<ul> <li>Must be constant at Compile Time</li> </ul> </li> <li><code>dropna</code>:boolean<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/rolling/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3,4,5]})\n...   return df.rolling(3,center=True).mean()\n&gt;&gt;&gt; f()\n     A\n0  NaN\n1  2.0\n2  3.0\n3  4.0\n4  NaN\n</code></pre> <p>For more information, please see the Window section.</p>"},{"location":"api_docs/pandas/dataframe/sample/","title":"<code>pd.DataFrame.sample</code>","text":"<p><code>pandas.DataFrame.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)</code></p>"},{"location":"api_docs/pandas/dataframe/sample/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>n</code>: Integer</li> <li><code>frac</code>: Float</li> <li><code>replace</code>: boolean</li> </ul>"},{"location":"api_docs/pandas/dataframe/sample/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.sample(1)\n&gt;&gt;&gt; f()\n   A  B  C\n2  3  6  9\n</code></pre>"},{"location":"api_docs/pandas/dataframe/select_dtypes/","title":"<code>pd.DataFrame.select_dtypes</code>","text":"<p><code>pandas.DataFrame.select_dtypes(include=None, exclude=None)</code></p>"},{"location":"api_docs/pandas/dataframe/select_dtypes/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>include</code>: string, type, List or tuple of string/type<ul> <li>Must be constant at Compile Time</li> </ul> </li> <li><code>exclude</code>: string, type, List or tuple of string/type<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/select_dtypes/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df= pd.DataFrame({\"A\": [1], \"B\": [\"X\"], \"C\": [pd.Timedelta(10, unit=\"D\")], \"D\": [True], \"E\": [3.1]})\n...   out_1 = df_l.select_dtypes(exclude=[np.float64, \"bool\"])\n...   out_2 = df_l.select_dtypes(include=\"int\")\n...   out_3 = df_l.select_dtypes(include=np.bool_, exclude=(np.int64, \"timedelta64[ns]\"))\n...   formated_out = \"\\n\".join([out_1.to_string(), out_2.to_string(), out_3.to_string()])\n...   return formated_out\n&gt;&gt;&gt; f()\n   A  B       C\n0  1  X 10 days\n  A\n0  1\n      D\n0  True\n</code></pre>"},{"location":"api_docs/pandas/dataframe/set_index/","title":"<code>pd.DataFrame.set_index</code>","text":"<p><code>pandas.DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)</code></p>"},{"location":"api_docs/pandas/dataframe/set_index/#supported-arguments","title":"Supported Arguments","text":"<ul> <li>keys: must be a constant string</li> </ul>"},{"location":"api_docs/pandas/dataframe/set_index/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]}, index = [\"X\", \"Y\", \"Z\"])\n...   return df.set_index(\"C\")\n&gt;&gt;&gt; f()\n   A  B\nC\n7  1  4\n8  2  5\n9  3  6\n</code></pre>"},{"location":"api_docs/pandas/dataframe/shape/","title":"<code>pd.DataFrame.shape</code>","text":"<p><code>pandas.DataFrame.shape</code></p>"},{"location":"api_docs/pandas/dataframe/shape/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [3,4,5]})\n...   return df.shape\n&gt;&gt;&gt; f()\n(3, 2)\n</code></pre>"},{"location":"api_docs/pandas/dataframe/shift/","title":"<code>pd.DataFrame.shift</code>","text":"<p><code>pandas.DataFrame.shift(periods=1, freq=None, axis=0, fill_value=NoDefault.no_default)</code></p>"},{"location":"api_docs/pandas/dataframe/shift/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>periods</code>: Integer</li> </ul>"},{"location":"api_docs/pandas/dataframe/shift/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,1,3], \"B\": [4,5,6]})\n...   return df.shift(1)\n&gt;&gt;&gt; f()\n     A    B\n0  NaN  NaN\n1  1.0  4.0\n2  1.0  5.0\n</code></pre> <p>Note</p> <p>Only supported for dataframes containing numeric, boolean, datetime.date and string types.</p>"},{"location":"api_docs/pandas/dataframe/size/","title":"<code>pd.DataFrame.size</code>","text":"<p><code>pandas.DataFrame.size</code></p> <pre><code>### Example Usage\n\n```py\n\n&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [3,4,5]})\n...   return df.size\n&gt;&gt;&gt; f()\n6\n```\n</code></pre>"},{"location":"api_docs/pandas/dataframe/sort_index/","title":"<code>pd.DataFrame.sort_index</code>","text":"<p><code>pandas.DataFrame.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None)</code></p>"},{"location":"api_docs/pandas/dataframe/sort_index/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>ascending</code>: boolean</li> <li><code>na_position</code>:constant String (\"first\" or \"last\")</li> </ul>"},{"location":"api_docs/pandas/dataframe/sort_index/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3]}, index=[1,None,3])\n...   return df.sort_index(ascending=False, na_position=\"last\")\n&gt;&gt;&gt; f()\n     A\n3    3\n1    1\nNaN  2\n</code></pre>"},{"location":"api_docs/pandas/dataframe/sort_values/","title":"<code>pd.DataFrame.sort_values</code>","text":"<p><code>pandas.DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)</code></p>"},{"location":"api_docs/pandas/dataframe/sort_values/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>by</code>: constant String or constant list of strings</li> <li><code>ascending</code>: boolean, list/tuple of boolean, with length equal to the number of key columns</li> <li><code>inplace</code>: Constant boolean</li> <li><code>na_position</code>: constant String (\"first\" or \"last\"), constant list/tuple of String, with length equal to the number of key columns</li> </ul>"},{"location":"api_docs/pandas/dataframe/sort_values/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,2,None], \"B\": [4, 5, 6, None]})\n...   df.sort_values(by=[\"A\", \"B\"], ascending=[True, False], na_position=[\"first\", \"last\"], inplace=True)\n...   return df\n&gt;&gt;&gt; f()\n      A     B\n3  &lt;NA&gt;  &lt;NA&gt;\n0     1     4\n2     2     6\n1     2     5\n</code></pre>"},{"location":"api_docs/pandas/dataframe/std/","title":"<code>pd.DataFrame.std</code>","text":"<p><code>pandas.DataFrame.std(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/dataframe/std/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>axis</code>: Integer (0 or 1)</li> <li>Must be constant at Compile Time</li> </ul>"},{"location":"api_docs/pandas/dataframe/std/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.std(axis=1)\n&gt;&gt;&gt; f()\n0    3.0\n1    3.0\n2    3.0\ndtype: float64\n</code></pre>"},{"location":"api_docs/pandas/dataframe/sum/","title":"<code>pd.DataFrame.sum</code>","text":"<p><code>pandas.DataFrame.sum(axis=None, skipna=None, level=None, numeric_only=None, min_count=0)</code></p>"},{"location":"api_docs/pandas/dataframe/sum/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>axis</code>: Integer (0 or 1)<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/sum/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.sum(axis=1)\n&gt;&gt;&gt; f()\n0    12\n1    15\n2    18\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/dataframe/tail/","title":"<code>pd.DataFrame.tail</code>","text":"<p><code>pandas.DataFrame.tail(n=5)</code></p>"},{"location":"api_docs/pandas/dataframe/tail/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>n</code>: Integer</li> </ul>"},{"location":"api_docs/pandas/dataframe/tail/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.DataFrame({\"A\": np.arange(1000)}).tail(3)\n&gt;&gt;&gt; f()\n      A\n997  997\n998  998\n999  999\n</code></pre>"},{"location":"api_docs/pandas/dataframe/take/","title":"<code>pd.DataFrame.take</code>","text":"<p><code>pandas.DataFrame.take(indices, axis=0, is_copy=None)</code></p>"},{"location":"api_docs/pandas/dataframe/take/#supported-arguments","title":"Supported Arguments","text":"<ul> <li>indices: scalar Integer, Pandas Integer Array, Numpy Integer Array, Integer Series</li> </ul>"},{"location":"api_docs/pandas/dataframe/take/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.take(pd.Series([-1,-2]))\n&gt;&gt;&gt; f()\n   A  B  C\n2  3  6  9\n1  2  5  8\n</code></pre>"},{"location":"api_docs/pandas/dataframe/to_csv/","title":"<code>pd.DataFrame.to_csv</code>","text":"<p><code>pandas.DataFrame.to_csv</code></p> <ul> <li><code>compression</code> argument defaults to <code>None</code> in JIT code. This is the only supported value of this argument.</li> <li><code>mode</code> argument supports only the default value <code>\"w\"</code>.</li> <li><code>errors</code> argument supports only the default value <code>strict</code>.</li> <li><code>storage_options</code> argument supports only the default value <code>None</code>.</li> </ul>"},{"location":"api_docs/pandas/dataframe/to_json/","title":"<code>pd.DataFrame.to_json</code>","text":"<p><code>pandas.DataFrame.to_json</code></p>"},{"location":"api_docs/pandas/dataframe/to_numpy/","title":"<code>pd.DataFrame.to_numpy</code>","text":"<p><code>pandas.DataFrame.to_numpy(dtype=None, copy=False, na_value=NoDefault.no_default)</code></p>"},{"location":"api_docs/pandas/dataframe/to_numpy/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>copy</code>: boolean</li> </ul>"},{"location":"api_docs/pandas/dataframe/to_numpy/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [3.1,4.2,5.3]})\n...   return df.to_numpy()\n&gt;&gt;&gt; f()\n[[1.  3.1]\n [2.  4.2]\n [3.  5.3]]\n</code></pre>"},{"location":"api_docs/pandas/dataframe/to_parquet/","title":"<code>pd.DataFrame.to_parquet</code>","text":"<p><code>pandas.DataFrame.to_parquet(path, engine='auto', compression='snappy', index=None, partition_cols=None, storage_options=None)</code></p>"},{"location":"api_docs/pandas/dataframe/to_parquet/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>path</code> is a required argument and must be a string. When writing distributed dataframes, the path refers to a directory of parquet files.</li> <li><code>engine</code> argument only supports <code>\"auto\"</code> and <code>\"pyarrow\"</code>. Default: <code>\"auto\"</code> which uses the pyarrow engine.</li> <li><code>compression</code> argument must be one of: <code>\"snappy\"</code>, <code>\"gzip\"</code>, <code>\"brotli\"</code>, <code>None</code>. Default: <code>\"snappy\"</code>.</li> <li><code>index</code> argument must be a constant bool or <code>None</code>. Default: <code>None</code>.</li> <li><code>partition_cols</code> argument is supported in most cases, except when the columns in the DataFrame cannot be determined at compile time. This must be a list of column names or <code>None</code>. Default: <code>None</code>.</li> <li><code>storage_options</code> argument supports only the default value <code>None</code>.</li> <li><code>row_group_size</code> argument can be used to specify the maximum size of the row-groups in the generated parquet files; the actual size of the written row-groups may be smaller then this value. This must be an integer. If not specified, Bodo writes row-groups with 1M rows.</li> </ul> <p>Note</p> <p>Bodo writes multiple files in parallel (one per core), and the total number of row-groups across all files is roughly <code>max(num_cores, total_rows / row_group_size)</code>. The size of the row groups can affect read performance significantly. In general, the dataset should have at least as many row-groups as the number of cores used for reading, but ideally a lot more. At the same time, the row-groups shouldn't be too small since this can lead to overheads at read time. For more details, refer to the parquet file format.</p>"},{"location":"api_docs/pandas/dataframe/to_parquet/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,1,3], \"B\": [4,5,6]})\n...   df.to_parquet(\"dataset.pq\")\n&gt;&gt;&gt; f()\n</code></pre>"},{"location":"api_docs/pandas/dataframe/to_sql/","title":"<code>pd.DataFrame.to_sql</code>","text":"<p><code>pandas.DataFrame.to_sql</code></p> <ul> <li>See Example Usage and more system specific instructions.</li> <li>Argument <code>con</code> is supported but only as a string form. SQLalchemy <code>connectable</code> is not supported.</li> <li>Argument <code>name</code>, <code>schema</code>, <code>if_exists</code>, <code>index</code>, <code>index_label</code>, <code>dtype</code>, <code>method</code> are supported.</li> <li>Argument <code>chunksize</code> is not supported.</li> </ul>"},{"location":"api_docs/pandas/dataframe/to_string/","title":"<code>pd.DataFrame.to_string</code>","text":"<ul> <li><code>pandas.DataFrame.to_string(buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, min_rows=None, max_cols=None, show_dimensions=False, decimal='.', line_width=None, max_colwidth=None, encoding=None)</code></li> </ul> <p>### Supported Arguments</p> <ul> <li><code>buf</code></li> <li><code>columns</code></li> <li><code>col_space</code></li> <li><code>header</code></li> <li><code>index</code></li> <li><code>na_rep</code></li> <li><code>formatters</code></li> <li><code>float_format</code></li> <li><code>sparsify</code></li> <li><code>index_names</code></li> <li><code>justify</code></li> <li><code>max_rows</code></li> <li><code>min_rows</code></li> <li><code>max_cols</code></li> <li><code>how_dimensions</code></li> <li><code>decimal</code></li> <li><code>line_width</code></li> <li><code>max_colwidth</code></li> <li><code>encoding</code></li> </ul> <p>### Example Usage</p> <p><pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3]})\n...   return df.to_string()\n&gt;&gt;&gt; f()\n   A\n0  1\n1  2\n2  3\n</code></pre>  !!! note     * This function is not optimized.     * When called on a distributed dataframe, the string returned for each rank will be reflective of the dataframe for that rank.</p>"},{"location":"api_docs/pandas/dataframe/values/","title":"<code>pd.DataFrame.values</code>","text":"<p><code>pandas.DataFrame.values</code> (only for numeric dataframes)</p>"},{"location":"api_docs/pandas/dataframe/values/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [3.1,4.2,5.3]})\n...   return df.values\n&gt;&gt;&gt; f()\n[[1.  3.1]\n [2.  4.2]\n [3.  5.3]]\n</code></pre>"},{"location":"api_docs/pandas/dataframe/var/","title":"<code>pd.DataFrame.var</code>","text":"<p><code>pandas.DataFrame.var(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/dataframe/var/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>axis</code>: Integer (0 or 1)<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/dataframe/var/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n...   return df.var(axis=1)\n&gt;&gt;&gt; f()\n0    9.0\n1    9.0\n2    9.0\ndtype: float64\n</code></pre>"},{"location":"api_docs/pandas/dataframe/where/","title":"pd.DataFrame.where","text":"<p><code>pandas.DataFrame.where(cond, other=np.nan, inplace=False, axis=1, level=None, errors='raise', try_cast=NoDefault.no_default)</code></p>"},{"location":"api_docs/pandas/dataframe/where/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>cond</code>: Boolean DataFrame, Boolean Series, Boolean Array<ul> <li>If 1-dimensional array or Series is provided, equivalent to Pandas <code>df.where</code> with <code>axis=1</code>.</li> </ul> </li> <li><code>other</code>: Scalar, DataFrame, Series, 1 or 2-D Array, <code>None</code><ul> <li>Data types in <code>other</code> must match corresponding entries in DataFrame.</li> <li><code>None</code> or omitting argument defaults to the respective <code>NA</code> value for each type.</li> </ul> </li> </ul> <p>Note</p> <p>DataFrame can contain categorical data if <code>other</code> is a scalar.</p>"},{"location":"api_docs/pandas/dataframe/where/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df, cond, other):\n...   return df.where(cond, other)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4.3, 2.4, 1.2]})\n&gt;&gt;&gt; cond = df &gt; 2\n&gt;&gt;&gt; other = df + 100\n&gt;&gt;&gt; f(df, cond, other)\n     A      B\n0  101    4.3\n1  102    2.4\n2    3  101.2\n</code></pre>"},{"location":"api_docs/pandas/dateoffsets/","title":"Date Offsets","text":"<p>Bodo supports a subset of the offset types in <code>pandas.tseries.offsets</code>:</p>"},{"location":"api_docs/pandas/dateoffsets/#dateoffset","title":"DateOffset","text":"<ul> <li><code>pd.tseries.offsets.DateOffset</code></li> <li><code>pd.tseries.offsets.MonthBegin</code> </li> <li><code>pd.tseries.offsets.MonthEnd</code> </li> <li><code>pd.tseries.offsets.Week</code> </li> </ul>"},{"location":"api_docs/pandas/dateoffsets/#properties","title":"Properties","text":"<ul> <li>pd.tseries.offsets.DateOffset.normalize` </li> <li><code>pd.tseries.offsets.DateOffset.n</code> </li> </ul>"},{"location":"api_docs/pandas/dateoffsets/#binary-operations","title":"Binary Operations","text":"<p>For all offsets, addition and subtraction with a scalar <code>datetime.date</code>, <code>datetime.datetime</code> or <code>pandas.Timestamp</code> is supported. Multiplication is also supported with a scalar integer.</p>"},{"location":"api_docs/pandas/dateoffsets/dateoffset/","title":"<code>pd.tseries.offsets.DateOffset</code>","text":"<p><code>pandas.tseries.offsets.DateOffset(n=1, normalize=False, years=None, months=None, weeks=None, days=None, hours=None, minutes=None, seconds=None, microseconds=None, nanoseconds=None, year=None, month=None, day=None, weekday=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None)</code></p>"},{"location":"api_docs/pandas/dateoffsets/dateoffset/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>n</code>: integer</li> <li><code>normalize</code>: boolean</li> <li><code>years</code>: integer</li> <li><code>months</code>: integer</li> <li><code>weeks</code>: integer</li> <li><code>days</code>: integer</li> <li><code>hours</code>:  integer</li> <li><code>minutes</code>: integer</li> <li><code>seconds</code>:  integer</li> <li><code>microseconds</code>:  integer</li> <li><code>nanoseconds</code>: integer</li> <li><code>year</code>:  integer</li> <li><code>month</code>:  integer</li> <li><code>weekday</code>: integer</li> <li><code>day</code>: integer</li> <li><code>hour</code>: integer</li> <li><code>minute</code>: integer</li> <li><code>second</code>: integer</li> <li><code>microsecond</code>: integer</li> <li><code>nanosecond</code>: integer</li> </ul>"},{"location":"api_docs/pandas/dateoffsets/dateoffset/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def f(ts):\n...     return ts + pd.tseries.offsets.DateOffset(n=4, normalize=True, weeks=11, hour=2)\n&gt;&gt;&gt; ts = pd.Timestamp(year=2020, month=10, day=30, hour=22)\n&gt;&gt;&gt; f(ts)\n\nTimestamp('2021-09-03 02:00:00')\n</code></pre>"},{"location":"api_docs/pandas/dateoffsets/monthbegin/","title":"<code>pd.tseries.offsets.MonthBegin</code>","text":"<p><code>pandas.tseries.offsets.MonthBegin(n=1, normalize=False)</code></p>"},{"location":"api_docs/pandas/dateoffsets/monthbegin/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>n</code>: integer</li> <li><code>normalize</code>: boolean</li> </ul>"},{"location":"api_docs/pandas/dateoffsets/monthbegin/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def f(ts):\n...     return ts + pd.tseries.offsets.MonthBegin(n=4, normalize=True)\n&gt;&gt;&gt; ts = pd.Timestamp(year=2020, month=10, day=30, hour=22)\n&gt;&gt;&gt; f(ts)\n\nTimestamp('2021-02-01 00:00:00')\n</code></pre>"},{"location":"api_docs/pandas/dateoffsets/monthend/","title":"<code>pd.tseries.offsets.MonthEnd</code>","text":"<p><code>pandas.tseries.offsets.MonthEnd(n=1, normalize=False)</code></p>"},{"location":"api_docs/pandas/dateoffsets/monthend/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>n</code>: integer</li> <li><code>normalize</code>: boolean</li> </ul>"},{"location":"api_docs/pandas/dateoffsets/monthend/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def f(ts):\n...     return ts + pd.tseries.offsets.MonthEnd(n=4, normalize=False)\n&gt;&gt;&gt; ts = pd.Timestamp(year=2020, month=10, day=30, hour=22)\n&gt;&gt;&gt; f(ts)\n\nTimestamp('2021-01-31 22:00:00')\n</code></pre>"},{"location":"api_docs/pandas/dateoffsets/n/","title":"<code>pd.tseries.offsets.DateOffset.n</code>","text":"<p><code>pandas.tseries.offsets.DateOffset.n</code></p>"},{"location":"api_docs/pandas/dateoffsets/normalize/","title":"pd.tseries.offsets.DateOffset.normalize`","text":"<p><code>pandas.tseries.offsets.DateOffset.normalize</code></p>"},{"location":"api_docs/pandas/dateoffsets/week/","title":"<code>pd.tseries.offsets.Week</code>","text":"<p><code>pandas.tseries.offsets.Week(n=1, normalize=False, weekday=None)</code></p>"},{"location":"api_docs/pandas/dateoffsets/week/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>n</code>: integer</li> <li><code>normalize</code>: boolean</li> <li><code>weekday</code>: integer</li> </ul>"},{"location":"api_docs/pandas/dateoffsets/week/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n&gt;&gt;&gt; def f(ts):\n...     return ts + pd.tseries.offsets.Week(n=4, normalize=True, weekday=5)\n&gt;&gt;&gt; ts = pd.Timestamp(year=2020, month=10, day=30, hour=22)\n&gt;&gt;&gt; f(ts)\n\nTimestamp('2020-11-21 00:00:00')\n</code></pre>"},{"location":"api_docs/pandas/general_functions/","title":"General Functions","text":"<p>General functions are the most commonly used functions in Pandas. They include functions for data manipulation, data cleaning, data merging, and more.</p>"},{"location":"api_docs/pandas/general_functions/#data-manipulations","title":"Data Manipulations","text":"Function Description pd.concat Concatenate pandas objects along a particular axis with optional set logic along the other axes. pd.crosstab Compute a simple cross-tabulation of two (or more) factors. pd.cut Bin values into discrete intervals. pd.qcut Quantile-based discretization function. pd.get_dummies Convert categorical variable into dummy/indicator variables. pd.merge Merge DataFrame or named Series objects with a database-style join. pd.pivot Reshape data (produce a \u201cpivot\u201d table) based on column values. pd.pivot_table Create a spreadsheet-style pivot table as a DataFrame. pd.unique Hash table-based unique."},{"location":"api_docs/pandas/general_functions/#top-level-missing-data","title":"Top Level Missing Data","text":"Function Description pd.isna Detect missing values. pd.notna Detect existing (non-missing) values. pd.isnull Detect missing values. pd.notnull Detect existing (non-missing) values."},{"location":"api_docs/pandas/general_functions/#top-level-conversions","title":"Top Level Conversions","text":"Function Description pd.to_numeric Convert argument to a numeric type."},{"location":"api_docs/pandas/general_functions/concat/","title":"<code>pd.concat</code>","text":"<p><code>pandas.concat(objs, axis=0, join=\"outer\", join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=None, copy=True)</code></p>"},{"location":"api_docs/pandas/general_functions/concat/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>objs</code> List or Tuple of DataFrames/Series <code>axis</code> Integer with either 0 or 1 <ul><li> Must be constant at  Compile Time </li></ul> <code>ignore_index</code> Boolean <ul><li> Must be constant at  Compile Time </li></ul> <p>Important</p> <p>Bodo currently concatenates local data chunks for distributed datasets, which does not preserve global order of concatenated objects in output.</p>"},{"location":"api_docs/pandas/general_functions/concat/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df1, df2):\n...     return pd.concat([df1, df2], axis=1)\n\n&gt;&gt;&gt; df1 = pd.DataFrame({\"A\": [3, 2, 1, -4, 7]})\n&gt;&gt;&gt; df2 = pd.DataFrame({\"B\": [3, 25, 1, -4, -24]})\n&gt;&gt;&gt; f(df1, df2)\n\nA   B\n0  3   3\n1  2  25\n2  1   1\n3 -4  -4\n4  7 -24\n</code></pre>"},{"location":"api_docs/pandas/general_functions/crosstab/","title":"<code>pd.crosstab</code>","text":"<p><code>pandas.crosstab(index, columns, values=None, rownames=None, colnames=None, aggfunc=None, margins=False, margins_name='All', dropna=True, normalize=False)</code></p>"},{"location":"api_docs/pandas/general_functions/crosstab/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>index</code> SeriesType <code>columns</code> SeriesType <p>Note</p> <p>Annotation of pivot values is required. For example, <code>@bodo.jit(pivots={'pt': ['small', 'large']})</code> declares the output table <code>pt</code> will have columns called <code>small</code> and <code>large</code>.</p>"},{"location":"api_docs/pandas/general_functions/crosstab/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit(pivots={\"pt\": [\"small\", \"large\"]})\n... def f(df):\n...   pt = pd.crosstab(df.A, df.C)\n...   return pt\n\n&gt;&gt;&gt; list_A = [\"foo\", \"foo\", \"bar\", \"bar\", \"bar\", \"bar\"]\n&gt;&gt;&gt; list_C = [\"small\", \"small\", \"large\", \"small\", \"small\", \"middle\"]\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": list_A, \"C\": list_C})\n&gt;&gt;&gt; f(df)\n\n         small  large\nindex\nfoo          2      0\nbar          2      1\n</code></pre>"},{"location":"api_docs/pandas/general_functions/cut/","title":"<code>pd.cut</code>","text":"<p><code>pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates=\"raise\", ordered=True)</code></p>"},{"location":"api_docs/pandas/general_functions/cut/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>x</code> Series or Array like <code>bins</code> Integer or Array like <code>include_lowest</code> Boolean"},{"location":"api_docs/pandas/general_functions/cut/#example-usage","title":"Example Usage","text":"<pre><code> &gt;&gt;&gt; @bodo.jit\n ... def f(S):\n ...   bins = 4\n ...   include_lowest = True\n ...   return pd.cut(S, bins, include_lowest=include_lowest)\n\n &gt;&gt;&gt; S = pd.Series(\n ...    [-2, 1, 3, 4, 5, 11, 15, 20, 22],\n ...    [\"a1\", \"a2\", \"a3\", \"a4\", \"a5\", \"a6\", \"a7\", \"a8\", \"a9\"],\n ...    name=\"ABC\",\n ... )\n &gt;&gt;&gt; f(S)\n\na1    (-2.025, 4.0]\na2    (-2.025, 4.0]\na3    (-2.025, 4.0]\na4    (-2.025, 4.0]\na5      (4.0, 10.0]\na6     (10.0, 16.0]\na7     (10.0, 16.0]\na8     (16.0, 22.0]\na9     (16.0, 22.0]\nName: ABC, dtype: category\nCategories (4, interval[float64, right]): [(-2.025, 4.0] &lt; (4.0, 10.0] &lt; (10.0, 16.0] &lt; (16.0, 22.0]]\n</code></pre>"},{"location":"api_docs/pandas/general_functions/date_range/","title":"<code>pd.date_range</code>","text":"<p><code>pandas.date_range(start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, **kwargs)</code></p>"},{"location":"api_docs/pandas/general_functions/date_range/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>start</code> String or Timestamp <code>end</code> String or Timestamp <code>periods</code> Integer <code>freq</code> String <ul><li> Must be a valid Pandas frequency </li></ul> <code>name</code> String <p>Note</p> <ul> <li>Exactly three of <code>start</code>, <code>end</code>, <code>periods</code>, and <code>freq</code> must   be provided.</li> <li>Bodo Does Not support <code>kwargs</code>, even for compatibility.</li> </ul>"},{"location":"api_docs/pandas/general_functions/date_range/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...     return pd.date_range(start=\"2018-04-24\", end=\"2018-04-27\", periods=3)\n\n&gt;&gt;&gt; f()\n\nDatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n              '2018-04-27 00:00:00'],\n             dtype='datetime64[ns]', freq=None)\n</code></pre>"},{"location":"api_docs/pandas/general_functions/get_dummies/","title":"<code>pd.get_dummies</code>","text":"<p><code>pandas.get_dummies(data, prefix=None, prefix_sep=\"_\", dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)</code></p>"},{"location":"api_docs/pandas/general_functions/get_dummies/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>data</code> Array or Series with Categorical dtypes Categories must be  known at compile time."},{"location":"api_docs/pandas/general_functions/get_dummies/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return pd.get_dummies(S)\n\n&gt;&gt;&gt; S = pd.Series([\"CC\", \"AA\", \"B\", \"D\", \"AA\", None, \"B\", \"CC\"]).astype(\"category\")\n&gt;&gt;&gt; f(S)\n\nAA  B  CC  D\n0   0  0   1  0\n1   1  0   0  0\n2   0  1   0  0\n3   0  0   0  1\n4   1  0   0  0\n5   0  0   0  0\n6   0  1   0  0\n7   0  0   1  0\n</code></pre>"},{"location":"api_docs/pandas/general_functions/isna/","title":"<code>pd.isna</code>","text":"<p><code>pandas.isna(obj)</code></p>"},{"location":"api_docs/pandas/general_functions/isna/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>obj</code> DataFrame, Series, Index, Array, or Scalar"},{"location":"api_docs/pandas/general_functions/isna/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return pd.isna(df)\n\n&gt;&gt;&gt; df = pd.DataFrame(\n...    {\"A\": [\"AA\", np.nan, \"\", \"D\", \"GG\"], \"B\": [1, 8, 4, -1, 2]},\n...    [1.1, -2.1, 7.1, 0.1, 3.1],\n... )\n&gt;&gt;&gt; f(df)\n\n       A      B\n1.1  False  False\n-2.1   True  False\n7.1  False  False\n0.1  False  False\n3.1  False  False\n</code></pre>"},{"location":"api_docs/pandas/general_functions/isnull/","title":"<code>pd.isnull</code>","text":"<p><code>pandas.isnull(obj)</code></p>"},{"location":"api_docs/pandas/general_functions/isnull/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>obj</code> DataFrame, Series, Index, Array, or  Scalar"},{"location":"api_docs/pandas/general_functions/isnull/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return pd.isnull(df)\n\n&gt;&gt;&gt; df = pd.DataFrame(\n...    {\"A\": [\"AA\", np.nan, \"\", \"D\", \"GG\"], \"B\": [1, 8, 4, -1, 2]},\n...    [1.1, -2.1, 7.1, 0.1, 3.1],\n... )\n&gt;&gt;&gt; f(df)\n\n       A      B\n1.1  False  False\n-2.1   True  False\n7.1  False  False\n0.1  False  False\n3.1  False  False\n</code></pre>"},{"location":"api_docs/pandas/general_functions/merge/","title":"<code>pd.merge</code>","text":"<p><code>pandas.merge(left, right, how=\"inner\", on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=(\"_x\", \"_y\"), copy=True, indicator=False, validate=None, _bodo_na_equal=True)</code></p>"},{"location":"api_docs/pandas/general_functions/merge/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>left</code> DataFrame <code>right</code> DataFrame <code>how</code> String <ul> <li> Must be one of <code>\"inner\"</code>, <code>\"outer\"</code>,  <code>\"left\"</code>, <code>\"right\"</code></li> <li> Must be constant at  Compile Time </li> </ul> <code>on</code> Column Name, List of Column Names, or General Merge Condition String (see merge-notes) <ul> <li> Must be constant at Compile Time </li> </ul> <code>left_on</code> Column Name or List of Column Names <ul><li> Must be constant at  Compile Time </li></ul> <code>right_on</code> Column Name or List of Column  Names <ul><li> Must be constant at  Compile Time </li></ul> <code>left_index</code> Boolean <ul><li> Must be constant at  Compile Time </li></ul> <code>right_index</code> Boolean <ul><li> Must be constant at  Compile Time </li></ul> <code>suffixes</code> Tuple of Strings <ul><li> Must be constant at  Compile Time </li></ul> <code>indicator</code> Boolean <ul><li> Must be constant at  Compile Time </li></ul> <code>_bodo_na_equal</code> Boolean <ul><li> Must be constant at  Compile Time </li> <li> This argument is  unique to Bodo and not  available in Pandas. If False, Bodo won't  consider NA/nan keys  as equal, which differs from Pandas. </li></ul> <p>Important</p> <p>The argument <code>_bodo_na_equal</code> is unique to Bodo and not available in Pandas. If it is <code>False</code>, Bodo won't consider NA/nan keys as equal, which differs from Pandas.</p>"},{"location":"api_docs/pandas/general_functions/merge/#merge-notes","title":"Merge Notes","text":"<ul> <li> <p>Output Ordering:</p> <p>The output dataframe is not sorted by default for better parallel performance (Pandas may preserve key order depending on <code>how</code>). One can use explicit sort if needed.</p> </li> <li> <p>General Merge Conditions:</p> <p>Within Pandas, the merge criteria supported by <code>pd.merge</code> are limited to equality between 1 or more pairs of keys. For some use cases, this is not sufficient and more generalized support is necessary. For example, with these limitations, a <code>left outer join</code> where <code>df1.A == df2.B &amp; df2.C &lt; df1.A</code> cannot be efficiently computed.</p> <p>Bodo supports these use cases by allowing users to pass general merge conditions to <code>pd.merge</code>. We plan to contribute this feature to Pandas to ensure full compatibility of Bodo and Pandas code.</p> <p>General merge conditions are performed by providing the condition as a string via the <code>on</code> argument. Columns in the left table are referred to by <code>left.{column name}</code> and columns in the right table are referred to by <code>right.{column name}</code>.</p> <p>Here's an example demonstrating the above:</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def general_merge(df1, df2):\n...   return df1.merge(df2, on=\"left.`A` == right.`B` &amp; right.`C` &lt; left.`A`\", how=\"left\")\n\n&gt;&gt;&gt; df1 = pd.DataFrame({\"col\": [2, 3, 5, 1, 2, 8], \"A\": [4, 6, 3, 9, 9, -1]})\n&gt;&gt;&gt; df2 = pd.DataFrame({\"B\": [1, 2, 9, 3, 2], \"C\": [1, 7, 2, 6, 5]})\n&gt;&gt;&gt; general_merge(df1, df2)\n\n   col  A     B     C\n0    2  4  &lt;NA&gt;  &lt;NA&gt;\n1    3  6  &lt;NA&gt;  &lt;NA&gt;\n2    5  3  &lt;NA&gt;  &lt;NA&gt;\n3    1  9     9     2\n4    2  9     9     2\n5    8 -1  &lt;NA&gt;  &lt;NA&gt;\n</code></pre> <p>These calls have a few additional requirements:</p> <ul> <li>The condition must be constant string.</li> <li>The condition must be of the form <code>cond_1 &amp; ... &amp; cond_N</code> where at least one <code>cond_i</code>   is a simple equality. This restriction will be removed in a future release.</li> <li>The columns specified in these conditions are limited to certain column types.   We currently support <code>boolean</code>, <code>integer</code>, <code>float</code>, <code>datetime64</code>, <code>timedelta64</code>, <code>datetime.date</code>,   and <code>string</code> columns.</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/general_functions/merge/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df1, df2):\n...   return pd.merge(df1, df2, how=\"inner\", on=\"key\")\n\n&gt;&gt;&gt; df1 = pd.DataFrame({\"key\": [2, 3, 5, 1, 2, 8], \"A\": np.array([4, 6, 3, 9, 9, -1], float)})\n&gt;&gt;&gt; df2 = pd.DataFrame({\"key\": [1, 2, 9, 3, 2], \"B\": np.array([1, 7, 2, 6, 5], float)})\n&gt;&gt;&gt; f(df1, df2)\n\nkey    A    B\n0    2  4.0  7.0\n1    2  4.0  5.0\n2    3  6.0  6.0\n3    1  9.0  1.0\n4    2  9.0  7.0\n5    2  9.0  5.0\n</code></pre>"},{"location":"api_docs/pandas/general_functions/notna/","title":"<code>pd.notna</code>","text":"<p><code>pandas.notna(obj)</code></p>"},{"location":"api_docs/pandas/general_functions/notna/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>obj</code> DataFrame, Series, Index, Array, or Scalar"},{"location":"api_docs/pandas/general_functions/notna/#example-usage","title":"Example Usage","text":"<pre><code> &gt;&gt;&gt; @bodo.jit\n ... def f(df):\n ...     return pd.notna(df)\n\n &gt;&gt;&gt; df = pd.DataFrame(\n ...    {\"A\": [\"AA\", np.nan, \"\", \"D\", \"GG\"], \"B\": [1, 8, 4, -1, 2]},\n ...    [1.1, -2.1, 7.1, 0.1, 3.1],\n ... )\n &gt;&gt;&gt; f(df)\n\n           A     B\n  1.1   True  True\n -2.1  False  True\n  7.1   True  True\n  0.1   True  True\n  3.1   True  True\n</code></pre>"},{"location":"api_docs/pandas/general_functions/notnull/","title":"<code>pd.notnull</code>","text":"<p><code>pandas.notnull(obj)</code></p>"},{"location":"api_docs/pandas/general_functions/notnull/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>obj</code> DataFrame, Series, Index, Array, or Scalar"},{"location":"api_docs/pandas/general_functions/notnull/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return pd.notnull(df)\n\n&gt;&gt;&gt; df = pd.DataFrame(\n...    {\"A\": [\"AA\", np.nan, \"\", \"D\", \"GG\"], \"B\": [1, 8, 4, -1, 2]},\n...    [1.1, -2.1, 7.1, 0.1, 3.1],\n... )\n&gt;&gt;&gt; f(df)\n\n       A     B\n1.1   True  True\n-2.1  False  True\n7.1   True  True\n0.1   True  True\n3.1   True  True\n</code></pre>"},{"location":"api_docs/pandas/general_functions/pivot/","title":"<code>pd.pivot</code>","text":"<p><code>pandas.pivot(data, values=None, index=None, columns=None)</code></p>"},{"location":"api_docs/pandas/general_functions/pivot/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>data</code> DataFrame <code>values</code> Constant Column Label or list of  labels <code>index</code> Constant Column Label or list of  labels <code>columns</code> Constant Column Label <p>Note</p> <p>The the number of columns and names of the output DataFrame won't be known at compile time. To update typing information on DataFrame you should pass it back to Python.</p>"},{"location":"api_docs/pandas/general_functions/pivot/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   df = pd.DataFrame({\"A\": [\"X\",\"X\",\"X\",\"X\",\"Y\",\"Y\"], \"B\": [1,2,3,4,5,6], \"C\": [10,11,12,20,21,22]})\n...   pivoted_tbl = pd.pivot(data, columns=\"A\", index=\"B\", values=\"C\")\n...   return pivoted_tbl\n&gt;&gt;&gt; f()\nA     X     Y\nB\n1  10.0   NaN\n2  11.0   NaN\n3  12.0   NaN\n4  20.0   NaN\n5   NaN  21.0\n6   NaN  22.0\n</code></pre>"},{"location":"api_docs/pandas/general_functions/pivot_table/","title":"<code>pd.pivot_table</code>","text":"<p><code>pandas.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True)</code></p>"},{"location":"api_docs/pandas/general_functions/pivot_table/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>data</code> DataFrame <code>values</code> Constant Column Label or list of  labels <code>index</code> Constant Column Label or list of  labels <code>columns</code> Constant Column Label <code>aggfunc</code> String Constant <p>Note</p> <p>This code takes two different paths depending on if pivot values are annotated. When pivot values are annotated then output columns are set to the annotated values. For example, <code>@bodo.jit(pivots={'pt': ['small', 'large']})</code> declares the output pivot table <code>pt</code> will have columns called <code>small</code> and <code>large</code>.</p> <p>If pivot values are not annotated, then the number of columns and names of the output DataFrame won't be known at compile time. To update typing information on DataFrame you should pass it back to Python.</p>"},{"location":"api_docs/pandas/general_functions/pivot_table/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit(pivots={'pivoted_tbl': ['X', 'Y']})\n... def f():\n...   df = pd.DataFrame({\"A\": [\"X\",\"X\",\"X\",\"X\",\"Y\",\"Y\"], \"B\": [1,2,3,4,5,6], \"C\": [10,11,12,20,21,22]})\n...   pivoted_tbl = pd.pivot_table(df, columns=\"A\", index=\"B\", values=\"C\", aggfunc=\"mean\")\n...   return pivoted_tbl\n&gt;&gt;&gt; f()\n      X     Y\nB\n1  10.0   NaN\n2  11.0   NaN\n3  12.0   NaN\n4  20.0   NaN\n5   NaN  21.0\n6   NaN  22.0\n</code></pre>"},{"location":"api_docs/pandas/general_functions/qcut/","title":"<code>pd.qcut</code>","text":"<p><code>pandas.qcut(x, q, labels=None, retbins=False, precision=3, duplicates=\"raise\")</code></p>"},{"location":"api_docs/pandas/general_functions/qcut/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>x</code> Series or Array like <code>q</code> Integer or Array like of floats"},{"location":"api_docs/pandas/general_functions/qcut/#example-usage","title":"Example Usage","text":"<pre><code> &gt;&gt;&gt; @bodo.jit\n ... def f(S):\n ...   q = 4\n ...   return pd.qcut(S, q)\n\n &gt;&gt;&gt; S = pd.Series(\n ...      [-2, 1, 3, 4, 5, 11, 15, 20, 22],\n ...      [\"a1\", \"a2\", \"a3\", \"a4\", \"a5\", \"a6\", \"a7\", \"a8\", \"a9\"],\n ...      name=\"ABC\",\n ... )\n &gt;&gt;&gt; f(S)\n\n a1    (-2.001, 3.0]\n a2    (-2.001, 3.0]\n a3    (-2.001, 3.0]\n a4       (3.0, 5.0]\n a5       (3.0, 5.0]\n a6      (5.0, 15.0]\n a7      (5.0, 15.0]\n a8     (15.0, 22.0]\n a9     (15.0, 22.0]\n Name: ABC, dtype: category\n Categories (4, interval[float64, right]): [(-2.001, 3.0] &lt; (3.0, 5.0] &lt; (5.0, 15.0] &lt; (15.0, 22.0]]\n</code></pre>"},{"location":"api_docs/pandas/general_functions/timedelta_range/","title":"<code>pd.timedelta_range</code>","text":"<p><code>pandas.timedelta_range(start=None, end=None, periods=None, freq=None, name=None, closed=None)</code></p>"},{"location":"api_docs/pandas/general_functions/timedelta_range/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>start</code> String or  Timedelta <code>end</code> String or  Timedelta <code>periods</code> Integer <code>freq</code> String <ul><li>   Must be a valid    Pandas  frequency </li></ul> <code>name</code> String <code>closed</code> String and one of   ('left', 'right') <p>Note</p> <ul> <li>Exactly three of <code>start</code>, <code>end</code>, <code>periods</code>, and <code>freq</code> must be provided.</li> <li>This function is not parallelized yet.</li> </ul>"},{"location":"api_docs/pandas/general_functions/timedelta_range/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...     return pd.timedelta_range(start=\"1 day\", end=\"11 days 1 hour\", periods=3)\n\n&gt;&gt;&gt; f()\n\nTimedeltaIndex(['1 days 00:00:00', '6 days 00:30:00', '11 days 01:00:00'], dtype='timedelta64[ns]', freq=None)\n</code></pre>"},{"location":"api_docs/pandas/general_functions/to_datetime/","title":"<code>pd.to_datetime</code>","text":"<p><code>pandas.to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False, utc=None, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix', cache=True)</code></p>"},{"location":"api_docs/pandas/general_functions/to_datetime/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>arg</code> Series, Array or scalar of integers  or strings <code>errors</code> String and one of ('ignore', 'raise', 'coerce') <code>dayfirst</code> Boolean <code>yearfirst</code> Boolean <code>utc</code> Boolean <code>format</code> String matching Pandas strftime/strptime <code>exact</code> Boolean <code>unit</code> String <ul><li> Must be a valid Pandas timedelta unit </li></ul> <code>infer_datetime_format</code> Boolean <code>origin</code> Scalar string or timestamp value <code>cache</code> Boolean <p>Note</p> <ul> <li>The function is not optimized.</li> <li>Bodo doesn't support Timezone-Aware datetime values</li> </ul>"},{"location":"api_docs/pandas/general_functions/to_datetime/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(val):\n...     return pd.to_datetime(val, format=\"%Y-%d-%m\")\n\n&gt;&gt;&gt; val = \"2016-01-06\"\n&gt;&gt;&gt; f(val)\n\nTimestamp('2016-06-01 00:00:00')\n</code></pre>"},{"location":"api_docs/pandas/general_functions/to_numeric/","title":"<code>pd.to_numeric</code>","text":"<p><code>pandas.to_numeric(arg, errors=\"raise\", downcast=None)</code></p>"},{"location":"api_docs/pandas/general_functions/to_numeric/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>arg</code> Series or Array <code>downcast</code> String and one of (<code>'integer'</code>, <code>'signed'</code>, <code>'unsigned'</code>, <code>'float'</code>) <ul><li> Must be constant at Compile Time </li></ul> <p>Note</p> <ul> <li>Output type is float64 by default</li> <li>Unlike Pandas, Bodo does not dynamically determine output type,   and does not downcast to the smallest numerical type.</li> <li><code>downcast</code> parameter should be used for type annotation of output.</li> </ul>"},{"location":"api_docs/pandas/general_functions/to_numeric/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return pd.to_numeric(S, errors=\"coerce\", downcast=\"integer\")\n\n&gt;&gt;&gt; S = pd.Series([\"1\", \"3\", \"12\", \"4\", None, \"-555\"])\n&gt;&gt;&gt; f(S)\n\n0       1\n1       3\n2      12\n3       4\n4    &lt;NA&gt;\n5    -555\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/general_functions/to_timedelta/","title":"<code>pd.to_timedelta</code>","text":"<p><code>pandas.to_timedelta(arg, unit=None, errors='raise')</code></p>"},{"location":"api_docs/pandas/general_functions/to_timedelta/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>arg</code> Series, Array or scalar of integers or strings <code>unit</code> String <ul><li>Must be a valid Pandastimedelta unit </li></ul> <p>Note</p> <p>Passing string data as <code>arg</code> is not optimized.</p>"},{"location":"api_docs/pandas/general_functions/to_timedelta/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return pd.to_timedelta(S, unit=\"D\")\n\n&gt;&gt;&gt; S = pd.Series([1.0, 2.2, np.nan, 4.2], [3, 1, 0, -2], name=\"AA\")\n&gt;&gt;&gt; f(val)\n\n3   1 days 00:00:00\n1   2 days 04:48:00\n0               NaT\n-2   4 days 04:48:00\nName: AA, dtype: timedelta64[ns]\n</code></pre>"},{"location":"api_docs/pandas/general_functions/unique/","title":"<code>pd.unique</code>","text":"<p><code>pandas.unique(values)</code></p>"},{"location":"api_docs/pandas/general_functions/unique/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>values</code> Series or 1-d array  with Categorical dtypes"},{"location":"api_docs/pandas/general_functions/unique/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return pd.unique(S)\n\n&gt;&gt;&gt; S = pd.Series([1, 2, 1, 3, 2, 1])\n&gt;&gt;&gt; f(S)\narray([1, 2, 3])\n</code></pre>"},{"location":"api_docs/pandas/groupby/","title":"GroupBy","text":"<p>The <code>groupby</code> method is used to group data in a DataFrame or Series based on a given column or index level. The grouped data can then be aggregated, transformed, or filtered using various methods.</p> <p>Bodo supports the following <code>groupby</code> methods:</p> Function Description <code>pd.core.groupby.Groupby.agg</code> Compute aggregation using one or more operations over the specified axis. <code>pd.core.groupby.DataFrameGroupby.aggregate</code> Compute aggregation using one or more operations over the specified axis. <code>pd.core.groupby.Groupby.apply</code> Apply a function to each group. <code>pd.core.groupby.Groupby.count</code> Count non-NA cells for each column or row. <code>pd.core.groupby.Groupby.cumsum</code> Compute the cumulative sum of the values. <code>pd.core.groupby.Groupby.first</code> Compute the first value of the group. <code>pd.core.groupby.Groupby.head</code> Return the first n rows of each group. <code>pd.core.groupby.DataFrameGroupby.idxmax</code> Compute the index of the maximum value. <code>pd.core.groupby.DataFrameGroupby.idxmin</code> Compute the index of the minimum value. <code>pd.core.groupby.Groupby.last</code> Compute the last value of the group. <code>pd.core.groupby.Groupby.max</code> Compute the maximum value of the group. <code>pd.core.groupby.Groupby.mean</code> Compute the mean value of the group. <code>pd.core.groupby.Groupby.median</code> Compute the median value of the group. <code>pd.core.groupby.Groupby.min</code> Compute the minimum value of the group. <code>pd.core.groupby.DataFrameGroupby.nunique</code> Compute the number of unique values in the group. <code>pd.core.groupby.DataFrameGroupby.ngroup</code> Compute a unique index number for each group. <code>pd.core.groupby.Groupby.pipe</code> Apply a function to each group. <code>pd.core.groupby.Groupby.prod</code> Compute the product of the group. <code>pd.core.groupby.Groupby.rolling</code> Provide rolling window calculations. <code>pd.Series.groupby</code> Group series using a mapper or by a series of columns. <code>pd.core.groupby.Groupby.shift</code> Shift the group by a number of periods. <code>pd.core.groupby.Groupby.size</code> Compute group sizes. <code>pd.core.groupby.Groupby.std</code> Compute the standard deviation of the group. <code>pd.core.groupby.Groupby.sum</code> Compute the sum of the group. <code>pd.core.groupby.Groupby.transform</code> Apply a function to each group. <code>pd.core.groupby.SeriesGroupBy.value_counts</code> Count unique values in the group. <code>pd.core.groupby.Groupby.var</code> Compute the variance of the group."},{"location":"api_docs/pandas/groupby/agg/","title":"<code>pd.core.groupby.Groupby.agg</code>","text":"<p><code>pandas.core.groupby.Groupby.agg(func, *args, **kwargs)</code></p>"},{"location":"api_docs/pandas/groupby/agg/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>func</code>: JIT function, callable defined within a JIT function, constant dictionary mapping column name to a function</li> <li>Additional arguments for <code>func</code> can be passed as additional arguments.</li> </ul> <p>Note</p> <ul> <li>Passing a list of functions is also supported if only one output column is selected.</li> <li>Output column names can be specified using keyword arguments and <code>pd.NamedAgg()</code>.</li> </ul>"},{"location":"api_docs/pandas/groupby/agg/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\", dropna=True).agg({\"A\": lambda x: max(x)})\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n        A\nB\n421  24.0\nf31   2.0\n</code></pre>"},{"location":"api_docs/pandas/groupby/aggregate/","title":"<code>pd.core.groupby.DataFrameGroupby.aggregate</code>","text":"<p><code>pandas.core.groupby.DataFrameGroupby.aggregate(func, *args, **kwargs)</code></p>"},{"location":"api_docs/pandas/groupby/aggregate/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>func</code>: JIT function, callable defined within a JIT function, constant dictionary mapping column name to a function</li> <li>Additional arguments for <code>func</code> can be passed as additional arguments.</li> </ul> <p>Note</p> <ul> <li>Passing a list of functions is also supported if only one output column is selected.</li> <li>Output column names can be specified using keyword arguments and <code>pd.NamedAgg()</code>.</li> </ul>"},{"location":"api_docs/pandas/groupby/aggregate/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\", dropna=True).agg({\"A\": lambda x: max(x)})\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n        A\nB\n421  24.0\nf31   2.0\n</code></pre>"},{"location":"api_docs/pandas/groupby/apply/","title":"<code>pd.core.groupby.Groupby.apply</code>","text":"<p><code>pandas.core.groupby.Groupby.apply(func, *args, **kwargs)</code></p>"},{"location":"api_docs/pandas/groupby/apply/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>func</code>: JIT function, callable defined within a JIT function that returns a DataFrame or Series</li> <li>Additional arguments for <code>func</code> can be passed as additional arguments.</li> </ul>"},{"location":"api_docs/pandas/groupby/apply/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df, y):\n...     return df.groupby(\"B\", dropna=True).apply(lambda group, y: group.sum(axis=1) + y, y=y)\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; y = 4\n&gt;&gt;&gt; f(df, y)\n\nB\n421  0          6.510\n   2          8.421\n   4     233260.000\n   6         16.210\n   8          6.510\n   10         8.421\n   12    233260.000\n   14        16.210\n   16         6.510\n   18         8.421\nf31  1     233260.000\n   3         16.210\n   5          6.510\n   7          8.421\n   9     233260.000\n   11        16.210\n   13         6.510\n   15         8.421\n   17    233260.000\n   19        16.210\ndtype: float64\n</code></pre>"},{"location":"api_docs/pandas/groupby/count/","title":"<code>pd.core.groupby.Groupby.count</code>","text":"<p><code>pandas.core.groupby.Groupby.count()</code></p>"},{"location":"api_docs/pandas/groupby/count/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").count()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n      A   C\nB\n421  10  10\nf31   5  10\n</code></pre>"},{"location":"api_docs/pandas/groupby/cumsum/","title":"<code>pd.core.groupby.Groupby.cumsum</code>","text":"<p><code>pandas.core.groupby.Groupby.cumsum(axis=0)</code></p> <p>Note</p> <p><code>cumsum</code> is only supported on numeric columns and is not supported on boolean columns</p>"},{"location":"api_docs/pandas/groupby/cumsum/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").cumsum()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n        A            C\n0     1.0        1.510\n1     2.0        2.421\n2    25.0   233233.510\n3     NaN       14.631\n4    26.0   233235.020\n5     4.0       17.052\n6    50.0   466467.020\n7     NaN       29.262\n8    51.0   466468.530\n9     6.0       31.683\n10   75.0   699700.530\n11    NaN       43.893\n12   76.0   699702.040\n13    8.0       46.314\n14  100.0   932934.040\n15    NaN       58.524\n16  101.0   932935.550\n17   10.0       60.945\n18  125.0  1166167.550\n19    NaN       73.155\n</code></pre>"},{"location":"api_docs/pandas/groupby/first/","title":"<code>pd.core.groupby.Groupby.first</code>","text":"<p><code>pandas.core.groupby.Groupby.first(numeric_only=False, min_count=-1)</code></p> <p>Note</p> <p><code>first</code> is not supported on columns with nested array types</p>"},{"location":"api_docs/pandas/groupby/first/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").first()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n       A      C\nB\n421  1.0  1.510\nf31  2.0  2.421\n</code></pre>"},{"location":"api_docs/pandas/groupby/groupby/","title":"<code>pd.DataFrame.groupby</code>","text":"<p><code>pandas.DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True)</code></p>"},{"location":"api_docs/pandas/groupby/groupby/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>by</code>: Column label or list of column labels<ul> <li>Must be constant at Compile Time</li> <li>This argument is required</li> </ul> </li> <li><code>as_index</code>: Boolean<ul> <li>Must be constant at Compile Time</li> </ul> </li> <li><code>dropna</code>: Boolean<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/groupby/groupby/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\", dropna=True, as_index=False).count()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n     B   A   C\n0  421  10  10\n1  f31   5  10\n</code></pre>"},{"location":"api_docs/pandas/groupby/head/","title":"<code>pd.core.groupby.Groupby.head</code>","text":"<p><code>pandas.core.groupby.Groupby.head(n=5)</code></p>"},{"location":"api_docs/pandas/groupby/head/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>n</code>: Non-negative integer<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/groupby/head/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").head()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n      A    B           C\n0   1.0  421       1.510\n1   2.0  f31       2.421\n2  24.0  421  233232.000\n3   NaN  f31      12.210\n4   1.0  421       1.510\n5   2.0  f31       2.421\n6  24.0  421  233232.000\n7   NaN  f31      12.210\n8   1.0  421       1.510\n9   2.0  f31       2.421\n</code></pre>"},{"location":"api_docs/pandas/groupby/idxmax/","title":"<code>pd.core.groupby.DataFrameGroupby.idxmax</code>","text":"<p><code>pandas.core.groupby.DataFrameGroupby.idxmax(axis=0, skipna=True)</code></p>"},{"location":"api_docs/pandas/groupby/idxmax/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").idxmax()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n     A  C\nB\n421  2  2\nf31  1  3\n</code></pre>"},{"location":"api_docs/pandas/groupby/idxmin/","title":"<code>pd.core.groupby.DataFrameGroupby.idxmin</code>","text":"<p><code>pandas.core.groupby.DataFrameGroupby.idxmin(axis=0, skipna=True)</code></p>"},{"location":"api_docs/pandas/groupby/idxmin/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").idxmin()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n     A  C\nB\n421  0  0\nf31  1  1\n</code></pre>"},{"location":"api_docs/pandas/groupby/last/","title":"<code>pd.core.groupby.Groupby.last</code>","text":"<p><code>pandas.core.groupby.Groupby.last(numeric_only=False, min_count=-1)</code></p> <p>Note</p> <p><code>last</code> is not supported on columns with nested array types</p>"},{"location":"api_docs/pandas/groupby/last/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").last()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n        A          C\nB\n421  24.0  233232.00\nf31   2.0      12.21\n</code></pre>"},{"location":"api_docs/pandas/groupby/max/","title":"<code>pd.core.groupby.Groupby.max</code>","text":"<p><code>pandas.core.groupby.Groupby.max(numeric_only=False, min_count=-1)</code></p> <p>Note</p> <ul> <li><code>max</code> is not supported on columns with nested array types.</li> <li>Categorical columns must be ordered.</li> </ul>"},{"location":"api_docs/pandas/groupby/max/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").max()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n        A          C\nB\n421  24.0  233232.00\nf31   2.0      12.21\n</code></pre>"},{"location":"api_docs/pandas/groupby/mean/","title":"<code>pd.core.groupby.Groupby.mean</code>","text":"<p><code>pandas.core.groupby.Groupby.mean(numeric_only=NoDefault.no_default)</code></p> <p>Note</p> <p><code>mean</code> is only supported on numeric columns and is not supported on boolean column</p>"},{"location":"api_docs/pandas/groupby/mean/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").mean()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n        A            C\nB\n421  12.5  116616.7550\nf31   2.0       7.3155\n</code></pre>"},{"location":"api_docs/pandas/groupby/median/","title":"<code>pd.core.groupby.Groupby.median</code>","text":"<p><code>pandas.core.groupby.Groupby.median(numeric_only=NoDefault.no_default)</code></p> <p>Note</p> <p><code>median</code> is only supported on numeric columns and is not supported on boolean column</p>"},{"location":"api_docs/pandas/groupby/median/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").median()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n        A            C\nB\n421  12.5  116616.7550\nf31   2.0       7.3155\n</code></pre>"},{"location":"api_docs/pandas/groupby/min/","title":"<code>pd.core.groupby.Groupby.min</code>","text":"<p><code>pandas.core.groupby.Groupby.min(numeric_only=False, min_count=-1)</code></p> <p>Note</p> <ul> <li><code>min</code> is not supported on columns with nested array types</li> <li>Categorical columns must be ordered.</li> </ul>"},{"location":"api_docs/pandas/groupby/min/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").min()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n       A      C\nB\n421  1.0  1.510\nf31  2.0  2.421\n</code></pre>"},{"location":"api_docs/pandas/groupby/ngroup/","title":"<code>pd.core.groupby.DataFrameGroupby.ngroup</code>","text":"<p><code>pandas.core.groupby.DataFrameGroupby.ngroup(ascending=True)</code></p>"},{"location":"api_docs/pandas/groupby/ngroup/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"A\").ngroup()\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"A\": [1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 1],\n...     \"B\": np.arange(12.0)\n... })\n&gt;&gt;&gt; f(df)\n0     1\n1     0\n2     3\n3     2\n4     4\n5     1\n6     0\n7     3\n8     2\n9     1\n10    0\n11    1\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/groupby/nunique/","title":"<code>pd.core.groupby.DataFrameGroupby.nunique</code>","text":"<p><code>pandas.core.groupby.DataFrameGroupby.nunique(dropna=True)</code></p>"},{"location":"api_docs/pandas/groupby/nunique/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>dropna</code>: boolean</li> </ul> <p>Note</p> <p><code>nunique</code> is not supported on columns with nested array types</p>"},{"location":"api_docs/pandas/groupby/nunique/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").nunique()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n     A  C\nB\n421  2  2\nf31  1  2\n</code></pre>"},{"location":"api_docs/pandas/groupby/pipe/","title":"<code>pd.core.groupby.Groupby.pipe</code>","text":"<p><code>pandas.core.groupby.Groupby.pipe(func, \\*args, **kwargs)</code></p>"},{"location":"api_docs/pandas/groupby/pipe/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>func</code>: JIT function, callable defined within a JIT function.<ul> <li>Additional arguments for <code>func</code> can be passed as additional arguments.</li> </ul> </li> </ul> <p>Note</p> <p><code>func</code> cannot be a tuple</p>"},{"location":"api_docs/pandas/groupby/pipe/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df, y):\n...     return df.groupby(\"B\").pipe(lambda grp, y: grp.sum() - y, y=y)\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; y = 5\n&gt;&gt;&gt; f(df, y)\n\n         A            C\nB\n421  120.0  1166162.550\nf31    5.0       68.155\n</code></pre>"},{"location":"api_docs/pandas/groupby/prod/","title":"<code>pd.core.groupby.Groupby.prod</code>","text":"<p><code>pandas.core.groupby.Groupby.prod(numeric_only=NoDefault.no_default, min_count=0)</code></p> <p>Note</p> <p><code>prod</code> is not supported on columns with nested array types</p>"},{"location":"api_docs/pandas/groupby/prod/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").prod()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n             A             C\nB\n421  7962624.0  5.417831e+27\nf31       32.0  2.257108e+07\n</code></pre>"},{"location":"api_docs/pandas/groupby/rolling/","title":"<code>pd.core.groupby.Groupby.rolling</code>","text":"<p><code>pandas.core.groupby.Groupby.rolling(window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None, method='single')</code></p>"},{"location":"api_docs/pandas/groupby/rolling/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>window</code>: Integer, String, Datetime, Timedelta</li> <li><code>min_periods</code>: Integer</li> <li><code>center</code>: Boolean</li> <li><code>on</code>: Column label<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul> <p>Note</p> <p>This is equivalent to performing the DataFrame API on each groupby. All operations of the rolling API can be used with groupby.</p>"},{"location":"api_docs/pandas/groupby/rolling/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").rolling(2).mean\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n           A            C\nB\n421 0    NaN          NaN\n    2    NaN          NaN\n    4   12.5  116616.7550\n    6    NaN       7.3155\n    8   12.5  116616.7550\n    10   NaN       7.3155\n    12  12.5  116616.7550\n    14   NaN       7.3155\n    16  12.5  116616.7550\n    18   NaN       7.3155\nf31 1   12.5  116616.7550\n    3    NaN       7.3155\n    5   12.5  116616.7550\n    7    NaN       7.3155\n    9   12.5  116616.7550\n    11   NaN       7.3155\n    13  12.5  116616.7550\n    15   NaN       7.3155\n    17  12.5  116616.7550\n    19   NaN       7.3155\n</code></pre>"},{"location":"api_docs/pandas/groupby/series_groupby/","title":"<code>pd.Series.groupby</code>","text":"<p><code>pandas.Series.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True)</code></p>"},{"location":"api_docs/pandas/groupby/series_groupby/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>by</code>: Array-like or Series data. This is not supported with Decimal or Categorical data.<ul> <li>Must be constant at Compile Time</li> </ul> </li> <li><code>level</code>: integer<ul> <li>Must be constant at Compile Time</li> <li>Only <code>level=0</code> is supported and not with MultiIndex.</li> </ul> </li> </ul> <p>Important</p> <p>You must provide exactly one of <code>by</code> and <code>level</code></p>"},{"location":"api_docs/pandas/groupby/series_groupby/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, by_series):\n...     return S.groupby(by_series).count()\n&gt;&gt;&gt; S = pd.Series([1, 2, 24, None] * 5)\n&gt;&gt;&gt; by_series = pd.Series([\"421\", \"f31\"] * 10)\n&gt;&gt;&gt; f(S, by_series)\n\n421    10\nf31     5\nName: , dtype: int64\n</code></pre> <p>Note</p> <p><code>Series.groupby</code> doesn't currently keep the name of the original Series.</p>"},{"location":"api_docs/pandas/groupby/shift/","title":"<code>pd.core.groupby.DataFrameGroupby.shift</code>","text":"<p><code>pandas.core.groupby.DataFrameGroupby.shift(periods=1, freq=None, axis=0, fill_value=None)</code></p> <p>Note</p> <p><code>shift</code> is not supported on columns with nested array types</p>"},{"location":"api_docs/pandas/groupby/shift/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").shift()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n       A           C\n0    NaN         NaN\n1    NaN         NaN\n2    1.0       1.510\n3    2.0       2.421\n4   24.0  233232.000\n5    NaN      12.210\n6    1.0       1.510\n7    2.0       2.421\n8   24.0  233232.000\n9    NaN      12.210\n10   1.0       1.510\n11   2.0       2.421\n12  24.0  233232.000\n13   NaN      12.210\n14   1.0       1.510\n15   2.0       2.421\n16  24.0  233232.000\n17   NaN      12.210\n18   1.0       1.510\n19   2.0       2.421\n</code></pre>"},{"location":"api_docs/pandas/groupby/size/","title":"<code>pd.core.groupby.Groupby.size</code>","text":"<p><code>pandas.core.groupby.Groupby.size()</code></p>"},{"location":"api_docs/pandas/groupby/size/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").size()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\nB\n421    10\nf31    10\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/groupby/std/","title":"<code>pd.core.groupby.Groupby.std</code>","text":"<p><code>pandas.core.groupby.Groupby.std(ddof=1)</code></p> <p>Note</p> <p><code>std</code> is only supported on numeric columns and is not supported on boolean column</p>"},{"location":"api_docs/pandas/groupby/std/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").std()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n             A              C\nB\n421  12.122064  122923.261366\nf31   0.000000       5.159256\n</code></pre>"},{"location":"api_docs/pandas/groupby/sum/","title":"<code>pd.core.groupby.Groupby.sum</code>","text":"<p><code>pandas.core.groupby.Groupby.sum(numeric_only=NoDefault.no_default, min_count=0)</code></p> <p>Note</p> <p><code>sum</code> is not supported on columns with nested array types</p>"},{"location":"api_docs/pandas/groupby/sum/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").sum()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n         A            C\nB\n421  125.0  1166167.550\nf31   10.0       73.155\n</code></pre>"},{"location":"api_docs/pandas/groupby/transform/","title":"<code>pd.core.groupby.DataFrameGroupby.transform</code>","text":"<p><code>pandas.core.groupby.DataFrameGroupby.transform(func, \\*args, engine=None, engine_kwargs=None, **kwargs)</code></p>"},{"location":"api_docs/pandas/groupby/transform/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>func</code>: Constant string, Python function from the builtins module that matches a supported operation</li> <li>Numpy functions cannot be provided.</li> </ul> <p>Note</p> <p>The supported builtin functions are <code>'count'</code>, <code>'first'</code>, <code>'last'</code>, <code>'min'</code>, <code>'max'</code>, <code>'mean'</code>, <code>'median'</code>, <code>'nunique'</code>, <code>'prod'</code>, <code>'std'</code>, <code>'sum'</code>, and <code>'var'</code></p>"},{"location":"api_docs/pandas/groupby/transform/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\", dropna=True).transform(max)\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n       A          C\n0   24.0  233232.00\n1    2.0      12.21\n2   24.0  233232.00\n3    2.0      12.21\n4   24.0  233232.00\n5    2.0      12.21\n6   24.0  233232.00\n7    2.0      12.21\n8   24.0  233232.00\n9    2.0      12.21\n10  24.0  233232.00\n11   2.0      12.21\n12  24.0  233232.00\n13   2.0      12.21\n14  24.0  233232.00\n15   2.0      12.21\n16  24.0  233232.00\n17   2.0      12.21\n18  24.0  233232.00\n19   2.0      12.21\n</code></pre>"},{"location":"api_docs/pandas/groupby/value_counts/","title":"<code>pd.core.groupby.SeriesGroupBy.value_counts</code>","text":"<p><code>pandas.core.groupby.SeriesGroupby.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)</code></p>"},{"location":"api_docs/pandas/groupby/value_counts/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>ascending</code>: boolean<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/groupby/value_counts/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.groupby(level=0).value_counts()\n&gt;&gt;&gt; S = pd.Series([1, 2, 24, None] * 5, index = [\"421\", \"f31\"] * 10)\n&gt;&gt;&gt; f(S)\n\n421  1.0     5\n     24.0    5\nf31  2.0     5\nName: , dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/groupby/var/","title":"<code>pd.core.groupby.Groupby.var</code>","text":"<p><code>pandas.core.groupby.Groupby.var(ddof=1)</code></p> <p>Note</p> <p><code>var</code> is only supported on numeric columns and is not supported on boolean column</p>"},{"location":"api_docs/pandas/groupby/var/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.groupby(\"B\").var()\n&gt;&gt;&gt; df = pd.DataFrame(\n...      {\n...          \"A\": [1, 2, 24, None] * 5,\n...          \"B\": [\"421\", \"f31\"] * 10,\n...          \"C\": [1.51, 2.421, 233232, 12.21] * 5\n...      }\n... )\n&gt;&gt;&gt; f(df)\n\n              A             C\nB\n421  146.944444  1.511013e+10\nf31    0.000000  2.661792e+01\n</code></pre>"},{"location":"api_docs/pandas/indexapi/","title":"Index objects","text":""},{"location":"api_docs/pandas/indexapi/#index","title":"Index","text":""},{"location":"api_docs/pandas/indexapi/#properties","title":"Properties","text":"<ul> <li><code>pd.Index.name</code> </li> <li><code>pd.Index.names</code> </li> <li><code>pd.Index.shape</code> </li> <li><code>pd.Index.size</code> </li> <li><code>pd.Index.empty</code> </li> <li><code>pd.Index.is_monotonic_increasing</code> </li> <li><code>pd.Index.is_monotonic_decreasing</code> </li> <li><code>pd.Index.values</code> </li> <li><code>pd.Index.nbytes</code> </li> <li><code>pd.Index.ndim</code> </li> <li><code>pd.Index.nlevels</code> </li> <li><code>pd.Index.dtype</code> </li> <li><code>pd.Index.inferred_type</code> </li> <li><code>pd.Index.is_all_dates</code> </li> <li><code>pd.Index.T</code> </li> </ul>"},{"location":"api_docs/pandas/indexapi/#type-information","title":"Type information","text":"<ul> <li><code>pd.Index.is_numeric</code> </li> <li><code>pd.Index.is_integer</code> </li> <li><code>pd.Index.is_floating</code> </li> <li><code>pd.Index.is_boolean</code> </li> <li><code>pd.Index.is_categorical</code> </li> <li><code>pd.Index.is_interval</code> </li> <li><code>pd.Index.is_object</code> </li> </ul>"},{"location":"api_docs/pandas/indexapi/#modifications-and-computations","title":"Modifications and computations","text":"<ul> <li><code>pd.Index.copy</code> </li> <li><code>pd.Index.get_loc</code> </li> <li><code>pd.Index.take</code> </li> <li><code>pd.Index.min</code> </li> <li><code>pd.Index.max</code> </li> <li><code>pd.Index.rename</code> </li> <li><code>pd.Index.duplicated</code> </li> <li><code>pd.Index.drop_duplicates</code> </li> <li><code>pd.Index.isin</code> </li> <li><code>pd.Index.unique</code> </li> <li><code>pd.Index.nunique</code> </li> <li><code>pd.Index.sort_values</code> </li> <li><code>pd.Index.argsort</code> </li> <li><code>pd.Index.all</code> </li> <li><code>pd.Index.any</code> </li> <li><code>pd.Index.argmax</code> </li> <li><code>pd.Index.argmin</code> </li> <li><code>pd.Index.where</code> </li> <li><code>pd.Index.putmask</code> </li> <li><code>pd.Index.union</code> </li> <li><code>pd.Index.intersection</code> </li> <li><code>pd.Index.difference</code> </li> <li><code>pd.Index.symmetric_difference</code> </li> <li><code>pd.Index.repeat</code> </li> </ul>"},{"location":"api_docs/pandas/indexapi/#missing-values","title":"Missing values","text":"<ul> <li><code>pd.Index.isna</code> </li> <li><code>pd.Index.isnull</code> </li> </ul>"},{"location":"api_docs/pandas/indexapi/#conversion","title":"Conversion","text":"<ul> <li><code>pd.Index.map</code> </li> <li><code>pd.Index.to_series</code> </li> <li><code>pd.Index.to_frame</code> </li> <li><code>pd.Index.to_numpy</code> </li> <li><code>pd.Index.to_list</code> </li> <li><code>pd.Index.tolist</code> </li> </ul>"},{"location":"api_docs/pandas/indexapi/#numeric-index","title":"Numeric Index","text":"<p>Numeric index objects <code>RangeIndex</code>, <code>Int64Index</code>, <code>UInt64Index</code> and <code>Float64Index</code> are supported as index to dataframes and series. Constructing them in Bodo functions, passing them to Bodo functions (unboxing), and returning them from Bodo functions (boxing) are also supported.</p> <ul> <li><code>pd.RangeIndex</code> </li> <li><code>pd.Int64Index</code> </li> <li><code>pd.UInt64Index</code> </li> <li><code>pd.Float64Index</code></li> </ul>"},{"location":"api_docs/pandas/indexapi/#datetimeindex","title":"DatetimeIndex","text":"<p><code>DatetimeIndex</code> objects are supported. They can be constructed, boxed/unboxed, and set as index to dataframes and series.</p> <ul> <li><code>pd.DateTimeIndex</code> </li> <li><code>pd.DateTimeIndex.year</code> </li> <li><code>pd.DateTimeIndex.month</code> </li> <li><code>pd.DateTimeIndex.day</code> </li> <li><code>pd.DateTimeIndex.hour</code> </li> <li><code>pd.DateTimeIndex.minute</code> </li> <li><code>pd.DateTimeIndex.second</code> </li> <li><code>pd.DateTimeIndex.microsecond</code> </li> <li><code>pd.DateTimeIndex.nanosecond</code> </li> <li><code>pd.DateTimeIndex.date</code> </li> <li><code>pd.DateTimeIndex.dayofyear</code> </li> <li><code>pd.DateTimeIndex.day_of_year</code> </li> <li><code>pd.DateTimeIndex.dayofweek</code> </li> <li><code>pd.DateTimeIndex.day_of_week</code> </li> <li><code>pd.DateTimeIndex.is_leap_year</code> </li> <li><code>pd.DateTimeIndex.is_month_start</code> </li> <li><code>pd.DateTimeIndex.is_month_end</code> </li> <li><code>pd.DateTimeIndex.is_quarter_start</code> </li> <li><code>pd.DateTimeIndex.is_quarter_end</code> </li> <li><code>pd.DateTimeIndex.is_year_start</code> </li> <li><code>pd.DateTimeIndex.is_year_end</code> </li> <li><code>pd.DateTimeIndex.week</code> </li> <li><code>pd.DateTimeIndex.weekday</code> </li> <li><code>pd.DateTimeIndex.weekofyear</code> </li> <li><code>pd.DateTimeIndex.quarter</code> </li> </ul> <p>Subtraction of <code>Timestamp</code> from <code>DatetimeIndex</code> and vice versa is supported.</p> <p>Comparison operators <code>==</code>, <code>!=</code>, <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code> between <code>DatetimeIndex</code> and a string of datetime are supported.</p>"},{"location":"api_docs/pandas/indexapi/#timedeltaindex","title":"TimedeltaIndex","text":"<p><code>TimedeltaIndex</code> objects are supported. They can be constructed, boxed/unboxed, and set as index to dataframes and series.</p> <ul> <li><code>pd.TimedeltaIndex</code> </li> <li><code>pd.TimedeltaIndex.days</code> </li> <li><code>pd.TimedeltaIndex.seconds</code> </li> <li><code>pd.TimedeltaIndex.microseconds</code> </li> <li><code>pd.TimedeltaIndex.nanoseconds</code> </li> </ul>"},{"location":"api_docs/pandas/indexapi/#periodindex","title":"PeriodIndex","text":"<p><code>PeriodIndex</code> objects can be boxed/unboxed and set as index to dataframes and series. Operations on them will be supported in upcoming releases.</p>"},{"location":"api_docs/pandas/indexapi/#binaryindex","title":"BinaryIndex","text":"<p><code>BinaryIndex</code> objects can be boxed/unboxed and set as index to dataframes and series. Operations on them will be supported in upcoming releases.</p>"},{"location":"api_docs/pandas/indexapi/#multiindex","title":"MultiIndex","text":"<ul> <li><code>pd.MultiIndex.from_product</code></li> </ul>"},{"location":"api_docs/pandas/indexapi/all/","title":"<code>pd.Index.all</code>","text":"<p><code>pandasIndex.all(*args, **kwargs)</code></p>"},{"location":"api_docs/pandas/indexapi/all/#supported-arguments-none","title":"Supported Arguments: None","text":"<p>Supported Index Types</p> <ul> <li>NumericIndex (only Integers or Booleans)</li> <li>RangeIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> </ul> <p>!!! info \"Important\"       Bodo diverges from the Pandas API for StringIndex and BinaryIndex by always returning a boolean instead of sometimes returning a string.</p>"},{"location":"api_docs/pandas/indexapi/all/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.all()\n\n&gt;&gt;&gt; I = pd.Index([1, 4, 9, 0, 3])\n&gt;&gt;&gt; f(I)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/indexapi/any/","title":"<code>pd.Index.any</code>","text":"<p><code>pandasIndex.any(*args, **kwargs)</code></p>"},{"location":"api_docs/pandas/indexapi/any/#supported-arguments-none","title":"Supported Arguments: None","text":"<p>Supported Index Types</p> <ul> <li>NumericIndex (only Integers or Booleans)</li> <li>RangeIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> </ul> <p>!!! info \"Important\"       Bodo diverges from the Pandas API for StringIndex and BinaryIndex by always returning a boolean instead of sometimes returning a string.</p>"},{"location":"api_docs/pandas/indexapi/any/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.any()\n\n&gt;&gt;&gt; I = pd.Index([1, 4, 9, 0, 3])\n&gt;&gt;&gt; f(I)\nTrue\n</code></pre>"},{"location":"api_docs/pandas/indexapi/argmax/","title":"<code>pd.Index.argmax</code>","text":"<p><code>pandasIndex.argmax(axis=None, skipna=True, *args, **kwargs)</code></p>"},{"location":"api_docs/pandas/indexapi/argmax/#supported-arguments-none","title":"Supported Arguments: None","text":"<p>Unsupported Index Types</p> <ul> <li>IntervalIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/argmax/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.argmax()\n\n&gt;&gt;&gt; I = pd.Index([1, 4, 9, 0, 3])\n&gt;&gt;&gt; f(I)\n2\n</code></pre>"},{"location":"api_docs/pandas/indexapi/argmin/","title":"<code>pd.Index.argmin</code>","text":"<p><code>pandasIndex.argmin(axis=None, skipna=True, *args, **kwargs)</code></p>"},{"location":"api_docs/pandas/indexapi/argmin/#supported-arguments-none","title":"Supported Arguments: None","text":"<p>Unsupported Index Types</p> <ul> <li>IntervalIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/argmin/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.argmin()\n\n&gt;&gt;&gt; I = pd.Index([1, 4, 9, 0, 3])\n&gt;&gt;&gt; f(I)\n3\n</code></pre>"},{"location":"api_docs/pandas/indexapi/argsort/","title":"<code>pd.Index.argsort</code>","text":"<p><code>pandasIndex.argsort(*args, **kwargs)</code></p>"},{"location":"api_docs/pandas/indexapi/argsort/#supported-arguments-none","title":"Supported Arguments: None","text":"<p>Unsupported Index Types</p> <ul> <li>IntervalIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/argsort/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.argsort()\n\n&gt;&gt;&gt; I = pd.Index([\"A\", \"L\", \"P\", \"H\", \"A\"])\n&gt;&gt;&gt; f(I)\narray([0, 4, 3, 1, 2])\n</code></pre>"},{"location":"api_docs/pandas/indexapi/copy/","title":"<code>pd.Index.copy</code>","text":"<p><code>pandasIndex.copy(name=None, deep=False, dtype=None, names=None)</code></p> <p>Unsupported Index Types</p> <ul> <li>MultiIndex</li> <li>IntervalIndex</li> </ul> <p>Supported arguments</p> <ul> <li><code>name</code></li> </ul>"},{"location":"api_docs/pandas/indexapi/copy/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.copy(name=\"new_name\")\n\n&gt;&gt;&gt; I = pd.Index([1,2,3], name = \"origial_name\")\n&gt;&gt;&gt; f(I)\nInt64Index([1, 2, 3], dtype='int64', name='new_name')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/date/","title":"<code>pd.DateTimeIndex.date</code>","text":"<p><code>pandasDatetimeIndex.date</code></p>"},{"location":"api_docs/pandas/indexapi/date/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.date\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\n[datetime.date(2019, 12, 31) datetime.date(2019, 12, 31) datetime.date(2019, 12, 31) datetime.date(2020, 1, 1) datetime.date(2020, 1, 1)]\n</code></pre>"},{"location":"api_docs/pandas/indexapi/datetimeindex/","title":"<code>pd.DateTimeIndex</code>","text":"<p><code>pandas.DatetimeIndex</code></p>"},{"location":"api_docs/pandas/indexapi/datetimeindex/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>data</code>: array-like of datetime64, Integer, or strings</li> </ul>"},{"location":"api_docs/pandas/indexapi/datetimeindex/#date-fields","title":"Date fields","text":""},{"location":"api_docs/pandas/indexapi/day/","title":"<code>pd.DateTimeIndex.day</code>","text":"<p><code>pandasDatetimeIndex.day</code></p>"},{"location":"api_docs/pandas/indexapi/day/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.day\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([31, 31, 31, 1, 1], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/day_of_week/","title":"<code>pd.DateTimeIndex.day_of_week</code>","text":"<p><code>pandasDatetimeIndex.day_of_week</code></p>"},{"location":"api_docs/pandas/indexapi/day_of_week/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.day_of_week\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([1, 1, 1, 2, 2], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/day_of_year/","title":"<code>pd.DateTimeIndex.day_of_year</code>","text":"<p><code>pandasDatetimeIndex.day_of_year</code></p>"},{"location":"api_docs/pandas/indexapi/day_of_year/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.day_of_year\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([365, 365, 365, 1, 1], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/dayofweek/","title":"<code>pd.DateTimeIndex.dayofweek</code>","text":"<p><code>pandasDatetimeIndex.dayofweek</code></p>"},{"location":"api_docs/pandas/indexapi/dayofweek/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.dayofweek\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([1, 1, 1, 2, 2], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/dayofyear/","title":"<code>pd.DateTimeIndex.dayofyear</code>","text":"<p><code>pandasDatetimeIndex.dayofyear</code></p>"},{"location":"api_docs/pandas/indexapi/dayofyear/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.dayofyear\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([365, 365, 365, 1, 1], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/days/","title":"<code>pd.TimedeltaIndex.days</code>","text":"<p><code>pandasTimedeltaIndex.days</code></p>"},{"location":"api_docs/pandas/indexapi/days/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.days\n\n&gt;&gt;&gt; I = pd.TimedeltaIndex([pd.Timedelta(3, unit=\"D\"))])\n&gt;&gt;&gt; f(I)\nInt64Index([3], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/difference/","title":"<code>pd.Index.difference</code>","text":"<p><code>pandasIndex.difference(other, sort=None)</code></p>"},{"location":"api_docs/pandas/indexapi/difference/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>other</code>: can be an Index, Series, or 1-dim numpy array with a matching type for the Index</li> </ul> <p>Supported Index Types</p> <ul> <li>NumericIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> <li>RangeIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> </ul> <p>Important</p> <p>Bodo diverges from the Pandas API for Index.difference() in several ways: the order of elements may be different and a NumericIndex is always returned instead of a RangeIndex.</p>"},{"location":"api_docs/pandas/indexapi/difference/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit(distributed=[\"I\", \"J\"])\n... def f(I, J):\n...    return I.difference(J)\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3, 4, 5])\n&gt;&gt;&gt; J = pd.Index([2, 4, 6, 8, 10, 12])\n&gt;&gt;&gt; f(I, J)\nInt64Index([1, 3, 5], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/drop_duplicates/","title":"<code>pd.Index.drop_duplicates</code>","text":"<p><code>pandasIndex.drop_duplicates(keep='first')</code></p>"},{"location":"api_docs/pandas/indexapi/drop_duplicates/#supported-arguments-none","title":"Supported Arguments: None","text":"<p>Unsupported Index Types</p> <ul> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/drop_duplicates/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.drop_duplicates()\n\n&gt;&gt;&gt; I = pd.Index([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"])\n&gt;&gt;&gt; f(I)\nIndex(['a', 'b', 'c'], dtype='object')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/dtype/","title":"<code>pd.Index.dtype</code>","text":"<p><code>pandasIndex.dtype</code></p> <p>Unsupported Index Types</p> <ul> <li>PeriodIndex</li> <li>IntervalIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/dtype/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.dtype\n\n&gt;&gt;&gt; I = pd.Index([1,2,3,4])\n&gt;&gt;&gt; f(I)\ndtype('int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/duplicated/","title":"<code>pd.Index.duplicated</code>","text":"<p><code>pandasIndex.duplicated(keep='first')</code></p>"},{"location":"api_docs/pandas/indexapi/duplicated/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/indexapi/duplicated/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.duplicated()\n\n&gt;&gt;&gt; idx = pd.Index(['a', 'b', None, 'a', 'c', None, 'd', 'b'])\n&gt;&gt;&gt; f(idx)\narray([False, False, False,  True, False,  True, False,  True])\n</code></pre>"},{"location":"api_docs/pandas/indexapi/empty/","title":"<code>pd.Index.empty</code>","text":"<p><code>pandasIndex.empty</code></p>"},{"location":"api_docs/pandas/indexapi/empty/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.empty\n\n&gt;&gt;&gt; I = pd.Index([\"A\", \"B\", \"C\"])\n&gt;&gt;&gt; f(I)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/indexapi/float64index/","title":"<code>pd.Float64Index</code>","text":"<p><code>pandas.Float64Index(data=None, dtype=None, copy=False, name=None)</code></p>"},{"location":"api_docs/pandas/indexapi/float64index/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>data</code>: list or array</li> <li><code>copy</code>: Boolean</li> <li><code>name</code>: String</li> </ul>"},{"location":"api_docs/pandas/indexapi/float64index/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n... return pd.Float64Index(np.arange(3))\n\n&gt;&gt;&gt; f()\nFloat64Index([0.0, 1.0, 2.0], dtype='float64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/float64index/#datetimeindex","title":"DatetimeIndex","text":"<p><code>DatetimeIndex</code> objects are supported. They can be constructed, boxed/unboxed, and set as index to dataframes and series.</p>"},{"location":"api_docs/pandas/indexapi/from_product/","title":"<code>pd.MultiIndex.from_product</code>","text":"<p><code>pandasMultiIndex.from_product</code>  (iterables and names supported as tuples, no parallel support yet)</p>"},{"location":"api_docs/pandas/indexapi/get_loc/","title":"<code>pd.Index.get_loc</code>","text":"<ul> <li><code>pandas.Index.get_loc(key, method=None, tolerance=None)</code></li> </ul> <p>Note</p> <p>Should be about as fast as standard python, maybe slightly slower.</p> <p>Unsupported Index Types</p> <ul> <li>CategoricalIndex</li> <li>MultiIndex</li> <li>IntervalIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/get_loc/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>key</code>: must be of same type as the index</li> </ul> <p>Important</p> <ul> <li>Only works for index with unique values (scalar return).</li> <li>Only works with replicated Index</li> </ul>"},{"location":"api_docs/pandas/indexapi/get_loc/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.get_loc(2)\n\n&gt;&gt;&gt; I = pd.Index([1,2,3])\n&gt;&gt;&gt; f(I)\n1\n</code></pre>"},{"location":"api_docs/pandas/indexapi/hour/","title":"<code>pd.DateTimeIndex.hour</code>","text":"<p><code>pandasDatetimeIndex.hour</code></p>"},{"location":"api_docs/pandas/indexapi/hour/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.hour\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([2, 12, 22, 9, 19], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/inferred_type/","title":"<code>pd.Index.inferred_type</code>","text":"<p><code>pandasIndex.inferred_type</code></p>"},{"location":"api_docs/pandas/indexapi/inferred_type/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.dtype\n\n&gt;&gt;&gt; I = pd.Index([\"A\", \"E\", \"I\", \"O\", \"U\"])\n&gt;&gt;&gt; f(I)\n'string'\n</code></pre>"},{"location":"api_docs/pandas/indexapi/int64index/","title":"<code>pd.Int64Index</code>","text":"<p><code>pandas.Int64Index(data=None, dtype=None, copy=False, name=None)</code></p>"},{"location":"api_docs/pandas/indexapi/int64index/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n... return pd.Int64Index(np.arange(3))\n\n&gt;&gt;&gt; f()\nInt64Index([0, 1, 2], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/intersection/","title":"<code>pd.Index.intersection</code>","text":"<p><code>pandasIndex.intersection(other, sort=None)</code></p>"},{"location":"api_docs/pandas/indexapi/intersection/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>other</code>: can be an Index, Series, or 1-dim numpy array with a matching type for the Index</li> </ul> <p>Supported Index Types</p> <ul> <li>NumericIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> <li>RangeIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> </ul> <p>Important</p> <p>Bodo diverges from the Pandas API for Index.intersection() in several ways: the default is sort=None, and a NumericIndex is always returned instead of a RangeIndex.</p>"},{"location":"api_docs/pandas/indexapi/intersection/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit(distributed=[\"I\", \"J\"])\n... def f(I, J):\n...    return I.intersection(J)\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3, 4, 5])\n&gt;&gt;&gt; J = pd.Index([2, 4, 6, 8, 10, 12])\n&gt;&gt;&gt; f(I, J)\nInt64Index([2, 4], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_all_dates/","title":"<code>pd.Index.is_all_dates</code>","text":"<p><code>pandasIndex.is_all_dates</code></p>"},{"location":"api_docs/pandas/indexapi/is_all_dates/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_all_dates\n\n&gt;&gt;&gt; I = pd.date_range(\"2018-01-01\", \"2018-01-06\")\n&gt;&gt;&gt; f(I)\nTrue\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_boolean/","title":"<code>pd.Index.is_boolean</code>","text":"<p><code>pandasIndex.is_boolean()</code></p>"},{"location":"api_docs/pandas/indexapi/is_boolean/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_boolean()\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3])\n&gt;&gt;&gt; f(I)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_categorical/","title":"<code>pd.Index.is_categorical</code>","text":"<p><code>pandasIndex.is_categorical()</code></p>"},{"location":"api_docs/pandas/indexapi/is_categorical/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_categorical()\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3])\n&gt;&gt;&gt; f(I)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_floating/","title":"<code>pd.Index.is_floating</code>","text":"<p><code>pandasIndex.is_floating()</code></p>"},{"location":"api_docs/pandas/indexapi/is_floating/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_floating()\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3])\n&gt;&gt;&gt; f(I)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_integer/","title":"<code>pd.Index.is_integer</code>","text":"<p><code>pandasIndex.is_integer()</code></p>"},{"location":"api_docs/pandas/indexapi/is_integer/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_integer()\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3])\n&gt;&gt;&gt; f(I)\nTrue\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_interval/","title":"<code>pd.Index.is_interval</code>","text":"<p><code>pandasIndex.is_interval()</code></p>"},{"location":"api_docs/pandas/indexapi/is_interval/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_interval()\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3])\n&gt;&gt;&gt; f(I)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_leap_year/","title":"<code>pd.DateTimeIndex.is_leap_year</code>","text":"<p><code>pandasDatetimeIndex.is_leap_year</code></p>"},{"location":"api_docs/pandas/indexapi/is_leap_year/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_leap_year\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\n[False False False True True]\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_monotonic_decreasing/","title":"<code>pd.Index.is_monotonic_decreasing</code>","text":"<p><code>pandasIndex.is_monotonic_decreasing</code></p> <p>Unsupported Index Types</p> <ul> <li>StringIndex</li> <li>BinaryIndex</li> <li>IntervalIndex</li> <li>CategoricalIndex</li> <li>PeriodIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/is_monotonic_decreasing/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_monotonic_decreasing\n\n&gt;&gt;&gt; I = pd.Index([1,2,3])\n&gt;&gt;&gt; f(I)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_monotonic_increasing/","title":"<code>pd.Index.is_monotonic_increasing</code>","text":"<p><code>pandasIndex.is_monotonic_increasing</code> and <code>pandas.Index.is_monotonic</code></p> <p>Unsupported Index Types</p> <ul> <li>StringIndex</li> <li>BinaryIndex</li> <li>IntervalIndex</li> <li>CategoricalIndex</li> <li>PeriodIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/is_monotonic_increasing/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_monotonic_increasing\n\n&gt;&gt;&gt; I = pd.Index([1,2,3])\n&gt;&gt;&gt; f(I)\nTrue\n\n&gt;&gt;&gt; @bodo.jit\n... def g(I):\n...   return I.is_monotonic\n\n&gt;&gt;&gt; I = pd.Index(1,2,3])\n&gt;&gt;&gt; g(I)\nTrue\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_month_end/","title":"<code>pd.DateTimeIndex.is_month_end</code>","text":"<p><code>pandasDatetimeIndex.is_month_end</code></p>"},{"location":"api_docs/pandas/indexapi/is_month_end/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_month_end\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([1, 1, 1, 0, 0], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_month_start/","title":"<code>pd.DateTimeIndex.is_month_start</code>","text":"<p><code>pandasDatetimeIndex.is_month_start</code></p>"},{"location":"api_docs/pandas/indexapi/is_month_start/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_month_start\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([0, 0, 0, 1, 1], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_numeric/","title":"<code>pd.Index.is_numeric</code>","text":"<p><code>pandasIndex.is_numeric()</code></p>"},{"location":"api_docs/pandas/indexapi/is_numeric/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_numeric()\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3])\n&gt;&gt;&gt; f(I)\nTrue\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_object/","title":"<code>pd.Index.is_object</code>","text":"<p><code>pandasIndex.is_object()</code></p> <p>Important</p> <p>Currently, Bodo diverges from the Pandas API for Indices of boolean values. Bodo always returns True, whereas Pandas returns False if the index was constructed from a pd.array of booleans.</p>"},{"location":"api_docs/pandas/indexapi/is_object/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_object()\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3])\n&gt;&gt;&gt; f(I)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_object/#modifications-and-computations","title":"Modifications and computations","text":""},{"location":"api_docs/pandas/indexapi/is_quarter_end/","title":"<code>pd.DateTimeIndex.is_quarter_end</code>","text":"<p><code>pandasDatetimeIndex.is_quarter_end</code></p>"},{"location":"api_docs/pandas/indexapi/is_quarter_end/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_quarter_end\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([1, 1, 1, 0, 0], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_quarter_start/","title":"<code>pd.DateTimeIndex.is_quarter_start</code>","text":"<p><code>pandasDatetimeIndex.is_quarter_start</code></p>"},{"location":"api_docs/pandas/indexapi/is_quarter_start/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_quarter_start\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([0, 0, 0, 1, 1], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_year_end/","title":"<code>pd.DateTimeIndex.is_year_end</code>","text":"<p><code>pandasDatetimeIndex.is_year_end</code></p>"},{"location":"api_docs/pandas/indexapi/is_year_end/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_year_end\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([1, 1, 1, 0, 0], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/is_year_start/","title":"<code>pd.DateTimeIndex.is_year_start</code>","text":"<p><code>pandasDatetimeIndex.is_year_start</code></p>"},{"location":"api_docs/pandas/indexapi/is_year_start/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.is_year_start\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([0, 0, 0, 1, 1], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/isin/","title":"<code>pd.Index.isin</code>","text":"<p><code>pandasIndex.isin(values)</code></p>"},{"location":"api_docs/pandas/indexapi/isin/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>values</code>: list-like or array-like of values</li> </ul> <p>Unsupported Index Types</p> <ul> <li>MultiIndex</li> <li>IntervalIndex</li> <li>PeriodIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/isin/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.isin([0, 2, 4])\n\n&gt;&gt;&gt; I = pd.Index([2, 4, 3, 4, 0, 3, 3, 5])\n&gt;&gt;&gt; f(I)\narray([ True,  True, False,  True,  True, False, False, False])\n</code></pre>"},{"location":"api_docs/pandas/indexapi/isna/","title":"<code>pd.Index.isna</code>","text":"<p><code>pandasIndex.isna()</code></p> <p>Unsupported Index Types</p> <ul> <li>MultiIndex</li> <li>IntervalIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/isna/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.isna()\n\n&gt;&gt;&gt; I = pd.Index([1,None,3])\n&gt;&gt;&gt; f(I)\n[False  True False]\n</code></pre>"},{"location":"api_docs/pandas/indexapi/isnull/","title":"<code>pd.Index.isnull</code>","text":"<p><code>pandasIndex.isnull()</code></p> <p>Unsupported Index Types</p> <ul> <li>MultiIndex</li> <li>IntervalIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/isnull/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.isnull()\n\n&gt;&gt;&gt; I = pd.Index([1,None,3])\n&gt;&gt;&gt; f(I)\n[False  True False]\n</code></pre>"},{"location":"api_docs/pandas/indexapi/isnull/#conversion","title":"Conversion","text":""},{"location":"api_docs/pandas/indexapi/map/","title":"<code>pd.Index.map</code>","text":"<p><code>pandasIndex.map(mapper, na_action=None)</code></p> <p>Unsupported Index Types</p> <ul> <li>MultiIndex</li> <li>IntervalIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/map/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>mapper</code>: must be a function, function cannot return tuple type</li> </ul>"},{"location":"api_docs/pandas/indexapi/map/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.map(lambda x: x + 2)\n\n&gt;&gt;&gt; I = pd.Index([1,None,3])\n&gt;&gt;&gt; f(I)\nFloat64Index([3.0, nan, 5.0], dtype='float64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/max/","title":"<code>pd.Index.max</code>","text":"<p><code>pandasIndex.max(axis=None, skipna=True, *args, **kwargs)</code></p>"},{"location":"api_docs/pandas/indexapi/max/#supported-arguments-none","title":"Supported Arguments: None","text":"<p>Supported Index Types</p> <ul> <li>NumericIndex</li> <li>RangeIndex</li> <li>CategoricalIndex</li> <li>TimedeltaIndex</li> <li>DatetimeIndex</li> </ul> <p>Important</p> <ul> <li>Bodo Does Not support <code>args</code> and <code>kwargs</code>, even for compatibility.</li> <li>For DatetimeIndex, will throw an error if all values in the index are null.</li> </ul>"},{"location":"api_docs/pandas/indexapi/max/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.min()\n\n&gt;&gt;&gt; I = pd.Index(pd.date_range(start=\"2018-04-24\", end=\"2018-04-25\", periods=5))\n&gt;&gt;&gt; f(I)\n2018-04-25 00:00:00\n</code></pre>"},{"location":"api_docs/pandas/indexapi/microsecond/","title":"<code>pd.DateTimeIndex.microsecond</code>","text":"<p><code>pandasDatetimeIndex.microsecond</code></p>"},{"location":"api_docs/pandas/indexapi/microsecond/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.microsecond\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 01:01:01\", end=\"2019-12-31 01:01:02\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([0, 250000, 500000, 750000, 0], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/microseconds/","title":"<code>pd.TimedeltaIndex.microseconds</code>","text":"<p><code>pandasTimedeltaIndex.microseconds</code></p>"},{"location":"api_docs/pandas/indexapi/microseconds/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.microseconds\n\n&gt;&gt;&gt; I = pd.TimedeltaIndex([pd.Timedelta(11, unit=\"micros\"))])\n&gt;&gt;&gt; f(I)\nInt64Index([11], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/min/","title":"<code>pd.Index.min</code>","text":"<p><code>pandasIndex.min(axis=None, skipna=True, *args, **kwargs)</code></p>"},{"location":"api_docs/pandas/indexapi/min/#supported-arguments-none","title":"Supported Arguments: None","text":"<p>Supported Index Types</p> <ul> <li>NumericIndex</li> <li>RangeIndex</li> <li>CategoricalIndex</li> <li>TimedeltaIndex</li> <li>DatetimeIndex</li> </ul> <p>Important</p> <ul> <li>Bodo Does Not support <code>args</code> and <code>kwargs</code>, even for compatibility.</li> <li>For DatetimeIndex, will throw an error if all values in the index are null.</li> </ul>"},{"location":"api_docs/pandas/indexapi/min/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.min()\n\n&gt;&gt;&gt; I = pd.Index(pd.date_range(start=\"2018-04-24\", end=\"2018-04-25\", periods=5))\n&gt;&gt;&gt; f(I)\n2018-04-24 00:00:00\n</code></pre>"},{"location":"api_docs/pandas/indexapi/minute/","title":"<code>pd.DateTimeIndex.minute</code>","text":"<p><code>pandasDatetimeIndex.minute</code></p>"},{"location":"api_docs/pandas/indexapi/minute/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.minute\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([32, 42, 52, 2, 12], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/month/","title":"<code>pd.DateTimeIndex.month</code>","text":"<p><code>pandasDatetimeIndex.month</code></p>"},{"location":"api_docs/pandas/indexapi/month/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.month\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([12, 12, 12, 1, 1], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/name/","title":"<code>pd.Index.name</code>","text":"<p><code>pandasIndex.name</code></p>"},{"location":"api_docs/pandas/indexapi/name/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.name\n\n&gt;&gt;&gt; I = pd.Index([1,2,3], name = \"hello world\")\n&gt;&gt;&gt; f(I)\n\"hello world\"\n</code></pre>"},{"location":"api_docs/pandas/indexapi/names/","title":"<code>pd.Index.names</code>","text":"<p><code>pandasIndex.names</code></p> <p>Important</p> <p>Bodo returns a tuple instead of a FrozenList.</p>"},{"location":"api_docs/pandas/indexapi/names/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.names\n\n&gt;&gt;&gt; I = pd.MultiIndex.from_product([[1, 2], [\"A\", \"B\"]], names=[\"C1\", \"C2\"])\n&gt;&gt;&gt; f(I)\n('C1', 'C2')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/nanosecond/","title":"<code>pd.DateTimeIndex.nanosecond</code>","text":"<p><code>pandasDatetimeIndex.nanosecond</code></p>"},{"location":"api_docs/pandas/indexapi/nanosecond/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.nanosecond\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 01:01:01.0000001\", end=\"2019-12-31 01:01:01.0000002\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([100, 125, 150, 175, 200], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/nanoseconds/","title":"<code>pd.TimedeltaIndex.nanoseconds</code>","text":"<p><code>pandasTimedeltaIndex.nanoseconds</code></p>"},{"location":"api_docs/pandas/indexapi/nanoseconds/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.nanoseconds\n\n&gt;&gt;&gt; I = pd.TimedeltaIndex([pd.Timedelta(7, unit=\"nanos\"))])\n&gt;&gt;&gt; f(I)\nInt64Index([7], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/nanoseconds/#periodindex","title":"PeriodIndex","text":"<p><code>PeriodIndex</code> objects can be boxed/unboxed and set as index to dataframes and series. Operations on them will be supported in upcoming releases.</p>"},{"location":"api_docs/pandas/indexapi/nanoseconds/#binaryindex","title":"BinaryIndex","text":"<p><code>BinaryIndex</code> objects can be boxed/unboxed and set as index to dataframes and series. Operations on them will be supported in upcoming releases.</p>"},{"location":"api_docs/pandas/indexapi/nanoseconds/#multiindex","title":"MultiIndex","text":""},{"location":"api_docs/pandas/indexapi/nbytes/","title":"<code>pd.Index.nbytes</code>","text":"<p><code>pandasIndex.nbytes</code></p> <p>Unsupported Index Types</p> <ul> <li>MultiIndex</li> <li>IntervalIndex</li> </ul> <p>Important</p> <p>Currently, Bodo upcasts all numeric index data types to 64 bitwidth.</p>"},{"location":"api_docs/pandas/indexapi/nbytes/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.nbytes\n\n&gt;&gt;&gt; I1 = pd.Index([1,2,3,4,5,6], dtype = np.int64)\n&gt;&gt;&gt; f(I1)\n48\n&gt;&gt;&gt; I2 = pd.Index([1,2,3], dtype = np.int64)\n&gt;&gt;&gt; f(I2)\n24\n&gt;&gt;&gt; I3 = pd.Index([1,2,3], dtype = np.int32)\n&gt;&gt;&gt; f(I3)\n24\n</code></pre>"},{"location":"api_docs/pandas/indexapi/ndim/","title":"<code>pd.Index.ndim</code>","text":"<p><code>pandasIndex.ndim</code></p>"},{"location":"api_docs/pandas/indexapi/ndim/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.ndim\n\n&gt;&gt;&gt; I = pd.Index([1,2,3,4])\n&gt;&gt;&gt; f(I)\n1\n</code></pre>"},{"location":"api_docs/pandas/indexapi/nlevels/","title":"<code>pd.Index.nlevels</code>","text":"<p><code>pandasIndex.nlevels</code></p>"},{"location":"api_docs/pandas/indexapi/nlevels/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.nlevels\n\n&gt;&gt;&gt; I = pd.MultiIndex.from_arrays([[1, 2, 3, 4],[\"A\", \"A\", \"B\", \"B\"]])\n&gt;&gt;&gt; f(I)\n2\n</code></pre>"},{"location":"api_docs/pandas/indexapi/nunique/","title":"<code>pd.Index.nunique</code>","text":"<p><code>pandasIndex.nunique(dropna=True)</code></p>"},{"location":"api_docs/pandas/indexapi/nunique/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>dropna</code>: can be True or False</li> </ul> <p>Unsupported Index Types</p> <ul> <li>IntervalIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/nunique/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.nunique()\n\n&gt;&gt;&gt; I = pd.Index([1, 5, 2, 1, 0, 1, 5, 2, 1])\n&gt;&gt;&gt; f(I)\n4\n</code></pre>"},{"location":"api_docs/pandas/indexapi/putmask/","title":"<code>pd.Index.putmask</code>","text":"<p><code>pandasIndex.putmask(cond, other=None)</code></p>"},{"location":"api_docs/pandas/indexapi/putmask/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>cond</code>: can be a Series or 1-dim array of booleans</li> <li><code>other</code>: can be a scalar, non-categorical Series, 1-dim numpy array or StringArray with a matching type for the Index</li> </ul> <p>Unsupported Index Types</p> <ul> <li>IntervalIndex</li> <li>MultiIndex</li> </ul> <p>Important</p> <p>Only supported for CategoricalIndex if the elements of other are the same as (or a subset of) the categories of the CategoricalIndex.</p>"},{"location":"api_docs/pandas/indexapi/putmask/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I, C, O):\n...   return I.putmask(C, O)\n\n&gt;&gt;&gt; I = pd.Index([\"A\", \"B\", \"C\", \"D\", \"E\"])\n&gt;&gt;&gt; C = pd.array([True, False, True, True, False])\n&gt;&gt;&gt; O = pd.Series([\"a\", \"e\", \"i\", \"o\", \"u\")\n&gt;&gt;&gt; f(I, C, O)\nIndex(['a', 'B', 'i', 'o', 'E'], dtype='object')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/quarter/","title":"<code>pd.DateTimeIndex.quarter</code>","text":"<p><code>pandasDatetimeIndex.quarter</code></p>"},{"location":"api_docs/pandas/indexapi/quarter/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.quarter\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([4, 4, 4, 1, 1], dtype='int64')\n</code></pre> <p>Subtraction of <code>Timestamp</code> from <code>DatetimeIndex</code> and vice versa is supported.</p> <p>Comparison operators <code>==</code>, <code>!=</code>, <code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code> between <code>DatetimeIndex</code> and a string of datetime are supported.</p>"},{"location":"api_docs/pandas/indexapi/quarter/#timedeltaindex","title":"TimedeltaIndex","text":"<p><code>TimedeltaIndex</code> objects are supported. They can be constructed, boxed/unboxed, and set as index to dataframes and series.</p>"},{"location":"api_docs/pandas/indexapi/rangeindex/","title":"<code>pd.RangeIndex</code>","text":"<p><code>pandas.RangeIndex(start=None, stop=None, step=None, dtype=None, copy=False, name=None)</code></p>"},{"location":"api_docs/pandas/indexapi/rangeindex/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>start</code>: integer</li> <li><code>stop</code>: integer</li> <li><code>step</code>: integer</li> <li><code>name</code>: String</li> </ul>"},{"location":"api_docs/pandas/indexapi/rangeindex/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.RangeIndex(0, 10, 2)\n\n&gt;&gt;&gt; f(I)\nRangeIndex(start=0, stop=10, step=2)\n</code></pre>"},{"location":"api_docs/pandas/indexapi/rename/","title":"<code>pd.Index.rename</code>","text":"<p><code>pandasIndex.rename(name, inplace=False)</code></p>"},{"location":"api_docs/pandas/indexapi/rename/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>name</code>: label or list of labels</li> </ul> <p>Unsupported Index Types</p> <ul> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/rename/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I, name):\n...   return I.rename(name)\n\n&gt;&gt;&gt; I = pd.Index([\"a\", \"b\", \"c\"])\n&gt;&gt;&gt; f(I, \"new_name\")\nIndex(['a', 'b', 'c'], dtype='object', name='new_name')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/repeat/","title":"<code>pd.Index.repeat</code>","text":"<p><code>pandasIndex.repeat(repeats, axis=None)</code></p>"},{"location":"api_docs/pandas/indexapi/repeat/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>repeat</code>: can be a non-negative integer or array of non-negative integers</li> </ul> <p>Supported Index Types</p> <ul> <li>NumericIndex</li> <li>StringIndex</li> <li>RangeIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> <li>CategoricalIndex</li> </ul> <p>Important</p> <p>If repeats is an integer array but its size is not the same as the length of I, undefined behavior may occur.</p>"},{"location":"api_docs/pandas/indexapi/repeat/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit(distributed=[\"I\"])\n... def f(I):\n...    return I.repeat(3)\n\n&gt;&gt;&gt; I = pd.Index([\"A\", \"B\", \"C\"])\n&gt;&gt;&gt; f(I)\nIndex(['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'], dtype='object')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/repeat/#missing-values","title":"Missing values","text":""},{"location":"api_docs/pandas/indexapi/second/","title":"<code>pd.DateTimeIndex.second</code>","text":"<p><code>pandasDatetimeIndex.second</code></p>"},{"location":"api_docs/pandas/indexapi/second/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.second\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([45, 35, 25, 15, 5], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/seconds/","title":"<code>pd.TimedeltaIndex.seconds</code>","text":"<p><code>pandasTimedeltaIndex.seconds</code></p>"},{"location":"api_docs/pandas/indexapi/seconds/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.seconds\n\n&gt;&gt;&gt; I = pd.TimedeltaIndex([pd.Timedelta(-2, unit=\"S\"))])\n&gt;&gt;&gt; f(I)\nInt64Index([-2], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/shape/","title":"<code>pd.Index.shape</code>","text":"<p><code>pandasIndex.shape</code></p>"},{"location":"api_docs/pandas/indexapi/shape/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.shape\n\n&gt;&gt;&gt; I = pd.Index([1,2,3])\n&gt;&gt;&gt; f(I)\n(3,)\n</code></pre>"},{"location":"api_docs/pandas/indexapi/size/","title":"<code>pd.Index.size</code>","text":"<p><code>pandasIndex.size</code></p>"},{"location":"api_docs/pandas/indexapi/size/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.size\n\n&gt;&gt;&gt; I = pd.Index([1,7,8,6])\n&gt;&gt;&gt; f(I)\n4\n</code></pre>"},{"location":"api_docs/pandas/indexapi/sort_values/","title":"<code>pd.Index.sort_values</code>","text":"<p><code>pandasIndex.sort_values(return_indexer=False, ascending=True, na_position=\"last\", key=None)</code></p>"},{"location":"api_docs/pandas/indexapi/sort_values/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>ascending</code>: can be True or False</li> <li><code>na_position</code>: can be \"first\" or \"last\"</li> </ul> <p>Unsupported Index Types</p> <ul> <li>IntervalIndex</li> <li>PeriodIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/sort_values/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.sort_values()\n\n&gt;&gt;&gt; I = pd.Index([0, -1, 1, -5, 8, -13, -2, 3])\n&gt;&gt;&gt; f(I)\nInt64Index([-13, -5, -2, -1, 0, 1, 3, 8], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/symmetric_difference/","title":"<code>pd.Index.symmetric_difference</code>","text":"<p><code>pandasIndex.symmetric_difference(other, sort=None)</code></p>"},{"location":"api_docs/pandas/indexapi/symmetric_difference/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>other</code>: can be an Index, Series, or 1-dim numpy array with a matching type for the Index</li> </ul> <p>Supported Index Types</p> <ul> <li>NumericIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> <li>RangeIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> </ul> <p>Important</p> <p>Bodo diverges from the Pandas API for Index.symmetric_difference() in several ways: the order of elements may be different and a NumericIndex is always returned instead of a RangeIndex.</p>"},{"location":"api_docs/pandas/indexapi/symmetric_difference/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit(distributed=[\"I\", \"J\"])\n... def f(I, J):\n...    return I.difference(J)\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3, 4, 5])\n&gt;&gt;&gt; J = pd.Index([2, 4, 6, 8, 10, 12])\n&gt;&gt;&gt; f(I, J)\nInt64Index([1, 3, 5, 6, 8, 10, 12], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/t/","title":"<code>pd.Index.T</code>","text":"<p><code>pandasIndex.T</code></p>"},{"location":"api_docs/pandas/indexapi/t/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.T\n\n&gt;&gt;&gt; I = pd.Index([\"A\", \"E\", \"I\", \"O\", \"U\"])\n&gt;&gt;&gt; f(I)\nIndex([\"A\", \"E\", \"I\", \"O\", \"U\"], dtype='object')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/t/#type-information","title":"Type information","text":""},{"location":"api_docs/pandas/indexapi/take/","title":"<code>pd.Index.take</code>","text":"<p><code>pandasIndex.take(indices, axis=0, allow_fill=True, fill_value=None, **kwargs)</code></p>"},{"location":"api_docs/pandas/indexapi/take/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>indices</code>:  can be boolean Array like, integer Array like, or slice</li> </ul> <p>Unsupported Index Types</p> <ul> <li>MultiIndex</li> <li>IntervalIndex</li> </ul> <p>Important</p> <p>Bodo Does Not support <code>kwargs</code>, even for compatibility.</p>"},{"location":"api_docs/pandas/indexapi/timedeltaindex/","title":"<code>pd.TimedeltaIndex</code>","text":"<p><code>pandasTimedeltaIndex(data=None, unit=None, freq=NoDefault.no_default, closed=None, dtype=dtype('&lt;m8[ns]'), copy=False, name=None)</code></p>"},{"location":"api_docs/pandas/indexapi/timedeltaindex/#supported-arguments","title":"Supported Arguments","text":"<p>-<code>data</code>:  must be array-like of timedelta64ns or Ingetger.</p>"},{"location":"api_docs/pandas/indexapi/to_frame/","title":"<code>pd.Index.to_frame</code>","text":"<p><code>pandasIndex.to_frame(index=True, name=None)</code></p>"},{"location":"api_docs/pandas/indexapi/to_frame/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>index</code>: can be a True or False</li> <li><code>name</code>: can be a string or int</li> </ul> <p>Unsupported Index Types</p> <ul> <li>IntervalIndex</li> <li>PeriodIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/to_frame/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.to_frame(index=False)\n\n&gt;&gt;&gt; I = pd.Index([\"A\", \"E\", \"I\", \"O\", \"U\", \"Y\"], name=\"vowels\")\n&gt;&gt;&gt; f(I)\n  vowels\n0      A\n1      E\n2      I\n3      O\n4      U\n5      Y\n</code></pre>"},{"location":"api_docs/pandas/indexapi/to_list/","title":"<code>pd.Index.to_list</code>","text":"<p><code>pandasIndex.to_list()</code></p> <p>Unsupported Index Types</p> <ul> <li>PeriodIndex</li> <li>IntervalIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/to_list/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.to_list()\n\n&gt;&gt;&gt; I = pd.RangeIndex(5, -1, -1)\n&gt;&gt;&gt; f(I)\n[5, 4, 3, 2, 1, 0]\n</code></pre>"},{"location":"api_docs/pandas/indexapi/to_numpy/","title":"<code>pd.Index.to_numpy</code>","text":"<p><code>pandasIndex.to_numpy(dtype=None, copy=True, na_value=None)</code></p>"},{"location":"api_docs/pandas/indexapi/to_numpy/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>copy</code>: can be a True or False</li> </ul> <p>Unsupported Index Types</p> <ul> <li>PeriodIndex</li> <li>MultiIndex</li> </ul> <p>Important</p> <p>Sometimes Bodo returns a Pandas array instead of a np.ndarray. Cases include a NumericIndex of integers containing nulls, or a CategoricalIndex.</p>"},{"location":"api_docs/pandas/indexapi/to_numpy/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.numpy()\n\n&gt;&gt;&gt; I = pd.Index([1, 9, -1, 3, 0, 1, 6])\n&gt;&gt;&gt; f(I)\n[ 1  9 -1  3  0  1  6]\n</code></pre>"},{"location":"api_docs/pandas/indexapi/to_series/","title":"<code>pd.Index.to_series</code>","text":"<p><code>pandasIndex.to_series(index=None, name=None)</code></p>"},{"location":"api_docs/pandas/indexapi/to_series/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>index</code>: can be a Index, Series, 1-dim numpy array, list, or tuple</li> <li><code>name</code>: can be a string or int</li> </ul> <p>Unsupported Index Types</p> <ul> <li>IntervalIndex</li> <li>PeriodIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/to_series/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I, J):\n...   return I.to_series(index=J)\n\n&gt;&gt;&gt; I = pd.Index([1, 4, 9, 0, 3])\n&gt;&gt;&gt; J = pd.Index([\"A\", \"B\", \"C\", \"D\", \"E\"])\n&gt;&gt;&gt; f(I, J)\nA    1\nB    4\nC    9\nD    0\nE    3\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/indexapi/tolist/","title":"<code>pd.Index.tolist</code>","text":"<p><code>pandasIndex.tolist()</code></p> <p>Unsupported Index Types</p> <ul> <li>PeriodIndex</li> <li>IntervalIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/tolist/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.tolist()\n\n&gt;&gt;&gt; I = pd.RangeIndex(5, -1, -1)\n&gt;&gt;&gt; f(I)\n[5, 4, 3, 2, 1, 0]\n</code></pre>"},{"location":"api_docs/pandas/indexapi/tolist/#numeric-index","title":"Numeric Index","text":"<p>Numeric index objects <code>RangeIndex</code>, <code>Int64Index</code>, <code>UInt64Index</code> and <code>Float64Index</code> are supported as index to dataframes and series. Constructing them in Bodo functions, passing them to Bodo functions (unboxing), and returning them from Bodo functions (boxing) are also supported.</p>"},{"location":"api_docs/pandas/indexapi/uint64index/","title":"<code>pd.UInt64Index</code>","text":"<p><code>pandas.UInt64Index(data=None, dtype=None, copy=False, name=None)</code></p>"},{"location":"api_docs/pandas/indexapi/uint64index/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n... return pd.UInt64Index([1,2,3])\n\n&gt;&gt;&gt; f()\nUInt64Index([0, 1, 2], dtype='uint64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/union/","title":"<code>pd.Index.union</code>","text":"<p><code>pandasIndex.union(other, sort=None)</code></p>"},{"location":"api_docs/pandas/indexapi/union/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>other</code>: can be an Index, Series, or 1-dim numpy array with a matching type for the Index</li> </ul> <p>Supported Index Types</p> <ul> <li>NumericIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> <li>RangeIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> </ul> <p>Important</p> <p>Bodo diverges from the Pandas API for Index.union() in several ways: duplicates are removed, the order of elements may be different, the shortcuts for returning the same Index are removed, and a NumericIndex is always returned instead of a RangeIndex.</p>"},{"location":"api_docs/pandas/indexapi/union/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit(distributed=[\"I\", \"J\"])\n... def f(I, J):\n...    return I.union(J)\n\n&gt;&gt;&gt; I = pd.Index([1, 2, 3, 4, 5])\n&gt;&gt;&gt; J = pd.Index([2, 4, 6, 8, 10, 12])\n&gt;&gt;&gt; f(I, J)\nInt64Index([1, 2, 3, 4, 5, 6, 8, 10, 12], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/unique/","title":"<code>pd.Index.unique</code>","text":"<p><code>pandasIndex.unique()</code></p> <p>Unsupported Index Types</p> <ul> <li>IntervalIndex</li> <li>PeriodIndex</li> <li>MultiIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/unique/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.unique()\n\n&gt;&gt;&gt; I = pd.Index([1, 5, 2, 1, 0, 1, 5, 2, 1, 3])\n&gt;&gt;&gt; f(I)\nInt64Index([1, 5, 2, 0, 3], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/values/","title":"<code>pd.Index.values</code>","text":"<p><code>pandasIndex.values</code></p> <p>Unsupported Index Types</p> <ul> <li>MultiIndex</li> <li>IntervalIndex</li> </ul>"},{"location":"api_docs/pandas/indexapi/values/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.values\n\n&gt;&gt;&gt; I = pd.Index([1,2,3])\n&gt;&gt;&gt; f(I)\n[1 2 3]\n</code></pre>"},{"location":"api_docs/pandas/indexapi/week/","title":"<code>pd.DateTimeIndex.week</code>","text":"<p><code>pandasDatetimeIndex.week</code></p>"},{"location":"api_docs/pandas/indexapi/week/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.week\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([1, 1, 1, 1, 1], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/weekday/","title":"<code>pd.DateTimeIndex.weekday</code>","text":"<p><code>pandasDatetimeIndex.weekday</code></p>"},{"location":"api_docs/pandas/indexapi/weekday/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.weekday\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([1, 1, 1, 2, 2], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/weekofyear/","title":"<code>pd.DateTimeIndex.weekofyear</code>","text":"<p><code>pandasDatetimeIndex.weekofyear</code></p>"},{"location":"api_docs/pandas/indexapi/weekofyear/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.weekofyear\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([1, 1, 1, 1,1], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/where/","title":"<code>pd.Index.where</code>","text":"<p><code>pandasIndex.where(cond, other=None)</code></p>"},{"location":"api_docs/pandas/indexapi/where/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>cond</code>: can be a Series or 1-dim array of booleans</li> <li><code>other</code>: can be a scalar, non-categorical Series, 1-dim numpy array or StringArray with a matching type for the Index</li> </ul> <p>Unsupported Index Types</p> <ul> <li>IntervalIndex</li> <li>MultiIndex</li> </ul> <p>Important</p> <p>Only supported for CategoricalIndex if the elements of other are the same as (or a subset of) the categories of the CategoricalIndex.</p>"},{"location":"api_docs/pandas/indexapi/where/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I, C, O):\n...   return I.where(C, O)\n\n&gt;&gt;&gt; I = pd.Index([\"A\", \"B\", \"C\", \"D\", \"E\"])\n&gt;&gt;&gt; C = pd.array([True, False, True, True, False])\n&gt;&gt;&gt; O = pd.Series([\"a\", \"e\", \"i\", \"o\", \"u\")\n&gt;&gt;&gt; f(I, C, O)\nIndex(['A', 'e', 'C', 'D', 'u'], dtype='object')\n</code></pre>"},{"location":"api_docs/pandas/indexapi/year/","title":"<code>pd.DateTimeIndex.year</code>","text":"<p><code>pandasDatetimeIndex.year</code></p>"},{"location":"api_docs/pandas/indexapi/year/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   return I.year\n\n&gt;&gt;&gt; I = pd.DatetimeIndex(pd.date_range(start=\"2019-12-31 02:32:45\", end=\"2020-01-01 19:12:05\", periods=5))\n&gt;&gt;&gt; f(I)\nInt64Index([2019, 2019, 2019, 2020, 2020], dtype='int64')\n</code></pre>"},{"location":"api_docs/pandas/io/","title":"Input/Output","text":"<p>See more in File IO, such as S3 and HDFS configuration requirements.</p> <ul> <li><code>pd.read_csv</code> </li> <li><code>pd.read_excel</code> </li> <li><code>pd.read_sql</code> </li> <li><code>pd.read_sql_table</code> </li> <li><code>pd.read_parquet</code> </li> <li><code>pd.read_json</code> </li> </ul>"},{"location":"api_docs/pandas/io/read_csv/","title":"pd.read_csv","text":"<p><code>pandas.read_csv</code></p> <ul> <li>example usage and more system specific instructions <code>filepath_or_buffer</code> should be a string and is required. It     could be pointing to a single CSV file, or a directory     containing multiple partitioned CSV files (must have <code>csv</code> file     extension inside directory).</li> <li>Arguments <code>sep</code>, <code>delimiter</code>, <code>header</code>, <code>names</code>, <code>index_col</code>,     <code>usecols</code>, <code>dtype</code>, <code>nrows</code>, <code>skiprows</code>, <code>chunksize</code>,     <code>parse_dates</code>, and <code>low_memory</code> are supported.</li> <li>Argument <code>anon</code> of <code>storage_options</code> is supported for S3     filepaths.</li> <li>Either <code>names</code> and <code>dtype</code> arguments should be provided to     enable type inference, or <code>filepath_or_buffer</code> should be     inferrable as a constant string. This is required so bodo can     infer the types at compile time, see compile time constants</li> <li><code>names</code>, <code>usecols</code>, <code>parse_dates</code> should be constant lists.</li> <li><code>dtype</code> should be a constant dictionary of strings and types.</li> <li><code>skiprows</code> must be an integer or list of integers and if it is     not a constant, <code>names</code> must be provided to enable type     inference.</li> <li><code>chunksize</code> is supported for uncompressed files only.</li> <li><code>low_memory</code> internally process file in chunks while parsing. In     Bodo this is set to <code>False</code> by default.</li> <li>When set to <code>True</code>, Bodo parses file in chunks but     like Pandas the entire file is read into a single DataFrame     regardless.</li> <li>If you want to load data in chunks, use the <code>chunksize</code>     argument.</li> <li>When a CSV file is read in parallel (distributed mode) and each     process reads only a portion of the file, reading columns that     contain line breaks is not supported.</li> <li> <p><code>_bodo_read_as_dict</code> is a Bodo specific argument which forces      the specified string columns to be read with dictionary-encoding.     Dictionary-encoding stores data in memory in an efficient     manner and is most effective when the column has many repeated values.     Read more about dictionary-encoded layout     here.</p> <p>For example: <pre><code>@bodo.jit\ndef impl(f):\n  df = pd.read_csv(f, _bodo_read_as_dict=[\"A\", \"B\", \"C\"])\n  return df\n</code></pre></p> </li> </ul>"},{"location":"api_docs/pandas/io/read_excel/","title":"pd.read_excel","text":"<p><code>pandas.read_excel</code></p> <ul> <li>output dataframe cannot be parallelized automatically yet.</li> <li>only arguments <code>io</code>, <code>sheet_name</code>, <code>header</code>, <code>names</code>, <code>comment</code>,     <code>dtype</code>, <code>skiprows</code>, <code>parse_dates</code> are supported.</li> <li><code>io</code> should be a string and is required.</li> <li>Either <code>names</code> and <code>dtype</code> arguments should be provided to     enable type inference, or <code>io</code> should be inferrable as a     constant string. This is required so bodo can infer the types at     compile time, see compile time constants</li> <li><code>sheet_name</code>, <code>header</code>, <code>comment</code>, and <code>skiprows</code> should be     constant if provided.</li> <li><code>names</code> and <code>parse_dates</code> should be constant lists if provided.</li> <li><code>dtype</code> should be a constant dictionary of strings and types if     provided.</li> </ul>"},{"location":"api_docs/pandas/io/read_json/","title":"<code>pd.read_json</code>","text":"<p><code>pandas.read_json</code></p> <ul> <li>Example usage and more system specific instructions</li> <li>Only supports reading JSON Lines text file format     (<code>pd.read_json(filepath_or_buffer, orient='records', lines=True)</code>)     and regular multi-line JSON     file(<code>pd.read_json(filepath_or_buffer, orient='records', lines=False)</code>).</li> <li>Argument <code>filepath_or_buffer</code> is supported: it can point to a     single JSON file, or a directory containing multiple partitioned     JSON files. When reading a directory, the JSON files inside the     directory must be JSON Lines text file     format with <code>json</code> file extension.</li> <li>Argument <code>orient = 'records'</code> is used as default, instead of     Pandas' default <code>'columns'</code> for dataframes. <code>'records'</code> is the     only supported value for <code>orient</code>.</li> <li>Argument <code>typ</code> is supported. <code>'frame'</code> is the only supported     value for <code>typ</code>.</li> <li><code>filepath_or_buffer</code> must be inferrable as a constant string.     This is required so bodo can infer the types at compile time,     see compile time constants.</li> <li>Arguments <code>convert_dates</code>, <code>precise_float</code>, <code>lines</code> are     supported.</li> <li>Argument <code>anon</code> of <code>storage_options</code> is supported for S3     filepaths.</li> </ul>"},{"location":"api_docs/pandas/io/read_parquet/","title":"pd.read_parquet","text":"<p><code>pandas.read_parquet</code></p> <ul> <li>example usage and more system specific instructions</li> <li>Arguments <code>path</code> and <code>columns</code> are supported. <code>columns</code> should     be a constant list of strings if provided.     <code>path</code> can be a string or list. If string, must be a path to a file     or a directory, or a glob string. If a list, must contain paths     to parquet files (not directories) or glob strings.</li> <li>Argument <code>anon</code> of <code>storage_options</code> is supported for S3     filepaths.</li> <li> <p>If <code>path</code> can be inferred as a constant (e.g. it is a function     argument), Bodo finds the schema from file at compilation time.     Otherwise, schema should be provided using the numba syntax.</p> <p>For example: <pre><code>@bodo.jit(locals={'df':{'A': bodo.float64[:],\n                        'B': bodo.string_array_type}})\ndef impl(f):\n  df = pd.read_parquet(f)\n  return df\n</code></pre></p> </li> <li> <p><code>_bodo_input_file_name_col</code> is a Bodo specific argument.     When specified, a column with this     name is added to the dataframe consisting of the name of the file the     row was read from. This is similar to SparkSQL's      <code>input_file_name</code> function.</p> <p>For example: <pre><code>@bodo.jit\ndef impl(f):\n  df = pd.read_parquet(f, _bodo_input_file_name_col=\"fname\")\n  return df\n</code></pre></p> </li> <li> <p><code>_bodo_read_as_dict</code> is a Bodo specific argument which forces      the specified string columns to be read with dictionary-encoding.     Bodo automatically loads string columns using dictionary     encoding when it determines it would be beneficial based on      a heuristic.     Dictionary-encoding stores data in memory in an efficient     manner and is most effective when the column has many repeated values.     Read more about dictionary-encoded layout     here.</p> <p>For example: <pre><code>@bodo.jit\ndef impl(f):\n  df = pd.read_parquet(f, _bodo_read_as_dict=[\"A\", \"B\", \"C\"])\n  return df\n</code></pre></p> </li> </ul>"},{"location":"api_docs/pandas/io/read_sql/","title":"pd.read_sql","text":"<p><code>pandas.read_sql</code></p> <ul> <li>example usage and more system specific instructions</li> <li>Argument <code>sql</code> is supported but only as a string form.     SQLalchemy <code>Selectable</code> is not supported. There is     no restriction on the form of the sql request.</li> <li>Argument <code>con</code> is supported but only as a string form.     SQLalchemy <code>connectable</code> is not supported.</li> <li>Argument <code>index_col</code> is supported.</li> <li>Arguments <code>chunksize</code>, <code>column</code>, <code>coerce_float</code>, <code>params</code> are     not supported.</li> </ul>"},{"location":"api_docs/pandas/io/read_sql_table/","title":"pd.read_sql_table","text":"<p><code>pandas.read_sql_table</code></p> <ul> <li>This API only supports reading Iceberg tables at the moment.</li> <li>See the Iceberg Section for example usage and more system specific instructions.</li> <li>Argument <code>table_name</code> is supported and must be the name of an Iceberg Table.</li> <li>Argument <code>con</code> is supported but only as a string form in a URL format.     SQLalchemy <code>connectable</code> is not supported.     It should be the absolute path to a Iceberg warehouse.     If using a Hadoop-based directory catalog, it should start with the URL scheme <code>iceberg://</code>.     If using a Thrift Hive catalog, it should start with the URL scheme <code>iceberg+thrift://</code></li> <li>Argument <code>schema</code> is supported and currently required for Iceberg tables. It must be the name     of the database schema. For Iceberg Tables, this is the directory name     in the warehouse (specified by <code>con</code>) where your table exists.</li> <li>Arguments <code>index_col</code>, <code>coerce_float</code>, <code>parse_dates</code>, <code>columns</code> and <code>chunksize</code> are     not supported.</li> <li>Arguments <code>_snapshot_id</code> and <code>_snapshot_timestamp_ms</code> are only available for Iceberg tables. These Arguments     are experimental and may change without warning. These arguments may be used to read a table from     a specific snapshot or point in time, which is known as \"time travel\" in Iceberg.</li> </ul>"},{"location":"api_docs/pandas/series/","title":"Series","text":"<p>Bodo provides extensive Series support. However, operations between Series (+, -, /, , *) do not implicitly align values based on their associated index values yet.</p>"},{"location":"api_docs/pandas/series/#attributes","title":"Attributes","text":"<ul> <li><code>pd.Series</code> </li> <li><code>pd.Series.index</code> </li> <li><code>pd.Series.values</code> </li> <li><code>pd.Series.dtype</code> </li> <li><code>pd.Series.shape</code> </li> <li><code>pd.Series.nbytes</code> </li> <li><code>pd.Series.ndim</code> </li> <li><code>pd.Series.size</code> </li> <li><code>pd.Series.T</code> </li> <li><code>pd.Series.memory_usage</code> </li> <li><code>pd.Series.hasnans</code> </li> <li><code>pd.Series.empty</code> </li> <li><code>pd.Series.dtypes</code> </li> <li><code>pd.Series.name</code> </li> </ul>"},{"location":"api_docs/pandas/series/#conversion","title":"Conversion","text":"<ul> <li><code>pd.Series.astype</code> </li> <li><code>pd.Series.copy</code> </li> <li><code>pd.Series.to_numpy</code> </li> <li><code>pd.Series.tolist</code> </li> </ul>"},{"location":"api_docs/pandas/series/#indexing-iteration","title":"Indexing, iteration","text":"<p>Location based indexing using <code>[]</code>, <code>iat</code>, and <code>iloc</code> is supported. Changing values of existing string Series using these operators is not supported yet.</p> <ul> <li><code>pd.Series.iat</code> </li> <li><code>pd.Series.iloc</code> </li> <li><code>pd.Series.loc</code> </li> </ul>"},{"location":"api_docs/pandas/series/#binary-operator-functions","title":"Binary operator functions","text":"<ul> <li><code>pd.Series.add</code> </li> <li><code>pd.Series.sub</code> </li> <li><code>pd.Series.mul</code> </li> <li><code>pd.Series.div</code> </li> <li><code>pd.Series.truediv</code> </li> <li><code>pd.Series.floordiv</code> </li> <li><code>pd.Series.mod</code> </li> <li><code>pd.Series.pow</code> </li> <li><code>pd.Series.radd</code> </li> <li><code>pd.Series.rsub</code> </li> <li><code>pd.Series.rmul</code> </li> <li><code>pd.Series.rdiv</code> </li> <li><code>pd.Series.rtruediv</code> </li> <li><code>pd.Series.rfloordiv</code> </li> <li><code>pd.Series.rmod</code> </li> <li><code>pd.Series.rpow</code> </li> <li><code>pd.Series.combine</code> </li> <li><code>pd.Series.round</code> </li> <li><code>pd.Series.lt</code> </li> <li><code>pd.Series.gt</code> </li> <li><code>pd.Series.le</code> </li> <li><code>pd.Series.ge</code> </li> <li><code>pd.Series.ne</code> </li> <li><code>pd.Series.eq</code> </li> <li><code>pd.Series.dot</code> </li> </ul>"},{"location":"api_docs/pandas/series/#function-application-groupby-window","title":"Function application, GroupBy &amp; Window","text":"<ul> <li><code>pd.Series.apply</code> </li> <li><code>pd.Series.map</code> </li> <li><code>pd.Series.groupby</code></li> <li><code>pd.Series.rolling</code></li> <li><code>pd.Series.pipe</code> </li> </ul>"},{"location":"api_docs/pandas/series/#computations-descriptive-stats","title":"Computations / Descriptive Stats","text":"<p>Statistical functions below are supported without optional arguments unless support is explicitly mentioned.</p> <ul> <li><code>pd.Series.abs</code> </li> <li><code>pd.Series.all</code> </li> <li><code>pd.Series.any</code> </li> <li><code>pd.Series.autocorr</code> </li> <li><code>pd.Series.between</code> </li> <li><code>pd.Series.corr</code> </li> <li><code>pd.Series.count</code> </li> <li><code>pd.Series.cov</code> </li> <li><code>pd.Series.cummin</code> </li> <li><code>pd.Series.cummax</code> </li> <li><code>pd.Series.cumprod</code> </li> <li><code>pd.Series.cumsum</code> </li> <li><code>pd.Series.describe</code> </li> <li><code>pd.Series.diff</code> </li> <li><code>pd.Series.kurt</code> </li> <li><code>pd.Series.max</code> </li> <li><code>pd.Series.mean</code> </li> <li><code>pd.Series.median</code> </li> <li><code>pd.Series.min</code> </li> <li><code>pd.Series.nlargest</code> </li> <li><code>pd.Series.nsmallest</code> </li> <li><code>pd.Series.pct_change</code> </li> <li><code>pd.Series.prod</code> </li> <li><code>pd.Series.product</code> </li> <li><code>pd.Series.quantile</code> </li> <li><code>pd.Series.rank</code> </li> <li><code>pd.Series.sem</code> </li> <li><code>pd.Series.skew</code> </li> <li><code>pd.Series.std</code> </li> <li><code>pd.Series.sum</code> </li> <li><code>pd.Series.var</code> </li> <li><code>pd.Series.kurtosis</code> </li> <li><code>pd.Series.unique</code> </li> <li><code>pd.Series.nunique</code> </li> <li><code>pd.Series.is_monotonic_increasing</code> </li> <li><code>pd.Series.is_monotonic_decreasing</code> </li> <li><code>pd.Series.value_counts</code> </li> </ul>"},{"location":"api_docs/pandas/series/#reindexing-selection-label-manipulation","title":"Reindexing / Selection / Label manipulation","text":"<ul> <li><code>pd.Series.drop_duplicates</code> </li> <li><code>pd.Series.duplicated</code> </li> <li><code>pd.Series.equals</code> </li> <li><code>pd.Series.first</code> </li> <li><code>pd.Series.head</code> </li> <li><code>pd.Series.idxmax</code> </li> <li><code>pd.Series.idxmin</code> </li> <li><code>pd.Series.isin</code> </li> <li><code>pd.Series.last</code> </li> <li><code>pd.Series.rename</code> </li> <li><code>pd.Series.reset_index</code> </li> <li><code>pd.Series.take</code> </li> <li><code>pd.Series.tail</code> </li> <li><code>pd.Series.where</code> </li> <li><code>pd.Series.mask</code> </li> </ul>"},{"location":"api_docs/pandas/series/#missing-data-handling","title":"Missing data handling","text":"<ul> <li><code>pd.Series.backfill</code> </li> <li><code>pd.Series.bfill</code> </li> <li><code>pd.Series.dropna</code> </li> <li><code>pd.Series.ffill</code> </li> <li><code>pd.Series.fillna</code> </li> <li><code>pd.Series.isna</code> </li> <li><code>pd.Series.isnull</code> </li> <li><code>pd.Series.notna</code> </li> <li><code>pd.Series.notnull</code> </li> <li><code>pd.Series.pad</code> </li> <li><code>pd.Series.replace</code> </li> </ul>"},{"location":"api_docs/pandas/series/#reshaping-sorting","title":"Reshaping, sorting","text":"<ul> <li><code>pd.Series.argsort</code> </li> <li><code>pd.Series.sort_values</code> </li> <li><code>pd.Series.sort_index</code> </li> <li><code>pd.Series.explode</code> </li> <li><code>pd.Series.repeat</code> </li> </ul>"},{"location":"api_docs/pandas/series/#time-series-related","title":"Time series-related","text":"<ul> <li><code>pd.Series.shift</code> </li> </ul>"},{"location":"api_docs/pandas/series/#datetime-properties","title":"Datetime properties","text":"<ul> <li><code>pd.Series.dt.date</code> </li> <li><code>pd.Series.dt.year</code> </li> <li><code>pd.Series.dt.month</code> </li> <li><code>pd.Series.dt.day</code> </li> <li><code>pd.Series.dt.hour</code> </li> <li><code>pd.Series.dt.minute</code> </li> <li><code>pd.Series.dt.second</code> </li> <li><code>pd.Series.dt.microsecond</code> </li> <li><code>pd.Series.dt.nanosecond</code> </li> <li><code>pd.Series.dt.day_of_week</code> </li> <li><code>pd.Series.dt.weekday</code> </li> <li><code>pd.Series.dt.dayofyear</code> </li> <li><code>pd.Series.dt.day_of_year</code> </li> <li><code>pd.Series.dt.quarter</code> </li> <li><code>pd.Series.dt.is_month_start</code> </li> <li><code>pd.Series.dt.is_month_end</code> </li> <li><code>pd.Series.dt.is_quarter_start</code> </li> <li><code>pd.Series.dt.is_quarter_end</code> </li> <li><code>pd.Series.dt.is_year_start</code> </li> <li><code>pd.Series.dt.is_year_end</code> </li> <li><code>pd.Series.dt.is_leap_year</code> </li> <li><code>pd.Series.dt.daysinmonth</code> </li> <li><code>pd.Series.dt.days_in_month</code> </li> </ul>"},{"location":"api_docs/pandas/series/#datetime-methods","title":"Datetime methods","text":"<ul> <li><code>pd.Series.dt.normalize</code> </li> <li><code>pd.Series.dt.strftime</code> </li> <li><code>pd.Series.dt.round</code> </li> <li><code>pd.Series.dt.floor</code> </li> <li><code>pd.Series.dt.ceil</code> </li> <li><code>pd.Series.dt.month_name</code> </li> <li><code>pd.Series.dt.day_name</code> </li> </ul>"},{"location":"api_docs/pandas/series/#string-handling","title":"String handling","text":"<ul> <li><code>pd.Series.str.capitalize</code> </li> <li><code>pd.Series.str.cat</code> </li> <li><code>pd.Series.str.center</code> </li> <li><code>pd.Series.str.contains</code> </li> <li><code>pd.Series.str.count</code> </li> <li><code>pd.Series.str.endswith</code> </li> <li><code>pd.Series.str.extract</code> </li> <li><code>pd.Series.str.extractall</code> </li> <li><code>pd.Series.str.find</code> </li> <li><code>pd.Series.str.get</code> </li> <li><code>pd.Series.str.join</code> </li> <li><code>pd.Series.str.len</code> </li> <li><code>pd.Series.str.ljust</code> </li> <li><code>pd.Series.str.lower</code> </li> <li><code>pd.Series.str.lstrip</code> </li> <li><code>pd.Series.str.pad</code> </li> <li><code>pd.Series.str.repeat</code> </li> <li><code>pd.Series.str.replace</code> </li> <li><code>pd.Series.str.rfind</code> </li> <li><code>pd.Series.str.rjist</code> </li> <li><code>pd.Series.str.restrip</code> </li> <li><code>pd.Series.str.slice</code> </li> <li><code>pd.Series.str.slice_replace</code> </li> <li><code>pd.Series.str.split</code> </li> <li><code>pd.Series.str.startswith</code> </li> <li><code>pd.Series.str.strip</code> </li> <li><code>pd.Series.str.swapcase</code> </li> <li><code>pd.Series.str.title</code> </li> <li><code>pd.Series.str.upper</code> </li> <li><code>pd.Series.str.zfill</code> </li> <li><code>pd.Series.str.isalnum</code> </li> <li><code>pd.Series.str.isalpha</code> </li> <li><code>pd.Series.str.isdigit</code> </li> <li><code>pd.Series.str.isspace</code> </li> <li><code>pd.Series.str.islower</code> </li> <li><code>pd.Series.str.isupper</code> </li> <li><code>pd.Series.str.istitle</code> </li> <li><code>pd.Series.str.isnumeric</code> </li> <li><code>pd.Series.str.isdecimal</code> </li> <li><code>pd.Series.str.encode</code> </li> </ul>"},{"location":"api_docs/pandas/series/#categorical-accessor","title":"Categorical accessor","text":"<ul> <li><code>pd.Series.cat.codes</code> </li> </ul>"},{"location":"api_docs/pandas/series/#serialization-io-conversion","title":"Serialization / IO / Conversion","text":"<ul> <li><code>pd.Series.to_csv</code> </li> <li><code>pd.Series.to_dict</code> </li> <li><code>pd.Series.to_frame</code> </li> </ul>"},{"location":"api_docs/pandas/series/#heterogeneous_series","title":"Heterogeneous Series","text":"<p>Bodo's Series implementation requires all elements to share a common data type. However, in situations where the size and types of the elements are constant at compile time, Bodo has some mixed type handling with its Heterogeneous Series type.</p> <p>Warning</p> <p>This type's primary purpose is for iterating through the rows of a DataFrame with different column types. You should not attempt to directly create Series with mixed types.</p> <p>Heterogeneous Series operations are a subset of those supported for Series and the supported operations are listed below.</p>"},{"location":"api_docs/pandas/series/#attributes_1","title":"Attributes","text":"<ul> <li><code>pd.Series.index</code> </li> <li><code>pd.Series.values</code> </li> <li><code>pd.Series.shape</code> </li> <li><code>pd.Series.ndim</code> </li> <li><code>pd.Series.size</code> </li> <li><code>pd.Series.T</code> </li> <li><code>pd.Series.empty</code> </li> <li><code>pd.Series.name</code> </li> </ul>"},{"location":"api_docs/pandas/series/abs/","title":"<code>pd.Series.abs</code>","text":"<p><code>pandas.Series.abs()</code></p>"},{"location":"api_docs/pandas/series/abs/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.abs()\n&gt;&gt;&gt; S = (pd.Series(np.arange(100)) % 7) - 2\n&gt;&gt;&gt; f(S)\n0     2\n1     1\n2     0\n3     1\n4     2\n     ..\n95    2\n96    3\n97    4\n98    2\n99    1\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/add/","title":"<code>pd.Series.add</code>","text":"<p><code>pandas.Series.add(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/add/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>  numeric scalar </li><li>  array with numeric data </li><li> Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.add</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/add/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.add(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      1001\n1      1001\n2      1001\n3      1001\n4      1001\n      ...\n995    1001\n996    1001\n997    1001\n998    1001\n999    1001\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/all/","title":"<code>pd.Series.all</code>","text":"<p><code>pandas.Series.all(axis=0, bool_only=None, skipna=True, level=None)</code></p>"},{"location":"api_docs/pandas/series/all/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <p>Bodo does not accept any additional arguments for Numpy compatibility</p>"},{"location":"api_docs/pandas/series/all/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.all()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/series/any/","title":"<code>pd.Series.any</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.any(axis=0, bool_only=None, skipna=True)</code></p>"},{"location":"api_docs/pandas/series/any/#argument-restrictions","title":"Argument Restrictions:","text":"<ul> <li><code>axis</code>: only supports default value <code>0</code>.</li> <li><code>bool_only</code>: only supports default value <code>None</code>.</li> <li><code>skipna</code>: only supports default value <code>True</code>.</li> </ul> <p>Note</p> <p>Argument <code>bool_only</code> has default value <code>None</code> that's different than Pandas default.</p> <p>Note</p> <p>Bodo does not accept any additional arguments for Numpy compatibility</p>"},{"location":"api_docs/pandas/series/any/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.any()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\nTrue\n</code></pre>"},{"location":"api_docs/pandas/series/apply/","title":"<code>pd.Series.apply</code>","text":"<ul> <li>pandas.Series.applyf(func, convert_dtype=True, args=(), **kwargs)</li> </ul>"},{"location":"api_docs/pandas/series/apply/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>func</code> Additional arguments for <code>func</code> can be passed as additional arguments. <ul><li>   JIT function or callable defined within a JIT function </li><li>   Numpy ufunc  </li><li>   Constant String which is the name of a supported Series method or Numpy ufunc  </li>"},{"location":"api_docs/pandas/series/apply/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...   return S.apply(lambda x: x ** 0.75)\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0      0.000000\n1      1.000000\n2      1.681793\n3      2.279507\n4      2.828427\n        ...\n95    30.429352\n96    30.669269\n97    30.908562\n98    31.147239\n99    31.385308\nLength: 100, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/argmax/","title":"<code>pd.Series.argmax</code>","text":"<p><code>pandas.Series.argmax(axis=None, skipna=True)</code></p>"},{"location":"api_docs/pandas/series/argmax/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <ul> <li>Series dtype must be a numeric type</li> <li>Bodo only accepts default values for <code>axis</code> and <code>skipna</code></li> </ul>"},{"location":"api_docs/pandas/series/argmax/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.argmax()\n&gt;&gt;&gt; S = pd.Series([4, -2, 3, 6, -1])\n&gt;&gt;&gt; f(S)\n3\n</code></pre>"},{"location":"api_docs/pandas/series/argmin/","title":"<code>pd.Series.argmin</code>","text":"<p><code>pandas.Series.argmin(axis=None, skipna=True)</code></p>"},{"location":"api_docs/pandas/series/argmin/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <ul> <li>Series dtype must be a numeric type</li> <li>Bodo only accepts default values for <code>axis</code> and <code>skipna</code></li> </ul>"},{"location":"api_docs/pandas/series/argmin/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.argmin()\n&gt;&gt;&gt; S = pd.Series([4, -2, 3, 6, -1])\n&gt;&gt;&gt; f(S)\n1\n</code></pre>"},{"location":"api_docs/pandas/series/argsort/","title":"<code>pd.Series.argsort</code>","text":"<p><code>pandas.Series.argsort(axis=0, kind='quicksort', order=None)</code></p>"},{"location":"api_docs/pandas/series/argsort/#supported-arguments-none","title":"Supported Arguments None","text":""},{"location":"api_docs/pandas/series/argsort/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.sort_values()\n&gt;&gt;&gt; S = pd.Series(np.arange(99, -1, -1), index=np.arange(100))\n&gt;&gt;&gt; f(S)\n0     99\n1     98\n2     97\n3     96\n4     95\n      ..\n95     4\n96     3\n97     2\n98     1\n99     0\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/astype/","title":"<code>pd.Series.astype</code>","text":"<p><code>pandas.Series.astype(dtype, copy=True, errors=\"raise\", _bodo_nan_to_str=True)</code></p>"},{"location":"api_docs/pandas/series/astype/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>dtype</code> <ul><li>   String (string must be parsable by <code>np.dtype</code>) </li><li>  Valid type (see types)</li><li>   The following functions: float, int, bool, str </li></ul> Must be constant at   Compile Time <code>copy</code> Boolean Must be constant at Compile Time <code>_bodo_nan_to_str</code> Boolean <ul><li> Must be constant at Compile Time </li><li> Argument unique to  Bodo. When <code>True</code> NA values in when converting to string are represented as NA  instead of a string representation of the  NA value  ''), the default  Pandas behavior."},{"location":"api_docs/pandas/series/astype/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.astype(np.float32)\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S)\n0        0.0\n1        1.0\n2        2.0\n3        3.0\n4        4.0\n      ...\n995    995.0\n996    996.0\n997    997.0\n998    998.0\n999    999.0\nLength: 1000, dtype: float32\n</code></pre>"},{"location":"api_docs/pandas/series/autocorr/","title":"<code>pd.Series.autocorr</code>","text":"<p><code>pandas.Series.autocorr(lag=1)</code></p>"},{"location":"api_docs/pandas/series/autocorr/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>lag</code> Integer"},{"location":"api_docs/pandas/series/autocorr/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.autocorr(3)\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n-0.49872171657407155\n</code></pre>"},{"location":"api_docs/pandas/series/backfill/","title":"<code>pd.Series.backfill</code>","text":"<p><code>pandas.Series.backfill(axis=None, inplace=False, limit=None, downcast=None)</code></p>"},{"location":"api_docs/pandas/series/backfill/#supported-arguments-none","title":"Supported Arguments None","text":""},{"location":"api_docs/pandas/series/backfill/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.backfill()\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S)\n0       1\n1       1\n2      -2\n3      -2\n4       5\n5       5\n6    &lt;NA&gt;\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/between/","title":"<code>pd.Series.between</code>","text":"<p><code>pandas.Series.between(left, right, inclusive='both')</code></p>"},{"location":"api_docs/pandas/series/between/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>left</code> Scalar matching the Series type <code>right</code> Scalar matching  the Series type <code>inclusive</code> One of (\"both\", \"neither\") Must be constant at Compile Time"},{"location":"api_docs/pandas/series/between/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.between(3, 5, \"both\")\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n0     False\n1     False\n2     False\n3      True\n4      True\n      ...\n95     True\n96     True\n97    False\n98    False\n99    False\nLength: 100, dtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/bfill/","title":"<code>pd.Series.bfill</code>","text":"<p><code>pandas.Series.bfill(axis=None, inplace=False, limit=None, downcast=None)</code></p>"},{"location":"api_docs/pandas/series/bfill/#supported-arguments-none","title":"Supported Arguments None","text":""},{"location":"api_docs/pandas/series/bfill/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.bfill()\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S)\n0       1\n1       1\n2      -2\n3      -2\n4       5\n5       5\n6    &lt;NA&gt;\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/cat.codes/","title":"<code>pd.Series.cat.codes</code>","text":"<p><code>pandas.Series.cat.codes</code></p> <p>Note</p> <p>If categories cannot be determined at compile time, then Bodo defaults to creating codes with an <code>int64</code>, which may differ from Pandas.</p>"},{"location":"api_docs/pandas/series/cat.codes/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.cat.codes\n&gt;&gt;&gt; S = pd.Series([\"a\", \"ce\", \"Erw\", \"a3\", \"@\"] * 10).astype(\"category\")\n&gt;&gt;&gt; f(S)\n0     2\n1     4\n2     1\n3     3\n4     0\n5     2\n6     4\n7     1\n8     3\n9     0\n10    2\n11    4\n12    1\n13    3\n14    0\n15    2\n16    4\n17    1\n18    3\n19    0\n20    2\n21    4\n22    1\n23    3\n24    0\n25    2\n26    4\n27    1\n28    3\n29    0\n30    2\n31    4\n32    1\n33    3\n34    0\n35    2\n36    4\n37    1\n38    3\n39    0\n40    2\n41    4\n42    1\n43    3\n44    0\n45    2\n46    4\n47    1\n48    3\n49    0\ndtype: int8\n</code></pre>"},{"location":"api_docs/pandas/series/cat.codes/#serialization-io-conversion","title":"Serialization / IO / Conversion","text":""},{"location":"api_docs/pandas/series/clip/","title":"<code>pd.Series.clip</code>","text":"<p><code>pandas.Series.clip(axis=0, inplace=False, lower=None, upper=None)</code></p>"},{"location":"api_docs/pandas/series/clip/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>lower</code> Scalar or series matching the Series type <code>upper</code> Scalar or series matching the Series type"},{"location":"api_docs/pandas/series/clip/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, lower, upper):\n...     return S.clip(lower, upper)\n&gt;&gt;&gt; S = pd.Series(pd.array([0, 1, 2, 3, 4, 5]), pd.array([1, 2, 2, 3, 1, 1]), pd.array([3, 3, 3, 3, 3, 4]))\n&gt;&gt;&gt; f(S)\n0     1\n1     2\n2     2\n3     3\n4     3\n5     4\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/combine/","title":"<code>pd.Series.combine</code>","text":"<p><code>pandas.Series.combine(other, func, fill_value=None)</code></p>"},{"location":"api_docs/pandas/series/combine/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>other</code> <ul><li>   Array  </li><li> Series  </li></ul> <code>func</code> -   Function that takes two scalar arguments and   returns a scalar  value. <code>fill_value</code> scalar Must be provided if the Series lengths aren't equal and the dtypes aren't floats."},{"location":"api_docs/pandas/series/combine/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.combine(other, lambda a, b: 2 * a + b)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      1002\n1      1003\n2      1004\n3      1005\n4      1006\n      ...\n995    1997\n996    1998\n997    1999\n998    2000\n999    2001\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/copy/","title":"<code>pd.Series.copy</code>","text":"<p><code>pandas.Series.copy(deep=True)</code></p>"},{"location":"api_docs/pandas/series/copy/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>deep</code> -   Boolean"},{"location":"api_docs/pandas/series/copy/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.copy()\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S)\n0        0\n1        1\n2        2\n3        3\n4        4\n      ...\n995    995\n996    996\n997    997\n998    998\n999    999\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/corr/","title":"<code>pd.Series.corr</code>","text":"<p><code>pandas.Series.corr(other, method='pearson', min_periods=None)</code></p>"},{"location":"api_docs/pandas/series/corr/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> Numeric Series or Array <p>Note</p> <p>Series type must be numeric</p>"},{"location":"api_docs/pandas/series/corr/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...     return S.cov(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; other = pd.Series(np.arange(100)) % 10\n&gt;&gt;&gt; f(S, other)\n0.004326329627279103\n</code></pre>"},{"location":"api_docs/pandas/series/count/","title":"<code>pd.Series.count</code>","text":"<p><code>pandas.Series.count(level=None)</code></p>"},{"location":"api_docs/pandas/series/count/#supported-arguments-none","title":"Supported Arguments None","text":""},{"location":"api_docs/pandas/series/count/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.count()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n100\n</code></pre>"},{"location":"api_docs/pandas/series/cov/","title":"<code>pd.Series.cov</code>","text":"<p><code>pandas.Series.cov(other, min_periods=None, ddof=1)</code></p>"},{"location":"api_docs/pandas/series/cov/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> -   Numeric Series or Array <code>ddof</code> -   Integer <p>Note</p> <p>Series type must be numeric</p>"},{"location":"api_docs/pandas/series/cov/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...     return S.cov(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; other = pd.Series(np.arange(100)) % 10\n&gt;&gt;&gt; f(S, other)\n0.025252525252525252\n</code></pre>"},{"location":"api_docs/pandas/series/cummax/","title":"<code>pd.Series.cummax</code>","text":"<p><code>pandas.Series.cummax(axis=None, skipna=True)</code></p>"},{"location":"api_docs/pandas/series/cummax/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments for Numpy compatibility</li> </ul>"},{"location":"api_docs/pandas/series/cummax/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.cummax()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n0     0\n1     1\n2     2\n3     3\n4     4\n     ..\n95    6\n96    6\n97    6\n98    6\n99    6\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/cummin/","title":"<code>pd.Series.cummin</code>","text":"<p><code>pandas.Series.cummin(axis=None, skipna=True)</code></p>"},{"location":"api_docs/pandas/series/cummin/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments for Numpy compatibility</li> </ul>"},{"location":"api_docs/pandas/series/cummin/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.cummin()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n0     0\n1     0\n2     0\n3     0\n4     0\n     ..\n95    0\n96    0\n97    0\n98    0\n99    0\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/cumprod/","title":"<code>pd.Series.cumprod</code>","text":"<p><code>pandas.Series.cumprod(axis=None, skipna=True)</code></p>"},{"location":"api_docs/pandas/series/cumprod/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments for Numpy compatibility</li> </ul>"},{"location":"api_docs/pandas/series/cumprod/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.cumprod()\n&gt;&gt;&gt; S = (pd.Series(np.arange(10)) % 7) + 1\n&gt;&gt;&gt; f(S)\n0        1\n1        2\n2        6\n3       24\n4      120\n5      720\n6     5040\n7     5040\n8    10080\n9    30240\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/cumsum/","title":"<code>pd.Series.cumsum</code>","text":"<p><code>pandas.Series.cumsum(axis=None, skipna=True)</code></p>"},{"location":"api_docs/pandas/series/cumsum/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments for Numpy compatibility</li> </ul>"},{"location":"api_docs/pandas/series/cumsum/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.cumsum()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n0       0\n1       1\n2       3\n3       6\n4      10\n     ...\n95    283\n96    288\n97    294\n98    294\n99    295\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/describe/","title":"<code>pd.Series.describe</code>","text":"<p><code>pandas.Series.describe(percentiles=None, include=None, exclude=None, datetime_is_numeric=False)</code></p>"},{"location":"api_docs/pandas/series/describe/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <p>Bodo only supports numeric and datetime64 types and assumes <code>datetime_is_numeric=True</code></p>"},{"location":"api_docs/pandas/series/describe/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.describe()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\ncount    100.000000\nmean       2.950000\nstd        2.021975\nmin        0.000000\n25%        1.000000\n50%        3.000000\n75%        5.000000\nmax        6.000000\ndtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/diff/","title":"<code>pd.Series.diff</code>","text":"<p><code>pandas.Series.diff(periods=1)</code></p>"},{"location":"api_docs/pandas/series/diff/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>periods</code> Integer <p>Note</p> <p>Bodo only supports numeric and datetime64 types</p>"},{"location":"api_docs/pandas/series/diff/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.diff(3)\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n0     NaN\n1     NaN\n2     NaN\n3     3.0\n4     3.0\n     ...\n95    3.0\n96    3.0\n97    3.0\n98   -4.0\n99   -4.0\nLength: 100, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/div/","title":"<code>pd.Series.div</code>","text":"<p><code>pandas.Series.div(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/div/#supported-arguments","title":"Supported Arguments","text":"<ul> <li> argument datatypes <code>other</code> <ul><li>  numeric scalar </li><li> array with numeric data </li><li> Series with numeric data </li></ul> <code>fill_value</code> numeric scalar </li> </ul> <p>Note</p> <p><code>Series.div</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/div/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.div(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0         0.001000\n1         0.002002\n2         0.003006\n3         0.004012\n4         0.005020\n          ...\n995     199.200000\n996     249.250000\n997     332.666667\n998     499.500000\n999    1000.000000\nLength: 1000, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/dot/","title":"<code>pd.Series.dot</code>","text":"<p><code>pandas.Series.dot(other)</code></p>"},{"location":"api_docs/pandas/series/dot/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> Series with numeric data <p>Note</p> <p><code>Series.dot</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/dot/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.dot(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n167167000\n</code></pre>"},{"location":"api_docs/pandas/series/dot/#function-application-groupby-window","title":"Function application, GroupBy &amp; Window","text":""},{"location":"api_docs/pandas/series/drop_duplicates/","title":"<code>pd.Series.drop_duplicates</code>","text":"<p><code>pandas.Series.drop_duplicates(keep='first', inplace=False)</code></p>"},{"location":"api_docs/pandas/series/drop_duplicates/#supported-arguments-none","title":"Supported Arguments None","text":""},{"location":"api_docs/pandas/series/drop_duplicates/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.drop_duplicates()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 10\n&gt;&gt;&gt; f(S)\n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/dropna/","title":"<code>pd.Series.dropna</code>","text":"<p><code>pandas.Series.dropna(axis=0, inplace=False, how=None)</code></p>"},{"location":"api_docs/pandas/series/dropna/#supported-arguments-none","title":"Supported Arguments None","text":""},{"location":"api_docs/pandas/series/dropna/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dropna()\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S)\n1     1\n3    -2\n5     5\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.ceil/","title":"<code>pd.Series.dt.ceil</code>","text":"<p><code>pandas.Series.dt.ceil(freq, ambiguous='raise', nonexistent='raise')</code></p>"},{"location":"api_docs/pandas/series/dt.ceil/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>freq</code> String Must be a valid fixed frequency alias"},{"location":"api_docs/pandas/series/dt.ceil/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.ceil(\"H\")\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2022', periods=30))\n&gt;&gt;&gt; f(S)\n0    2022-01-01 00:00:00\n1    2022-01-01 08:00:00\n2    2022-01-01 15:00:00\n3    2022-01-01 23:00:00\n4    2022-01-02 06:00:00\n5    2022-01-02 14:00:00\n6    2022-01-02 21:00:00\n7    2022-01-03 05:00:00\n8    2022-01-03 12:00:00\n9    2022-01-03 20:00:00\n10   2022-01-04 03:00:00\n11   2022-01-04 10:00:00\n12   2022-01-04 18:00:00\n13   2022-01-05 01:00:00\n14   2022-01-05 09:00:00\n15   2022-01-05 16:00:00\n16   2022-01-06 00:00:00\n17   2022-01-06 07:00:00\n18   2022-01-06 15:00:00\n19   2022-01-06 22:00:00\n20   2022-01-07 05:00:00\n21   2022-01-07 13:00:00\n22   2022-01-07 20:00:00\n23   2022-01-08 04:00:00\n24   2022-01-08 11:00:00\n25   2022-01-08 19:00:00\n26   2022-01-09 02:00:00\n27   2022-01-09 10:00:00\n28   2022-01-09 17:00:00\n29   2022-01-10 00:00:00\ndtype: datetime64[ns]\n</code></pre>"},{"location":"api_docs/pandas/series/dt.date/","title":"`pd.Series.dt.date","text":"<p>`pandas.Series.dt.date</p>"},{"location":"api_docs/pandas/series/dt.date/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.date\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2022', periods=30))\n&gt;&gt;&gt; f(S)\n0     2022-01-01\n1     2022-01-01\n2     2022-01-01\n3     2022-01-01\n4     2022-01-02\n5     2022-01-02\n6     2022-01-02\n7     2022-01-03\n8     2022-01-03\n9     2022-01-03\n10    2022-01-04\n11    2022-01-04\n12    2022-01-04\n13    2022-01-05\n14    2022-01-05\n15    2022-01-05\n16    2022-01-05\n17    2022-01-06\n18    2022-01-06\n19    2022-01-06\n20    2022-01-07\n21    2022-01-07\n22    2022-01-07\n23    2022-01-08\n24    2022-01-08\n25    2022-01-08\n26    2022-01-09\n27    2022-01-09\n28    2022-01-09\n29    2022-01-10\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/dt.day/","title":"<code>pd.Series.dt.day</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.day</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.day/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.day\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0      1\n1      8\n2     18\n3     25\n4      2\n5     10\n6     17\n7     24\n8      1\n9      9\n10    17\n11    24\n12     3\n13    11\n14    18\n15    26\n16     2\n17    10\n18    17\n19    25\n20     2\n21    11\n22    18\n23    26\n24     3\n25    10\n26    17\n27    25\n28     2\n29    10\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.day_name/","title":"<code>pd.Series.dt.day_name</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.day_name(locale=None)</code></p>"},{"location":"api_docs/pandas/series/dt.day_name/#argument-restrictions","title":"Argument Restrictions:","text":"<ul> <li><code>locale</code>: only supports default value <code>None</code>.</li> </ul> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.day_name/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.day_name()\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2022', periods=30))\n&gt;&gt;&gt; f(S)\n0      Saturday\n1      Saturday\n2      Saturday\n3      Saturday\n4        Sunday\n5        Sunday\n6        Sunday\n7        Monday\n8        Monday\n9        Monday\n10      Tuesday\n11      Tuesday\n12      Tuesday\n13    Wednesday\n14    Wednesday\n15    Wednesday\n16    Wednesday\n17     Thursday\n18     Thursday\n19     Thursday\n20       Friday\n21       Friday\n22       Friday\n23     Saturday\n24     Saturday\n25     Saturday\n26       Sunday\n27       Sunday\n28       Sunday\n29       Monday\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/dt.day_name/#string-handling","title":"String handling","text":""},{"location":"api_docs/pandas/series/dt.day_of_week/","title":"<code>pd.Series.dt.day_of_week</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.day_of_week</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.day_of_week/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.day_of_week\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0     5\n1     1\n2     4\n3     0\n4     3\n5     6\n6     2\n7     5\n8     1\n9     4\n10    1\n11    4\n12    0\n13    3\n14    6\n15    2\n16    5\n17    1\n18    4\n19    0\n20    4\n21    0\n22    3\n23    6\n24    2\n25    5\n26    1\n27    4\n28    0\n29    4\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.day_of_year/","title":"<code>pd.Series.dt.day_of_year</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.day_of_year</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.day_of_year/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.day_of_year\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0       1\n1      39\n2      77\n3     115\n4     153\n5     191\n6     229\n7     267\n8     305\n9     343\n10     17\n11     55\n12     93\n13    131\n14    169\n15    207\n16    245\n17    283\n18    321\n19    359\n20     33\n21     71\n22    109\n23    147\n24    185\n25    223\n26    261\n27    299\n28    337\n29     10\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.dayofweek/","title":"<code>pd.Series.dt.dayofweek</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.dayofweek</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.dayofyear/","title":"<code>pd.Series.dt.dayofyear</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.dayofyear</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.dayofyear/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.dayofyear\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0       1\n1      39\n2      77\n3     115\n4     153\n5     191\n6     229\n7     267\n8     305\n9     343\n10     17\n11     55\n12     93\n13    131\n14    169\n15    207\n16    245\n17    283\n18    321\n19    359\n20     33\n21     71\n22    109\n23    147\n24    185\n25    223\n26    261\n27    299\n28    337\n29     10\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.days_in_month/","title":"<code>pd.Series.dt.days_in_month</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.days_in_month</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.days_in_month/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.days_in_month\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='12/31/2024', periods=30))\n&gt;&gt;&gt; f(S)\n0     31\n1     28\n2     31\n3     30\n4     30\n5     31\n6     31\n7     30\n8     31\n9     31\n10    31\n11    28\n12    31\n13    31\n14    30\n15    31\n16    31\n17    31\n18    30\n19    31\n20    31\n21    31\n22    30\n23    31\n24    30\n25    31\n26    30\n27    31\n28    30\n29    31\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.days_in_month/#datetime-methods","title":"Datetime methods","text":""},{"location":"api_docs/pandas/series/dt.daysinmonth/","title":"<code>pd.Series.dt.daysinmonth</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.daysinmonth</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.daysinmonth/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.daysinmonth\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='12/31/2024', periods=30))\n&gt;&gt;&gt; f(S)\n0     31\n1     28\n2     31\n3     30\n4     30\n5     31\n6     31\n7     30\n8     31\n9     31\n10    31\n11    28\n12    31\n13    31\n14    30\n15    31\n16    31\n17    31\n18    30\n19    31\n20    31\n21    31\n22    30\n23    31\n24    30\n25    31\n26    30\n27    31\n28    30\n29    31\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.floor/","title":"<code>pd.Series.dt.floor</code>","text":"<p><code>pandas.Series.dt.floor(freq, ambiguous='raise', nonexistent='raise')</code></p>"},{"location":"api_docs/pandas/series/dt.floor/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>freq</code> String Must be a valid fixed frequency alias"},{"location":"api_docs/pandas/series/dt.floor/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.floor(\"H\")\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2022', periods=30))\n&gt;&gt;&gt; f(S)\n0    2022-01-01 00:00:00\n1    2022-01-01 07:00:00\n2    2022-01-01 14:00:00\n3    2022-01-01 22:00:00\n4    2022-01-02 05:00:00\n5    2022-01-02 13:00:00\n6    2022-01-02 20:00:00\n7    2022-01-03 04:00:00\n8    2022-01-03 11:00:00\n9    2022-01-03 19:00:00\n10   2022-01-04 02:00:00\n11   2022-01-04 09:00:00\n12   2022-01-04 17:00:00\n13   2022-01-05 00:00:00\n14   2022-01-05 08:00:00\n15   2022-01-05 15:00:00\n16   2022-01-05 23:00:00\n17   2022-01-06 06:00:00\n18   2022-01-06 14:00:00\n19   2022-01-06 21:00:00\n20   2022-01-07 04:00:00\n21   2022-01-07 12:00:00\n22   2022-01-07 19:00:00\n23   2022-01-08 03:00:00\n24   2022-01-08 10:00:00\n25   2022-01-08 18:00:00\n26   2022-01-09 01:00:00\n27   2022-01-09 09:00:00\n28   2022-01-09 16:00:00\n29   2022-01-10 00:00:00\ndtype: datetime64[ns]\n</code></pre>"},{"location":"api_docs/pandas/series/dt.hour/","title":"<code>pd.Series.dt.hour</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.hour</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.hour/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.hour\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0      0\n1      2\n2      4\n3      7\n4      9\n5     12\n6     14\n7     17\n8     19\n9     22\n10     0\n11     3\n12     5\n13     8\n14    10\n15    13\n16    15\n17    18\n18    20\n19    23\n20     1\n21     4\n22     6\n23     9\n24    11\n25    14\n26    16\n27    19\n28    21\n29     0\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.is_leap_year/","title":"<code>pd.Series.dt.is_leap_year</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.is_leap_year</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.is_leap_year/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.is_leap_year\n&gt;&gt;&gt; S = pd.Series([pd.Timestamp('2020-01-01'), pd.Timestamp('2021-01-01')])\n&gt;&gt;&gt; f(S)\n0      True\n1     False\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/dt.is_month_end/","title":"<code>pd.Series.dt.is_month_end</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.is_month_end</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.is_month_end/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.is_month_end\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='12/31/2024', periods=30))\n&gt;&gt;&gt; f(S)\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14    False\n15    False\n16    False\n17    False\n18    False\n19    False\n20    False\n21    False\n22    False\n23    False\n24    False\n25    False\n26    False\n27    False\n28    False\n29     True\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/dt.is_month_start/","title":"<code>pd.Series.dt.is_month_start</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.is_month_start</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.is_month_start/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.is_month_start\n&gt;&gt;&gt; SS = pd.Series(pd.date_range(start='1/1/2022', end='12/31/2024', periods=30))\n&gt;&gt;&gt; f(S)\n0      True\n1     False\n2     False\n3     False\n4      True\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14    False\n15    False\n16    False\n17    False\n18    False\n19    False\n20    False\n21    False\n22    False\n23    False\n24    False\n25     True\n26    False\n27    False\n28    False\n29    False\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/dt.is_quarter_end/","title":"<code>pd.Series.dt.is_quarter_end</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.is_quarter_end</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.is_quarter_end/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.is_quarter_end\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='12/31/2024', periods=30))\n&gt;&gt;&gt; f(S)\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14    False\n15    False\n16    False\n17    False\n18    False\n19    False\n20    False\n21    False\n22    False\n23    False\n24    False\n25    False\n26    False\n27    False\n28    False\n29     True\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/dt.is_quarter_start/","title":"<code>pd.Series.dt.is_quarter_start</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.is_quarter_start</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.is_quarter_start/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.is_quarter_start\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='12/31/2024', periods=30))\n&gt;&gt;&gt; f(S)\n0      True\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14    False\n15    False\n16    False\n17    False\n18    False\n19    False\n20    False\n21    False\n22    False\n23    False\n24    False\n25    False\n26    False\n27    False\n28    False\n29    False\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/dt.is_year_end/","title":"<code>pd.Series.dt.is_year_end</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.is_year_end</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.is_year_end/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.is_year_end\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='12/31/2024', periods=30))\n&gt;&gt;&gt; f(S)\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14    False\n15    False\n16    False\n17    False\n18    False\n19    False\n20    False\n21    False\n22    False\n23    False\n24    False\n25    False\n26    False\n27    False\n28    False\n29     True\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/dt.is_year_start/","title":"<code>pd.Series.dt.is_year_start</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.is_year_start</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.is_year_start/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.is_year_start\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='12/31/2024', periods=30))\n&gt;&gt;&gt; f(S)\n0      True\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14    False\n15    False\n16    False\n17    False\n18    False\n19    False\n20    False\n21    False\n22    False\n23    False\n24    False\n25    False\n26    False\n27    False\n28    False\n29    False\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/dt.microsecond/","title":"<code>pd.Series.dt.microsecond</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.microsecond</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.microsecond/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.microsecond\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0          0\n1     931034\n2     862068\n3     793103\n4     724137\n5     655172\n6     586206\n7     517241\n8     448275\n9     379310\n10    310344\n11    241379\n12    172413\n13    103448\n14     34482\n15    965517\n16    896551\n17    827586\n18    758620\n19    689655\n20    620689\n21    551724\n22    482758\n23    413793\n24    344827\n25    275862\n26    206896\n27    137931\n28     68965\n29         0\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.minute/","title":"<code>pd.Series.dt.minute</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.minute</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.minute/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.minute\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0      0\n1     28\n2     57\n3     26\n4     55\n5     24\n6     53\n7     22\n8     51\n9     20\n10    49\n11    18\n12    47\n13    16\n14    45\n15    14\n16    43\n17    12\n18    41\n19    10\n20    39\n21     8\n22    37\n23     6\n24    35\n25     4\n26    33\n27     2\n28    31\n29     0\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.month/","title":"<code>pd.Series.dt.month</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.month</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.month/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.month\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0      1\n1      2\n2      3\n3      4\n4      6\n5      7\n6      8\n7      9\n8     11\n9     12\n10     1\n11     2\n12     4\n13     5\n14     6\n15     7\n16     9\n17    10\n18    11\n19    12\n20     2\n21     3\n22     4\n23     5\n24     7\n25     8\n26     9\n27    10\n28    12\n29     1\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.month_name/","title":"<code>pd.Series.dt.month_name</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.month_name(locale=None)</code></p>"},{"location":"api_docs/pandas/series/dt.month_name/#argument-restrictions","title":"Argument Restrictions:","text":"<ul> <li><code>locale</code>: only supports default value <code>None</code>.</li> </ul> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.month_name/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.month_name()\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0       January\n1      February\n2         March\n3         April\n4          June\n5          July\n6        August\n7     September\n8      November\n9      December\n10      January\n11     February\n12        April\n13          May\n14         June\n15         July\n16    September\n17      October\n18     November\n19     December\n20     February\n21        March\n22        April\n23          May\n24         July\n25       August\n26    September\n27      October\n28     December\n29      January\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/dt.nanosecond/","title":"<code>pd.Series.dt.nanosecond</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.nanosecond</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.nanosecond/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.nanosecond\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0       0\n1     483\n2     966\n3     448\n4     932\n5     416\n6     896\n7     380\n8     864\n9     348\n10    832\n11    312\n12    792\n13    280\n14    760\n15    248\n16    728\n17    208\n18    696\n19    176\n20    664\n21    144\n22    624\n23    104\n24    584\n25     80\n26    560\n27     40\n28    520\n29      0\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.normalize/","title":"<code>pd.Series.dt.normalize</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.normalize()</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.normalize/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.normalize()\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2022', periods=30))\n&gt;&gt;&gt; f(S)\n0    2022-01-01\n1    2022-01-01\n2    2022-01-01\n3    2022-01-01\n4    2022-01-02\n5    2022-01-02\n6    2022-01-02\n7    2022-01-03\n8    2022-01-03\n9    2022-01-03\n10   2022-01-04\n11   2022-01-04\n12   2022-01-04\n13   2022-01-05\n14   2022-01-05\n15   2022-01-05\n16   2022-01-05\n17   2022-01-06\n18   2022-01-06\n19   2022-01-06\n20   2022-01-07\n21   2022-01-07\n22   2022-01-07\n23   2022-01-08\n24   2022-01-08\n25   2022-01-08\n26   2022-01-09\n27   2022-01-09\n28   2022-01-09\n29   2022-01-10\ndtype: datetime64[ns]\n</code></pre>"},{"location":"api_docs/pandas/series/dt.quarter/","title":"<code>pd.Series.dt.quarter</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.quarter</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.quarter/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.quarter\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0     1\n1     1\n2     1\n3     2\n4     2\n5     3\n6     3\n7     3\n8     4\n9     4\n10    1\n11    1\n12    2\n13    2\n14    2\n15    3\n16    3\n17    4\n18    4\n19    4\n20    1\n21    1\n22    2\n23    2\n24    3\n25    3\n26    3\n27    4\n28    4\n29    1\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.round/","title":"<code>pd.Series.dt.round</code>","text":"<p><code>pandas.Series.dt.round(freq, ambiguous='raise', nonexistent='raise')</code></p>"},{"location":"api_docs/pandas/series/dt.round/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>freq</code> String Must be a valid fixedfrequency alias"},{"location":"api_docs/pandas/series/dt.round/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.round(\"H\")\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2022', periods=30))\n&gt;&gt;&gt; f(S)\n0    2022-01-01 00:00:00\n1    2022-01-01 07:00:00\n2    2022-01-01 15:00:00\n3    2022-01-01 22:00:00\n4    2022-01-02 06:00:00\n5    2022-01-02 13:00:00\n6    2022-01-02 21:00:00\n7    2022-01-03 04:00:00\n8    2022-01-03 12:00:00\n9    2022-01-03 19:00:00\n10   2022-01-04 02:00:00\n11   2022-01-04 10:00:00\n12   2022-01-04 17:00:00\n13   2022-01-05 01:00:00\n14   2022-01-05 08:00:00\n15   2022-01-05 16:00:00\n16   2022-01-05 23:00:00\n17   2022-01-06 07:00:00\n18   2022-01-06 14:00:00\n19   2022-01-06 22:00:00\n20   2022-01-07 05:00:00\n21   2022-01-07 12:00:00\n22   2022-01-07 20:00:00\n23   2022-01-08 03:00:00\n24   2022-01-08 11:00:00\n25   2022-01-08 18:00:00\n26   2022-01-09 02:00:00\n27   2022-01-09 09:00:00\n28   2022-01-09 17:00:00\n29   2022-01-10 00:00:00\ndtype: datetime64[ns]\n</code></pre>"},{"location":"api_docs/pandas/series/dt.second/","title":"<code>pd.Series.dt.second</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.second</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.second/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.second\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0      0\n1     57\n2     55\n3     53\n4     51\n5     49\n6     47\n7     45\n8     43\n9     41\n10    39\n11    37\n12    35\n13    33\n14    31\n15    28\n16    26\n17    24\n18    22\n19    20\n20    18\n21    16\n22    14\n23    12\n24    10\n25     8\n26     6\n27     4\n28     2\n29     0\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.strftime/","title":"<code>pd.Series.dt.strftime</code>","text":"<p><code>pandas.Series.dt.strftime(date_format)</code></p>"},{"location":"api_docs/pandas/series/dt.strftime/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>date_format</code> String Must be a valid datetime format string"},{"location":"api_docs/pandas/series/dt.strftime/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.strftime(\"%B %d, %Y, %r\")\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2022', periods=30))\n&gt;&gt;&gt; f(S)\n0     January 01, 2022, 12:00:00 AM\n1     January 01, 2022, 07:26:53 AM\n2     January 01, 2022, 02:53:47 PM\n3     January 01, 2022, 10:20:41 PM\n4     January 02, 2022, 05:47:35 AM\n5     January 02, 2022, 01:14:28 PM\n6     January 02, 2022, 08:41:22 PM\n7     January 03, 2022, 04:08:16 AM\n8     January 03, 2022, 11:35:10 AM\n9     January 03, 2022, 07:02:04 PM\n10    January 04, 2022, 02:28:57 AM\n11    January 04, 2022, 09:55:51 AM\n12    January 04, 2022, 05:22:45 PM\n13    January 05, 2022, 12:49:39 AM\n14    January 05, 2022, 08:16:33 AM\n15    January 05, 2022, 03:43:26 PM\n16    January 05, 2022, 11:10:20 PM\n17    January 06, 2022, 06:37:14 AM\n18    January 06, 2022, 02:04:08 PM\n19    January 06, 2022, 09:31:02 PM\n20    January 07, 2022, 04:57:55 AM\n21    January 07, 2022, 12:24:49 PM\n22    January 07, 2022, 07:51:43 PM\n23    January 08, 2022, 03:18:37 AM\n24    January 08, 2022, 10:45:31 AM\n25    January 08, 2022, 06:12:24 PM\n26    January 09, 2022, 01:39:18 AM\n27    January 09, 2022, 09:06:12 AM\n28    January 09, 2022, 04:33:06 PM\n29    January 10, 2022, 12:00:00 AM\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/dt.weekday/","title":"<code>pd.Series.dt.weekday</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.weekday</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.weekday/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.weekday\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0     5\n1     1\n2     4\n3     0\n4     3\n5     6\n6     2\n7     5\n8     1\n9     4\n10    1\n11    4\n12    0\n13    3\n14    6\n15    2\n16    5\n17    1\n18    4\n19    0\n20    4\n21    0\n22    3\n23    6\n24    2\n25    5\n26    1\n27    4\n28    0\n29    4\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dt.year/","title":"<code>pd.Series.dt.year</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.dt.year</code></p> <p>Note</p> <p>Input must be a Series of <code>datetime64</code> data.</p>"},{"location":"api_docs/pandas/series/dt.year/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dt.year\n&gt;&gt;&gt; S = pd.Series(pd.date_range(start='1/1/2022', end='1/10/2025', periods=30))\n&gt;&gt;&gt; f(S)\n0     2022\n1     2022\n2     2022\n3     2022\n4     2022\n5     2022\n6     2022\n7     2022\n8     2022\n9     2022\n10    2023\n11    2023\n12    2023\n13    2023\n14    2023\n15    2023\n16    2023\n17    2023\n18    2023\n19    2023\n20    2024\n21    2024\n22    2024\n23    2024\n24    2024\n25    2024\n26    2024\n27    2024\n28    2024\n29    2025\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/dtype/","title":"<code>pd.Series.dtype</code>","text":"<p><code>pandas.Series.dtype</code> (object data types such as dtype of string series not supported yet)</p>"},{"location":"api_docs/pandas/series/dtype/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dtype\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S)\ndtype('int64')\n</code></pre>"},{"location":"api_docs/pandas/series/dtypes/","title":"<code>pd.Series.dtypes</code>","text":"<p><code>pandas.Series.dtypes</code></p>"},{"location":"api_docs/pandas/series/dtypes/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.dtypes\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S)\ndtype('int64')\n</code></pre>"},{"location":"api_docs/pandas/series/duplicated/","title":"<code>pd.Series.duplicated</code>","text":"<p><code>pandas.Series.duplicated(keep='first')</code></p>"},{"location":"api_docs/pandas/series/duplicated/#supported-arguments-none","title":"Supported Arguments None","text":""},{"location":"api_docs/pandas/series/duplicated/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...  return S.duplicated()\n&gt;\n&gt;&gt;&gt; S = pd.Series([1, 2, 1, np.nan, 3, 2, np.nan, 4])\n0    False\n1    False\n2     True\n3    False\n4    False\n5     True\n6     True\n7    False\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/empty/","title":"pd.Series.empty","text":"<ul> <li><code>pandas.Series.empty</code></li> </ul>"},{"location":"api_docs/pandas/series/empty/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.apply(lambda row: row.empty, axis=1)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": np.arange(100), \"B\": [\"A\", \"b\"] * 50})\n&gt;&gt;&gt; f(df)\n0     False\n1     False\n2     False\n3     False\n4     False\n      ...\n95    False\n96    False\n97    False\n98    False\n99    False\nLength: 100, dtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/eq/","title":"<code>pd.Series.eq</code>","text":"<p><code>pandas.Series.eq(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/eq/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>  numeric scalar </li><li>  array with numeric data </li><li> Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.eq</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/eq/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.eq(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      False\n1      False\n2      False\n3      False\n4      False\n      ...\n995    False\n996    False\n997    False\n998    False\n999    False\nLength: 1000, dtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/equals/","title":"<code>pd.Series.equals</code>","text":"<p><code>pandas.Series.equals(other)</code></p>"},{"location":"api_docs/pandas/series/equals/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> Series <p>Note</p> <p>Series and <code>other</code> must contain scalar values in each row</p>"},{"location":"api_docs/pandas/series/equals/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...     return S.equals(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 10\n&gt;&gt;&gt; other = pd.Series(np.arange(100)) % 5\n&gt;&gt;&gt; f(S, other)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/series/explode/","title":"<code>pd.Series.explode</code>","text":"<p><code>pandas.Series.explode(ignore_index=False)</code></p>"},{"location":"api_docs/pandas/series/explode/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <p>Bodo's output type may differ from Pandas because Bodo must convert to a nullable type at compile time.</p>"},{"location":"api_docs/pandas/series/explode/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.explode()\n&gt;&gt;&gt; S = pd.Series([np.arange(i) for i in range(10)])\n&gt;&gt;&gt; f(S)\n0    &lt;NA&gt;\n1       0\n2       0\n2       1\n3       0\n3       1\n3       2\n4       0\n4       1\n4       2\n4       3\n5       0\n5       1\n5       2\n5       3\n5       4\n6       0\n6       1\n6       2\n6       3\n6       4\n6       5\n7       0\n7       1\n7       2\n7       3\n7       4\n7       5\n7       6\n8       0\n8       1\n8       2\n8       3\n8       4\n8       5\n8       6\n8       7\n9       0\n9       1\n9       2\n9       3\n9       4\n9       5\n9       6\n9       7\n9       8\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/ffill/","title":"<code>pd.Series.ffill</code>","text":"<p><code>pandas.Series.ffill(axis=None, inplace=False, limit=None, downcast=None)</code></p>"},{"location":"api_docs/pandas/series/ffill/#supported-arguments-none","title":"Supported Arguments None","text":""},{"location":"api_docs/pandas/series/ffill/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.ffill()\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S)\n0    &lt;NA&gt;\n1       1\n2       1\n3      -2\n4      -2\n5       5\n6       5\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/fillna/","title":"<code>pd.Series.fillna</code>","text":"<p><code>pandas.Series.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)</code></p>"},{"location":"api_docs/pandas/series/fillna/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>value</code> Scalar <code>method</code> One of (\"bfill\", \"backfill\", \"ffill\", and \"pad\") Must be constant at Compile Time <code>inplace</code> Boolean Must be constant at Compile Time <ul> <li>If <code>value</code> is provided then <code>method</code> must be <code>None</code> and     vice-versa</li> <li>If <code>method</code> is provided then <code>inplace</code> must be <code>False</code></li> </ul>"},{"location":"api_docs/pandas/series/fillna/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.fillna(-1)\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S)\n0    -1\n1     1\n2    -1\n3    -2\n4    -1\n5     5\n6    -1\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/first/","title":"<code>pd.Series.first</code>","text":"<p><code>pandas.Series.first(offset)</code></p>"},{"location":"api_docs/pandas/series/first/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>offset</code> String or Offset type String argument be a valid frequency alias <p>Note</p> <p>Series must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted.</p>"},{"location":"api_docs/pandas/series/first/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, offset):\n...     return S.first(offset)\n&gt;&gt;&gt; S = pd.Series(np.arange(100), index=pd.date_range(start='1/1/2022', end='12/31/2024', periods=100))\n&gt;&gt;&gt; f(S, \"2M\")\n2022-01-01 00:00:00.000000000    0\n2022-01-12 01:27:16.363636363    1\n2022-01-23 02:54:32.727272727    2\n2022-02-03 04:21:49.090909091    3\n2022-02-14 05:49:05.454545454    4\n2022-02-25 07:16:21.818181818    5\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/floordiv/","title":"<code>pd.Series.floordiv</code>","text":"<p><code>pandas.Series.floordiv(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/floordiv/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>   numeric scalar </li><li> array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.floordiv</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/floordiv/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.floordiv(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0         0\n1         0\n2         0\n3         0\n4         0\n      ...\n995     199\n996     249\n997     332\n998     499\n999    1000\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/ge/","title":"<code>pd.Series.ge</code>","text":"<p><code>pandas.Series.ge(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/ge/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>  numeric scalar </li><li>  array with numeric data </li><li> Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.ge</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/ge/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.ge(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      False\n1      False\n2      False\n3      False\n4      False\n      ...\n995     True\n996     True\n997     True\n998     True\n999     True\nLength: 1000, dtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/groupby/","title":"<code>pd.Series.groupby</code>","text":"<p><code>pandas.Series.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True)</code></p>"},{"location":"api_docs/pandas/series/groupby/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>by</code> Array-like or Series data. This is not supported with Decimal or Categorical data. Must be constant at  Compile Time <code>level</code> integer <ul><li> Must be constant at  Compile Time </li><li> Only <code>level=0</code> is supported and not  with MultiIndex. <p>You must provide exactly one of <code>by</code> and <code>level</code></p>"},{"location":"api_docs/pandas/series/groupby/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, by_series):\n...     return S.groupby(by_series).count()\n&gt;&gt;&gt; S = pd.Series([1, 2, 24, None] * 5)\n&gt;&gt;&gt; by_series = pd.Series([\"421\", \"f31\"] * 10)\n&gt;&gt;&gt; f(S, by_series)\n&gt;\n421    10\nf31     5\nName: , dtype: int64\n</code></pre> <p>Note</p> <p><code>Series.groupby</code> doesn't currently keep the name of the original Series.</p>"},{"location":"api_docs/pandas/series/gt/","title":"<code>pd.Series.gt</code>","text":"<p><code>pandas.Series.gt(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/gt/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>  numeric scalar </li><li>  array with numeric data </li><li> Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.gt</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/gt/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.gt(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      False\n1      False\n2      False\n3      False\n4      False\n      ...\n995     True\n996     True\n997     True\n998     True\n999     True\nLength: 1000, dtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/hasnans/","title":"<code>pd.Series.hasnans</code>","text":"<p><code>pandas.Series.hasnans</code></p>"},{"location":"api_docs/pandas/series/hasnans/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.hasnans\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/series/head/","title":"<code>pd.Series.head</code>","text":"<p><code>pandas.Series.head(n=5)</code></p>"},{"location":"api_docs/pandas/series/head/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>n</code> Integer"},{"location":"api_docs/pandas/series/head/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.head(10)\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/iat/","title":"<code>pd.Series.iat</code>","text":"<p><code>pandas.Series.iat</code></p> <p>We only support indexing using <code>iat</code> using a pair of integers</p>"},{"location":"api_docs/pandas/series/iat/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, i):\n...   return S.iat[i]\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S, 27)\n27\n</code></pre>"},{"location":"api_docs/pandas/series/idxmax/","title":"<code>pd.Series.idxmax</code>","text":"<p><code>pandas.Series.idxmax(axis=0, skipna=True)</code></p>"},{"location":"api_docs/pandas/series/idxmax/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <p>Bodo does not accept any additional arguments for Numpy compatibility</p>"},{"location":"api_docs/pandas/series/idxmax/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.idxmax()\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; S[(S % 3 == 0)] = 100\n&gt;&gt;&gt; f(S)\n0\n</code></pre>"},{"location":"api_docs/pandas/series/idxmin/","title":"<code>pd.Series.idxmin</code>","text":"<p><code>pandas.Series.idxmin(axis=0, skipna=True)</code></p>"},{"location":"api_docs/pandas/series/idxmin/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <p>Bodo does not accept any additional arguments for Numpy compatibility</p>"},{"location":"api_docs/pandas/series/idxmin/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.idxmin()\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; S[(S % 3 == 0)] = 100\n&gt;&gt;&gt; f(S)\n1\n</code></pre>"},{"location":"api_docs/pandas/series/iloc/","title":"<code>pd.Series.iloc</code>","text":"<p><code>pandas.Series.iloc</code></p> <ul> <li> <p>getitem:</p> <ul> <li><code>Series.iloc</code> supports single integer indexing (returns a     scalar) <code>S.iloc[0]</code></li> <li><code>Series.iloc</code> supports list/array/series of integers/bool     (returns a Series) <code>S.iloc[[0,1,2]]</code></li> <li><code>Series.iloc</code> supports integer slice (returns a Series)     <code>S.iloc[[0:2]]</code></li> </ul> </li> <li> <p>setitem:</p> <ul> <li>Supports the same cases as getitem but the array type must be     mutable (i.e. numeric array)</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/series/iloc/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, idx):\n...   return S.iloc[idx]\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S, [1, 4, 29])\n1      1\n4      4\n29    29\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/is_monotonic_decreasing/","title":"<code>pd.Series.is_monotonic_decreasing</code>","text":"<p>`pandas.Series.is_monotonic_decreasing</p>"},{"location":"api_docs/pandas/series/is_monotonic_decreasing/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.is_monotonic_decreasing\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\nFalse\n</code></pre>"},{"location":"api_docs/pandas/series/is_monotonic_increasing/","title":"<code>pd.Series.is_monotonic_increasing</code>","text":"<p>`pandas.Series.is_monotonic_increasing</p>"},{"location":"api_docs/pandas/series/is_monotonic_increasing/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.is_monotonic_increasing\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\nTrue\n</code></pre>"},{"location":"api_docs/pandas/series/isin/","title":"<code>pd.Series.isin</code>","text":"<p><code>pandas.Series.isin(values)</code></p>"},{"location":"api_docs/pandas/series/isin/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>values</code> <ul><li>   Series </li><li> Array </li><li> List </li></ul> <p>Note</p> <p><code>values</code> argument supports both distributed array/Series and replicated list/array/Series</p>"},{"location":"api_docs/pandas/series/isin/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.isin([3, 11, 98])\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0     False\n1     False\n2     False\n3      True\n4     False\n      ...\n95    False\n96    False\n97    False\n98     True\n99    False\nLength: 100, dtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/isna/","title":"<code>pd.Series.isna</code>","text":"<p><code>pandas.Series.isna()</code></p>"},{"location":"api_docs/pandas/series/isna/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.isna()\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S)\n0     True\n1    False\n2     True\n3    False\n4     True\n5    False\n6     True\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/isnull/","title":"<code>pd.Series.isnull</code>","text":"<p><code>pandas.Series.isnull()</code></p>"},{"location":"api_docs/pandas/series/isnull/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.isnull()\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S)\n0     True\n1    False\n2     True\n3    False\n4     True\n5    False\n6     True\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/kurt/","title":"<code>pd.Series.kurt</code>","text":"<p><code>pandas.Series.kurt(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/series/kurt/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>skipna</code> Boolean <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/kurt/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.kurt()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n-1.269562153611973\n</code></pre>"},{"location":"api_docs/pandas/series/kurtosis/","title":"<code>pd.Series.kurtosis</code>","text":"<p><code>pandas.Series.kurtosis(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/series/kurtosis/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>skipna</code> Boolean <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/kurtosis/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.kurtosis()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n-1.269562153611973\n</code></pre>"},{"location":"api_docs/pandas/series/last/","title":"<code>pd.Series.last</code>","text":"<p><code>pandas.Series.last(offset)</code></p>"},{"location":"api_docs/pandas/series/last/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>offset</code> -   String or Offset type -   String argument be a valid frequency alias <p>Note</p> <p>Series must have a valid DatetimeIndex and is assumed to already be sorted. This function have undefined behavior if the DatetimeIndex is not sorted.</p>"},{"location":"api_docs/pandas/series/last/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, offset):\n...     return S.last(offset)\n&gt;&gt;&gt; S = pd.Series(np.arange(100), index=pd.date_range(start='1/1/2022', end='12/31/2024', periods=100))\n&gt;&gt;&gt; f(S, \"2M\")\n2024-11-05 16:43:38.181818176    94\n2024-11-16 18:10:54.545454544    95\n2024-11-27 19:38:10.909090912    96\n2024-12-08 21:05:27.272727264    97\n2024-12-19 22:32:43.636363632    98\n2024-12-31 00:00:00.000000000    99\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/le/","title":"<code>pd.Series.le</code>","text":"<p><code>pandas.Series.le(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/le/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>  numeric scalar </li><li>  array with numeric data </li><li> Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.le</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/le/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.le(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0       True\n1       True\n2       True\n3       True\n4       True\n      ...\n995    False\n996    False\n997    False\n998    False\n999    False\nLength: 1000, dtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/loc/","title":"<code>pd.Series.loc</code>","text":"<p><code>pandas.Series.loc</code></p> <ul> <li> <p>getitem:</p> <ul> <li><code>Series.loc</code> supports list/array of booleans</li> <li><code>Series.loc</code> supports integer with RangeIndex</li> </ul> </li> <li> <p>setitem:</p> <ul> <li><code>Series.loc</code> supports list/array of booleans</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/series/loc/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, idx):\n...   return S.loc[idx]\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S, S &lt; 10)\n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/loc/#binary-operator-functions","title":"Binary operator functions:","text":""},{"location":"api_docs/pandas/series/lt/","title":"<code>pd.Series.lt</code>","text":"<p><code>pandas.Series.lt(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/lt/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>  numeric scalar </li><li>  array with numeric data </li><li> Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.lt</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/lt/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.lt(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0       True\n1       True\n2       True\n3       True\n4       True\n      ...\n995    False\n996    False\n997    False\n998    False\n999    False\nLength: 1000, dtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/map/","title":"<code>pd.Series.map</code>","text":"<p><code>pandas.Series.map(arg, na_action=None)</code></p>"},{"location":"api_docs/pandas/series/map/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>arg</code> <ul><li>   Dictionary   </li><li>   JIT function or callable defined within a JIT function </li><li>   Constant String which refers to a supported Series method or Numpy  ufunc  </li><li>   Numpy ufunc  </li></ul>"},{"location":"api_docs/pandas/series/map/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...   return S.map(lambda x: x ** 0.75)\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0      0.000000\n1      1.000000\n2      1.681793\n3      2.279507\n4      2.828427\n        ...\n95    30.429352\n96    30.669269\n97    30.908562\n98    31.147239\n99    31.385308\nLength: 100, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/mask/","title":"<code>pd.Series.mask</code>","text":"<p><code>pandas.Series.mask(cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=NoDefault.no_default)</code></p>"},{"location":"api_docs/pandas/series/mask/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>cond</code> <ul><li>  boolean array  </li>   1d bool numpy array </ul> <code>other</code> <ul><li>  1d numpy array </li>   scalar     </ul> <p>Note</p> <p>Series can contain categorical data if <code>other</code> is a scalar</p>"},{"location":"api_docs/pandas/series/mask/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.mask((S % 3) != 0, 0)\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0      0\n1      0\n2      0\n3      3\n4      0\n      ..\n95     0\n96    96\n97     0\n98     0\n99    99\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/mask/#missing-data-handling","title":"Missing data handling","text":""},{"location":"api_docs/pandas/series/max/","title":"<code>pd.Series.max</code>","text":"<p><code>pandas.Series.max(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/series/max/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/max/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.max()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n6\n</code></pre>"},{"location":"api_docs/pandas/series/mean/","title":"<code>pd.Series.mean</code>","text":"<p><code>pandas.Series.mean(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/series/mean/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/mean/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.mean()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n2.95\n</code></pre>"},{"location":"api_docs/pandas/series/median/","title":"<code>pd.Series.median</code>","text":"<p><code>pandas.Series.median(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/series/median/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>skipna</code> Boolean <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/median/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.median()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n3.0\n</code></pre>"},{"location":"api_docs/pandas/series/memory_usage/","title":"<code>pd.Series.memory_usage</code>","text":"<p><code>pandas.Series.memory_usage(index=True, deep=False)</code></p>"},{"location":"api_docs/pandas/series/memory_usage/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>index</code> Boolean Must be constant at Compile Time <p>Note</p> <p>This tracks the number of bytes used by Bodo which may differ from the Pandas values.</p>"},{"location":"api_docs/pandas/series/memory_usage/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.memory_usage()\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S)\n8024\n</code></pre>"},{"location":"api_docs/pandas/series/min/","title":"<code>pd.Series.min</code>","text":"<p><code>pandas.Series.min(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/series/min/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/min/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.min()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n0\n</code></pre>"},{"location":"api_docs/pandas/series/mod/","title":"<code>pd.Series.mod</code>","text":"<p><code>pandas.Series.mod(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/mod/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>   numeric scalar </li><li> array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.mod</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/mod/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.mod(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      1\n1      2\n2      3\n3      4\n4      5\n      ..\n995    1\n996    1\n997    2\n998    1\n999    0\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/mul/","title":"<code>pd.Series.mul</code>","text":"<p><code>pandas.Series.mul(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/mul/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>   numeric scalar  </li><li>  array with numeric data </li><li> Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.mul</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/mul/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.mul(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      1000\n1      1998\n2      2994\n3      3988\n4      4980\n      ...\n995    4980\n996    3988\n997    2994\n998    1998\n999    1000\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/name/","title":"pd.Series.name","text":"<ul> <li><code>pandas.Series.name</code></li> </ul>"},{"location":"api_docs/pandas/series/name/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.apply(lambda row: row.name, axis=1)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": np.arange(100), \"B\": [\"A\", \"b\"] * 50})\n&gt;&gt;&gt; f(df)\n0      0\n1      1\n2      2\n3      3\n4      4\n      ..\n95    95\n96    96\n97    97\n98    98\n99    99\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/nbytes/","title":"<code>pd.Series.nbytes</code>","text":"<p><code>pandas.Series.nbytes</code></p> <p>Note</p> <p>This tracks the number of bytes used by Bodo which may differ from the Pandas values.</p>"},{"location":"api_docs/pandas/series/nbytes/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.nbytes\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S)\n8000\n</code></pre>"},{"location":"api_docs/pandas/series/ndim/","title":"pd.Series.ndim","text":"<ul> <li><code>pandas.Series.ndim</code></li> </ul>"},{"location":"api_docs/pandas/series/ndim/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.apply(lambda row: row.ndim, axis=1)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": np.arange(100), \"B\": [\"A\", \"b\"] * 50})\n&gt;&gt;&gt; f(df)\n0     1\n1     1\n2     1\n3     1\n4     1\n..\n95    1\n96    1\n97    1\n98    1\n99    1\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/ne/","title":"<code>pd.Series.ne</code>","text":"<p><code>pandas.Series.ne(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/ne/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>  numeric scalar </li><li>  array with numeric data </li><li> Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.ne</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/ne/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.ne(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      True\n1      True\n2      True\n3      True\n4      True\n      ...\n995    True\n996    True\n997    True\n998    True\n999    True\nLength: 1000, dtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/nlargest/","title":"<code>pd.Series.nlargest</code>","text":"<p><code>pandas.Series.nlargest(n=5, keep='first')</code></p>"},{"location":"api_docs/pandas/series/nlargest/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>n</code> Integer <p>Note</p> <p>Series type must be numeric</p>"},{"location":"api_docs/pandas/series/nlargest/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.nlargest(20)\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n20    6\n27    6\n41    6\n34    6\n55    6\n13    6\n83    6\n90    6\n6     6\n69    6\n48    6\n76    6\n62    6\n97    6\n19    5\n5     5\n26    5\n61    5\n12    5\n68    5\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/notna/","title":"<code>pd.Series.notna</code>","text":"<p><code>pandas.Series.notna()</code></p>"},{"location":"api_docs/pandas/series/notna/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.notna()\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S)\n0    False\n1     True\n2    False\n3     True\n4    False\n5     True\n6    False\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/notnull/","title":"<code>pd.Series.notnull</code>","text":"<p><code>pandas.Series.notnull()</code></p>"},{"location":"api_docs/pandas/series/notnull/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.notnull()\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S)\n0    False\n1     True\n2    False\n3     True\n4    False\n5     True\n6    False\ndtype: bool\n</code></pre>"},{"location":"api_docs/pandas/series/nsmallest/","title":"<code>pd.Series.nsmallest</code>","text":"<p><code>pandas.Series.nsmallest(n=5, keep='first')</code></p>"},{"location":"api_docs/pandas/series/nsmallest/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>n</code> Integer <p>Note</p> <p>Series type must be numeric</p>"},{"location":"api_docs/pandas/series/nsmallest/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.nsmallest(20)\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n63    0\n7     0\n56    0\n98    0\n77    0\n91    0\n49    0\n42    0\n35    0\n84    0\n28    0\n21    0\n70    0\n0     0\n14    0\n43    1\n1     1\n57    1\n15    1\n36    1\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/nunique/","title":"<code>pd.Series.nunique</code>","text":"<p><code>pandas.Series.nunique(dropna=True)</code></p>"},{"location":"api_docs/pandas/series/nunique/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>dropna</code> Boolean"},{"location":"api_docs/pandas/series/nunique/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.nunique()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n7\n</code></pre>"},{"location":"api_docs/pandas/series/pad/","title":"<code>pd.Series.pad</code>","text":"<p><code>pandas.Series.pad(axis=None, inplace=False, limit=None, downcast=None)</code></p>"},{"location":"api_docs/pandas/series/pad/#supported-arguments-none","title":"Supported Arguments None","text":""},{"location":"api_docs/pandas/series/pad/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.pad()\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S)\n0    &lt;NA&gt;\n1       1\n2       1\n3      -2\n4      -2\n5       5\n6       5\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/pct_change/","title":"<code>pd.Series.pct_change</code>","text":"<p><code>pandas.Series.pct_change(periods=1, fill_method='pad', limit=None, freq=None)</code></p>"},{"location":"api_docs/pandas/series/pct_change/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>periods</code> Integer <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to shift</li> </ul>"},{"location":"api_docs/pandas/series/pct_change/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.pct_change(3)\n&gt;&gt;&gt; S = (pd.Series(np.arange(100)) % 7) + 1\n&gt;&gt;&gt; f(S)\n0          NaN\n1          NaN\n2          NaN\n3     3.000000\n4     1.500000\n        ...\n95    1.500000\n96    1.000000\n97    0.750000\n98   -0.800000\n99   -0.666667\nLength: 100, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/pipe/","title":"<code>pd.Series.pipe</code>","text":"<ul> <li>pandas.Series.pipe(func, *args, **kwargs)</li> </ul>"},{"location":"api_docs/pandas/series/pipe/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>func</code> JIT function or callable defined within a JIT function. Additional arguments for <code>func</code> can be passed as additional arguments. <p>Note</p> <p><code>func</code> cannot be a tuple</p>"},{"location":"api_docs/pandas/series/pipe/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     def g(row, y):\n...         return row + y\n...\n...     def f(row):\n...         return row * 2\n...\n...     return S.pipe(h).pipe(g, y=32)\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0      32\n1      34\n2      36\n3      38\n4      40\n     ...\n95    222\n96    224\n97    226\n98    228\n99    230\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/pipe/#computations-descriptive-stats","title":"Computations / Descriptive Stats","text":"<p>Statistical functions below are supported without optional arguments unless support is explicitly mentioned.</p>"},{"location":"api_docs/pandas/series/pow/","title":"<code>pd.Series.pow</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.pow(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/pow/#argument-restrictions","title":"Argument Restrictions:","text":"<ul> <li><code>other</code>: must be a numeric scalar or Series, Index, Array, List, or Tuple with numeric data.</li> <li><code>level</code>: only supports default value <code>None</code>.</li> <li><code>fill_value</code>: (optional, defaults to <code>None</code>) must be <code>Integer</code>, <code>Float</code> or <code>Boolean</code>.</li> <li><code>axis</code>: only supports default value <code>0</code>.</li> </ul> <p>Note</p> <p>Input must be a Series of <code>Integer</code> or <code>Float</code> data.</p>"},{"location":"api_docs/pandas/series/pow/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.pow(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0                        1\n1                        0\n2     -5459658280481875879\n3                        0\n4      3767675092665006833\n              ...\n995        980159361278976\n996           988053892081\n997              994011992\n998                 998001\n999                   1000\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/prod/","title":"<code>pd.Series.prod</code>","text":"<p><code>pandas.Series.prod(axis=None, skipna=True, level=None, numeric_only=None, min_count=0)</code></p>"},{"location":"api_docs/pandas/series/prod/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>skipna</code> Boolean <code>min_count</code> Integer <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/prod/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.prod()\n&gt;&gt;&gt; S = (pd.Series(np.arange(20)) % 3) + 1\n&gt;&gt;&gt; f(S)\n93312\n</code></pre>"},{"location":"api_docs/pandas/series/product/","title":"<code>pd.Series.product</code>","text":"<p><code>pandas.Series.product(axis=None, skipna=True, level=None, numeric_only=None, min_count=0)</code></p>"},{"location":"api_docs/pandas/series/product/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>skipna</code> Boolean <code>min_count</code> Integer <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/product/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.product()\n&gt;&gt;&gt; S = (pd.Series(np.arange(20)) % 3) + 1\n&gt;&gt;&gt; f(S)\n93312\n</code></pre>"},{"location":"api_docs/pandas/series/quantile/","title":"<code>pd.Series.quantile</code>","text":"<p><code>pandas.Series.quantile(q=0.5, interpolation='linear')</code></p>"},{"location":"api_docs/pandas/series/quantile/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>q</code> <ul><li>   Float in [0.0, 1.0] </li><li>  Iterable of floats in [0.0, 1.0] </li>&lt;/ul"},{"location":"api_docs/pandas/series/quantile/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.quantile([0.25, 0.5, 0.75])\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n0.25    1.0\n0.50    3.0\n0.75    5.0\ndtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/radd/","title":"<code>pd.Series.radd</code>","text":"<p><code>pandas.Series.radd(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/radd/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>   numeric scalar </li><li> array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.radd</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/radd/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.radd(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      1001\n1      1001\n2      1001\n3      1001\n4      1001\n      ...\n995    1001\n996    1001\n997    1001\n998    1001\n999    1001\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/rank/","title":"<code>pd.Series.rank</code>","text":"<p><code>pandas.Series.rank(axis=0, method='average', numeric_only=NoDefault.no_default, na_option='keep', ascending=True, pct=False)</code></p>"},{"location":"api_docs/pandas/series/rank/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>method</code> String in <code>na_option</code> String in <code>ascending</code> Boolean <code>pct</code> Boolean <p>Note</p> <ul> <li>Using <code>method='first'</code>  with <code>ascending=False</code> is currently unsupported.</li> </ul>"},{"location":"api_docs/pandas/series/rank/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.rank(method='dense', na_option='bottom', pct=True)\n&gt;&gt;&gt; S = pd.Series([np.nan, 4, 2, 4, 8, np.nan])\n&gt;&gt;&gt; f(S)\n0    1.00\n1    0.50\n2    0.25\n3    0.50\n4    0.75\n5    1.00\ndtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/rdiv/","title":"<code>pd.Series.rdiv</code>","text":"<p><code>pandas.Series.rdiv(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/rdiv/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>   numeric scalar </li><li> array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.rdiv</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/rdiv/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.rdiv(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      1000.000000\n1       499.500000\n2       332.666667\n3       249.250000\n4       199.200000\n          ...\n995       0.005020\n996       0.004012\n997       0.003006\n998       0.002002\n999       0.001000\nLength: 1000, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/rename/","title":"<code>pd.Series.rename</code>","text":"<p><code>pandas.Series.rename(index=None, *, axis=None, copy=True, inplace=False, level=None, errors='ignore')</code></p>"},{"location":"api_docs/pandas/series/rename/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>index</code> -   String <code>axis</code> -   Any value. Bodo ignores this argument entirely, which i consistent with Pandas."},{"location":"api_docs/pandas/series/rename/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.rename(\"a\")\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0      0\n1      1\n2      2\n3      3\n4      4\n      ..\n95    95\n96    96\n97    97\n98    98\n99    99\nName: a, Length: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/repeat/","title":"<code>pd.Series.repeat</code>","text":"<p><code>pandas.Series.repeat(repeats, axis=None)</code></p>"},{"location":"api_docs/pandas/series/repeat/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>repeats</code> <ul><li>   Integer </li><li>   Array-like of integers the same length as the Series </li></ul>"},{"location":"api_docs/pandas/series/repeat/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.repeat(3)\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0      0\n0      0\n0      0\n1      1\n1      1\n      ..\n98    98\n98    98\n99    99\n99    99\n99    99\nLength: 300, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/repeat/#combining-comparing-joining-merging","title":"Combining / comparing / joining / merging","text":""},{"location":"api_docs/pandas/series/replace/","title":"<code>pd.Series.replace</code>","text":"<p><code>pandas.Series.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad')</code></p>"},{"location":"api_docs/pandas/series/replace/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>to_replace</code> <ul><li>   Scalar  </li><li>   List of Scalars  </li><li>   Dictionary mapping scalars of the  same type </li></ul> <code>value</code> -   Scalar If <code>to_replace</code> is not a  scalar, value must be  <code>None</code>"},{"location":"api_docs/pandas/series/replace/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, replace_dict):\n...     return S.replace(replace_dict)\n&gt;&gt;&gt; S = pd.Series(pd.array([None, 1, None, -2, None, 5, None]))\n&gt;&gt;&gt; f(S, {1: -2, -2: 5, 5: 27})\n0    &lt;NA&gt;\n1      -2\n2    &lt;NA&gt;\n3       5\n4    &lt;NA&gt;\n5      27\n6    &lt;NA&gt;\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/replace/#reshaping-sorting","title":"Reshaping, sorting","text":""},{"location":"api_docs/pandas/series/reset_index/","title":"<code>pd.Series.reset_index</code>","text":"<p><code>pandas.Series.reset_index(level=None, drop=False, name=None, inplace=False)</code></p>"},{"location":"api_docs/pandas/series/reset_index/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>level</code> <ul> <li> Integer</li><li> Boolean</li> </ul> Must be constant at Compile Time <code>drop</code> Boolean &lt;&gt; <li> Must be constant at Compile Time </li> <li> If <code>False</code>, Index name must be known at compilation time </li> <p>Note</p> <p>For MultiIndex case, only dropping all levels is supported.</p>"},{"location":"api_docs/pandas/series/reset_index/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.reset_index()\n&gt;&gt;&gt; S = pd.Series(np.arange(100), index=pd.RangeIndex(100, 200, 1, name=\"b\"))\n&gt;&gt;&gt; f(S)\n      b   0\n0   100   0\n1   101   1\n2   102   2\n3   103   3\n4   104   4\n..  ...  ..\n95  195  95\n96  196  96\n97  197  97\n98  198  98\n99  199  99\n&gt;\n[100 rows x 2 columns]\n</code></pre>"},{"location":"api_docs/pandas/series/rfloordiv/","title":"<code>pd.Series.rfloordiv</code>","text":"<p><code>pandas.Series.rfloordiv(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/rfloordiv/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>   numeric scalar </li><li> array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.rfloordiv</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/rfloordiv/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.rfloordiv(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      1000\n1       499\n2       332\n3       249\n4       199\n      ...\n995       0\n996       0\n997       0\n998       0\n999       0\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/rmod/","title":"<code>pd.Series.rmod</code>","text":"<p><code>pandas.Series.rmod(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/rmod/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>   numeric scalar </li><li> array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.rmod</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/rmod/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.rmod(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      0\n1      1\n2      2\n3      1\n4      1\n      ..\n995    5\n996    4\n997    3\n998    2\n999    1\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/rmul/","title":"<code>pd.Series.rmul</code>","text":"<p><code>pandas.Series.rmul(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/rmul/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>   numeric scalar </li><li> array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.rmul</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/rmul/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.rmul(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      1000\n1      1998\n2      2994\n3      3988\n4      4980\n      ...\n995    4980\n996    3988\n997    2994\n998    1998\n999    1000\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/rolling/","title":"<code>pd.Series.rolling</code>","text":"<p><code>pandas.Series.rolling(window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None, method='single')</code></p>"},{"location":"api_docs/pandas/series/rolling/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>window</code> <ul><li>  Integer </li><li>  String representing a Time Offset  </li><li>  Timedelta </li></ul> <code>min_periods</code> Integer <code>center</code> Boolean"},{"location":"api_docs/pandas/series/rolling/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.rolling(2).mean()\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0      NaN\n  1      0.5\n  2      1.5\n  3      2.5\n  4      3.5\n        ...\n  95    94.5\n  96    95.5\n  97    96.5\n  98    97.5\n  99    98.5\n  Length: 100, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/round/","title":"<code>pd.Series.round</code>","text":"<p><code>pandas.Series.round(decimals=0)</code></p>"},{"location":"api_docs/pandas/series/round/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> Series with numeric data <p>Note</p> <p><code>Series.round</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/round/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...   return S.round(2)\n&gt;&gt;&gt; S = pd.Series(np.linspace(100, 1000))\n&gt;&gt;&gt; f(S)\n0      100.00\n1      118.37\n2      136.73\n3      155.10\n4      173.47\n5      191.84\n6      210.20\n7      228.57\n8      246.94\n9      265.31\n10     283.67\n11     302.04\n12     320.41\n13     338.78\n14     357.14\n15     375.51\n16     393.88\n17     412.24\n18     430.61\n19     448.98\n20     467.35\n21     485.71\n22     504.08\n23     522.45\n24     540.82\n25     559.18\n26     577.55\n27     595.92\n28     614.29\n29     632.65\n30     651.02\n31     669.39\n32     687.76\n33     706.12\n34     724.49\n35     742.86\n36     761.22\n37     779.59\n38     797.96\n39     816.33\n40     834.69\n41     853.06\n42     871.43\n43     889.80\n44     908.16\n45     926.53\n46     944.90\n47     963.27\n48     981.63\n49    1000.00\ndtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/rpow/","title":"<code>pd.Series.rpow</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.rpow(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/rpow/#argument-restrictions","title":"Argument Restrictions:","text":"<ul> <li><code>other</code>: must be a numeric scalar or Series, Index, Array, List, or Tuple with numeric data.</li> <li><code>level</code>: only supports default value <code>None</code>.</li> <li><code>fill_value</code>: (optional, defaults to <code>None</code>) must be <code>Integer</code>, <code>Float</code> or <code>Boolean</code>.</li> <li><code>axis</code>: only supports default value <code>0</code>.</li> </ul> <p>Note</p> <p>Input must be a Series of <code>Integer</code> or <code>Float</code> data.</p>"},{"location":"api_docs/pandas/series/rpow/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.rpow(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0                     1000\n1                   998001\n2                994011992\n3             988053892081\n4          980159361278976\n              ...\n995    3767675092665006833\n996                      0\n997   -5459658280481875879\n998                      0\n999                      1\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/rsub/","title":"<code>pd.Series.rsub</code>","text":"<p><code>pandas.Series.rsub(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/rsub/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>   numeric scalar </li><li> array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.rsub</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/rsub/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.rsub(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      999\n1      997\n2      995\n3      993\n4      991\n      ...\n995   -991\n996   -993\n997   -995\n998   -997\n999   -999\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/rtruediv/","title":"<code>pd.Series.rtruediv</code>","text":"<p><code>pandas.Series.rtruediv(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/rtruediv/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>   numeric scalar </li><li> array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.rtruediv</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/rtruediv/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.rtruediv(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0      1000.000000\n1       499.500000\n2       332.666667\n3       249.250000\n4       199.200000\n          ...\n995       0.005020\n996       0.004012\n997       0.003006\n998       0.002002\n999       0.001000\nLength: 1000, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/sem/","title":"<code>pd.Series.sem</code>","text":"<p><code>pandas.Series.sem(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/series/sem/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>skipna</code> Boolean <code>ddof</code> Integer <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/sem/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.sem()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n0.20219752318917852\n</code></pre>"},{"location":"api_docs/pandas/series/series/","title":"<code>pd.Series</code>","text":"<ul> <li><code>pandas.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)</code></li> </ul>"},{"location":"api_docs/pandas/series/series/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>data</code> <ul><li> Series type</li> <li>List type</li><li>  Array type <li><li>  Constant  Dictionary  </li><li>  None </li> <code>index</code> SeriesType <code>dtype</code> <ul><li>  Numpy or Pandas Type </li> <li>String name for Numpy/Pandas Type  </li></ul> <ul><li> Must be constant at Compile Time </li><li> String/Data Type must be one of the  supported types (see <code>Series.astype()</code>) </li></ul> <code>name</code> String <p>Note</p> <p>If <code>data</code> is a Series and <code>index</code> is provided, implicit alignment is not performed yet.</p>"},{"location":"api_docs/pandas/series/series/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...     return pd.Series(np.arange(1000), dtype=np.float64, name=\"my_series\")\n&gt;&gt;&gt; f()\n0        0.0\n1        1.0\n2        2.0\n3        3.0\n4        4.0\n      ...\n995    995.0\n996    996.0\n997    997.0\n998    998.0\n999    999.0\nName: my_series, Length: 1000, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/series/#attributes","title":"Attributes","text":""},{"location":"api_docs/pandas/series/series_index/","title":"pd.Series.index","text":"<ul> <li><code>pandas.Series.index</code></li> </ul>"},{"location":"api_docs/pandas/series/series_index/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.apply(lambda row: len(row.index), axis=1)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": np.arange(100), \"B\": [\"A\", \"b\"] * 50})\n&gt;&gt;&gt; f(df)\n0     2\n1     2\n2     2\n3     2\n4     2\n    ..\n95    2\n96    2\n97    2\n98    2\n99    2\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/shape/","title":"pd.Series.shape","text":"<ul> <li><code>pandas.Series.shape</code></li> </ul>"},{"location":"api_docs/pandas/series/shape/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.apply(lambda row: row.shape, axis=1)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": np.arange(100), \"B\": [\"A\", \"b\"] * 50})\n&gt;&gt;&gt; f(df)\n0     (2,)\n1     (2,)\n2     (2,)\n3     (2,)\n4     (2,)\n      ...\n95    (2,)\n96    (2,)\n97    (2,)\n98    (2,)\n99    (2,)\nLength: 100, dtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/shift/","title":"<code>pd.Series.shift</code>","text":"<p><code>pandas.Series.shift(periods=1, freq=None, axis=0, fill_value=None)</code></p>"},{"location":"api_docs/pandas/series/shift/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>periods</code> Integer <p>Note</p> <p>This data type for the series must be one of: -   Integer -   Float -   Boolean -   datetime.data -   datetime64 -   timedelta64 -   string</p>"},{"location":"api_docs/pandas/series/shift/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.shift(1)\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0      NaN\n1      0.0\n2      1.0\n3      2.0\n4      3.0\n      ...\n95    94.0\n96    95.0\n97    96.0\n98    97.0\n99    98.0\nLength: 100, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/shift/#datetime-properties","title":"Datetime properties","text":""},{"location":"api_docs/pandas/series/size/","title":"pd.Series.size","text":"<ul> <li><code>pandas.Series.size</code></li> </ul>"},{"location":"api_docs/pandas/series/size/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.apply(lambda row: row.size, axis=1)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": np.arange(100), \"B\": [\"A\", \"b\"] * 50})\n&gt;&gt;&gt; f(df)\n0     2\n1     2\n2     2\n3     2\n4     2\n..\n95    2\n96    2\n97    2\n98    2\n99    2\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/skew/","title":"<code>pd.Series.skew</code>","text":"<p><code>pandas.Series.skew(axis=None, skipna=None, level=None, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/series/skew/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>skipna</code> Boolean <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/skew/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.skew()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n0.032074996591991714\n</code></pre>"},{"location":"api_docs/pandas/series/sort_index/","title":"<code>pd.Series.sort_index</code>","text":"<p><code>pandas.Series.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None)</code></p>"},{"location":"api_docs/pandas/series/sort_index/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>ascending</code> Boolean <code>na_position</code> One of (\"first\", \"last\") Must be constant at  Compile Time"},{"location":"api_docs/pandas/series/sort_index/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.sort_index()\n&gt;&gt;&gt; S = pd.Series(np.arange(100), index=np.arange(99, -1, -1))\n&gt;&gt;&gt; f(S)\n0     99\n1     98\n2     97\n3     96\n4     95\n      ..\n95     4\n96     3\n97     2\n98     1\n99     0\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/sort_values/","title":"<code>pd.Series.sort_values</code>","text":"<p><code>pandas.Series.sort_values(axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)</code></p>"},{"location":"api_docs/pandas/series/sort_values/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>ascending</code> Boolean <code>na_position</code> One of (\"first\", \"last\") Must be constant at  Compile Time"},{"location":"api_docs/pandas/series/sort_values/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.sort_values()\n&gt;&gt;&gt; S = pd.Series(np.arange(99, -1, -1), index=np.arange(100))\n&gt;&gt;&gt; f(S)\n99     0\n98     1\n97     2\n96     3\n95     4\n      ..\n4     95\n3     96\n2     97\n1     98\n0     99\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/std/","title":"<code>pd.Series.std</code>","text":"<p><code>pandas.Series.std(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/series/std/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>skipna</code> Boolean <code>ddof</code> Integer <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/std/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.std()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n2.021975231891785\n</code></pre>"},{"location":"api_docs/pandas/series/str.capitalize/","title":"<code>pd.Series.str.capitalize</code>","text":"<p><code>pandas.Series.str.capitalize()</code></p>"},{"location":"api_docs/pandas/series/str.capitalize/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.capitalize()\n&gt;&gt;&gt; S = pd.Series([\"a\", \"ce\", \"Erw\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0       A\n1      Ce\n2     Erw\n3      A3\n4       @\n5     A n\n6    ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.casefold/","title":"<code>pd.Series.str.casefold</code>","text":"<p><code>pandas.Series.str.casefold()</code></p>"},{"location":"api_docs/pandas/series/str.casefold/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.casefold()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"CE\", \"Erw\", \"A3\", \"@\", \"\u00df\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0       a\n1      ce\n2     erw\n3      a3\n4       @\n5      ss\n6    ^ ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.cat/","title":"<code>pd.Series.str.cat</code>","text":"<p><code>pandas.Series.str.cat(others=None, sep=None, na_rep=None, join='left')</code></p>"},{"location":"api_docs/pandas/series/str.cat/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>others</code> DataFrame <code>sep</code> String"},{"location":"api_docs/pandas/series/str.cat/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, df):\n...     return S.str.cat(df, \",\")\n&gt;&gt;&gt; S = pd.Series([\"s1\", \"s2\", \"s3\", None, \"s5\"])\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": [\"a1\", \"a2\", \"a3\", \"a4\", \"a5\"], \"B\": [\"b1\", \"b2\", None, \"b4\", \"b5\"]})\n&gt;&gt;&gt; f(S, df)\n0    s1,a1,b1\n1    s2,a2,b2\n2         NaN\n3         NaN\n4    s5,a5,b5\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.center/","title":"<code>pd.Series.str.center</code>","text":"<p><code>pandas.Series.str.center(width, fillchar=' ')</code></p>"},{"location":"api_docs/pandas/series/str.center/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>width</code> Integer <code>fillchar</code> String with a single character"},{"location":"api_docs/pandas/series/str.center/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.center(4)\n&gt;&gt;&gt; S = pd.Series([\"a\", \"ce\", \"Erw\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0     a\n1     ce\n2    Erw\n3     a3\n4     @\n5    a n\n6    ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.contains/","title":"<code>pd.Series.str.contains</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.str.contains(pat, case=True, flags=0, na=None, regex=True)</code></p>"},{"location":"api_docs/pandas/series/str.contains/#argument-restrictions","title":"Argument Restrictions:","text":"<ul> <li><code>pat</code>: must be type <code>String</code>.</li> <li><code>case</code>: must be a compile time constant and must be type <code>Boolean</code>.</li> <li><code>flags</code>: must be type <code>Integer</code>.</li> <li><code>na</code>: only supports default value <code>None</code>.</li> <li><code>regex</code>: must be a compile time constant and must be type <code>Boolean</code>.</li> </ul> <p>Note</p> <p>Input must be a Series of <code>String</code> data.</p> <p>Note</p> <p>Argument <code>na</code> has default value <code>None</code> that's different than Pandas default.</p>"},{"location":"api_docs/pandas/series/str.contains/#example-usage","title":"Example Usage:","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.contains(\"a.+\")\n&gt;&gt;&gt; S = pd.Series([\"a\", \"ce\", \"Erw\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    False\n1    False\n2    False\n3     True\n4    False\n5     True\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.count/","title":"<code>pd.Series.str.count</code>","text":"<p><code>pandas.Series.str.count(pat, flags=0)</code></p>"},{"location":"api_docs/pandas/series/str.count/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>pat</code> String <code>flags</code> Integer <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.count(\"w\")\n&gt;&gt;&gt; S = pd.Series([\"a\", \"ce\", \"Erw\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    1\n1    2\n2    3\n3    2\n4    0\n5    2\n6    2\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/str.encode/","title":"<code>pd.Series.str.encode</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.str.encode(encoding, errors='strict')</code></p>"},{"location":"api_docs/pandas/series/str.encode/#argument-restrictions","title":"Argument Restrictions:","text":"argument datatypes other requirements <code>encoding</code> String <code>errors</code> String <p>Note</p> <p>Input must be a Series of <code>String</code> data.</p>"},{"location":"api_docs/pandas/series/str.encode/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.encode(\"ascii\")\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0       b'A'\n1      b'ce'\n2      b'14'\n3       b' '\n4       b'@'\n5     b'a n'\n6    b'^ Ef'\ndtype: large_binary[pyarrow]\n</code></pre>"},{"location":"api_docs/pandas/series/str.endswith/","title":"<code>pd.Series.str.endswith</code>","text":"<p><code>pandas.Series.str.endswith(pat, na=None)</code></p>"},{"location":"api_docs/pandas/series/str.endswith/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>pat</code> String <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.endswith(\"e\")\n&gt;&gt;&gt; S = pd.Series([\"a\", \"ce\", \"Erw\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    False\n1     True\n2    False\n3    False\n4    False\n5    False\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.extract/","title":"<code>pd.Series.str.extract</code>","text":"<p><code>pandas.Series.str.extract(pat, flags=0, expand=True)</code></p>"},{"location":"api_docs/pandas/series/str.extract/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>pat</code> -   String Must be constant at Compile Time <code>flags</code> -   Integer Must be constant at Compile Time <code>expand</code> -   Boolean Must be constant at Compile Time <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.extract(\"(a|e)\")\n&gt;&gt;&gt; S = pd.Series([\"a\", \"ce\", \"Erw\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n     0\n0    a\n1    e\n2  NaN\n3    a\n4  NaN\n5    a\n6  NaN\n</code></pre>"},{"location":"api_docs/pandas/series/str.extractall/","title":"<code>pd.Series.str.extractall</code>","text":"<p><code>pandas.Series.str.extractall(pat, flags=0)</code></p>"},{"location":"api_docs/pandas/series/str.extractall/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>pat</code> String Must be constant at Compile Time <code>flags</code> Integer Must be constant at Compile Time <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.extractall(\"(a|n)\")\n&gt;&gt;&gt; S = pd.Series([\"a\", \"ce\", \"Erw\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n         0\n  match\n0 0      a\n3 0      a\n5 0      a\n  1      n\n</code></pre>"},{"location":"api_docs/pandas/series/str.find/","title":"<code>pd.Series.str.find</code>","text":"<p><code>pandas.Series.str.find(sub, start=0, end=None)</code></p>"},{"location":"api_docs/pandas/series/str.find/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>sub</code> String <code>start</code> Integer <code>end</code> Integer <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.find(\"a3\", start=1)\n&gt;&gt;&gt; S = pd.Series([\"Aa3\", \"cea3\", \"14a3\", \" a3\", \"a3@\", \"a n3\", \"^ Ea3f\"])\n&gt;&gt;&gt; f(S)\n0     1\n1     2\n2     2\n3     1\n4    -1\n5    -1\n6     3\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/str.fullmatch/","title":"<code>pd.Series.str.fullmatch</code>","text":"<p><code>pandas.Series.str.fullmatch(pat, case=True, flags=0, na=np.nan)</code></p>"},{"location":"api_docs/pandas/series/str.fullmatch/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>pat</code> String <code>case</code> Boolean <code>flags</code> Integer <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.fullmatch(\"ab.*\")\n&gt;&gt;&gt; S = pd.Series([\"abcdefg\", \"cab\", \"abc @123\", \"ABC\", \"Abcd\"])\n&gt;&gt;&gt; f(S)\n0     True\n1     False\n2     True\n3     False\n4     False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.get/","title":"<code>pd.Series.str.get</code>","text":"<p><code>pandas.Series.str.get(i)</code></p>"},{"location":"api_docs/pandas/series/str.get/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>i</code> Integer <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.get(1)\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    NaN\n1      e\n2      4\n3    NaN\n4    NaN\n5\n6\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.index/","title":"<code>pd.Series.str.index</code>","text":"<p><code>pandas.Series.str.index(sub, start=0, end=None)</code></p>"},{"location":"api_docs/pandas/series/str.index/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>sub</code> String <code>start</code> Integer <code>end</code> Integer <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.index(\"a\", start=1)\n&gt;&gt;&gt; S = pd.Series([\"Aa3\", \"cea3\", \"14a3\", \" a3\", \"^ Ea3f\", \"aaa\"])\n&gt;&gt;&gt; f(S)\n0     1\n1     2\n2     2\n3     1\n4     3\n5     1\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/str.isalnum/","title":"<code>pd.Series.str.isalnum</code>","text":"<p><code>pandas.Series.str.isalnum()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.isalnum()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0     True\n1     True\n2     True\n3    False\n4    False\n5    False\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.isalpha/","title":"<code>pd.Series.str.isalpha</code>","text":"<p><code>pandas.Series.str.isalpha()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.isalpha()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0     True\n1     True\n2    False\n3    False\n4    False\n5    False\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.isdecimal/","title":"<code>pd.Series.str.isdecimal</code>","text":"<p><code>pandas.Series.str.isdecimal()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.isdecimal()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    False\n1    False\n2     True\n3    False\n4    False\n5    False\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.isdecimal/#categorical-accessor","title":"Categorical accessor","text":""},{"location":"api_docs/pandas/series/str.isdigit/","title":"<code>pd.Series.str.isdigit</code>","text":"<p><code>pandas.Series.str.isdigit()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.isdigit()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    False\n1    False\n2     True\n3    False\n4    False\n5    False\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.islower/","title":"<code>pd.Series.str.islower</code>","text":"<p><code>pandas.Series.str.islower()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.islower()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    False\n1     True\n2    False\n3     True\n4    False\n5     True\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.isnumeric/","title":"<code>pd.Series.str.isnumeric</code>","text":"<p><code>pandas.Series.str.isnumeric()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.isnumeric()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    False\n1    False\n2     True\n3    False\n4    False\n5    False\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.isspace/","title":"<code>pd.Series.str.isspace</code>","text":"<p><code>pandas.Series.str.isspace()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.isspace()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    False\n1    False\n2    False\n3     True\n4    False\n5    False\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.istitle/","title":"<code>pd.Series.str.istitle</code>","text":"<p><code>pandas.Series.str.istitle()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.istitle()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0     True\n1    False\n2    False\n3    False\n4    False\n5    False\n6     True\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.isupper/","title":"<code>pd.Series.str.isupper</code>","text":"<p><code>pandas.Series.str.isupper()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.isupper()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \"a3\", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0     True\n1    False\n2    False\n3    False\n4    False\n5    False\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.join/","title":"<code>pd.Series.str.join</code>","text":"<p><code>pandas.Series.str.join(sep)</code></p>"},{"location":"api_docs/pandas/series/str.join/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>sep</code> String <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.join(\",\")\n&gt;&gt;&gt; S = pd.Series([[\"a\", \"fe\", \"@23\"], [\"a\", \"b\"], [], [\"c\"]])\n&gt;&gt;&gt; f(S)\n0    a,fe,@23\n1         a,b\n2\n3           c\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.len/","title":"<code>pd.Series.str.len</code>","text":"<p><code>pandas.Series.str.len()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.len()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    1\n1    2\n2    2\n3    1\n4    1\n5    3\n6    4\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/str.ljust/","title":"<code>pd.Series.str.ljust</code>","text":"<p><code>pandas.Series.str.ljust(width, fillchar=' ')</code></p>"},{"location":"api_docs/pandas/series/str.ljust/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>width</code> Integer <code>fillchar</code> String with a single character <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.ljust(5, fillchar=\",\")\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    A,,,,\n1    ce,,,\n2    14,,,\n3     ,,,,\n4    @,,,,\n5    a n,,\n6    ^ Ef,\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.lower/","title":"<code>pd.Series.str.lower</code>","text":"<p><code>pandas.Series.str.lower()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.lower()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0       a\n1      ce\n2      14\n3\n4       @\n5     a n\n6    ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.lstrip/","title":"<code>pd.Series.str.lstrip</code>","text":"<p><code>pandas.Series.str.lstrip(to_strip=None)</code></p>"},{"location":"api_docs/pandas/series/str.lstrip/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>to_strip</code> String <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.lstrip(\"c\")\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0       A\n1       e\n2      14\n3\n4       @\n5     a n\n6    ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.pad/","title":"<code>pd.Series.str.pad</code>","text":"<p>Link to Pandas documentation</p> <p><code>pandas.Series.str.pad(width, side='left', fillchar=' ')</code></p>"},{"location":"api_docs/pandas/series/str.pad/#argument-restrictions","title":"Argument Restrictions:","text":"<ul> <li><code>width</code>: must be type <code>Integer</code>.</li> <li><code>side</code>: must be a compile time constant and must be <code>\"left\"</code>, <code>\"right\"</code> or <code>\"both\"</code>.</li> <li><code>fillchar</code>: must be type <code>Character</code>.</li> </ul> <p>Note</p> <p>Input must be a Series of <code>String</code> data.</p>"},{"location":"api_docs/pandas/series/str.pad/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.pad(5)\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0        A\n1       ce\n2       14\n3\n4        @\n5      a n\n6     ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.partition/","title":"<code>pd.Series.str.partition</code>","text":"<p><code>pandas.Series.str.partition(sep=' ', expand=True)</code></p>"},{"location":"api_docs/pandas/series/str.partition/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>sep</code> String <code>expand</code> Boolean <p>Note</p> <p>Bodo currently only supports expand=True.</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.partition()\n&gt;&gt;&gt; S = pd.Series([\"alphabet soup is delicious\", \"hello     world\", \"goodbye\"])\n&gt;&gt;&gt; f(S)\n          0  1                  2\n0  alphabet     soup is delicious\n1     hello                 world\n2   goodbye                      \n</code></pre>"},{"location":"api_docs/pandas/series/str.removeprefix/","title":"<code>pd.Series.str.removeprefix</code>","text":"<p><code>pandas.Series.str.removeprefix(prefix)</code></p>"},{"location":"api_docs/pandas/series/str.removeprefix/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>prefix</code> String <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.removeprefix(\"ab\")\n&gt;&gt;&gt; S = pd.Series([\"a\", \"ab\", \"abc\", \" abcd\", \"a bcd\", \"abcd\", \"xab\"])\n&gt;&gt;&gt; f(S)\n0        a\n1         \n2        c\n3     abcd\n4    a bcd\n5       cd\n6      xab\ndtype: string\n</code></pre>"},{"location":"api_docs/pandas/series/str.removesuffix/","title":"<code>pd.Series.str.removesuffix</code>","text":"<p><code>pandas.Series.str.removesuffix(suffix)</code></p>"},{"location":"api_docs/pandas/series/str.removesuffix/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>suffix</code> String <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.removesuffix(\"b\")\n&gt;&gt;&gt; S = pd.Series([\"a\", \"ab\", \"abc\", \" abcd\", \"a bcd\", \"abcd\", \"xab\"])\n&gt;&gt;&gt; f(S)\n0        a\n1        a\n2      abc\n3     abcd\n4    a bcd\n5     abcd\n6       xa\ndtype: string\n</code></pre>"},{"location":"api_docs/pandas/series/str.repeat/","title":"<code>pd.Series.str.repeat</code>","text":"<p><code>pandas.Series.str.repeat(repeats)</code></p>"},{"location":"api_docs/pandas/series/str.repeat/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>repeats</code> <ul><li>   Integer </li><li>  Array Like containing integers </li></ul> If <code>repeats</code> is array  like, then it must be the same length as the Series. <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.repeat(2)\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0          AA\n1        cece\n2        1414\n3\n4          @@\n5      a na n\n6    ^ Ef^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.replace/","title":"<code>pd.Series.str.replace</code>","text":"<p><code>pandas.Series.str.replace(pat, repl, n=- 1, case=None, flags=0, regex=None)</code></p>"},{"location":"api_docs/pandas/series/str.replace/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>regex</code></li> </ul> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.replace(\"(a|e)\", \"yellow\")\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0           A\n1     cyellow\n2          14\n3\n4           @\n5    yellow n\n6        ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.rfind/","title":"<code>pd.Series.str.rfind</code>","text":"<p><code>pandas.Series.str.rfind(sub, start=0, end=None)</code></p>"},{"location":"api_docs/pandas/series/str.rfind/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>sub</code> String <code>start</code> Integer <code>end</code> Integer <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.rfind(\"a3\", start=1)\n&gt;&gt;&gt; S = pd.Series([\"Aa3\", \"cea3\", \"14a3\", \" a3\", \"a3@\", \"a n3\", \"^ Ea3f\"])\n&gt;&gt;&gt; f(S)\n0     1\n1     2\n2     2\n3     1\n4    -1\n5    -1\n6     3\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/str.rindex/","title":"<code>pd.Series.str.rindex</code>","text":"<p><code>pandas.Series.str.index(sub, start=0, end=None)</code></p>"},{"location":"api_docs/pandas/series/str.rindex/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>sub</code> String <code>start</code> Integer <code>end</code> Integer <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.rindex(\"i\")\n&gt;&gt;&gt; S = pd.Series([\"alphabet soup is delicious\", \"eieio\", \"iguana\"])\n&gt;&gt;&gt; f(S)\n0     22\n1     3\n2     0\ndtype: Int64\n</code></pre>"},{"location":"api_docs/pandas/series/str.rjust/","title":"<code>pd.Series.str.rjist</code>","text":"<p><code>pandas.Series.str.rjust(width, fillchar=' ')</code></p> <p>Supported arguments`:</p> argument datatypes <code>width</code> Integer <code>fillchar</code> String with a single character <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.rjust(10)\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0             A\n1            ce\n2            14\n3\n4             @\n5           a n\n6          ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.rstrip/","title":"<code>pd.Series.str.restrip</code>","text":"<p><code>pandas.Series.str.rstrip(to_strip=None)</code></p>"},{"location":"api_docs/pandas/series/str.rstrip/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>to_strip</code> String <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.rstrip(\"n\")\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0       A\n1      ce\n2      14\n3\n4       @\n5      a\n6    ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.slice/","title":"<code>pd.Series.str.slice</code>","text":"<p><code>pandas.Series.str.slice(start=None, stop=None, step=None)</code></p>"},{"location":"api_docs/pandas/series/str.slice/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>start</code> Integer <code>stop</code> Integer <code>step</code> Integer <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.slice(1, 4)\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    A\n1    c\n2    1\n3\n4    @\n5    a\n6    #\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.slice_replace/","title":"<code>pd.Series.str.slice_replace</code>","text":"<p><code>pandas.Series.str.slice_replace(start=None, stop=None, repl=None)</code></p>"},{"location":"api_docs/pandas/series/str.slice_replace/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>start</code> Integer <code>stop</code> Integer <code>repl</code> String <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.slice_replace(1, 4)\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    A\n1    c\n2    1\n3\n4    @\n5    a\n6    #\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.split/","title":"<code>pd.Series.str.split</code>","text":"<p><code>pandas.Series.str.split(pat=None, n=-1, expand=False)</code></p>"},{"location":"api_docs/pandas/series/str.split/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>pat</code> String <code>n</code> Integer <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.split(\" \")\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0        [A]\n1       [ce]\n2       [14]\n3       [, ]\n4        [@]\n5     [a, n]\n6    [#, Ef]\ndtype: object\n</code></pre> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.split(\" \", n=2)\n&gt;&gt;&gt; S = pd.Series([\"alphabet soup is delicious\", \"hello world\", \"oh what a beautiful morning\"])\n&gt;&gt;&gt; f(S)\n0     [alphabet, soup, is delicious]\n1                     [hello, world]\n2    [oh, what, a beautiful morning]\ndtype: object\n</code></pre> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.split(n=1)\n&gt;&gt;&gt; S = pd.Series([\"alphabet soup is delicious\", \"hello \\n\\tworld\", \"oh what a beautiful morning\"])\n&gt;&gt;&gt; f(S)\n0     [alphabet, soup is delicious]\n1                     [hello, world]\n2    [oh, what a beautiful morning]\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.startswith/","title":"<code>pd.Series.str.startswith</code>","text":"<p><code>pandas.Series.str.startswith(pat, na=None)</code></p>"},{"location":"api_docs/pandas/series/str.startswith/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>pat</code> String <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.startswith(\"A\")\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0     True\n1    False\n2    False\n3    False\n4    False\n5    False\n6    False\ndtype: boolean\n</code></pre>"},{"location":"api_docs/pandas/series/str.strip/","title":"<code>pd.Series.str.strip</code>","text":"<p><code>pandas.Series.str.strip(to_strip=None)</code></p>"},{"location":"api_docs/pandas/series/str.strip/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>to_strip</code> String <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.strip(\"n\")\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0       A\n1      ce\n2      14\n3\n4       @\n5      a\n6    ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.swapcase/","title":"<code>pd.Series.str.swapcase</code>","text":"<p><code>pandas.Series.str.swapcase()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.swapcase()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0       a\n1      CE\n2      14\n3\n4       @\n5     A N\n6    ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.title/","title":"<code>pd.Series.str.title</code>","text":"<p><code>pandas.Series.str.title()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.title()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0       A\n1      Ce\n2      14\n3\n4       @\n5     A N\n6    ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.upper/","title":"<code>pd.Series.str.upper</code>","text":"<p><code>pandas.Series.str.upper()</code></p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.upper()\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0       A\n1      CE\n2      14\n3\n4       @\n5     A N\n6    ^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/str.zfill/","title":"<code>pd.Series.str.zfill</code>","text":"<p><code>pandas.Series.str.zfill(width)</code></p>"},{"location":"api_docs/pandas/series/str.zfill/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>width</code> Integer <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.str.zfill(5)\n&gt;&gt;&gt; S = pd.Series([\"A\", \"ce\", \"14\", \" \", \"@\", \"a n\", \"^ Ef\"])\n&gt;&gt;&gt; f(S)\n0    0000A\n1    000ce\n2    00014\n3    0000\n4    0000@\n5    00a n\n6    0^ Ef\ndtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/sub/","title":"<code>pd.Series.sub</code>","text":"<p><code>pandas.Series.sub(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/sub/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>  numeric scalar </li><li>  array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.sub</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/sub/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.sub(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0     -999\n1     -997\n2     -995\n3     -993\n4     -991\n      ...\n995    991\n996    993\n997    995\n998    997\n999    999\nLength: 1000, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/sum/","title":"<code>pd.Series.sum</code>","text":"<p><code>pandas.Series.sum(axis=None, skipna=None, level=None, numeric_only=None, min_count=0)</code></p>"},{"location":"api_docs/pandas/series/sum/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>skipna</code> Boolean <code>min_count</code> Integer <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/sum/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.sum()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n295\n</code></pre>"},{"location":"api_docs/pandas/series/t/","title":"pd.Series.T","text":"<ul> <li><code>pandas.Series.T</code></li> </ul>"},{"location":"api_docs/pandas/series/t/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.apply(lambda row: row.T.size, axis=1)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": np.arange(100), \"B\": [\"A\", \"b\"] * 50})\n&gt;&gt;&gt; f(df)\n0     2\n1     2\n2     2\n3     2\n4     2\n    ..\n95    2\n96    2\n97    2\n98    2\n99    2\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/tail/","title":"<code>pd.Series.tail</code>","text":"<p><code>pandas.Series.tail(n=5)</code></p>"},{"location":"api_docs/pandas/series/tail/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>n</code> Integer"},{"location":"api_docs/pandas/series/tail/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.tail(10)\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n90    90\n91    91\n92    92\n93    93\n94    94\n95    95\n96    96\n97    97\n98    98\n99    99\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/take/","title":"<code>pd.Series.take</code>","text":"<p><code>pandas.Series.take(indices, axis=0, is_copy=None)</code></p>"},{"location":"api_docs/pandas/series/take/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>indices</code> Array like with integer data To have distributed  data <code>indices</code> must be an array with the same distribution as S. <p>Note</p> <p>Bodo does not accept any additional arguments for Numpy compatibility</p>"},{"location":"api_docs/pandas/series/take/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.take([2, 7, 4, 19])\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n2      2\n7      7\n4      4\n19    19\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/to_csv/","title":"<code>pd.Series.to_csv</code>","text":"<p><code>pandas.Series.to_csv(path_or_buf=None, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict', storage_options=None)</code></p>"},{"location":"api_docs/pandas/series/to_dict/","title":"<code>pd.Series.to_dict</code>","text":"<p><code>pandas.Series.to_dict(into=&lt;class 'dict'&gt;)</code></p>"},{"location":"api_docs/pandas/series/to_dict/#supported-arguments-none","title":"Supported Arguments None","text":"<p>Note</p> <ul> <li>This method is not parallelized since dictionaries are not     parallelized.</li> <li>This method returns a typedDict, which maintains typing information if passing the dictionary between JIT code and regular Python. This can be converted to a regular Python dictionary by using the <code>dict</code> constructor.</li> </ul>"},{"location":"api_docs/pandas/series/to_dict/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.to_dict()\n&gt;&gt;&gt; S = pd.Series(np.arange(10))\n&gt;&gt;&gt; dict(f(S))\n{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n</code></pre>"},{"location":"api_docs/pandas/series/to_frame/","title":"<code>pd.Series.to_frame</code>","text":"<p><code>pandas.Series.to_frame(name=None)</code></p>"},{"location":"api_docs/pandas/series/to_frame/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>name</code> String Must be constant at Compile Time <p>Note</p> <p>If <code>name</code> is not provided Series name must be a known constant</p>"},{"location":"api_docs/pandas/series/to_frame/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.to_frame(\"my_column\")\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S)\n     my_column\n0            0\n1            1\n2            2\n3            3\n4            4\n..         ...\n995        995\n996        996\n997        997\n998        998\n999        999\n</code></pre> <p>[1000 rows x 1 columns]</p>"},{"location":"api_docs/pandas/series/to_numpy/","title":"<code>pd.Series.to_numpy</code>","text":"<p><code>pandas.Series.to_numpy(dtype=None, copy=False, na_value=None)</code></p>"},{"location":"api_docs/pandas/series/to_numpy/#supported-arguments-none","title":"Supported Arguments None","text":""},{"location":"api_docs/pandas/series/to_numpy/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.to_numpy()\n&gt;&gt;&gt; S = pd.Series(np.arange(1000))\n&gt;&gt;&gt; f(S)\narray([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n       637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n       650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n       663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n       676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n       689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n       715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n       728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n       741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753,\n       754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n       767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779,\n       780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792,\n       793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805,\n       806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,\n       819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831,\n       832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844,\n       845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857,\n       858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870,\n       871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883,\n       884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896,\n       897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909,\n       910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922,\n       923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935,\n       936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948,\n       949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961,\n       962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974,\n       975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987,\n       988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999])\n</code></pre>"},{"location":"api_docs/pandas/series/tolist/","title":"<code>pd.Series.tolist</code>","text":"<p><code>pandas.Series.tolist()</code></p> <p>Note</p> <p>Calling <code>tolist</code> on a non-float array with NA values with cause a runtime exception.</p>"},{"location":"api_docs/pandas/series/tolist/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.tolist()\n&gt;&gt;&gt; S = pd.Series(np.arange(50))\n&gt;&gt;&gt; f(S)\n[0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49]\n</code></pre>"},{"location":"api_docs/pandas/series/tolist/#indexing-iteration","title":"Indexing, iteration:","text":"<p>Location based indexing using <code>[]</code>, <code>iat</code>, and <code>iloc</code> is supported. Changing values of existing string Series using these operators is not supported yet.</p>"},{"location":"api_docs/pandas/series/truediv/","title":"<code>pd.Series.truediv</code>","text":"<p><code>pandas.Series.truediv(other, level=None, fill_value=None, axis=0)</code></p>"},{"location":"api_docs/pandas/series/truediv/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>other</code> <ul><li>  numeric scalar </li><li> array with numeric data </li><li>  Series with numeric data </li></ul> <code>fill_value</code> numeric scalar <p>Note</p> <p><code>Series.truediv</code> is only supported on Series of numeric data.</p>"},{"location":"api_docs/pandas/series/truediv/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S, other):\n...   return S.truediv(other)\n&gt;&gt;&gt; S = pd.Series(np.arange(1, 1001))\n&gt;&gt;&gt; other = pd.Series(reversed(np.arange(1, 1001)))\n&gt;&gt;&gt; f(S, other)\n0         0.001000\n1         0.002002\n2         0.003006\n3         0.004012\n4         0.005020\n          ...\n995     199.200000\n996     249.250000\n997     332.666667\n998     499.500000\n999    1000.000000\nLength: 1000, dtype: float64\n</code></pre>"},{"location":"api_docs/pandas/series/unique/","title":"<code>pd.Series.unique</code>","text":"<p><code>pandas.Series.unique()</code></p> <p>Note</p> <p>The output is assumed to be \"small\" relative to input and is replicated. Use <code>Series.drop_duplicates()</code> if the output should remain distributed.</p>"},{"location":"api_docs/pandas/series/unique/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.unique()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n[0 1 2 3 4 5 6]\n</code></pre>"},{"location":"api_docs/pandas/series/value_counts/","title":"<code>pd.Series.value_counts</code>","text":"<p><code>pandas.Series.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)</code></p>"},{"location":"api_docs/pandas/series/value_counts/#supported-arguments","title":"Supported Arguments","text":"argument datatypes other requirements <code>normalize</code> Boolean Must be constant at Compile Time <code>sort</code> Boolean Must be constant at Compile Time <code>ascending</code> Boolean <code>bins</code> <ul><li> Integer </li>  Array-like of integers </ul>"},{"location":"api_docs/pandas/series/value_counts/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.value_counts()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n0    15\n1    15\n2    14\n3    14\n4    14\n5    14\n6    14\ndtype: int64\n</code></pre>"},{"location":"api_docs/pandas/series/value_counts/#reindexing-selection-label-manipulation","title":"Reindexing / Selection / Label manipulation","text":""},{"location":"api_docs/pandas/series/values/","title":"pd.Series.values","text":"<ul> <li><code>pandas.Series.values</code></li> </ul>"},{"location":"api_docs/pandas/series/values/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.apply(lambda row: row.values, axis=1)\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": np.arange(100), \"B\": [\"A\", \"b\"] * 50})\n&gt;&gt;&gt; f(df)\n0      (0, A)\n1      (1, b)\n2      (2, A)\n3      (3, b)\n4      (4, A)\n      ...\n95    (95, b)\n96    (96, A)\n97    (97, b)\n98    (98, A)\n99    (99, b)\nLength: 100, dtype: object\n</code></pre>"},{"location":"api_docs/pandas/series/var/","title":"<code>pd.Series.var</code>","text":"<p><code>pandas.Series.var(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)</code></p>"},{"location":"api_docs/pandas/series/var/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>skipna</code> Boolean <code>ddof</code> Integer <p>Note</p> <ul> <li>Series type must be numeric</li> <li>Bodo does not accept any additional arguments to pass to the function</li> </ul>"},{"location":"api_docs/pandas/series/var/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.var()\n&gt;&gt;&gt; S = pd.Series(np.arange(100)) % 7\n&gt;&gt;&gt; f(S)\n4.088383838383838\n</code></pre>"},{"location":"api_docs/pandas/series/where/","title":"<code>pd.Series.where</code>","text":"<p><code>pandas.Series.where(cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=NoDefault.no_default)</code></p>"},{"location":"api_docs/pandas/series/where/#supported-arguments","title":"Supported Arguments","text":"argument datatypes <code>cond</code> <ul><li>  boolean array </li><li>   1d bool numpy array </li></ul> <code>other</code> <ul><li>   1d numpy array </li><li> scalar   </li></ul> <p>Note</p> <p>Series can contain categorical data if <code>other</code> is a scalar</p>"},{"location":"api_docs/pandas/series/where/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(S):\n...     return S.where((S % 3) != 0, 0)\n&gt;&gt;&gt; S = pd.Series(np.arange(100))\n&gt;&gt;&gt; f(S)\n0      0\n1      1\n2      2\n3      0\n4      4\n      ..\n95    95\n96     0\n97    97\n98    98\n99     0\nLength: 100, dtype: int64\n</code></pre>"},{"location":"api_docs/pandas/timedelta/","title":"Timedelta","text":"<p>Timedelta functionality is documented in <code>pandas.Timedelta</code>.</p> <ul> <li><code>pd.Timedelta</code> </li> <li><code>pd.Timedelta.components</code> </li> <li><code>pd.Timedelta.days</code> </li> <li><code>pd.Timedelta.delta</code> </li> <li><code>pd.Timedelta.microseconds</code> </li> <li><code>pd.Timedelta.nanoseconds</code> </li> <li><code>pd.Timedelta.seconds</code> </li> <li><code>pd.Timedelta.value</code> </li> <li><code>pd.Timedelta.ceil</code> </li> <li><code>pd.Timedelta.floor</code> </li> <li><code>pd.Timedelta.round</code> </li> <li><code>pd.Timedelta.to_numpy</code> </li> <li><code>pd.Timedelta.to_pytimedelta</code> </li> <li><code>pd.Timedelta.to_timedelta64</code> </li> <li><code>pd.Timedelta.total_seconds</code> </li> </ul>"},{"location":"api_docs/pandas/timedelta/ceil/","title":"<code>pd.Timedelta.ceil</code>","text":"<p><code>pandas.Timedelta.ceil(freq)</code></p>"},{"location":"api_docs/pandas/timedelta/ceil/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>freq</code>: String</li> </ul>"},{"location":"api_docs/pandas/timedelta/ceil/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23).ceil(\"D\")\n&gt;&gt;&gt; f()\n11 days 00:00:00\n</code></pre>"},{"location":"api_docs/pandas/timedelta/components/","title":"<code>pd.Timedelta.components</code>","text":"<p><code>pandas.Timedelta.components</code></p>"},{"location":"api_docs/pandas/timedelta/components/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23).components\n&gt;&gt;&gt; f()\nComponents(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23, nanoseconds=0)\n</code></pre>"},{"location":"api_docs/pandas/timedelta/days/","title":"<code>pd.Timedelta.days</code>","text":"<p><code>pandas.Timedelta.days</code></p>"},{"location":"api_docs/pandas/timedelta/days/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23).days\n&gt;&gt;&gt; f()\n10\n</code></pre>"},{"location":"api_docs/pandas/timedelta/delta/","title":"<code>pd.Timedelta.delta</code>","text":"<p><code>pandas.Timedelta.delta</code></p>"},{"location":"api_docs/pandas/timedelta/delta/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(microseconds=23).delta\n&gt;&gt;&gt; f()\n23000\n</code></pre>"},{"location":"api_docs/pandas/timedelta/floor/","title":"<code>pd.Timedelta.floor</code>","text":"<p><code>pandas.Timedelta.floor</code></p>"},{"location":"api_docs/pandas/timedelta/floor/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>freq</code>: String</li> </ul>"},{"location":"api_docs/pandas/timedelta/floor/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23).floor(\"D\")\n&gt;&gt;&gt; f()\n10 days 00:00:00\n</code></pre>"},{"location":"api_docs/pandas/timedelta/microseconds/","title":"<code>pd.Timedelta.microseconds</code>","text":"<p><code>pandas.Timedelta.microseconds</code></p>"},{"location":"api_docs/pandas/timedelta/microseconds/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23).microseconds\n&gt;&gt;&gt; f()\n23\n</code></pre>"},{"location":"api_docs/pandas/timedelta/nanoseconds/","title":"<code>pd.Timedelta.nanoseconds</code>","text":"<p><code>pandas.Timedelta.nanoseconds</code></p>"},{"location":"api_docs/pandas/timedelta/nanoseconds/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23).nanoseconds\n&gt;&gt;&gt; f()\n0\n</code></pre>"},{"location":"api_docs/pandas/timedelta/round/","title":"<code>pd.Timedelta.round</code>","text":"<p><code>pandas.Timedelta.round</code></p>"},{"location":"api_docs/pandas/timedelta/round/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>freq</code>: String</li> </ul>"},{"location":"api_docs/pandas/timedelta/round/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return (pd.Timedelta(days=10, hours=12).round(\"D\"), pd.Timedelta(days=10, hours=13).round(\"D\"))\n&gt;&gt;&gt; f()\n(Timedelta('10 days 00:00:00'), Timedelta('11 days 00:00:00'))\n</code></pre>"},{"location":"api_docs/pandas/timedelta/seconds/","title":"<code>pd.Timedelta.seconds</code>","text":"<p><code>pandas.Timedelta.seconds</code></p>"},{"location":"api_docs/pandas/timedelta/seconds/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(\"10 nanoseconds\").nanoseconds\n&gt;&gt;&gt; f()\n10\n</code></pre>"},{"location":"api_docs/pandas/timedelta/timedelta/","title":"<code>pd.Timedelta</code>","text":"<p><code>pandas.Timedelta(value=&lt;object object\\&gt;, unit=\"ns\", days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0)</code></p>"},{"location":"api_docs/pandas/timedelta/timedelta/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>value</code>: Integer (with constant string unit argument), String, Pandas Timedelta, datetime Timedelta</li> <li><code>unit</code>: Constant String. Only has an effect when passing an integer <code>value</code>, see here for allowed values.</li> <li><code>days</code>: Integer</li> <li><code>seconds</code>: Integer</li> <li><code>microseconds</code>: Integer</li> <li><code>milliseconds</code>: Integer</li> <li><code>minutes</code>: Integer</li> <li><code>hours</code>: Integer</li> <li><code>weeks</code>: Integer</li> </ul>"},{"location":"api_docs/pandas/timedelta/timedelta/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   td1 = pd.Timedelta(\"10 Seconds\")\n...   td2 = pd.Timedelta(10, unit= \"W\")\n...   td3 = pd.Timedelta(days= 10, hours=2, microseconds= 23)\n...   return (td1, td2, td3)\n&gt;&gt;&gt; f()\n(Timedelta('0 days 00:00:10'), Timedelta('70 days 00:00:00'), Timedelta('10 days 02:00:00.000023'))\n</code></pre>"},{"location":"api_docs/pandas/timedelta/to_numpy/","title":"<code>pd.Timedelta.to_numpy</code>","text":"<p><code>pandas.Timedelta.to_numpy()</code></p>"},{"location":"api_docs/pandas/timedelta/to_numpy/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23).to_numpy()\n&gt;&gt;&gt; f()\n871623013023000 nanoseconds\n</code></pre>"},{"location":"api_docs/pandas/timedelta/to_pytimedelta/","title":"<code>pd.Timedelta.to_pytimedelta</code>","text":"<p><code>pandas.Timedelta.to_pytimedelta()</code></p>"},{"location":"api_docs/pandas/timedelta/to_pytimedelta/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23).to_pytimedelta()\n&gt;&gt;&gt; f()\n10 days, 2:07:03.013023\n</code></pre>"},{"location":"api_docs/pandas/timedelta/to_timedelta64/","title":"<code>pd.Timedelta.to_timedelta64</code>","text":"<p><code>pandas.Timedelta.to_timedelta64()</code></p>"},{"location":"api_docs/pandas/timedelta/to_timedelta64/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23).to_timedelta64()\n&gt;&gt;&gt; f()\n871623013023000 nanoseconds\n</code></pre>"},{"location":"api_docs/pandas/timedelta/total_seconds/","title":"<code>pd.Timedelta.total_seconds</code>","text":"<p><code>pandas.Timedelta.total_seconds()</code></p>"},{"location":"api_docs/pandas/timedelta/total_seconds/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(days=10, hours=2, minutes=7, seconds=3, milliseconds=13, microseconds=23).total_seconds()\n&gt;&gt;&gt; f()\n871623.013023\n</code></pre>"},{"location":"api_docs/pandas/timedelta/value/","title":"<code>pd.Timedelta.value</code>","text":"<p><code>pandas.Timedelta.value</code></p>"},{"location":"api_docs/pandas/timedelta/value/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timedelta(\"13 nanoseconds\").value\n&gt;&gt;&gt; f()\n13\n</code></pre>"},{"location":"api_docs/pandas/timestamp/","title":"Timestamp","text":"<p>Timestamp functionality is documented in <code>pandas.Timestamp</code>.</p> <ul> <li><code>pd.Timestamp</code> </li> <li><code>pd.Timestamp.day</code> </li> <li><code>pd.Timestamp.hour</code> </li> <li><code>pd.Timestamp.microsecond</code> </li> <li><code>pd.Timestamp.month</code> </li> <li><code>pd.Timestamp.nanosecond</code> </li> <li><code>pd.Timestamp.second</code> </li> <li><code>pd.Timestamp.year</code> </li> <li><code>pd.Timestamp.dayofyear</code> </li> <li><code>pd.Timestamp.day_of_year</code> </li> <li><code>pd.Timestamp.dayofweek</code> </li> <li><code>pd.Timestamp.day_of_week</code> </li> <li><code>pd.Timestamp.days_in_month</code> </li> <li><code>pd.Timestamp.daysinmonth</code> </li> <li><code>pd.Timestamp.is_leap_year</code> </li> <li><code>pd.Timestamp.is_month_start</code> </li> <li><code>pd.Timestamp.is_month_end</code> </li> <li><code>pd.Timestamp.is_quarter_start</code> </li> <li><code>pd.Timestamp.is_quarter_end</code> </li> <li><code>pd.Timestamp.is_year_start</code> </li> <li><code>pd.Timestamp.is_year_end</code> </li> <li><code>pd.Timestamp.quarter</code> </li> <li><code>pd.Timestamp.week</code> </li> <li><code>pd.Timestamp.weekofyear</code> </li> <li><code>pd.Timestamp.value</code> </li> <li><code>pd.Timestamp.ceil</code> </li> <li><code>pd.Timestamp.date</code> </li> <li><code>pd.Timestamp.day_name</code> </li> <li><code>pd.Timestamp.floor</code> </li> <li><code>pd.Timestamp.isocalendar</code> </li> <li><code>pd.Timestamp.isoformat</code> </li> <li><code>pd.Timestamp.month_name</code> </li> <li><code>pd.Timestamp.normalize</code> </li> <li><code>pd.Timestamp.round</code> </li> <li><code>pd.Timestamp.strftime</code> </li> <li><code>pd.Timestamp.toordinal</code> </li> <li><code>pd.Timestamp.weekday</code> </li> <li><code>pd.Timestamp.now</code> </li> </ul>"},{"location":"api_docs/pandas/timestamp/ceil/","title":"<code>pd.Timestamp.ceil</code>","text":"<p><code>pandasTimestamp.ceil(freq, ambiguous='raise', nonexistent='raise')</code></p>"},{"location":"api_docs/pandas/timestamp/ceil/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>freq</code>: string</li> </ul>"},{"location":"api_docs/pandas/timestamp/ceil/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123).ceil(\"D\")\n...   return (ts1, ts2)\n&gt;&gt;&gt; f()\n(Timestamp('2021-12-09 09:57:44.114123'), Timestamp('2021-12-10 00:00:00'))\n</code></pre>"},{"location":"api_docs/pandas/timestamp/date/","title":"<code>pd.Timestamp.date</code>","text":"<p><code>pandasTimestamp.date()</code></p>"},{"location":"api_docs/pandas/timestamp/date/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123).date()\n...   return (ts1, ts2)\n&gt;&gt;&gt; f()\n(Timestamp('2021-12-09 09:57:44.114123'), datetime.date(2021, 12, 9))\n</code></pre>"},{"location":"api_docs/pandas/timestamp/day/","title":"<code>pd.Timestamp.day</code>","text":"<p><code>pandasTimestamp.day</code></p>"},{"location":"api_docs/pandas/timestamp/day/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.day\n&gt;&gt;&gt; f()\n9\n</code></pre>"},{"location":"api_docs/pandas/timestamp/day_name/","title":"<code>pd.Timestamp.day_name</code>","text":"<p><code>pandasTimestamp.day_name(*args, **kwargs)</code></p>"},{"location":"api_docs/pandas/timestamp/day_name/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/timestamp/day_name/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   day_1 = pd.Timestamp(year=2021, month=12, day=9).day_name()\n...   day_2 = pd.Timestamp(year=2021, month=12, day=10).day_name()\n...   day_3 = pd.Timestamp(year=2021, month=12, day=11).day_name()\n...   return (day_1, day_2, day_3)\n&gt;&gt;&gt; f()\n('Thursday', 'Friday', 'Saturday')\n</code></pre>"},{"location":"api_docs/pandas/timestamp/day_of_week/","title":"<code>pd.Timestamp.day_of_week</code>","text":"<p><code>pandasTimestamp.day_of_week</code></p>"},{"location":"api_docs/pandas/timestamp/day_of_week/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.day_of_week\n&gt;&gt;&gt; f()\n3\n</code></pre>"},{"location":"api_docs/pandas/timestamp/day_of_year/","title":"<code>pd.Timestamp.day_of_year</code>","text":"<p><code>pandasTimestamp.day_of_year</code></p>"},{"location":"api_docs/pandas/timestamp/day_of_year/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.day_of_year\n&gt;&gt;&gt; f()\n343\n</code></pre>"},{"location":"api_docs/pandas/timestamp/dayofweek/","title":"<code>pd.Timestamp.dayofweek</code>","text":"<p><code>pandasTimestamp.dayofweek</code></p>"},{"location":"api_docs/pandas/timestamp/dayofweek/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.day_of_year\n&gt;&gt;&gt; f()\n343\n</code></pre>"},{"location":"api_docs/pandas/timestamp/dayofyear/","title":"<code>pd.Timestamp.dayofyear</code>","text":"<p><code>pandasTimestamp.dayofyear</code></p>"},{"location":"api_docs/pandas/timestamp/dayofyear/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.dayofyear\n&gt;&gt;&gt; f()\n343\n</code></pre>"},{"location":"api_docs/pandas/timestamp/days_in_month/","title":"<code>pd.Timestamp.days_in_month</code>","text":"<p><code>pandasTimestamp.days_in_month</code></p>"},{"location":"api_docs/pandas/timestamp/days_in_month/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.days_in_month\n&gt;&gt;&gt; f()\n31\n</code></pre>"},{"location":"api_docs/pandas/timestamp/daysinmonth/","title":"<code>pd.Timestamp.daysinmonth</code>","text":"<p><code>pandasTimestamp.daysinmonth</code></p>"},{"location":"api_docs/pandas/timestamp/daysinmonth/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.daysinmonth\n&gt;&gt;&gt; f()\n31\n</code></pre>"},{"location":"api_docs/pandas/timestamp/floor/","title":"<code>pd.Timestamp.floor</code>","text":"<p><code>pandasTimestamp.floor(freq, ambiguous='raise', nonexistent='raise')</code></p>"},{"location":"api_docs/pandas/timestamp/floor/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>freq</code>: string</li> </ul>"},{"location":"api_docs/pandas/timestamp/floor/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123).ceil(\"D\")\n...   return (ts1, ts2)\n&gt;&gt;&gt; f()\n(Timestamp('2021-12-09 09:57:44.114123'), Timestamp('2021-12-09 00:00:00'))\n</code></pre>"},{"location":"api_docs/pandas/timestamp/hour/","title":"<code>pd.Timestamp.hour</code>","text":"<p><code>pandasTimestamp.hour</code></p>"},{"location":"api_docs/pandas/timestamp/hour/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.hour\n&gt;&gt;&gt; f()\n9\n</code></pre>"},{"location":"api_docs/pandas/timestamp/is_leap_year/","title":"<code>pd.Timestamp.is_leap_year</code>","text":"<p><code>pandasTimestamp.is_leap_year</code></p>"},{"location":"api_docs/pandas/timestamp/is_leap_year/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2020, month=2,day=2)\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return (ts1.is_leap_year, ts2.is_leap_year)\n&gt;&gt;&gt; f()\n(True, False)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/is_month_end/","title":"<code>pd.Timestamp.is_month_end</code>","text":"<p><code>pandasTimestamp.is_month_end</code></p>"},{"location":"api_docs/pandas/timestamp/is_month_end/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=31)\n...   ts2 = pd.Timestamp(year=2021, month=12, day=30)\n...   return (ts1.is_month_end, ts2.is_month_end)\n&gt;&gt;&gt; f()\n(True, False)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/is_month_start/","title":"<code>pd.Timestamp.is_month_start</code>","text":"<p><code>pandasTimestamp.is_month_start</code></p>"},{"location":"api_docs/pandas/timestamp/is_month_start/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=1)\n...   ts2 = pd.Timestamp(year=2021, month=12, day=2)\n...   return (ts1.is_month_start, ts2.is_month_start)\n&gt;&gt;&gt; f()\n(True, False)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/is_quarter_end/","title":"<code>pd.Timestamp.is_quarter_end</code>","text":"<p><code>pandasTimestamp.is_quarter_end</code></p>"},{"location":"api_docs/pandas/timestamp/is_quarter_end/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=9, day=30)\n...   ts2 = pd.Timestamp(year=2021, month=10, day=1)\n...   return (ts1.is_quarter_start, ts2.is_quarter_start)\n&gt;&gt;&gt; f()\n(True, False)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/is_quarter_start/","title":"<code>pd.Timestamp.is_quarter_start</code>","text":"<p><code>pandasTimestamp.is_quarter_start</code></p>"},{"location":"api_docs/pandas/timestamp/is_quarter_start/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=9, day=30)\n...   ts2 = pd.Timestamp(year=2021, month=10, day=1)\n...   return (ts1.is_quarter_start, ts2.is_quarter_start)\n&gt;&gt;&gt; f()\n(False, True)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/is_year_end/","title":"<code>pd.Timestamp.is_year_end</code>","text":"<p><code>pandasTimestamp.is_year_end</code></p>"},{"location":"api_docs/pandas/timestamp/is_year_end/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=31)\n...   ts2 = pd.Timestamp(year=2021, month=1, day=1)\n...   return (ts1.is_year_end, ts2.is_year_end)\n&gt;&gt;&gt; f()\n(True, False)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/is_year_start/","title":"<code>pd.Timestamp.is_year_start</code>","text":"<p><code>pandasTimestamp.is_year_start</code></p>"},{"location":"api_docs/pandas/timestamp/is_year_start/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=31)\n...   ts2 = pd.Timestamp(year=2021, month=1, day=1)\n...   return (ts1.is_year_start, ts2.is_year_start)\n&gt;&gt;&gt; f()\n(False, True)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/isocalendar/","title":"<code>pd.Timestamp.isocalendar</code>","text":"<p><code>pandasTimestamp.isocalendar()</code></p>"},{"location":"api_docs/pandas/timestamp/isocalendar/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123).isocalendar()\n...   return (ts1, ts2)\n&gt;&gt;&gt; f()\n(2021, 49, 4)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/isoformat/","title":"<code>pd.Timestamp.isoformat</code>","text":"<p><code>pandasTimestamp.isoformat()</code></p>"},{"location":"api_docs/pandas/timestamp/isoformat/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123).isocalendar()\n...   return (ts1, ts2)\n&gt;&gt;&gt; f()\n'2021-12-09T09:57:44'\n</code></pre>"},{"location":"api_docs/pandas/timestamp/microsecond/","title":"<code>pd.Timestamp.microsecond</code>","text":"<p><code>pandasTimestamp.microsecond</code></p>"},{"location":"api_docs/pandas/timestamp/microsecond/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.microsecond\n&gt;&gt;&gt; f()\n114123\n</code></pre>"},{"location":"api_docs/pandas/timestamp/month/","title":"<code>pd.Timestamp.month</code>","text":"<p><code>pandasTimestamp.month</code></p>"},{"location":"api_docs/pandas/timestamp/month/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.month\n&gt;&gt;&gt; f()\nmonth\n</code></pre>"},{"location":"api_docs/pandas/timestamp/month_name/","title":"<code>pd.Timestamp.month_name</code>","text":"<p><code>pandasTimestamp.month_name(locale=None)</code></p>"},{"location":"api_docs/pandas/timestamp/month_name/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/timestamp/month_name/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timestamp(year=2021, month=12, day=9).month_name()\n&gt;&gt;&gt; f()\n'December'\n</code></pre>"},{"location":"api_docs/pandas/timestamp/nanosecond/","title":"<code>pd.Timestamp.nanosecond</code>","text":"<p><code>pandasTimestamp.nanosecond</code></p>"},{"location":"api_docs/pandas/timestamp/nanosecond/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(12, unit=\"ns\")\n...   return ts2.nanosecond\n&gt;&gt;&gt; f()\n12\n</code></pre>"},{"location":"api_docs/pandas/timestamp/normalize/","title":"<code>pd.Timestamp.normalize</code>","text":"<p><code>pandasTimestamp.normalize()</code></p>"},{"location":"api_docs/pandas/timestamp/normalize/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123).normalize()\n...   return (ts1, ts2)\n&gt;&gt;&gt; f()\nTimestamp('2021-12-09 00:00:00')\n</code></pre>"},{"location":"api_docs/pandas/timestamp/now/","title":"<code>pd.Timestamp.now</code>","text":"<p><code>pandasTimestamp.now(tz=None)</code></p>"},{"location":"api_docs/pandas/timestamp/now/#supported-arguments","title":"Supported Arguments:","text":"<ul> <li><code>tz</code>: constant string, integer, or None</li> </ul>"},{"location":"api_docs/pandas/timestamp/now/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timestamp.now()\n&gt;&gt;&gt; f()\nTimestamp('2021-12-10 10:54:06.457168')\n</code></pre>"},{"location":"api_docs/pandas/timestamp/quarter/","title":"<code>pd.Timestamp.quarter</code>","text":"<p><code>pandasTimestamp.quarter</code></p>"},{"location":"api_docs/pandas/timestamp/quarter/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=1)\n...   ts2 = pd.Timestamp(year=2021, month=9, day=1)\n...   return (ts1.quarter, ts2.quarter)\n&gt;&gt;&gt; f()\n(4, 3)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/round/","title":"<code>pd.Timestamp.round</code>","text":"<p><code>pandasTimestamp.round(freq, ambiguous='raise', nonexistent='raise')</code></p>"},{"location":"api_docs/pandas/timestamp/round/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>freq</code>: string</li> </ul>"},{"location":"api_docs/pandas/timestamp/round/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=9, hour = 12).round()\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 13).round()\n...   return (ts1, ts2)\n&gt;&gt;&gt; f()\n(Timestamp('2021-12-09 00:00:00'),Timestamp('2021-12-10 00:00:00'))\n</code></pre>"},{"location":"api_docs/pandas/timestamp/second/","title":"<code>pd.Timestamp.second</code>","text":"<p><code>pandasTimestamp.second</code></p>"},{"location":"api_docs/pandas/timestamp/second/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.second\n&gt;&gt;&gt; f()\n44\n</code></pre>"},{"location":"api_docs/pandas/timestamp/strftime/","title":"<code>pd.Timestamp.strftime</code>","text":"<p><code>pandasTimestamp.strftime(format)</code></p>"},{"location":"api_docs/pandas/timestamp/strftime/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>format</code>: string</li> </ul>"},{"location":"api_docs/pandas/timestamp/strftime/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timestamp(year=2021, month=12, day=9, hour = 12).strftime('%Y-%m-%d %X')\n&gt;&gt;&gt; f()\n'2021-12-09 12:00:00'\n</code></pre>"},{"location":"api_docs/pandas/timestamp/timestamp/","title":"<code>pd.Timestamp</code>","text":"<ul> <li>pandas.Timestamp(ts_input=&lt;object object&gt;, freq=None, tz=None, unit=None, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None, tzinfo=None, *, fold=None)</li> </ul>"},{"location":"api_docs/pandas/timestamp/timestamp/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>ts_input</code>: string, integer, timestamp, datetimedate</li> <li><code>unit</code>: constant string</li> <li><code>year</code>: integer</li> <li><code>month</code>: integer</li> <li><code>day</code>: integer</li> <li><code>hour</code>: integer</li> <li><code>minute</code>: integer</li> <li><code>second</code>: integer</li> <li><code>microsecond</code>: integer</li> <li><code>nanosecond</code>: integer</li> </ul>"},{"location":"api_docs/pandas/timestamp/timestamp/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return I.copy(name=\"new_name\")\n...   ts1 = pd.Timestamp('2021-12-09 09:57:44.114123')\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   ts3 = pd.Timestamp(100, unit=\"days\")\n...   ts4 = pd.Timestamp(datetime.date(2021, 12, 9), hour = 9, minute=57, second=44, microsecond=114123)\n...   return (ts1, ts2, ts3, ts4)\n&gt;&gt;&gt; f()\n(Timestamp('2021-12-09 09:57:44.114123'), Timestamp('2021-12-09 09:57:44.114123'), Timestamp('1970-04-11 00:00:00'), Timestamp('2021-12-09 09:57:44.114123'))\n</code></pre>"},{"location":"api_docs/pandas/timestamp/toordinal/","title":"<code>pd.Timestamp.toordinal</code>","text":"<p><code>pandasTimestamp.toordinal()</code></p>"},{"location":"api_docs/pandas/timestamp/toordinal/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timestamp(year=2021, month=12, day=9).toordinal()\n&gt;&gt;&gt; f()\n738133\n</code></pre>"},{"location":"api_docs/pandas/timestamp/value/","title":"<code>pd.Timestamp.value</code>","text":"<p><code>pandasTimestamp.value</code></p>"},{"location":"api_docs/pandas/timestamp/value/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   return pd.Timestamp(12345, unit=\"ns\").value\n&gt;&gt;&gt; f()\n12345\n</code></pre>"},{"location":"api_docs/pandas/timestamp/week/","title":"<code>pd.Timestamp.week</code>","text":"<p><code>pandasTimestamp.week</code></p>"},{"location":"api_docs/pandas/timestamp/week/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=9, day=1)\n...   ts2 = pd.Timestamp(year=2021, month=9, day=20)\n...   return (ts1.week, ts2.week)\n&gt;&gt;&gt; f()\n(35, 38)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/weekday/","title":"<code>pd.Timestamp.weekday</code>","text":"<p><code>pandasTimestamp.weekday()</code></p>"},{"location":"api_docs/pandas/timestamp/weekday/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=12, day=9)\n...   ts2 = pd.Timestamp(year=2021, month=12, day=10)\n...   return (ts1.weekday(), ts2.weekday())\n&gt;&gt;&gt; f()\n(3, 4)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/weekofyear/","title":"<code>pd.Timestamp.weekofyear</code>","text":"<p><code>pandasTimestamp.weekofyear</code></p>"},{"location":"api_docs/pandas/timestamp/weekofyear/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts1 = pd.Timestamp(year=2021, month=9, day=1)\n...   ts2 = pd.Timestamp(year=2021, month=9, day=20)\n...   return (ts1.weekofyear, ts2.weekofyear)\n&gt;&gt;&gt; f()\n(35, 38)\n</code></pre>"},{"location":"api_docs/pandas/timestamp/year/","title":"<code>pd.Timestamp.year</code>","text":"<p><code>pandasTimestamp.year</code></p>"},{"location":"api_docs/pandas/timestamp/year/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f():\n...   ts2 = pd.Timestamp(year=2021, month=12, day=9, hour = 9, minute=57, second=44, microsecond=114123)\n...   return ts2.year\n&gt;&gt;&gt; f()\n2021\n</code></pre>"},{"location":"api_docs/pandas/window/","title":"Window","text":"<p>Bodo supports the following window functions:</p> <ul> <li><code>pd.core.window.rolling.Rolling.count</code> </li> <li><code>pd.core.window.rolling.Rolling.sum</code> </li> <li><code>pd.core.window.rolling.Rolling.mean</code> </li> <li><code>pd.core.window.rolling.Rolling.median</code> </li> <li><code>pd.core.window.rolling.Rolling.var</code> </li> <li><code>pd.core.window.rolling.Rolling.std</code> </li> <li><code>pd.core.window.rolling.Rolling.min</code> </li> <li><code>pd.core.window.rolling.Rolling.max</code> </li> <li><code>pd.core.window.rolling.Rolling.corr</code> </li> <li><code>pd.core.window.rolling.Rolling.cov</code> </li> <li><code>pd.core.window.rolling.Rolling.apply</code> </li> </ul>"},{"location":"api_docs/pandas/window/apply/","title":"<code>pd.core.window.rolling.Rolling.apply</code>","text":"<p><code>pandas.core.window.rolling.apply(func, raw=False, engine=None, engine_kwargs=None, args=None, kwargs=None)</code></p>"},{"location":"api_docs/pandas/window/apply/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>func</code>: JIT function or callable defined within a JIT function<ul> <li>Must be constant at Compile Time</li> </ul> </li> <li><code>raw</code>: boolean<ul> <li>Must be constant at Compile Time</li> </ul> </li> </ul>"},{"location":"api_docs/pandas/window/apply/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df = pd.DataFrame({\"A\": [1,2,3,4,-5,-6,-7]})\n...   return df.rolling(3).apply(lambda x: True if x.sum() &gt; 0 else False)\nA\n0  NaN\n1  NaN\n2  1.0\n3  1.0\n4  1.0\n5  0.0\n6  0.0\n</code></pre>"},{"location":"api_docs/pandas/window/corr/","title":"<code>pd.core.window.rolling.Rolling.corr</code>","text":"<p><code>pandas.core.window.rolling.Rolling.corr(other=None, pairwise=None, ddof=1)</code></p>"},{"location":"api_docs/pandas/window/corr/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>other</code>: DataFrame or Series (cannot contain nullable Integer Types)</li> <li>Required</li> <li>If called with a DataFrame, <code>other</code> must be a DataFrame. If called with a Series, <code>other</code> must be a Series.</li> </ul>"},{"location":"api_docs/pandas/window/corr/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df1 = pd.DataFrame({\"A\": [1,2,3,4,5,6,7]})\n...   df2 = pd.DataFrame({\"A\": [1,2,3,4,-5,-6,-7]})\n...   return df1.rolling(3).corr(df2)\n        A\n0       NaN\n1       NaN\n2  1.000000\n3  1.000000\n4 -0.810885\n5 -0.907841\n6 -1.000000\n</code></pre>"},{"location":"api_docs/pandas/window/count/","title":"<code>pd.core.window.rolling.Rolling.count</code>","text":"<p><code>pandas.core.window.rolling.Rolling.count()</code></p>"},{"location":"api_docs/pandas/window/count/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df = pd.DataFrame({\"A\": [1,2,3,4,5], \"B\": [6,7,None,9,10]})\n...   return df.rolling(3).count()\nA    B\n0  1.0  1.0\n1  2.0  2.0\n2  3.0  3.0\n3  3.0  2.0\n4  3.0  2.0\n5  3.0  2.0\n6  3.0  3.0\n</code></pre>"},{"location":"api_docs/pandas/window/cov/","title":"<code>pd.core.window.rolling.Rolling.cov</code>","text":"<p><code>pandas.core.window.rolling.Rolling.cov(other=None, pairwise=None, ddof=1)</code></p>"},{"location":"api_docs/pandas/window/cov/#supported-arguments","title":"Supported Arguments","text":"<ul> <li><code>other</code>: DataFrame or Series (cannot contain nullable Integer Types)</li> <li>Required</li> <li>If called with a DataFrame, <code>other</code> must be a DataFrame. If called with a Series, <code>other</code> must be a Series.</li> </ul>"},{"location":"api_docs/pandas/window/cov/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df1 = pd.DataFrame({\"A\": [1,2,3,4,5,6,7]})\n...   df2 = pd.DataFrame({\"A\": [1,2,3,4,-5,-6,-7]})\n...   return df1.rolling(3).cov(df2)\nA\n0  NaN\n1  NaN\n2  1.0\n3  1.0\n4 -4.0\n5 -5.0\n6 -1.0\n</code></pre>"},{"location":"api_docs/pandas/window/max/","title":"<code>pd.core.window.rolling.Rolling.max</code>","text":"<p><code>pandas.core.window.rolling.Rolling.max(engine=None, engine_kwargs=None)</code></p>"},{"location":"api_docs/pandas/window/max/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/window/max/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df = pd.DataFrame({\"A\": [1,2,3,4,5,6,7], \"B\": [8,9,10,None,11,12,13]})\n...   return df.rolling(3).max()\n  A     B\n0  NaN   NaN\n1  NaN   NaN\n2  3.0  10.0\n3  4.0   NaN\n4  5.0   NaN\n5  6.0   NaN\n6  7.0  13.0\n</code></pre>"},{"location":"api_docs/pandas/window/mean/","title":"<code>pd.core.window.rolling.Rolling.mean</code>","text":"<p><code>pandas.core.window.rolling.Rolling.mean(engine=None, engine_kwargs=None)</code></p>"},{"location":"api_docs/pandas/window/mean/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/window/mean/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df = pd.DataFrame({\"A\": [1,2,3,4,5,6,7], \"B\": [8,9,10,None,11,12,13]})\n...   return df.rolling(3).mean()\n  A     B\n0  NaN   NaN\n1  NaN   NaN\n2  2.0   9.0\n3  3.0   NaN\n4  4.0   NaN\n5  5.0   NaN\n6  6.0  12.0\n</code></pre>"},{"location":"api_docs/pandas/window/median/","title":"<code>pd.core.window.rolling.Rolling.median</code>","text":"<p><code>pandas.core.window.rolling.Rolling.median(engine=None, engine_kwargs=None)</code></p>"},{"location":"api_docs/pandas/window/median/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/window/median/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df = pd.DataFrame({\"A\": [1,2,3,4,5,6,7], \"B\": [8,9,10,None,11,12,13]})\n...   return df.rolling(3).median()\n  A     B\n0  NaN   NaN\n1  NaN   NaN\n2  2.0   9.0\n3  3.0   NaN\n4  4.0   NaN\n5  5.0   NaN\n6  6.0  12.0\n</code></pre>"},{"location":"api_docs/pandas/window/min/","title":"<code>pd.core.window.rolling.Rolling.min</code>","text":"<p><code>pandas.core.window.rolling.Rolling.min(engine=None, engine_kwargs=None)</code></p>"},{"location":"api_docs/pandas/window/min/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/window/min/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df = pd.DataFrame({\"A\": [1,2,3,4,5,6,7], \"B\": [8,9,10,None,11,12,13]})\n...   return df.rolling(3).min()\n  A     B\n0  NaN   NaN\n1  NaN   NaN\n2  1.0   8.0\n3  2.0   NaN\n4  3.0   NaN\n5  4.0   NaN\n6  5.0  11.0\n</code></pre>"},{"location":"api_docs/pandas/window/std/","title":"<code>pd.core.window.rolling.Rolling.std</code>","text":"<p><code>pandas.core.window.rolling.Rolling.std(ddof=1)</code></p>"},{"location":"api_docs/pandas/window/std/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/window/std/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df = pd.DataFrame({\"A\": [1,2,3,4,5,6,7], \"B\": [8,9,10,None,11,12,13]})\n...   return df.rolling(3).std()\n  A    B\n0  NaN  NaN\n1  NaN  NaN\n2  1.0  1.0\n3  1.0  NaN\n4  1.0  NaN\n5  1.0  NaN\n6  1.0  1.0\n</code></pre>"},{"location":"api_docs/pandas/window/sum/","title":"<code>pd.core.window.rolling.Rolling.sum</code>","text":"<p><code>pandas.core.window.rolling.Rolling.sum(engine=None, engine_kwargs=None)</code></p>"},{"location":"api_docs/pandas/window/sum/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/window/sum/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df = pd.DataFrame({\"A\": [1,2,3,4,5,6,7], \"B\": [8,9,10,None,11,12,13]})\n...   return df.rolling(3).sum()\n    A     B\n0   NaN   NaN\n1   NaN   NaN\n2   6.0  27.0\n3   9.0   NaN\n4  12.0   NaN\n5  15.0   NaN\n6  18.0  36.0\n</code></pre>"},{"location":"api_docs/pandas/window/var/","title":"<code>pd.core.window.rolling.Rolling.var</code>","text":"<p><code>pandas.core.window.rolling.Rolling.var(ddof=1)</code></p>"},{"location":"api_docs/pandas/window/var/#supported-arguments-none","title":"Supported Arguments: None","text":""},{"location":"api_docs/pandas/window/var/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(I):\n...   df = pd.DataFrame({\"A\": [1,2,3,4,5,6,7], \"B\": [8,9,10,None,11,12,13]})\n...   return df.rolling(3).var()\n  A    B\n0  NaN  NaN\n1  NaN  NaN\n2  1.0  1.0\n3  1.0  NaN\n4  1.0  NaN\n5  1.0  NaN\n6  1.0  1.0\n</code></pre>"},{"location":"api_docs/python/","title":"Bodo JIT Compiler Python Support API Reference","text":"<ul> <li>Bodo Parallel API Reference</li> <li>Pandas Reference</li> <li>NumPy Reference</li> <li>Python UDFs</li> <li>Miscellaneous Python APIs</li> <li>Machine Learning Libraries</li> </ul>"},{"location":"api_docs/sql/","title":"BodoSQL","text":"<p>BodoSQL provides high performance and scalable SQL query execution using Bodo's HPC capabilities and optimizations. It also provides native Python/SQL integration as well as SQL to Pandas conversion for the first time.</p> <p>The following topics provide detailed information on how to use BodoSQL. </p>"},{"location":"api_docs/sql/#aliasing","title":"Aliasing","text":"<p>How aliasing works in BodoSQL with internal naming conventions.</p>"},{"location":"api_docs/sql/#user-defined-functions-and-table-functions","title":"User Defined Functions and Table Functions","text":"<p>How to define and use UDFs and User Defined Table Functions (UDTFs) in BodoSQL.</p>"},{"location":"api_docs/sql/#caching-and-parameterized-queries","title":"Caching and Parameterized Queries","text":"<p>How to cache queries and use named parameters in BodoSQL.</p>"},{"location":"api_docs/sql/#io-handling","title":"I/O handling","text":"<p>How to handle I/O operations in BodoSQL.</p>"},{"location":"api_docs/sql/#bodosqlcontext-api","title":"BodoSQLContext API","text":"<p>How to use the BodoSQLContext API to interact with BodoSQL.</p>"},{"location":"api_docs/sql/#tablepath-api","title":"TablePath API","text":"<p>How to use the TablePath API to load data into BodoSQL.</p>"},{"location":"api_docs/sql/#database-catalogs","title":"Database Catalogs","text":"<p>How to use database catalogs in BodoSQL.</p>"},{"location":"api_docs/sql/#identifier-case-sensitivity","title":"Identifier Case Sensitivity","text":"<p>How identifier case sensitivity works in BodoSQL.</p>"},{"location":"api_docs/sql/#performance-considerations","title":"Performance Considerations","text":"<p>Factors that affect performance when using BodoSQL.</p>"},{"location":"api_docs/sql/#query-support","title":"Query Support","text":"<p>A full API reference guide for BodoSQL SQL syntax and functions. Click on each specific topic to see a full list of supported API.</p> <ul> <li>DDL Statements</li> <li>Query Syntax</li> <li>Aggregations and Window Functions</li> <li>Array Functions</li> <li>Casting Functions</li> <li>Context Functions</li> <li>Control Flow Functions</li> <li>Data Generation Functions</li> <li>Numeric Functions</li> <li>Object Functions</li> <li>Operators</li> <li>Regular Expressions</li> <li>String Functions</li> <li>Table Functions</li> <li>Timestamp Functions</li> <li>Type Predicates</li> </ul>"},{"location":"api_docs/sql/aliasing/","title":"Aliasing","text":"<p>In all but the most trivial cases, BodoSQL generates internal names to avoid conflicts in the intermediate Dataframes. By default, BodoSQL does not rename the columns for the final output of a query using a consistent approach. For example the query:</p> <p><pre><code>bc.sql(\"SELECT SUM(A) FROM table1 WHERE B &gt; 4\")\n</code></pre> Results in an output column named <code>$EXPR0</code>. To reliably reference this column later in your code, we highly recommend using aliases for all columns that are the final outputs of a query, such as:</p> <pre><code>bc.sql(\"SELECT SUM(A) as sum_col FROM table1 WHERE B &gt; 4\")\n</code></pre> <p>Note</p> <p>BodoSQL supports using aliases generated in <code>SELECT</code> inside <code>GROUP BY</code> and <code>HAVING</code> in the same query, but you cannot do so with <code>WHERE</code></p>"},{"location":"api_docs/sql/bodosqlcontext/","title":"BodoSQLContext API","text":"<p>The <code>BodoSQLContext</code> API is the primary interface for executing SQL queries. It performs two roles:</p> <ol> <li>Registering data and connection information to load tables of interest.</li> <li>Forwarding SQL queries to the BodoSQL engine for compilation and execution. This is done via the      <code>bc.sql(query)</code> method, where <code>bc</code> is a <code>BodoSQLContext</code> object.</li> </ol> <p>A <code>BodoSQLContext</code> can be defined in regular Python and passed as an argument to JIT functions or can be defined directly inside JIT functions. We recommend defining and modifying a <code>BodoSQLContext</code> in regular Python whenever possible.</p> <p>For example:</p> <pre><code>bc = bodosql.BodoSQLContext(\n    {\n        \"T1\": bodosql.TablePath(\"my_file_path.pq\", \"parquet\"),\n    },\n    catalog=bodosql.SnowflakeCatalog(\n        username,\n        password,\n        account_name,\n        warehouse_name,\n        database name,\n    )\n)\n\n@bodo.jit\ndef f(bc):\n    return bc.sql(\"select t1.A, t2.B from t1, catalogSchema.t2 where t1.C &gt; 5 and t1.D = catalogSchema.t2.D\")\n</code></pre>"},{"location":"api_docs/sql/bodosqlcontext/#api-reference","title":"API Reference","text":"<ul> <li> <p><code>bodosql.BodoSQLContext(tables: Optional[Dict[str, Union[pandas.DataFrame|TablePath]]] = None, catalog: Optional[DatabaseCatalog] = None)</code> </p> <p>Defines a <code>BodoSQLContext</code> with the given local tables and catalog.</p> <p>Arguments</p> <ul> <li> <p><code>tables</code>: A dictionary that maps a name used in a SQL query to a <code>DataFrame</code> or <code>TablePath</code> object.</p> </li> <li> <p><code>catalog</code>: A <code>DatabaseCatalog</code> used to load tables from a remote database (e.g. Snowflake).</p> </li> </ul> </li> <li> <p><code>bodosql.BodoSQLContext.sql(self, query: str, params_dict: Optional[Dict[str, Any] = None, distributed: list|set|bool = set(), replicated: list|set|bool = set(), **jit_options)</code> </p> <p>Executes a SQL query using the tables registered in this <code>BodoSQLContext</code>.</p> <p>Arguments</p> <ul> <li><code>query</code>: The SQL query to execute. This function generates code that is compiled so the <code>query</code> argument is required to be a compile time constant.</li> </ul> </li> <li> <p><code>params_dict</code>: A dictionary that maps a SQL usable name to Python variables. For more information please     refer to the BodoSQL named parameters section.</p> </li> <li> <p><code>distributed</code>, <code>replicated</code>, and other JIT options are passed to Bodo JIT. See Bodo distributed flags documentation for more details.      Example code:</p> <pre><code>df = pd.DataFrame({\"A\": np.arange(10), \"B\": np.ones(10)})\nbc = bodosql.BodoSQLContext({\"T1\": df})\nout_df = bc.sql(\"select sum(B) from T1 group by A\", distributed=[\"T1\"])\n</code></pre> <p>Returns</p> <p>A <code>DataFrame</code> that results from executing the query.</p> </li> <li> <p><code>bodosql.BodoSQLContext.add_or_replace_view(self, name: str, table: Union[pandas.DataFrame, TablePath])</code> </p> <p>Create a new <code>BodoSQLContext</code> from an existing <code>BodoSQLContext</code> by adding or replacing a table.</p> <p>Arguments</p> <ul> <li> <p><code>name</code>: The name of the table to add. If the name already exists references to that table are removed from the new context.</p> </li> <li> <p><code>table</code>: The table object to add. <code>table</code> must be a <code>DataFrame</code> or <code>TablePath</code> object.</p> </li> </ul> <p>Returns</p> <p>A new <code>BodoSQLContext</code> that retains the tables and catalogs from the old <code>BodoSQLContext</code> and inserts the new table specified.</p> <p>Note</p> <p>This DOES NOT update the given context. Users should always use the <code>BodoSQLContext</code> object returned from the function call. e.g. <code>bc = bc.add_or_replace_view(\"t1\", table)</code></p> </li> <li> <p><code>bodosql.BodoSQLContext.remove_view(self, name: str)</code> </p> <p>Creates a new <code>BodoSQLContext</code> from an existing context by removing the table with the given name. If the name does not exist, a <code>BodoError</code> is thrown.</p> <p>Arguments</p> <ul> <li><code>name</code>: The name of the table to remove.</li> </ul> <p>Returns</p> <p>A new <code>BodoSQLContext</code> that retains the tables and catalogs from the old <code>BodoSQLContext</code> minus the table specified.</p> <p>Note</p> <p>This DOES NOT update the given context. Users should always use the <code>BodoSQLContext</code> object returned from the function call. e.g. <code>bc = bc.remove_view(\"t1\")</code></p> </li> <li> <p><code>bodosql.BodoSQLContext.add_or_replace_catalog(self, catalog: DatabaseCatalog)</code> </p> <p>Create a new <code>BodoSQLContext</code> from an existing context by replacing the <code>BodoSQLContext</code> object's <code>DatabaseCatalog</code> with a new catalog.</p> <p>Arguments</p> <ul> <li><code>catalog</code>: The catalog to insert.</li> </ul> <p>Returns</p> <p>A new <code>BodoSQLContext</code> that retains tables from the old <code>BodoSQLContext</code> but replaces the old catalog with the new catalog specified.</p> <p>Note</p> <p>This DOES NOT update the given context. Users should always use the <code>BodoSQLContext</code> object returned from the function call. e.g. <code>bc = bc.add_or_replace_catalog(catalog)</code></p> </li> <li> <p><code>bodosql.BodoSQLContext.remove_catalog(self)</code> </p> <p>Create a new <code>BodoSQLContext</code> from an existing context by removing its <code>DatabaseCatalog</code>.</p> <p>Returns</p> <p>A new <code>BodoSQLContext</code> that retains tables from the old <code>BodoSQLContext</code> but removes the old catalog.</p> <p>Note</p> <p>This DOES NOT update the given context. Users should always use the <code>BodoSQLContext</code> object returned from the function call. e.g. <code>bc = bc.remove_catalog()</code></p> </li> </ul>"},{"location":"api_docs/sql/bodosqlerrors/","title":"Common Bodo SQL Errors","text":"<p>BodoSQL can raise a number of different errors when parsing SQL queries. This page contains a list of commonly encountered parsing errors and their causes.</p> <ul> <li>A binary operation was used on two types for which it is not supported.</li> </ul> <pre><code>\"Cannot apply 'OP' to arguments of type '&lt;SQL_TYPE_ENUM&gt; OP &lt;SQL_TYPE_ENUM&gt;'\"\n</code></pre> <p>This error can be resolved by casting either side of the expression to a common type.</p> <p></p> <ul> <li>The format string passed to STR_TO_DATE is not a valid SQL format string.</li> </ul> <pre><code>\"STR_TO_DATE contains an invalid format string\"\n</code></pre> <p>See <code>DATE_FORMAT</code> for the list of supported SQL format characters.</p> <p></p> <ul> <li>The format string passed to STR_TO_DATE is a valid SQL format string, which contains one or more escape  characters that BodoSQL does not currently support.</li> </ul> <pre><code>\"STR_TO_DATE contains an invalid escape character (escape char)\"\n</code></pre> <p>For the list of supported SQL format characters, see <code>DATE_FORMAT</code>.</p> <p></p> <ul> <li>The specified column (<code>COL_NAME</code>) of one of the pandas DataFrames used to initialize a BodoSQLContext has an unsupported type.</li> </ul> <pre><code>\"Pandas column 'COL_NAME' with type PANDAS_TYPE not supported in BodoSQL.\"\n</code></pre> <p>For the list of supported pandas types,  see here.</p> <p></p> <ul> <li>The parser encountered something other than a query at a location where a query was expected.</li> </ul> <pre><code>\"Non-query expression encountered in illegal context\"\n</code></pre> <p>See here for the syntax of a select clause.</p> <p></p> <ul> <li>The table name specified in a SQL query doesn't match a table name registered in the BodoSQLContext.</li> </ul> <pre><code>\"Object 'tablename' not found\"\n</code></pre> <p>Generally, this is caused by misnaming a table when initializing the BodoSqlContext, or misnaming a table in the query itself.  If you encounter this issue please check BodoSQL's case sensitivity rules.</p> <p></p> <ul> <li>The query attempted to select a column from one or more tables, and the column wasn't present in any of them.</li> </ul> <pre><code>\"Column 'COL_NAME' not found in any table\"\n</code></pre> <p>Generally, this is caused by misnaming a column while initializing the BodoSQLContext, or misnaming a column in the query itself.   If you encounter this issue please check BodoSQL's case sensitivity rules.</p> <p></p> <ul> <li>The query attempted to select a column for two or more tables, and the column was present in multiple tables.</li> </ul> <pre><code>\"Column 'COL_NAME' is ambiguous\"\n</code></pre> <p></p> <p>Generally, this can be resolved by the specifying the origin table like so:</p> <p><code>Select A from table1, table2</code> \u2192 <code>Select table1.A from table1, table2</code></p> <ul> <li>The types of arguments supplied to the function don't match the types supported by the function.</li> </ul> <pre><code>\"Cannot apply 'FN_NAME' to arguments of type 'FN_NAME(&lt;ARG1_SQL_TYPE&gt;, &lt;ARG2_SQL_TYPE&gt;, ...)'. Supported form(s): 'FN_NAME(&lt;ARG1_SQL_TYPE&gt;, &lt;ARG2_SQL_TYPE&gt;, ...)'\"\n</code></pre> <p>This can be resolved by explicitly casting the problematic argument(s) to the appropriate type.</p> <p></p> <ul> <li>Either BodoSQL doesn't support the function or an incorrect number of arguments was supplied.</li> </ul> <pre><code>\"No match found for function signature FN_NAME(&lt;ARG1_SQL_TYPE&gt;, &lt;ARG2_SQL_TYPE&gt;, ...)\"\n</code></pre> <p>This generally means that the function you are trying to use does not support the types you have provided or may not be supported in BodoSQL.</p> <p></p> <ul> <li>A Window function that does not support windows with a <code>ROWS_BETWEEN</code> clause was called over a window containing a <code>ROWS_BETWEEN</code> clause.</li> </ul> <pre><code>\"ROW/RANGE not allowed with RANK, DENSE_RANK or ROW_NUMBER functions\"\n</code></pre> <p>In addition to the RANK, DENSE_RANK, or ROW_NUMBER functions listed in the error message, LEAD and LAG also have this requirement.  The list of window aggregations we support, and their calling syntax can be found here.</p> <p></p> <ul> <li>BodoSQL was unable to parse your SQL because the query contained unsupported syntax.</li> </ul> <pre><code>\"Encountered \"KEYWORD\" at line X, column Y. Was expecting one of: ...\"\n</code></pre> <p>There are a variety of reasons this could occur, but here are some of the common ones:</p> <ul> <li>A typo in one of the query words, for example <code>groupby</code> instead of <code>group by</code>. In this situation <code>line X, column Y</code> should point you to the first typo.</li> <li>All the components are legal SQL keywords, but they are used in an incorrect order. Please refer to our support syntax to check for legal constructions. If you believe your query should be supported please file an issue.</li> <li>Trying to use double-quotes for a string literal (i.e. <code>py\"example\"</code> instead of <code>'example'</code>)</li> <li>Unclosed parenthesis or trailing commas</li> </ul> <p></p> <ul> <li>A parameter was not properly registered in the BodoSQLContext.</li> </ul> <pre><code>\"SQL query contains a unregistered parameter: '@param_name'\"\n</code></pre> <p>This is often caused by failing to pass the parameter to <code>BodoSQLContext.sql()</code> or using an incorrect name in either the query or the registration. For more information on named parameters, see here.</p> <p></p> <ul> <li>A non-dataframe value was used to initialize a BodoSQLContext.</li> </ul> <pre><code>\"BodoSQLContext(): 'table' values must be DataFrames\"\n</code></pre> <p>The dictionary used to initialize a BodoSQLContext must map string table names to pandas DataFrames.</p>"},{"location":"api_docs/sql/case_sensitivity/","title":"Identifier Case Sensitivity","text":"<p>In BodoSQL, by default all identifiers not wrapped in quotes are automatically converted to upper case. If you are a Snowflake user who is using either the Snowflake Catalog or Table Path API, then this should not impact you and the rules will be the same as Snowflake (i.e. identifiers are case-insensitive unless wrapped in quotes during table creation). See here for the Snowflake documentation..</p> <p>This means that the following queries are equivalent:</p> <pre><code>SELECT A FROM table1\n</code></pre> <pre><code>SELECT a FROM TABLE1\n</code></pre> <p>When providing column or table names, identifiers will only match if the original name is in uppercase For example, the following code will fail to compile because there is no match for TABLE1:</p> <pre><code>@bodo.jit\ndef f(filename):\n    df1 = pd.read_parquet(filename)\n    bc = bodosql.BodoSQLContext({\"table1\": df1})\n    return bc.sql(\"SELECT A FROM table1\")\n</code></pre> <p>To match non-uppercase names you can use quotes to specify the name exactly as it appears in the BodoSQLContext definition or the columns of a DataFrame. For example:</p> <pre><code>@bodo.jit\ndef f(filename):\n    df1 = pd.read_parquet(filename)\n    bc = bodosql.BodoSQLContext({\"table1\": df1})\n    return bc.sql(\"SELECT A FROM \\\"table1\\\"\")\n</code></pre> <p>Similarly if you want an alias to be case sensitive then you will also need it to be wrapped in quotes:</p> <pre><code>SELECT A as \"myIdentifier\" FROM table1\n</code></pre> <p>If you provide DataFrames directly from Python or are using the TablePath API to load Parquet files, then please be advised that the column names will be required to match exactly and for ease of use we highly recommend using uppercase column names.</p> <p>BodoSQL's identifier handling behavior can be controlled by an environment variable <code>BODO_SQL_STYLE</code> which has two currently supported values:</p> <ul> <li><code>SNOWFLAKE</code> (the default)</li> <li><code>SPARK</code></li> </ul> <p>If <code>BODO_SQL_STYLE</code> is set to <code>SPARK</code>, then a different set of case sensitivity rules will be used:</p> <ul> <li>Identifiers, both with and without quotes, will not have their casing altered.</li> <li>All identifier matching is case-insensitive.</li> </ul>"},{"location":"api_docs/sql/data_types/","title":"Supported DataFrame Data Types","text":"<p>BodoSQL uses its internal Python tables to represent SQL tables in memory and converts SQL types to corresponding Python types which are used by Bodo. Below is a table mapping SQL types used in BodoSQL to their respective Python types and Bodo data types.</p> <p> SQL Type(s) Equivalent Python Type Bodo Data Type <code>BOOLEAN</code> <code>np.bool_</code> <code>bodo.bool_</code> <code>TINYINT</code> <code>np.int8</code> <code>bodo.int8</code> <code>SMALLINT</code> <code>np.int16</code> <code>bodo.int16</code> <code>INT</code> <code>np.int32</code> <code>bodo.int32</code> <code>BIGINT</code> <code>np.int64</code> <code>bodo.int64</code> <code>FLOAT</code> <code>np.float32</code> <code>bodo.float32</code> <code>DOUBLE</code> <code>np.float64</code> <code>bodo.float64</code> <code>VARCHAR</code>, <code>CHAR</code> <code>str</code> <code>bodo.string_type</code> <code>VARBINARY</code>, <code>BINARY</code> <code>bytes</code> <code>bodo.bytes_type</code> <code>DATE</code> <code>datetime.date</code> <code>bodo.datetime_date_type</code> <code>TIME</code> <code>bodo.Time</code> <code>bodo.TimeType</code> <code>TIMESTAMP_NTZ</code> <code>pd.Timestamp</code> <code>bodo.PandasTimestampType(None)</code> <code>TIMESTAMP_LTZ</code> <code>pd.Timestamp</code> <code>bodo.PandasTimestampType(local_tz)</code> <code>TIMESTAMP_TZ</code> <code>bodo.TimestampTZ</code> <code>bodo.timestamptz_type</code> <code>INTERVAL(day-time)</code> <code>np.timedelta64[ns]</code> <code>bodo.timedelta64ns</code> <code>ARRAY</code> <code>pyarrow.large_list</code> <code>bodo.ArrayItemArray</code> <code>MAP</code> <code>pyarrow.map</code> <code>bodo.MapScalarType</code> <code>NULL</code> <code>pyarrow.NA</code> <code>bodo.null_dtype</code> <p></p> <p>BodoSQL may be able to handle additional column types if the data is unused. When loading data from Snowflake or other sources, BodoSQL will treat Decimal columns as either BigInt or Float64 depending on the column's scale and precision.</p>"},{"location":"api_docs/sql/data_types/#unsigned-types","title":"Unsigned Types","text":"<p>Although SQL does not explicitly support unsigned types, BodoSQL typically maintains the types of the existing DataFrames registered in a [BodoSQLContext]. If these types are unsigned, then this may result in different behavior than expected. We always recommend working with signed types to avoid any potential issues.</p>"},{"location":"api_docs/sql/data_types/#timestamp_tz","title":"TIMESTAMP_TZ","text":"<p>Note that <code>bodo.TimestampTZ</code> in python is a custom type provided by the Bodo library. In <code>sql</code> this datatype is compatible with Snowflake's TIMESTAMP_TZ.</p> <p><code>TIMESTAMP_TZ</code> stores a timestamp along with a <code>UTC</code> offset with a resolution of minutes. This offset can be arbitrary, but it is not dependant on the timestamp value. In other words, it is not aware of timezones and changes in offset such as <code>DST</code>. While most operations will use the timestamp value (not <code>UTC</code>), any comparison between two <code>TIMESTAMP_TZ</code> values will treat them as equal if their <code>UTC</code> time is equal. For example:</p> <p><pre><code>SELECT '2024-01-01 00:00:00 +00:00'::timestamptz = '2024-01-01 01:00:00 +01:00'::timestamptz\n</code></pre> The above query will output a row with <code>TRUE</code>  - the timestamps are the same with respect to <code>UTC</code> even though their values without the offset are different.</p> <p><pre><code>SELECT '2024-01-01 00:00:00 +00:00'::timestamptz = '2024-01-01 00:00:00 +05:00'::timestamptz\n</code></pre> The above query will output a row with <code>False</code> - the timestamps are not the same with respect to <code>UTC</code> even though their values without the offset are equal.</p> <p>This means that grouping by a <code>TIMESTAMP_TZ</code> value will follow the same equality rules above, and we make no guarantees about what the offset of the key for a group will be - only guarantee is that the key's <code>UTC</code> timestamp is equal to all values for that group. For example, consider the following table:</p> A B 2023-01-01 00:00:00 +00:00 1 2023-01-01 01:00:00 +01:00 1 2023-01-01 00:00:00 +01:00 1 2023-01-01 01:00:00 +00:00 1 2023-01-02 00:00:00 +00:00 1 2023-01-02 01:00:00 +01:00 1 <p>Where <code>A</code> is a <code>TIMESTAMP_TZ</code> and <code>B</code> is a <code>NUMBER</code>. Note that rows <code>0</code> and <code>1</code> have equal values for <code>A</code>. Similarly rows <code>2</code> and <code>3</code> are equal in terms of <code>A</code>, and same for rows <code>4</code> and <code>5</code>. Then, both of the following are valid results for <code>SELECT A, sum(B) FROM table GROUP BY A</code>:</p> A B 2023-01-01 00:00:00 +00:00 2 2023-01-01 00:00:00 +01:00 2 2023-01-02 00:00:00 +00:00 2 A B 2023-01-01 01:00:00 +01:00 2 2023-01-01 00:00:00 +01:00 2 2023-01-02 01:00:00 +01:00 2 <p>Note that these aren't the only two possibilities - for the query above there are <code>8</code> possible results.</p> <p>If you need to compare values by their local timestamp instead of their UTC timestamp, consider casting to <code>timestampntz</code>. For the same input table above, here's what the result of <code>SELECT A::timestampntz FROM table</code> would look like:</p> A::timestampntz 2023-01-01 00:00:00 2023-01-01 01:00:00 2023-01-01 00:00:00 2023-01-01 01:00:00 2023-01-02 00:00:00 2023-01-02 01:00:00 <p>Note that this model of equality also holds during <code>JOIN</code>s:</p> <p>Table 1: | A                          | B | |----------------------------|---| | 2023-01-01 00:00:00 +00:00 | 1 | | 2024-02-02 00:00:00 +00:00 | 2 |</p> <p>Table 2: | A                          | |----------------------------| | 2023-01-01 00:00:00 +01:00 | | 2023-01-01 00:00:00 +02:00 | | 2023-01-01 00:00:00 +03:00 | | 2024-02-02 00:00:00 +01:00 | | 2024-02-02 00:00:00 +02:00 | | 2024-02-02 00:00:00 +03:00 |</p> <p>The result of <code>SELECT TABLE1.A, TABLE2.A, B FROM TABLE1 JOIN TABLE2 ON TABLE1.A=TABLE2.A</code> would be:</p> TABLE1.A TABLE2.A B 2023-01-01 00:00:00 +00:00 2023-01-01 00:00:00 +01:00 1 2023-01-01 00:00:00 +00:00 2023-01-01 00:00:00 +02:00 1 2023-01-01 00:00:00 +00:00 2023-01-01 00:00:00 +03:00 1 2024-02-02 00:00:00 +00:00 2024-02-02 00:00:00 +01:00 2 2024-02-02 00:00:00 +00:00 2024-02-02 00:00:00 +02:00 2 2024-02-02 00:00:00 +00:00 2024-02-02 00:00:00 +03:00 2 <p>Aside from comparison most other operations will treat <code>TIMESTAMP_TZ</code> as it's local timestamp, for example <code>SELECT EXTRACT(HOUR from '2024-01-02 03:04:05 +06:07'::timestamptz)</code> should return <code>3</code> (even though the <code>UTC</code> timestamp would have an hour of <code>21</code>).</p>"},{"location":"api_docs/sql/data_types/#timestamp_tz-interaction-with-snowflake","title":"TIMESTAMP_TZ interaction with Snowflake","text":"<p>Note that reading <code>TIMESTAMP_TZ</code> values to or from Snowflake may change the session parameter <code>TIMESTAMP_TZ_OUTPUT_FORMAT</code>. If your query relies on custom values for <code>TIMESTAMP_TZ_OUTPUT_FORMAT</code> you may experience unexpected behavior.</p>"},{"location":"api_docs/sql/data_types/#timestamp_tz-limitations","title":"TIMESTAMP_TZ limitations","text":"<p>Currently only the following aggregation functions are supported with <code>TIMESTAMP_TZ</code>. Future releases will expand this list.</p> <ul> <li>min/max</li> <li>first/last/any_value</li> <li>count</li> <li>mode</li> </ul> <p>Additionally, <code>TIMESTAMP_TZ</code> is not supported in semi-structured data (arrays,  and objects).</p>"},{"location":"api_docs/sql/data_types/#supported-literals","title":"Supported Literals","text":"<p>BodoSQL supports the following literal types:</p> <ul> <li><code>array_literal</code></li> <li><code>boolean_literal</code></li> <li><code>datetime_literal</code></li> <li><code>float_literal</code></li> <li><code>integer_literal</code></li> <li><code>interval_literal</code></li> <li><code>object_literal</code></li> <li><code>string_literal</code></li> <li><code>binary_literal</code></li> </ul>"},{"location":"api_docs/sql/data_types/#array_literal","title":"Array Literal","text":"<p>Syntax:</p> <pre><code>&lt;[&gt; [expr[, expr...]] &lt;]&gt;\n</code></pre> <p>where <code>&lt;[&gt;</code> and <code>&lt;]&gt;</code> indicate literal <code>[</code> and <code>]</code>s, and <code>expr</code> is any expression.</p> <p>Array literals are lists of comma separated expressions wrapped in square brackets.</p> <p>Note that BodoSQL currently only supports homogenous lists, and all <code>expr</code>s must coerce to a single type.</p>"},{"location":"api_docs/sql/data_types/#boolean_literal","title":"Boolean Literal","text":"<p>Syntax:</p> <pre><code>TRUE | FALSE\n</code></pre> <p>Boolean literals are case-insensitive.</p>"},{"location":"api_docs/sql/data_types/#datetime_literal","title":"Datetime Literal","text":"<p>Syntax:</p> <pre><code>DATE 'yyyy-mm-dd' |\nTIME 'HH:mm:ss' |\nTIMESTAMP 'yyyy-mm-dd' |\nTIMESTAMP 'yyyy-mm-dd HH:mm:ss'\n</code></pre>"},{"location":"api_docs/sql/data_types/#float_literal","title":"Float Literal","text":"<p>Syntax:</p> <pre><code>[ + | - ] { digit [ ... ] . [ digit [ ... ] ] | . digit [ ... ] }\n</code></pre> <p>where digit is any numeral from 0 to 9</p>"},{"location":"api_docs/sql/data_types/#integer_literal","title":"Integer Literal","text":"<p>Syntax:</p> <pre><code>[ + | - ] digit [ ... ]\n</code></pre> <p>where digit is any numeral from 0 to 9</p>"},{"location":"api_docs/sql/data_types/#interval_literal","title":"Interval Literal","text":"<p>Syntax:</p> <pre><code>INTERVAL integer_literal interval_type\n</code></pre> <p>Where integer_literal is a valid integer literal and interval type is one of:</p> <pre><code>DAY[S] | HOUR[S] | MINUTE[S] | SECOND[S]\n</code></pre> <p>In addition, we also have limited support for <code>YEAR[S]</code> and <code>MONTH[S]</code>. These literals cannot be stored in columns and currently are only supported for operations involving add and sub.</p>"},{"location":"api_docs/sql/data_types/#object_literal","title":"Object Literal","text":"<p>Syntax:</p> <pre><code>{['k1': `v1`[, 'k2': `v2`, ...]]}\n</code></pre> <p>Where each <code>ki</code> is a unique string literal, and each <code>vi</code> is an expression. Obeys the same semantics as the function <code>OBJECT_CONSTRUCT</code> , so any pair where the key or value is null is omitted, and for now BodoSQL only supports when all values are the same type.</p>"},{"location":"api_docs/sql/data_types/#string_literal","title":"String Literal","text":"<p>Syntax:</p> <pre><code>'char [ ... ]'\n</code></pre> <p>Where char is a character literal in a Python string.</p>"},{"location":"api_docs/sql/data_types/#binary_literal","title":"Binary Literal","text":"<p>Syntax:</p> <pre><code>X'hex [ ... ]'\n</code></pre> <p>Where hex is a hexadecimal character between 0-F.</p>"},{"location":"api_docs/sql/database_catalogs/","title":"Database Catalogs","text":"<p>Database Catalogs are configuration objects that grant BodoSQL access to load tables from a database. For example, when a user wants to load data from Snowflake, a user will create a <code>SnowflakeCatalog</code> to grant BodoSQL access to their Snowflake account and load the tables of interest.</p> <p>A database catalog can be registered during the construction of the <code>BodoSQLContext</code> by passing it in as a parameter, or can be manually set using the <code>BodoSQLContext.add_or_replace_catalog</code> API. Currently, a <code>BodoSQLContext</code> can support at most one database catalog.</p> <p>When using a catalog in a <code>BodoSQLContext</code> we strongly recommend creating the <code>BodoSQLContext</code> once in regular Python and then passing the <code>BodoSQLContext</code> as an argument to JIT functions. There is no benefit to creating the <code>BodoSQLContext</code> in JIT and this could increase compilation time.</p> <pre><code>catalog = bodosql.SnowflakeCatalog(\n    username,\n    password,\n    account_name,\n    \"DEMO_WH\", # warehouse name\n    \"SNOWFLAKE_SAMPLE_DATA\", # database name\n)\nbc = bodosql.BodoSQLContext({\"LOCAL_TABLE1\": df1}, catalog=catalog)\n\n@bodo.jit\ndef run_query(bc):\n    return bc.sql(\"SELECT r_name, local_id FROM TPCH_SF1.REGION, local_table1 WHERE R_REGIONKEY = local_table1.region_key ORDER BY r_name\")\n\nrun_query(bc)\n</code></pre> <p>Database catalogs can be used alongside local, in-memory <code>DataFrame</code> or <code>TablePath</code> tables. If a table is specified without a schema then BodoSQL resolves the table in the following order:</p> <ol> <li>Default Catalog Schema</li> <li>Local (in-memory) DataFrames / TablePath names</li> </ol> <p>An error is raised if the table cannot be resolved after searching through both of these data sources.</p> <p>This ordering indicates that in the event of a name conflict between a table in the database catalog and a local table, the table in the database catalog is used.</p> <p>If a user wants to use the local table instead, the user can explicitly specify the table with the local schema <code>__BODOLOCAL__</code>.</p> <p>For example:</p> <pre><code>SELECT A from __BODOLOCAL__.table1\n</code></pre> <p>Currently, BodoSQL supports catalogs for Snowflake and a user's FileSystem. Support for other data storage systems will be added in future releases.</p>"},{"location":"api_docs/sql/database_catalogs/#snowflake-catalog-api","title":"SnowflakeCatalog","text":"<p>With a Snowflake Catalog, users only have to specify their Snowflake connection once. They can then access any tables of interest in their Snowflake account. Currently, a Snowflake Catalog requires a default <code>DATABASE</code> (e.g., <code>USE DATABASE</code>), as shown below.</p> <pre><code>catalog = bodosql.SnowflakeCatalog(\n    username,\n    password,\n    account_name,\n    \"DEMO_WH\", # warehouse name\n    \"SNOWFLAKE_SAMPLE_DATA\", # default database name\n)\nbc = bodosql.BodoSQLContext(catalog=catalog)\n\n@bodo.jit\ndef run_query(bc):\n    return bc.sql(\"SELECT r_name FROM TPCH_SF1.REGION ORDER BY r_name\")\n\nrun_query(bc)\n</code></pre> <p>BodoSQL does not currently support Snowflake syntax for specifying defaults and session parameters (e.g. <code>USING SCHEMA &lt;NAME&gt;</code>). Instead users can pass any session parameters through the optional <code>connection_params</code> argument, which accepts a <code>Dict[str, str]</code> for each session parameter. For example, users can provide a default schema to simplify the previous example.</p> <pre><code>catalog = bodosql.SnowflakeCatalog(\n    username,\n    password,\n    account,\n    \"DEMO_WH\", # warehouse name\n    \"SNOWFLAKE_SAMPLE_DATA\", # database name\n    connection_params={\"schema\": \"TPCH_SF1\"}\n)\nbc = bodosql.BodoSQLContext(catalog=catalog)\n\n@bodo.jit\ndef run_query(bc):\n    return bc.sql(\"SELECT r_name FROM REGION ORDER BY r_name\")\n\nrun_query(bc)\n</code></pre> <p>Internally, Bodo uses the following connections to Snowflake:</p> <ol> <li>A JDBC connection to lazily fetch metadata.</li> <li>The Snowflake-Python-Connector's distributed fetch API to load batches of arrow data.</li> </ol>"},{"location":"api_docs/sql/database_catalogs/#api-reference","title":"API Reference","text":"<ul> <li> <p><code>bodosql.SnowflakeCatalog(username: str, password: str, account: str, warehouse: str, database: str, connection_params: Optional[Dict[str, str]] = None, iceberg_volume: Optional[str] = None)</code> </p> <p>Constructor for <code>SnowflakeCatalog</code>. This allows users to execute queries on tables stored in Snowflake when the <code>SnowflakeCatalog</code> object is registered with a <code>BodoSQLContext</code>.</p> <p>Arguments</p> <ul> <li> <p><code>username</code>: Snowflake account username.</p> </li> <li> <p><code>password</code>: Snowflake account password.</p> </li> <li> <p><code>account</code>: Snowflake account name.</p> </li> <li> <p><code>warehouse</code>: Snowflake warehouse to use when loading data.</p> </li> <li> <p><code>database</code>: Name of Snowflake database to load data from. The Snowflake     Catalog is currently restricted to using a single Snowflake <code>database</code>.</p> </li> <li> <p><code>connection_params</code>: A dictionary of Snowflake session parameters.</p> </li> <li> <p><code>iceberg_volume</code>: The name of a storage volume to use for writing Iceberg tables. When provided any tables created by BodoSQL will be written as    an Iceberg table.</p> </li> </ul> </li> </ul>"},{"location":"api_docs/sql/database_catalogs/#supported-query-types","title":"Supported Query Types","text":"<p>The <code>SnowflakeCatalog</code> currently supports the following types of SQL queries:</p> <ul> <li><code>SELECT</code></li> <li><code>INSERT INTO</code></li> <li><code>DELETE</code></li> <li><code>CREATE TABLE AS</code></li> </ul>"},{"location":"api_docs/sql/database_catalogs/#fs-catalog-api","title":"FileSystemCatalog","text":"<p>The <code>FileSystemCatalog</code> allows users to read and write tables using their local file system or S3 storage without needing access to a proper database. To use this catalog, you will have to select a root directory. The catalog will treat each subdirectory as a schema that you can also specify. We recommend always using at least one schema to avoid any potential issues with table resolution. For example, the following code shows how a user could read a table called <code>MY_TABLE</code> that is located at <code>s3://my_bucket/MY_SCHEMA/MY_TABLE</code>.</p> <pre><code>catalog = bodosql.FileSystemCatalog(\n    \"s3://my_bucket\", # root directory\n)\nbc = bodosql.BodoSQLContext(catalog=catalog)\n\n@bodo.jit\ndef run_query(bc):\n    return bc.sql(\"SELECT * FROM MY_SCHEMA.MY_TABLE\")\n\nrun_query(bc)\n</code></pre> <p>When working with tables in the <code>FileSystemCatalog</code>, BodoSQL uses the full name of any directory or file as the object's name and is case sensitive. When constructing a query you must follow the BodoSQL rules for identifier case sensitivity.</p> <p>To simplify your queries you can also provide a default schema resolution path to the <code>FileSystemCatalog</code> constructor. For example, this code provides a default schema of <code>MY_SCHEMA.other_schema</code> for loading <code>OTHER_TABLE</code> from <code>s3://my_bucket/MY_SCHEMA/other_schema/OTHER_TABLE</code>.</p> <pre><code>catalog = bodosql.FileSystemCatalog(\n    \"s3://my_bucket\",\n    default_schema=\"MY_SCHEMA.\\\"other_schema\\\"\"\n)\nbc = bodosql.BodoSQLContext(catalog=catalog)\n\n@bodo.jit\ndef run_query(bc):\n    return bc.sql(\"SELECT * FROM OTHER_TABLE\")\n\nrun_query(bc)\n</code></pre>"},{"location":"api_docs/sql/database_catalogs/#api-reference_1","title":"API Reference","text":"<ul> <li> <p><code>bodosql.FileSystemCatalog(root: str, default_write_format: str = \"iceberg\", default_schema: str = \".\")</code> </p> <p>Constructor for <code>FileSystemCatalog</code>. This allows users to try a file system as a database for querying or writing tables with a <code>BodoSQLContext</code>.</p> <p>Arguments</p> <ul> <li> <p><code>root</code>: Filesystem path that provides the root directory for the database. This can either be a local file system path or an S3 path.</p> </li> <li> <p><code>default_write_format</code>: The default format to use when writing tables using <code>create table as</code>. This can be either <code>iceberg</code> or <code>parquet</code>.</p> </li> <li> <p><code>default_schema</code>: The default schema to use when resolving tables. This should be a <code>.</code> separated string that represents the path to the default schema.    Each value separated by a <code>.</code> should be treated as its own SQL identifier. If no default schema is provided the root directory is used.</p> </li> </ul> </li> </ul>"},{"location":"api_docs/sql/database_catalogs/#supported-query-types_1","title":"Supported Query Types","text":"<p>The <code>FileSystemCatalog</code> currently supports the following types of SQL queries:</p> <ul> <li><code>SELECT</code></li> <li><code>CREATE TABLE AS</code></li> </ul>"},{"location":"api_docs/sql/database_catalogs/#supported-table-types","title":"Supported Table Types","text":"<p>The <code>FileSystemCatalog</code> currently only supports reading Iceberg tables. It can write tables as either Iceberg or Parquet, depending on the <code>default_write_format</code> parameter. When writing tables, any specified schema must already exist as directories in the file system. Future releases will provide additional table support.</p>"},{"location":"api_docs/sql/database_catalogs/#s3-support","title":"S3 Support","text":"<p>The <code>FileSystemCatalog</code> supports reading and writing tables from S3. When using S3, the <code>root</code> parameter should be an s3 uri. To access S3 BodoSQL uses the following environment variables to connect to S3:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_REGION</code></li> </ul> <p>If you encounter any issues connecting to s3 or accessing a table, please ensure that these environment variables are set. For more information please refer to the AWS documentation.</p>"},{"location":"api_docs/sql/database_catalogs/#REST-catalog-api","title":"RESTCatalog","text":"<p>The <code>RESTCatalog</code> allows users to read and write tables to and from REST Iceberg catalogs. To use this catalog, you will need to provide the uri of the rest catalog and a token or credential.</p> <pre><code>catalog = bodosql.RESTTablesCatalog(\n    warehouse=\"warehouse_name\",\n    rest_uri=\"http://rest_uri\",\n    token=\"token\",\n)\nbc = bodosql.BodoSQLContext(catalog=catalog)\ndf = bc.sql(\"SELECT * FROM MY_SCHEMA.MY_TABLE\")\n</code></pre> <p>When constructing a query you must follow the BodoSQL rules for identifier case sensitivity.</p>"},{"location":"api_docs/sql/database_catalogs/#api-reference_2","title":"API Reference","text":"<p><pre><code>bodosql.RESTCatalog(\n    warehouse: str,\n    rest_uri: str,\n    token: str | None = None,\n    credential: str | None = None,\n    scope: str | None = None,\n    default_schema: str | None = None)\n</code></pre> Arguments</p> <pre><code>- `warehouse`: Name of the REST Iceberg catalog warehuose to connect to\n- rest_uri: URI of the REST Iceberg catalog to connect to\n- token: Token to use for authentication, if credential is not provided\n- credential: Credential to use for authentication, if token is not provided\n- scope: Scope to authenticate with, not always required\n- default_schema: Default schema to use when resolving tables. If not provided, the root schema is used.\n</code></pre>"},{"location":"api_docs/sql/database_catalogs/#supported-query-types_2","title":"Supported Query Types","text":"<p>The <code>RESTCatalog</code> currently supports the following types of SQL queries:</p> <ul> <li><code>SELECT</code></li> <li><code>CREATE TABLE AS</code></li> </ul>"},{"location":"api_docs/sql/database_catalogs/#glue-catalog-api","title":"GlueCatalog","text":"<p>The <code>GlueCatalog</code> allows users to read and write tables using AWS Glue. To use this catalog, you will have to select a warehouse that is the Glue S3 bucket. The bucket must exist. The <code>s3://</code> prefix is optional. I.e., both <code>s3://bucket_name</code> and <code>bucket_name</code> are valid.</p> <pre><code>catalog = bodosql.GlueCatalog(\n    warehouse=\"warehouse_name\",\n)\nbc = bodosql.BodoSQLContext(catalog=catalog)\n\n@bodo.jit\ndef run_query(bc):\n    return bc.sql(\"SELECT * FROM MY_SCHEMA.MY_TABLE\")\n\nrun_query(bc)\n</code></pre> <p>When constructing a query you must follow the BodoSQL rules for identifier case sensitivity.</p>"},{"location":"api_docs/sql/database_catalogs/#authentication-authorization","title":"Authentication / Authorization","text":"<p>Before creating a catalog for the Glue S3 bucket, you must first ensure that the cloud config role for the workspace has access to the Glue S3 bucket with the following permissions: <pre><code>{\n    \"Sid\": \"BodoPlatformCatalog\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\n        \"s3:ListBucket\"\n    ],\n    \"Resource\": \"arn:aws:s3:::&lt;backet&gt;\"\n}\n</code></pre></p> <p>The following are the steps to find the cloud config role:   * find the <code>Cloud Config uuid</code> in the workpace details.   * find the <code>Cloud Config</code> that has the same uuid in the <code>Cloud Configurations</code> page.   * find the <code>Role ARN</code> in the details of the <code>Cloud Config</code>. It may look like <code>BodoPlatformUser-XXXXXXXX</code>   if it was created using <code>Cloud Formation</code> or <code>Access Key</code>. </p> <p>The Bodo clusters that run the queries on the Glue Catalog need to be created with an instance role. This instance role needs to have read/write access to the Glue S3 bucket and also appropreate Glue permissions. The exact set of Glue permissions depend on the queries on the Glue database. The following is  an example that could work for most of the use cases.</p> <pre><code>{\n    \"Sid\": \"BodoPlatformAccessGlueCatalog\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\n        \"glue:CreateDatabase\",\n        \"glue:CreateSchema\",\n        \"glue:CreateTable\",\n        \"glue:DeleteDatabase\",\n        \"glue:DeleteSchema\",\n        \"glue:DeleteTable\",\n        \"glue:GetDatabase\",\n        \"glue:GetDatabases\",\n        \"glue:GetSchema\",\n        \"glue:GetSchemaByDefinition\",\n        \"glue:GetSchemaVersion\",\n        \"glue:GetSchemaVersionsDiff\",\n        \"glue:GetTable\",\n        \"glue:GetTables\",\n        \"glue:GetTableVersion\",\n        \"glue:GetTableVersions\",\n        \"glue:ListSchemas\",\n        \"glue:ListSchemaVersions\",\n        \"glue:PutSchemaVersionMetadata\",\n        \"glue:QuerySchemaVersionMetadata\",\n        \"glue:RegisterSchemaVersion\",\n        \"glue:RemoveSchemaVersionMetadata\",\n        \"glue:UpdateDatabase\",\n        \"glue:UpdateSchema\",\n        \"glue:UpdateTable\"\n    ],\n    \"Resource\": [\n      \"arn:aws:glue:*:*:catalog\",\n      \"arn:aws:glue:*:*:database/*&lt;bodo-db&gt;*\",\n      \"arn:aws:glue:*:*:table/*&lt;bodo-db&gt;*/&lt;table&gt;\",\n      \"arn:aws:glue:*:*:connection/*&lt;bodo-connection&gt;*\",\n      \"arn:aws:glue:*:*:session/*&lt;bodo-session&gt;*\"\n    ]\n}, {\n    \"Sid\": \"BodoPlatformAccessGlueBucket\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\n        \"s3:*\",\n    ],\n   \"Resource\": \"arn:aws:s3:::&lt;backet&gt;\"\n}\n</code></pre> <p>For workspaces that have PrivateLink enabled, or have no internet access, users need to create AWS Glue endpoint in the VPC to allow access to Glue.</p>"},{"location":"api_docs/sql/database_catalogs/#api-reference_3","title":"API Reference","text":"<ul> <li> <p><code>bodosql.GlueCatalog(warehouse: str)</code> </p> <p>Constructor for <code>GlueCatalog</code>. This allows users to use an AWS Glue Iceberg Warehouse as a database for querying or writing tables with a <code>BodoSQLContext</code>.</p> <p>Arguments</p> <ul> <li><code>warehouse</code>: Name of the Glue S3 bucket, with or without the <code>s3://</code> prefix. I.e., both <code>s3://bucket_name</code> and <code>bucket_name</code> are valid.</li> </ul> </li> </ul>"},{"location":"api_docs/sql/database_catalogs/#supported-query-types_3","title":"Supported Query Types","text":"<p>The <code>GlueCatalog</code> currently supports the following types of SQL queries:</p> <ul> <li><code>SELECT</code></li> <li><code>CREATE TABLE AS</code></li> </ul>"},{"location":"api_docs/sql/database_catalogs/#s3-tables-catalog-api","title":"S3TablesCatalog","text":"<p>The <code>S3TablesCatalog</code> allows users to read and write tables to and from S3 Tables. To use this catalog, you will need to provide the S3 table bucket arn. The bucket must exist.</p> <pre><code>catalog = bodosql.S3TablesCatalog(\n    warehouse=\"warehouse_name\",\n)\nbc = bodosql.BodoSQLContext(catalog=catalog)\ndf = bc.sql(\"SELECT * FROM MY_SCHEMA.MY_TABLE\")\n</code></pre> <p>When constructing a query you must follow the BodoSQL rules for identifier case sensitivity.</p>"},{"location":"api_docs/sql/database_catalogs/#authentication-authorization_1","title":"Authentication / Authorization","text":"<p>Refer to AWS' documentation</p>"},{"location":"api_docs/sql/database_catalogs/#api-reference_4","title":"API Reference","text":"<ul> <li> <p><code>bodosql.S3TablesCatalog(warehouse: str)</code> </p> <p>Constructor for <code>S3TablesCatalog</code>. This allows users to use an AWS S3 Tables Iceberg Warehouse as a database for querying or writing tables with a <code>BodoSQLContext</code>.</p> <p>Arguments</p> <ul> <li><code>warehouse</code>: Arn of the S3 table bucket</li> </ul> </li> </ul>"},{"location":"api_docs/sql/database_catalogs/#supported-query-types_4","title":"Supported Query Types","text":"<p>The <code>S3TablesCatalog</code> currently supports the following types of SQL queries:</p> <ul> <li><code>SELECT</code></li> <li><code>CREATE TABLE AS</code></li> </ul>"},{"location":"api_docs/sql/io_handling/","title":"IO Handling","text":"<p>BodoSQL is great for compute based SQL queries, but you cannot yet access external storage directly from SQL. Instead, you can load and store data using Bodo and various Python APIs. Here we explain a couple common methods for loading data.</p>"},{"location":"api_docs/sql/io_handling/#pandas-io-in-jit-function-with-sql-query","title":"Pandas IO in JIT function with SQL Query","text":"<p>The most common way to load data is to first use Pandas APIs to load a DataFrame inside a JIT function and then to use that DataFrame inside a BodoSQLContext.</p> <pre><code>def f(f1, f2):\n    df1 = pd.read_parquet(f1)\n    df2 = pd.read_parquet(f2)\n    bc = bodosql.BodoSQLContext(\n        {\n            \"T1\": df1,\n            \"T2\": df2,\n        }\n    )\n    return bc.sql(\"select t1.A, t2.B from t1, t2 where t1.C &gt; 5 and t1.D = t2.D\")\n</code></pre>"},{"location":"api_docs/sql/io_handling/#pandas-io-in-a-jit-function-separate-from-query","title":"Pandas IO in a JIT Function Separate from Query","text":"<p>The previous approach works well for most individual queries. However, when running several queries on the same dataset, it should ideally be loaded once for all queries. To do this, you can structure your JIT code to contain a single load function at the beginning. For example:</p> <pre><code>@bodo.jit\ndef load_data(f1, f2):\n    df1 = pd.read_parquet(f1)\n    df2 = pd.read_parquet(f2)\n    return df1, df2\n\ndef q1(df1, df2):\n    bc = bodosql.BodoSQLContext(\n        {\n            \"T1\": df1,\n            \"T2\": df2,\n        }\n    )\n    return bc.sql(\"select t1.A, t2.B from t1, t2 where t1.C &gt; 5 and t1.D = t2.D\")\n\n...\n\n@bodo.jit\ndef run_queries(f1, f2):\n    df1, df2 = load_data(f1, f2)\n    print(q1(df1, df2))\n    print(q2(df2))\n    print(q3(df1))\n    ...\n\nrun_queries(f1, f2)\n</code></pre> <p>This approach prevents certain optimizations, such as filter pushdown. However, the assumption here is that you will use the entire DataFrame across the various benchmarks, so no optimization is useful by itself. In addition, any optimizations that can apply to all queries can be done explicitly inside <code>load_data</code>. For example, if all queries are operate on a single day's data with <code>df1</code>, you can write that filter in <code>load_data</code> to limit IO and filter pushdown will be performed.</p> <pre><code>@bodo.jit\ndef load_data(f1, f2, target_date):\n    df1 = pd.read_parquet(f1)\n    # Applying this filter limits how much data is loaded.\n    df1 = df1[df1.date_val == target_date]\n    df2 = pd.read_parquet(f2)\n    return df1, df2\n\n@bodo.jit\ndef run_queries(f1, f2, target_date):\n    df1, df2 = load_data(f1, f2, target_date)\n    ...\n\nrun_queries(f1, f2, target_date)\n</code></pre>"},{"location":"api_docs/sql/named_params/","title":"BodoSQL Caching &amp; Parameterized Queries","text":"<p>BodoSQL can reuse Bodo caching to avoid recompilation when used inside a JIT function. BodoSQL caching works the same as Bodo, so for example:</p> <pre><code>@bodo.jit(cache=True)\ndef f(filename):\n    df1 = pd.read_parquet(filename)\n    bc = bodosql.BodoSQLContext({\"TABLE1\": df1})\n    df2 = bc.sql(\"SELECT A FROM table1 WHERE B &gt; 4\")\n    print(df2.A.sum())\n</code></pre> <p>This will avoid recompilation so long as the DataFrame scheme stored in <code>filename</code> has the same schema and the code does not change.</p> <p>To enable caching for queries with scalar parameters that you may want to adjust between runs, we introduce a feature called parameterized queries. In a parameterized query, the SQL query replaces a constant/scalar value with a variable, which we call a named parameter. In addition, the query is passed a dictionary of parameters which maps each name to a corresponding Python variable.</p> <p>For example, if in the above SQL query we wanted to replace 4 with other integers, we could rewrite our query as:</p> <pre><code>bc.sql(\"SELECT A FROM table1 WHERE B @var\", {\"var\": python_var})\n</code></pre> <p>Now anywhere that <code>@var</code> is used, the value of python_var at runtime will be used instead. This can be used in caching, because python_var can be provided as an argument to the JIT function itself, thus enabling changing the filter without recompiling. The full example looks like this:</p> <pre><code>@bodo.jit(cache=True)\ndef f(filename, python_var):\n    df1 = pd.read_parquet(filename)\n    bc = bodosql.BodoSQLContext({\"TABLE1\": df1})\n    df2 = bc.sql(\"SELECT A FROM table1 WHERE B @var\", {\"var\": python_var})\n    print(df2.A.sum())\n</code></pre> <p>Named parameters cannot be used in places that require a constant value to generate the correct implementation (e.g. TimeUnit in EXTRACT).</p> <p>Note</p> <p>Named parameters are case sensitive, so <code>@var</code> and <code>@VAR</code> are different identifiers.</p>"},{"location":"api_docs/sql/performance/","title":"Performance Considerations","text":"<p>This section discusses some factors which affect performance when using BodoSQL.</p>"},{"location":"api_docs/sql/performance/#snowflake-views","title":"Snowflake Views","text":"<p>Users may define views within their Snowflake account to enable greater query reuse. Views may constitute performance bottlenecks because if a view is evaluated in Snowflake Bodo will need to wait for the result before it can fetch data and may have less access to optimizations.</p> <p>To improve performance in these circumstances Bodo will attempt to expand any views into the body of the query to allow Bodo to operate on the underlying tables. When this occurs users should face no performance penalty for using views in their queries. However there are a few situations in which this is not possible, namely</p> <ul> <li>The Snowflake User passed to Bodo does not have permissions to determine the view definition.</li> <li>The Snowflake User passed to Bodo does not have permissions to     read all of the underlying tables.</li> <li>The view is a materialized or secure view.</li> </ul> <p>If for any reason Bodo is unable to expand the view, then the query will execute treating the view as a table and delegate it to Snowflake.</p>"},{"location":"api_docs/sql/performance/#vectorized-execution","title":"Vectorized Execution","text":"<p>Bodo uses a vectorized query execution model for SQL queries. In this model, the query plan is split into several stages or pipelines, each comprised of multiple operators.  Each operator of the pipeline performs computations on batches of data and passes on the result to the next operator. This enables several performance and reliability benefits:</p> <ul> <li>It is possible to overlap compute and I/O. When reading data, BodoSQL can start executing compute tasks on the chunks of data that have been read while the rest is waiting on I/O. Similarly, BodoSQL can start writing the chunks of data that have finished being computed while the rest is still in progress.</li> <li>There is increased CPU cache locality when working on smaller chunks, potentially increasing runtime performance.</li> <li>This prevents out-of-memory (OOM) errors for most use cases. For example, in simple aggregations, data can be read and aggregated incrementally, instead of needing to read all the data and then performing the aggregation on the entire table.</li> </ul> <p>For operators like Join, Aggregate, Sort and some Window functions, we use specialized operators that can spill to disk for reliable execution even when the intermediates are too big to fit in memory. </p> <p>Note that not every operator in BodoSQL fully supports vectorized execution yet. Some operations still require BodoSQL to have the entire data in main memory before starting any compute.</p> <p>The best way to determine if a query is using vectorized execution in BodoSQL is to check the query plan with <code>bc.generate_plan(query)</code>. BodoSQL generates two special RelNodes when it needs to break vectorized execution: <code>CombineStreamsExchange</code> is used to gather the chunked batches of data into a single in-memory table (potentially causing an OOM error if the table is big enough) so that whatever operators occur next can operate on the non-streamed data, and <code>SeparateStreamsExchange</code> is used to split up such a table back into multiple chunks so that subsequent operators can use vectorized execution.</p> <p>For example, consider the abstract BodoSQL plan below:</p> <pre><code>TableCreate\n  NodeA\n    SeparateStreamsExchange\n      NodeB\n        CombineStreamsExchange\n          TableScan\n</code></pre> <p>At runtime, BodoSQL will perform the table scan in batches, and as it receives each batch it will  combine them into a single table. Once that singular table has been created, the operation described  by <code>NodeB</code> is run on the entire table. Afterward, the output of <code>NodeB</code> is split up into smaller chunks that are fed into <code>NodeA</code>, which processes each chunk one at a time. Finally, the output of <code>NodeA</code>  is fed into the table create operation, which writes each chunk as it receives it rather than waiting for <code>NodeA</code> to finish processing every chunk.</p> <p>Some notable examples of operations that do not currently use vectorized execution:</p> <ul> <li>Some set operators (<code>INTERSECT</code> and <code>MINUS</code>).</li> <li>Aggregations without <code>GROUP BY</code>.</li> </ul> <p>Additionally, BodoSQL's support for vectorized execution of window functions is a limited and experimental feature.</p>"},{"location":"api_docs/sql/sql_udfs/","title":"User Defined Functions (UDFs) and User Defined Table Functions (UDTFs)","text":"<p>BodoSQL supports using Snowflake UDFs and UDTFs in queries and views. To make UDFs and UDTFs available in BodoSQL, you must first register and define them inside your Snowflake account using the appropriate <code>create function</code> command. Once the function is created, so long as your user can access the function's metadata, BodoSQL can process queries that use the function.</p>"},{"location":"api_docs/sql/sql_udfs/#usage","title":"Usage","text":"<p>A UDF is used like any other SQL function, except that there are two possible calling conventions.</p> <p><code>MY_UDF(arg1, arg2, ..., argN)</code></p> <p><code>MY_UDF(name1=&gt;arg1, name2=&gt;arg2, ..., nameN=&gt;argN)</code></p> <p>When calling a function you must either pass all arguments positionally or by name (you cannot mix these). If you pass the arguments by name, then you can pass them in any order. For example, the following calls are are equivalent.</p> <pre><code>select my_udf(name1=&gt;1, name2=&gt;2) as A, my_udf(name2=&gt;2, name1=&gt;1) as B\n</code></pre> <p>When calling a UDTF you must wrap the function in a <code>TABLE()</code> call and then you may use the function anywhere a table can be used. For example:</p> <pre><code>select * from table(my_udtf(1))\n</code></pre> <p>To reference columns from another table in the UDTF, you can use a comma join, optionally alongside the <code>lateral</code> keyword. For example:</p> <pre><code>select * from my_table, table(my_udtf(N=&gt;A))\n</code></pre> <p>or</p> <pre><code>select * from my_table, LATERAL(table(my_udtf(N=&gt;A)))\n</code></pre>"},{"location":"api_docs/sql/sql_udfs/#calling-convention-best-practices","title":"Calling Convention Best Practices","text":"<p>When calling either a UDF or a UDTF, we strongly recommend always using the named calling convention. This is because UDFs support overloaded definitions and using distinct names is the safest way to ensure you are calling the correct function. For more information see this section of the Snowflake Documentation. Even if you are not currently using an overloaded function, we encourage this practice in case the function is overloaded in the future.</p>"},{"location":"api_docs/sql/sql_udfs/#requirements","title":"Requirements","text":"<p>BodoSQL must be able to execute the UDF directly from its definition. To do this, BodoSQL needs to be able to both obtain the definition and execute it, producing the following requirements:</p> <ul> <li>The function must be written in SQL.</li> <li>All elements of the function body must be supported within BodoSQL.</li> <li>The user executing Bodo must have access to any tables or views referenced   within the function body.</li> <li>The function must not be defined using the secure keyword.</li> <li>The function must not be defined using the external keyword.</li> </ul> <p>In addition, there are a couple other limitations to be aware of due to gaps in the available metadata:</p> <ul> <li>At this time, we cannot support default values because the default is not stored in   the metadata. These functions can still be executed by providing the default values.</li> <li>Some special characters in argument names, especially commas or spaces, may not compile   because they are not properly escaped within the Snowflake metadata.</li> </ul>"},{"location":"api_docs/sql/sql_udfs/#performance","title":"Performance","text":"<p>BodoSQL supports UDFs and UDTFs by inlining the function body directly into the body of the query. This means that users of these functions should achieve the same performance as if they had written the function body directly into the query.</p> <p>For complex UDFs or UDTFs, naively executing the function body may require producing a correlated subquery, an operation in which a query must be executed once per row in another table. This can cause a significant performance hit, so BodoSQL undergoes a process called decorrelation to rewrite the query in terms of much more efficient joins. If BodoSQL is not able to rewrite a query, then it will raise an error indicating a correlation could not be fully removed.</p>"},{"location":"api_docs/sql/sql_udfs/#overloaded-definition-priority","title":"Overloaded Definition Priority","text":"<p>As mentioned above, Snowflake UDFs support overloaded definitions. This means that you can define the same function name multiple times with different argument signatures, and a function will be selected by determining the \"best match\", possibly through implicit casting.</p> <p>BodoSQL supports this functionality, but if there is no exact match, then BodoSQL cannot guarantee equivalent Snowflake behavior. Snowflake states which implicit casts are legal, but it provides no promises as to which function will be selected in the case of multiple possible matches requiring implicit casts.</p> <p>When BodoSQL encounters a UDF call, without an exact match, we look at the implicit cast priority of each possible UDF defintions as shown in the table below.</p> <p> Source Type Target Option 1 Target Option 2 Target Option 3 Target Option 4 Target Option 5 Target Option 6 Target Option 7 Target Option 8 Target Option 9 Target Option 10 ARRAY VARIANT BOOLEAN VARCHAR VARIANT DATE TIMESTAMP_LTZ TIMESTAMP_NTZ VARCHAR VARIANT DOUBLE BOOLEAN VARIANT VARCHAR NUMBER NUMBER DOUBLE BOOLEAN VARIANT VARCHAR OBJECT VARIANT TIME VARCHAR TIMESTAMP_NTZ TIMESTAMP_LTZ VARCHAR DATE TIME VARIANT TIMESTAMP_LTZ TIMESTAMP_NTZ VARCHAR DATE TIME VARIANT VARCHAR BOOLEAN DATE DOUBLE TIMESTAMP_LTZ TIMESTAMP_NTZ NUMBER TIME VARIANT VARIANT ARRAY BOOLEAN OBJECT VARCHAR DATE TIME TIMESTAMP_LTZ TIMESTAMP_NTZ DOUBLE NUMBER <p></p> <p>Here, the lower the option number, the higher the priority, with exact matches having priority 0 and being omitted. If there is no function with an exact match then we compute the closest signature by computing the \"priority\" of the required cast for each argument based on the above table and selecting the implementation with the smallest sum of distances. If we encounter a tie then we select the earliest defined function based on the metadata. While this may not match Snowflake in all situations, we have found that in common cases (e.g., differing by a single argument), this gives us behavior consistent with Snowflake.</p> <p>However, as we add further type support or expand our UDF infrastructure, this matching system is subject to change. As a result, we strongly recommend using a unique name for each argument and only using the named calling convention to avoid any potential issues.</p>"},{"location":"api_docs/sql/tablepath/","title":"TablePath API","text":"<p>The <code>TablePath</code> API is a general purpose IO interface to specify IO sources. This API is meant as an alternative to natively loading tables in Python inside JIT functions. The <code>TablePath</code> API stores the user-defined data location and the storage type to load a table of interest. For example, here is some sample code that loads two DataFrames from parquet using the <code>TablePath</code> API.</p> <pre><code>bc = bodosql.BodoSQLContext(\n    {\n        \"T1\": bodosql.TablePath(\"my_file_path1.pq\", \"parquet\"),\n        \"T2\": bodosql.TablePath(\"my_file_path2.pq\", \"parquet\"),\n    }\n)\n\n@bodo.jit\ndef f(bc):\n    return bc.sql(\"select t1.A, t2.B from t1, t2 where t1.C &gt; 5 and t1.D = t2.D\")\n</code></pre> <p>Here, the <code>TablePath</code> constructor doesn't load any data. Instead, a <code>BodoSQLContext</code> internally generates code to load the tables of interest after parsing the SQL query. Note that a <code>BodoSQLContext</code> loads all used tables from I/O on every query, which means that if users would like to perform multiple queries on the same data, they should consider loading the DataFrames once in a separate JIT function.</p>"},{"location":"api_docs/sql/tablepath/#api-reference","title":"API Reference","text":"<ul> <li> <p><code>bodosql.TablePath(file_path: str, file_type: str, *, conn_str: Optional[str] = None, reorder_io: Optional[bool] = None, statistics_file: Optional[str] = None)</code> </p> <p>Specifies how a DataFrame should be loaded from IO by a BodoSQL query. This can only load data when used with a <code>BodoSQLContext</code> constructor.</p> <p>Arguments</p> <ul> <li> <p><code>file_path</code>: Path to IO file or name of the table for SQL. This must constant at compile time if used inside JIT.</p> </li> <li> <p><code>file_type</code>: Type of file to load as a string. Supported values are <code>\"parquet\"</code> and <code>\"sql\"</code>. This must constant at compile time if used inside JIT.</p> </li> <li> <p><code>conn_str</code>: Connection string used to connect to a SQL DataBase, equivalent to the conn argument to <code>pandas.read_sql</code>. This must be constant at compile time if used inside JIT and must be None if not loading from a SQL DataBase.</p> </li> <li> <p><code>reorder_io</code>: Boolean flag determining when to load IO. If <code>False</code>, all used tables are loaded before executing any of the query. If <code>True</code>, tables are loaded just before first use inside the query, which often results in decreased peak memory usage as each table is partially processed before loading the next table. The default value, <code>None</code>, behaves like <code>True</code>, but this may change in the future. This must be constant at compile time if used inside JIT.</p> </li> <li> <p><code>statistics_file</code>: Path to a statistics file (JSON) for the table. This is only supported for <code>\"parquet\"</code> file type. The supported keys are <code>\"row_count\"</code> and <code>\"ndv\"</code>. <code>\"row_count\"</code>, if provided, should be the number of rows in the Parquet dataset. <code>\"ndv\"</code>, if provided, should be a dictionary mapping the column names to the estimated number of distinct values in the column. It is valid to provide the NDV estimates for only some of the columns.</p> </li> </ul> </li> </ul>"},{"location":"api_docs/sql/ddl/","title":"DDL","text":"<p>BodoSQL currently supports the following Data Definition Language (DDL) statements.</p> <ul> <li>ALTER TABLE</li> <li>ALTER VIEW</li> <li>CREATE SCHEMA</li> <li>CREATE TABLE</li> <li>CREATE VIEW</li> <li>DESCRIBE SCHEMA</li> <li>DESCRIBE TABLE</li> <li>DESCRIBE VIEW</li> <li>DROP SCHEMA</li> <li>DROP TABLE</li> <li>DROP VIEW</li> <li>SHOW OBJECTS</li> <li>SHOW SCHEMAS</li> <li>SHOW TABLES</li> <li>SHOW TBLPROPERTIES</li> <li>SHOW VIEWS</li> </ul>"},{"location":"api_docs/sql/ddl/alter_table/","title":"ALTER TABLE","text":"<p>Modifies the properties, columns, or constraints for an existing table from the current/specified schema.</p> <p>See the Snowflake documentation and Iceberg documentation for more details.</p> <p>Currently, BodoSQL only supports the following operations:</p>"},{"location":"api_docs/sql/ddl/alter_table/#renaming-a-table","title":"Renaming a table","text":"<pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt; RENAME TO &lt;new_table_name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/alter_table/#adding-columns","title":"Adding columns","text":"<pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt; \n    ADD [ COLUMN ] [ IF NOT EXISTS ] &lt;new_column_name&gt; &lt;column_datatype&gt;\n</code></pre> Warning <ul> <li> <p>This operation is currently only supported for Iceberg.</p> </li> <li> <p>Only a subset of Iceberg types are supported for <code>ADD COLUMN</code>. The syntax corresponding to the Iceberg types are as follows:</p> Syntax Iceberg Type DECIMAL, NUMERIC decimal(38, 0) NUMBER(P, S), DECIMAL(P, S) decimal(p, s) INT, INTEGER, SMALLINT, TINYINT, BYTEINT int BIGINT long FLOAT, FLOAT4, FLOAT8 float DOUBLE, DOUBLE PRECISION, REAL double VARCHAR, CHAR, CHARACTER, STRING, TEXT, BINARY, VARBINARY string BOOLEAN boolean DATE date TIME time DATETIME, TIMESTAMP, TIMESTAMP_NTZ timestamp <p>Note that adding nested types such as <code>struct&lt;x: double, y: double&gt;</code> is not supported yet. As such, column names including periods are disallowed in order to prevent ambiguity.</p> </li> </ul>"},{"location":"api_docs/sql/ddl/alter_table/#dropping-columns","title":"Dropping columns","text":"<pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt; \n    DROP [ COLUMN ] [ IF EXISTS ] &lt;column_name&gt; [ , &lt;column_name&gt;, ...]\n</code></pre> Note <ul> <li> <p>This operation is currently only supported for Iceberg.</p> </li> <li> <p><code>DROP COLUMN</code> can be used to drop nested columns and fields of structs.</p> <p>To do so, use <code>.</code> separated field names:</p> <pre><code>-- Example\nALTER TABLE tblname DROP COLUMN colname.fieldname\n</code></pre> <p>Multiple nested columns are also supported:</p> <pre><code>-- Example\nALTER TABLE tblname DROP COLUMN colname.structname.fieldname\n</code></pre> </li> </ul>"},{"location":"api_docs/sql/ddl/alter_table/#renaming-columns","title":"Renaming columns","text":"<pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt; \n    RENAME COLUMN &lt;column_name&gt; TO &lt;new_column_name&gt;\n</code></pre> Note <ul> <li>This operation is currently only supported for Iceberg.</li> <li>Nested columns can also be renamed. For example, <pre><code>ALTER TABLE table1 RENAME COLUMN column1.field1 TO field2\n</code></pre> will rename the nested field <code>field1</code> within the <code>column1</code> struct to <code>field2</code>.</li> </ul>"},{"location":"api_docs/sql/ddl/alter_table/#altering-columns","title":"Altering columns","text":"<pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt;\n    ALTER [ COLUMN ] &lt;column_name&gt; alterColumnAction\n</code></pre> <p>Currently, the following options for <code>alterColumnAction</code> are supported:</p>"},{"location":"api_docs/sql/ddl/alter_table/#setting-column-comments","title":"Setting column comments","text":"<pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt;\n    ALTER [ COLUMN ] &lt;column_name&gt; COMMENT 'comment_string'\n</code></pre>"},{"location":"api_docs/sql/ddl/alter_table/#changing-nullability-of-columns","title":"Changing nullability of columns","text":"<pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt;\n    ALTER [ COLUMN ] &lt;column_name&gt; DROP NOT NULL\n</code></pre> <p>This will change a required column (a column that cannot hold NULL values) to an optional column.</p> <p>Note</p> <p>All <code>ALTER COLUMN</code> operations are currently only supported for Iceberg.</p>"},{"location":"api_docs/sql/ddl/alter_table/#setting-unsetting-table-properties","title":"Setting / unsetting table properties","text":"<p><code>ALTER TABLE SET</code> is used to set table-wide properties. If a particular property was already set, this overrides the old value with the new one. <code>ALTER TABLE UNSET</code> is used to drop table properties.</p> <p>Note</p> <p>This operation is currently only supported for Iceberg.</p> <pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt; \n    SET ( PROPERTY | PROPERTIES | TAG | TAGS | TBLPROPERTY | TBLPROPERTIES ) \n    '&lt;tag_name&gt;' = '&lt;tag_value&gt;' [ , '&lt;tag_name&gt;' = '&lt;tag_value&gt;' ... ]\n</code></pre> <pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt; \n    UNSET ( PROPERTY | PROPERTIES | TAG | TAGS | TBLPROPERTY | TBLPROPERTIES ) \n    [ IF EXISTS ] '&lt;tag_name&gt;'[ , '&lt;tag_name&gt;' ... ]\n</code></pre>"},{"location":"api_docs/sql/ddl/alter_table/#setting-unsetting-table-comments","title":"Setting / unsetting table comments","text":"<p>This operation functions as an alias for <code>ALTER TABLE SET PROPERTY COMMENT='comment'</code>.</p> <p>Note</p> <p>This operation is currently only supported for Iceberg.</p> <pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt; SET COMMENT '&lt;comment&gt;'\n</code></pre> <pre><code>ALTER TABLE [ IF EXISTS ] &lt;name&gt; UNSET COMMENT\n</code></pre>"},{"location":"api_docs/sql/ddl/alter_view/","title":"ALTER VIEW","text":"<p>Modifies the properties for an existing view from the current/specified schema.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/alter_view/#syntax","title":"Syntax","text":"<p>Currently, BodoSQL only supports the following syntax:</p> <pre><code>ALTER VIEW [ IF EXISTS ] &lt;name&gt; RENAME TO &lt;new_name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/create_schema/","title":"CREATE SCHEMA","text":"<p>Creates a new schema in the current/specified database.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/create_schema/#syntax","title":"Syntax","text":"<p>Currently, BodoSQL only supports the following syntax:</p> <pre><code>CREATE [ OR REPLACE ] SCHEMA [ IF NOT EXISTS ] &lt;name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/create_table/","title":"CREATE TABLE","text":"<p>Creates a new table in the current/specified schema.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/create_table/#syntax","title":"Syntax","text":"<p>Currently, BodoSQL only supports the <code>CREATE TABLE ... AS SELECT</code> (CTAS) form, and only with the following syntax:</p> <pre><code>CREATE [ OR REPLACE ] TABLE [ IF NOT EXISTS ] &lt;name&gt;\n[ ( &lt;col_name&gt; [ &lt;col_type&gt; ] , &lt;col_name&gt; [ &lt;col_type&gt; ] , ... ) ]\n[ COMMENT = &lt;string&gt; ] \n[ TAG | TAGS | PROPERTY | PROPERTIES | TBLPROPERTY | TBLPROPERTIES (&lt;key1&gt; = &lt;value1&gt;, &lt;key2&gt; = &lt;value2&gt;, ...) ]\nAS &lt;query&gt;\n</code></pre> <p>BodoSQL can parse additional forms of <code>CREATE TABLE</code> syntax, though they currently do not have any effects:</p> <ul> <li>The <code>TRANSIENT</code> keyword</li> <li>The <code>LOCAL</code>, <code>GLOBAL</code>, <code>TEMP</code>, <code>TEMPORARY</code> or <code>VOLATILE</code> keywords</li> <li>A <code>COPY GRANTS</code> clause</li> <li>A <code>CLUSTER BY</code> clause</li> </ul>"},{"location":"api_docs/sql/ddl/create_view/","title":"CREATE VIEW","text":"<p>Creates a new view in the current/specified schema.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/create_view/#syntax","title":"Syntax","text":"<p>Currently, BodoSQL only supports the following syntax:</p> <pre><code>CREATE [ OR REPLACE ] VIEW [ IF NOT EXISTS ] &lt;name&gt; \n[ ( &lt;column_list&gt; ) ]\nas &lt;select statement&gt;\n</code></pre> <p>BodoSQL can parse additional forms of <code>CREATE VIEW</code> syntax purely for the purposes of allowing view inlining:</p> <ul> <li>The <code>SECURE</code> keyword</li> <li>The <code>LOCAL</code>, <code>GLOBAL</code>, <code>TEMP</code>, <code>TEMPORARY</code> or <code>VOLATILE</code> keywords</li> <li>The <code>RECURSIVE</code> keyword</li> <li>A <code>COPY GRANTS</code> clause</li> <li>A view-level <code>COMMENT</code></li> <li>A view-level <code>WITH TAG</code> clause</li> <li>A view-level <code>ROW ACCESS POLICY</code> clause</li> </ul>"},{"location":"api_docs/sql/ddl/describe_schema/","title":"DESCRIBE SCHEMA","text":"<p>Describes the schema. For example, lists the tables and views in the schema.</p> <p>For Iceberg catalogs, lists the tables and views of the namespace.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/describe_schema/#syntax","title":"Syntax","text":"<pre><code>DESC[RIBE] SCHEMA &lt;name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/describe_schema/#output","title":"Output","text":"<p>The output provides object properties and metadata in the following columns:</p> Column DESCRIPTION CREATED_ON The timestamp at which the object was created, as a string type. NAME The name of the object. KIND The kind of the object. <p>For Iceberg tables, the <code>CREATED_ON</code> field will be <code>None</code>.</p>"},{"location":"api_docs/sql/ddl/describe_schema/#examples","title":"Examples","text":"<pre><code>DESCRIBE SCHEMA sample_schema;\n\n+-------------------------------+----------------+-------------------+\n| CREATED_ON                    | NAME           | KIND              |\n|-------------------------------+----------------+-------------------|\n| 2022-06-23 01:00:00.000 -0700 | SAMPLE_TABLE_1 | TABLE             |\n| 2022-06-23 02:00:00.000 -0700 | SAMPLE_VIEW_1  | VIEW              |\n+-------------------------------+----------------+-------------------+\n</code></pre>"},{"location":"api_docs/sql/ddl/describe_table/","title":"DESCRIBE TABLE","text":"<p>Describes either the columns in a table or the current values, as well as the default values, for the stage properties for a table.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/describe_table/#syntax","title":"Syntax","text":"<p>Currently, BodoSQL only supports the following syntax:</p> <pre><code>DESC[RIBE] TABLE &lt;name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/describe_view/","title":"DESCRIBE VIEW","text":"<p>Describes the columns in a view.</p> <p>This function can also be used to describe the columns in a table.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/describe_view/#syntax","title":"Syntax","text":"<p>Currently, BodoSQL only supports the following syntax:</p> <pre><code>DESC[RIBE] VIEW &lt;name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/drop_schema/","title":"DROP SCHEMA","text":"<p>Removes a schema from the current/specified database.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/drop_schema/#syntax","title":"Syntax","text":"<p>Currently, BodoSQL only supports the following syntax:</p> <pre><code>DROP SCHEMA [ IF EXISTS ]\n</code></pre>"},{"location":"api_docs/sql/ddl/drop_table/","title":"DROP TABLE","text":"<p>Removes a table from the current/specified schema. </p> <p>BodoSQL supports the optional keyword <code>PURGE</code> which specifies that the underlying data and metadata files should be deleted, as opposed to just making the table no longer accessible. </p> <p>The effect of this keyword depends on which catalog is used.  For a Snowflake catalog, it is a no-op since Snowflake does not currently support the purge command.  For Iceberg catalogs, the command may tell the catalog that the data files are marked for deletion, but they may or may not be deleted right away.</p> <p>The optional keyword <code>CASCADE</code> or <code>RESTRICT</code> is a no-op in Iceberg catalogs, but it is used in Snowflake to specify whether the table can be dropped if foreign keys exist that reference the table.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/drop_table/#syntax","title":"Syntax","text":"<p>Currently, BodoSQL only supports the following syntax:</p> <pre><code>DROP TABLE [ IF EXISTS ] &lt;name&gt; [ CASCADE | RESTRICT ] [ PURGE ]\n</code></pre>"},{"location":"api_docs/sql/ddl/drop_view/","title":"DROP VIEW","text":"<p>Removes a view from the current/specified schema.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/drop_view/#syntax","title":"Syntax","text":"<p>Currently, BodoSQL only supports the following syntax:</p> <pre><code>DROP VIEW [ IF EXISTS ] &lt;name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/show_objects/","title":"SHOW OBJECTS","text":"<p>Lists the tables and views for which you have access privileges for a specified database or schema.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/show_objects/#syntax","title":"Syntax","text":"<pre><code>SHOW [ TERSE ] OBJECTS IN [&lt;database_name&gt;.]&lt;schema_name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/show_objects/#usage-notes","title":"Usage notes","text":"<p><code>SHOW OBJECTS</code> returns the following columns as <code>string</code> types unless otherwise mentioned:</p> <ul> <li><code>CREATED_ON</code></li> <li><code>NAME</code></li> <li><code>SCHEMA_NAME</code></li> <li><code>KIND</code></li> <li><code>COMMENT</code></li> <li><code>CLUSTER_BY</code></li> <li><code>ROWS</code> - type <code>Decimal(38, 0)</code></li> <li><code>BYTES</code> - type <code>Decimal(38, 0)</code></li> <li><code>OWNER</code></li> <li><code>RETENTION_TIME</code></li> <li><code>OWNER_ROLE_TYPE</code></li> </ul> <p>See the Snowflake documentation for descriptions of the columns.</p> <p>For Iceberg catalogs, only a subset of the columns are supported. The rest will always be set to <code>NULL</code>. Supported columns are:</p> <ul> <li><code>NAME</code></li> <li><code>SCHEMA_NAME</code></li> <li><code>KIND</code></li> <li><code>COMMENT</code></li> </ul> <p>The <code>TERSE</code> option will return only the following output columns, regardless of catalog:</p> <ul> <li><code>CREATED_ON</code></li> <li><code>NAME</code></li> <li><code>SCHEMA_NAME</code></li> <li><code>KIND</code></li> </ul> <p>All columns will be of type <code>string</code>.</p>"},{"location":"api_docs/sql/ddl/show_schemas/","title":"SHOW SCHEMAS","text":"<p>Lists the schemas for which you have access privileges for a specified database.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/show_schemas/#syntax","title":"Syntax","text":"<pre><code>SHOW [ TERSE ] SCHEMAS IN &lt;database_name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/show_schemas/#usage-notes","title":"Usage notes","text":"<p><code>SHOW SCHEMAS</code> returns the following columns as <code>string</code> types unless otherwise mentioned:</p> <ul> <li><code>CREATED_ON</code></li> <li><code>NAME</code></li> <li><code>IS_DEFAULT</code></li> <li><code>IS_CURRENT</code></li> <li><code>DATABASE_NAME</code></li> <li><code>OWNER</code></li> <li><code>COMMENT</code></li> <li><code>OPTIONS</code></li> <li><code>RETENTION_TIME</code></li> <li><code>OWNER_ROLE_TYPE</code></li> </ul> <p>See the Snowflake documentation for descriptions of the columns.</p> <p>For Iceberg catalogs, only a subset of the columns are supported. The rest will always be set to <code>NULL</code>. Supported columns are:</p> <ul> <li><code>NAME</code></li> <li><code>DATABASE_NAME</code></li> </ul> <p>The <code>TERSE</code> option will return only the following output columns, regardless of catalog:</p> <ul> <li><code>CREATED_ON</code></li> <li><code>NAME</code></li> <li><code>SCHEMA_NAME</code></li> <li><code>KIND</code></li> </ul> <p>All columns will be of type <code>string</code>.</p>"},{"location":"api_docs/sql/ddl/show_tables/","title":"SHOW TABLES","text":"<p>Lists the tables for which you have access privileges for a specified database or schema.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/show_tables/#syntax","title":"Syntax","text":"<pre><code>SHOW [ TERSE ] TABLES IN [&lt;database_name&gt;.]&lt;schema_name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/show_tables/#usage-notes","title":"Usage notes","text":"<p><code>SHOW TABLES</code> returns the following columns as <code>string</code> types unless otherwise mentioned:</p> <ul> <li><code>CREATED_ON</code></li> <li><code>NAME</code></li> <li><code>SCHEMA_NAME</code></li> <li><code>KIND</code></li> <li><code>COMMENT</code></li> <li><code>CLUSTER_BY</code></li> <li><code>ROWS</code> - type <code>Decimal(38, 0)</code></li> <li><code>BYTES</code> - type <code>Decimal(38, 0)</code></li> <li><code>OWNER</code></li> <li><code>RETENTION_TIME</code></li> <li><code>AUTOMATIC_CLUSTERING</code></li> <li><code>CHANGE_TRACKING</code></li> <li><code>IS_EXTERNAL</code></li> <li><code>ENABLE_SCHEMA_EVOLUTION</code></li> <li><code>OWNER_ROLE_TYPE</code></li> <li><code>IS_EVENT</code></li> <li><code>IS_HYBRID</code></li> <li><code>IS_ICEBERG</code></li> <li><code>IS_IMMUTABLE</code></li> </ul> <p>See the Snowflake documentation for descriptions of the columns.</p> <p>For Iceberg catalogs, only a subset of the columns are supported. The rest will always be set to <code>NULL</code>. Supported columns are:</p> <ul> <li><code>NAME</code></li> <li><code>SCHEMA_NAME</code></li> <li><code>KIND</code></li> <li><code>COMMENT</code></li> <li><code>IS_ICEBERG</code></li> </ul> <p>The <code>TERSE</code> option will return only the following output columns, regardless of catalog:</p> <ul> <li><code>CREATED_ON</code></li> <li><code>NAME</code></li> <li><code>SCHEMA_NAME</code></li> <li><code>KIND</code></li> </ul> <p>All columns will be of type <code>string</code>.</p>"},{"location":"api_docs/sql/ddl/show_tblproperties/","title":"SHOW TBLPROPERTIES","text":"<p>Returns the value of a table property given an optional value for a property key. If no key is specified then all the properties are returned.</p> <p>Note</p> <p>This operation is an Iceberg-only operation.</p>"},{"location":"api_docs/sql/ddl/show_tblproperties/#syntax","title":"Syntax","text":"<pre><code>SHOW ( TBLPROPERTIES | PROPERTIES | TAGS ) &lt;table_identifier&gt; [ ('property_key') ] \n</code></pre>"},{"location":"api_docs/sql/ddl/show_tblproperties/#examples","title":"Examples","text":"<pre><code>SHOW TBLPROPERTIES my_table;\n\n  +---------------------+----------+\n  |key                  |value     |\n  +---------------------+----------+\n  |property_1           |value_1   |\n  |property_2           |value_2   |\n  |property_3           |value_3   |\n  +---------------------+----------+\n\nSHOW TBLPROPERTIES my_table ('property_1');\n\n  +----------+\n  |value     |\n  +----------+\n  |value_1   |\n  +----------+\n</code></pre>"},{"location":"api_docs/sql/ddl/show_views/","title":"SHOW VIEWS","text":"<p>Lists the views for which you have access privileges for a specified database or schema.</p> <p>See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/ddl/show_views/#syntax","title":"Syntax","text":"<pre><code>SHOW [ TERSE ] VIEWS IN [&lt;database_name&gt;.]&lt;schema_name&gt;\n</code></pre>"},{"location":"api_docs/sql/ddl/show_views/#usage-notes","title":"Usage notes","text":"<p><code>SHOW VIEWS</code> returns the following columns as <code>string</code> types unless otherwise mentioned:</p> <ul> <li><code>CREATED_ON</code></li> <li><code>NAME</code></li> <li><code>RESERVED</code></li> <li><code>SCHEMA_NAME</code></li> <li><code>COMMENT</code></li> <li><code>OWNER</code></li> <li><code>TEXT</code></li> <li><code>IS_SECURE</code></li> <li><code>IS_MATERIALIZED</code></li> <li><code>OWNER_ROLE_TYPE</code></li> <li><code>CHANGE_TRACKING</code></li> </ul> <p>See the Snowflake documentation for descriptions of the columns.</p> <p>For Iceberg catalogs, only a subset of the columns are supported. The rest will always be set to <code>NULL</code>. Supported columns are:</p> <ul> <li><code>NAME</code></li> <li><code>SCHEMA_NAME</code></li> <li><code>COMMENT</code></li> </ul> <p>The <code>TERSE</code> option will return only the following output columns, regardless of catalog:</p> <ul> <li><code>CREATED_ON</code></li> <li><code>NAME</code></li> <li><code>SCHEMA_NAME</code></li> <li><code>KIND</code></li> </ul> <p>All columns will be of type <code>string</code>.</p>"},{"location":"api_docs/sql/dml/","title":"DML","text":"<p>BodoSQL currently supports the following Data Modification Language (DML) statements.</p> <ul> <li>Insert Into</li> </ul>"},{"location":"api_docs/sql/dml/insert_into/","title":"INSERT INTO","text":"<p>Updates a table by inserting one or more rows into the table.</p> <p>See the Snowflake documentation for more details.</p> <p>Currently, BodoSQL only supports the following syntax:</p> <pre><code>INSERT INTO &lt;name&gt; [ ( &lt;target_col_name&gt; [ , ... ] ) ]\n  {\n    VALUES ( { &lt;value&gt; } [ , ... ] ) [ , ( ... ) ]  |\n    &lt;query&gt;\n  }\n</code></pre>"},{"location":"api_docs/sql/functions/operators/","title":"Operators","text":""},{"location":"api_docs/sql/functions/operators/#arithmetic","title":"Arithmetic","text":"<p>BodoSQL currently supports the following arithmetic operators:</p> <ul> <li><code>+</code> (addition)</li> <li><code>-</code> (subtraction)</li> <li><code>*</code> (multiplication)</li> <li><code>/</code> (true division)</li> <li><code>%</code> (modulo)</li> </ul>"},{"location":"api_docs/sql/functions/operators/#comparison","title":"Comparison","text":"<p>BodoSQL currently supports the following comparison operators:</p> <ul> <li><code>=</code> (equal to)</li> <li><code>&gt;</code> (greater than)</li> <li><code>&lt;</code> (less than)</li> <li><code>&gt;=</code> (greater than or equal to)</li> <li><code>&lt;=</code> (less than or equal to)</li> <li><code>&lt;&gt;</code> (not equal to)</li> <li><code>!=</code> (not equal to)</li> <li><code>&lt;=&gt;</code> (equal to or both inputs are null)</li> </ul>"},{"location":"api_docs/sql/functions/operators/#logical","title":"Logical","text":"<p>BodoSQL currently supports the following logical operators:</p> <ul> <li><code>AND</code></li> <li><code>OR</code></li> <li><code>NOT</code></li> </ul>"},{"location":"api_docs/sql/functions/operators/#string","title":"String","text":"<p>BodoSQL currently supports the following string operators:</p> <ul> <li><code>||</code> (string concatenation)</li> </ul>"},{"location":"api_docs/sql/functions/regex/","title":"Regex Functions","text":"<p>BodoSQL currently uses Python's regular expression library via the <code>re</code> module. Although this may be subject to change, it means that there are several deviations from the behavior of Snowflake's regular expression functions (see here for snowflake documentation). The key points and major deviations are noted below:</p> <ul> <li> <p>Snowflake uses a superset of the POSIX ERE regular expression syntax. This means that BodoSQL can utilize several syntactic forms of regular expressions that Snowflake cannot (see here for Python re documentation). However, there are several features that POSIX ERE has that Python's <code>re</code> does not:</p> </li> <li> <p>POSIX character classes (see here for a full list). BodoSQL does support these as macros for character sets. In other words, <code>[[:lower:]]</code> is transformed into <code>[a-z]</code>. However, this form of replacement cannot be escaped. Additionally, any character classes that are supposed to include the null terminator <code>\\x00</code> instead start at <code>\\x01</code></p> </li> <li> <p>Equivalence classes (not supported by BodoSQL).</p> </li> <li> <p>Returning the longest match when using alternation patterns (BodoSQL returns the leftmost match).</p> </li> <li> <p>The regex functions can optionally take in a flag argument. The flag is a string whose characters control how matches to patterns occur. The following characters have meaning when contained in the flag string:</p> </li> <li> <p><code>'c'</code>: case-sensitive matching (the default behavior)</p> </li> <li><code>'i'</code>: case-insensitive matching (if both 'c' and 'i' are provided, whichever one occurs last is used)</li> <li><code>'m'</code>: allows anchor patterns to interact with the start/end of each line, not just the start/end of the entire string.</li> <li><code>'s'</code>: allows the <code>.</code> metacharacter to capture newline characters</li> <li> <p><code>'e'</code>: see <code>REGEXP_SUBSTR</code>/<code>REGEXP_INSTR</code></p> </li> <li> <p>Currently, BodoSQL supports the lazy <code>?</code> operator whereas Snowflake does not. So for example, in Snowflake, the pattern <code>`(.*?),'</code> would match with as many characters as possible so long as the last character was a comma. However, in BodoSQL, the match would end as soon as the first comma.</p> </li> <li> <p>Currently, BodoSQL supports the following regexp features which should crash when done in Snowflake: <code>(?...)</code>, <code>\\A</code>, <code>\\Z</code>, <code>\\1</code>, <code>\\2</code>, <code>\\3</code>, etc.</p> </li> <li> <p>Currently, BodoSQL requires the pattern argument and the flag argument (if provided) to be string literals as opposed to columns or expressions.</p> </li> <li> <p>Currently, extra backslashes may be required to escape certain characters if they have meaning in Python. The amount of backslashes required to properly escape a character depends on the usage.</p> </li> <li> <p>All matches are non-overlapping.</p> </li> <li> <p>If any of the numeric arguments are zero or negative, or the <code>group_num</code> argument is out of bounds, an error is raised. The only exception is <code>REGEXP_REPLACE</code>, which allows its occurrence argument to be zero.</p> </li> </ul> <p>BodoSQL currently supports the following regex functions:</p>"},{"location":"api_docs/sql/functions/regex/#regexp_like","title":"REGEXP_LIKE","text":"<ul> <li> <p><code>REGEXP_LIKE(str, pattern[, flag])</code></p> <p>Returns <code>true</code> if the entire string matches with the pattern. If <code>flag</code> is not provided, <code>''</code> is used.</p> <p>If the pattern is empty, then <code>true</code> is returned if the string is also empty.</p> <p>For example:</p> <ul> <li> <p>2 arguments: Returns <code>true</code> if <code>A</code> is a 5-character string where the first character is an a, the last character is a z, and the middle 3 characters are also lowercase characters (case-sensitive). <pre><code>SELECT REGEXP_LIKE(A, 'a[a-z]{3}z')\n</code></pre></p> </li> <li> <p>3 arguments: Returns <code>true</code> if <code>A</code> starts with the letters <code>'THE'</code> (case-insensitive). <pre><code>SELECT REGEXP_LIKE(A, 'THE.*', 'i')\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"api_docs/sql/functions/regex/#regexp_count","title":"REGEXP_COUNT","text":"<ul> <li> <p><code>REGEXP_COUNT(str, pattern[, position[, flag]])</code></p> <p>Returns the number of times the string contains matches to the pattern, starting at the location specified by the <code>position</code> argument (with 1-indexing). If <code>position</code> is not provided, <code>1</code> is used. If <code>flag</code> is not provided, <code>''</code> is used.</p> <p>If the pattern is empty, 0 is returned.</p> <p>For example:</p> <ul> <li> <p>2 arguments: Returns the number of times that any letters occur in <code>A</code>. <pre><code>SELECT REGEXP_COUNT(A, '[[:alpha:]]')\n</code></pre></p> </li> <li> <p>3 arguments: Returns the number of times that any digit characters occur in <code>A</code>, not including the first 5 characters. <pre><code>SELECT REGEXP_COUNT(A, '\\d', 6)\n</code></pre></p> </li> <li> <p>4 arguments: Returns the number of times that a substring occurs in <code>A</code> that contains two ones with any character (including newlines) in between. <pre><code>SELECT REGEXP_COUNT(A, '1.1', 1, 's')\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"api_docs/sql/functions/regex/#regexp_replace","title":"REGEXP_REPLACE","text":"<ul> <li> <p><code>REGEXP_REPLACE(str, pattern[, replacement[, position[, occurrence[, flag]]]])</code></p> <p>Returns the version of the inputted string where each match to the pattern is replaced by the replacement string, starting at the location specified by the <code>position</code> argument (with 1-indexing). The occurrence argument specifies which match to replace, where 0 means replace all occurrences. If <code>replacement</code> is not provided, <code>''</code> is used. If <code>position</code> is not provided, <code>1</code> is used. If <code>occurrence</code> is not provided, <code>0</code> is used. If <code>flag</code> is not provided, <code>''</code> is used.</p> <p>If there are an insufficient number of matches, or the pattern is empty, the original string is returned.</p> <p>Note</p> <p>back-references in the replacement pattern are supported, but may require additional backslashes to work correctly.</p> <p>For example:</p> <ul> <li> <p>2 arguments: Deletes all whitespace in <code>A</code>. <pre><code>SELECT REGEXP_REPLACE(A, '[[:space:]]')\n</code></pre></p> </li> <li> <p>3 arguments: Replaces all occurrences of <code>'hate'</code> in <code>A</code> with <code>'love'</code> (case-sensitive). <pre><code>SELECT REGEXP_REPLACE(A, 'hate', 'love')\n</code></pre></p> </li> <li> <p>4 arguments: Replaces all occurrences of two consecutive digits in <code>A</code> with the same two digits reversed, excluding the first 2 characters. <pre><code>SELECT REGEXP_REPLACE(A, '(\\d)(\\d)', '\\\\\\\\2\\\\\\\\1', 3)\n</code></pre></p> </li> <li> <p>5 arguments: Replaces the first character in <code>A</code> with an underscore. <pre><code>SELECT REGEXP_REPLACE(A, '^.', '_', 1, 2)\n</code></pre></p> </li> <li> <p>6 arguments: Removes the first and last word from each line of <code>A</code> that contains at least 3 words. <pre><code>SELECT REGEXP_REPLACE(A, '^\\w+ (.*) \\w+$', '\\\\\\\\1', 0, 1, 'm')\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"api_docs/sql/functions/regex/#regexp_substr","title":"REGEXP_SUBSTR","text":"<ul> <li> <p><code>REGEXP_SUBSTR(str, pattern[, position[, occurrence[, flag[, group_num]]]])</code></p> <p>Returns the substring of the original string that caused a match with the pattern, starting at the location specified by the <code>position</code> argument (with 1-indexing). The occurrence argument specifies which match to extract (with 1-indexing). If <code>position</code> is not provided, <code>1</code> is used. If <code>occurrence</code> is not provided, <code>1</code> is used. If <code>flag</code> is not provided, <code>''</code> is used. If <code>group_num</code> is not provided, and <code>flag</code> contains <code>'e</code>', <code>1</code> is used. If <code>group_num</code> is provided but the flag does not contain <code>e</code>, then it behaves as if it did. If the flag does contain <code>e</code>, then one of the subgroups of the match is returned instead of the entire match. The subgroup returned corresponds to the <code>group_num</code> argument (with 1-indexing).</p> <p>If there are an insufficient number of matches, or the pattern is empty, <code>NULL</code> is returned.</p> <p>For example:</p> <ul> <li> <p>2 arguments: Returns the first number that occurs inside of <code>A</code>. <pre><code>SELECT REGEXP_SUBSTR(A, '\\d+')\n</code></pre></p> </li> <li> <p>3 arguments: Returns the first punctuation symbol that occurs inside of <code>A</code>, excluding the first 10 characters. <pre><code>SELECT REGEXP_SUBSTR(A, '[[:punct:]]', 11)\n</code></pre></p> </li> <li> <p>4 arguments: Returns the fourth occurrence of two consecutive lowercase vowels in <code>A</code>. <pre><code>SELECT REGEXP_SUBSTR(A, '[aeiou]{2}', 1, 4)\n</code></pre></p> </li> <li> <p>5 arguments: Returns the first 3+ character substring of <code>A</code> that starts with and ends with a vowel (case-insensitive, and it can contain newline characters). <pre><code>SELECT REGEXP_SUBSTR(A, '[aeiou].+[aeiou]', 1, 1, 'im')\n</code></pre></p> </li> <li> <p>6 arguments: Looks for third occurrence in <code>A</code> of a number followed by a colon, a space, and a word that starts with <code>'a'</code> (case-sensitive) and returns the word that starts with <code>'a'</code>. <pre><code>SELECT REGEXP_SUBSTR(A, '(\\d+): (a\\w+)', 1, 3, 'e', 2)\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"api_docs/sql/functions/regex/#regexp_instr","title":"REGEXP_INSTR","text":"<ul> <li> <p><code>REGEXP_INSTR(str, pattern[, position[, occurrence[, option[, flag[, group_num]]]]])</code></p> <p>Returns the location within the original string that caused a match with the pattern, starting at the location specified by the <code>position</code> argument (with 1-indexing). The occurrence argument specifies which match to extract (with 1-indexing). The option argument specifies whether to return the start of the match (if <code>0</code>) or the first location after the end of the match (if <code>1</code>). If <code>position</code> is not provided, <code>1</code> is used. If <code>occurrence</code> is not provided, <code>1</code> is used. If <code>option</code> is not provided, <code>0</code> is used. If <code>flag</code> is not provided, <code>''</code> is used. If <code>group_num</code> is not provided, and <code>flag</code> contains <code>'e</code>', <code>1</code> is used. If <code>group_num</code> is provided but the flag does not contain <code>e</code>, then it behaves as if it did. If the flag does contain <code>e</code>, then the location of one of the subgroups of the match is returned instead of the location of the entire match. The subgroup returned corresponds to the <code>group_num</code> argument (with 1-indexing).</p> <p>If there are an insufficient number of matches, or the pattern is empty, <code>0</code> is returned.</p> <ul> <li> <p>2 arguments: Returns the index of the first <code>'#'</code> in <code>A</code>. <pre><code>SELECT REGEXP_INSTR(A, '#')\n</code></pre></p> </li> <li> <p>3 arguments: Returns the starting index of the first occurrence of 3 consecutive digits in <code>A</code>, excluding the first 3 characters.  ```sql SELECT REGEXP_INSTR(A, '\\d{3}', 4) <pre><code>- 4 arguments: Returns the starting index of the 9th word sandwiched between angle brackets in `A`.\n```sql\nSELECT REGEXP_INSTR(A, '&lt;\\w+&gt;', 1, 9)\n</code></pre></p> </li> <li> <p>5 arguments: Returns the ending index of the first substring of <code>A</code> that starts and ends with non-ascii characters. <pre><code>SELECT REGEXP_INSTR(A, '[^[:ascii:]].*[^[:ascii:]]', 1, 1, 1)\n</code></pre></p> </li> <li> <p>6 arguments: Returns the starting index of the second line of <code>A</code> that begins with an uppercase vowel. <pre><code>SELECT REGEXP_INSTR(A, '^[AEIOU].*', 1, 2, 0, 'm')\n</code></pre></p> </li> <li> <p>7 arguments: Looks for the first substring of <code>A</code> that has the format of a name in a phonebook (i.e. <code>Lastname, Firstname</code>) and returns the starting index of the first name. <pre><code>SELECT REGEXP_INSTR(A, '([[:upper]][[:lower:]]+), ([[:upper]][[:lower:]]+)', 1, 1, 0, 'e', 2)\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"api_docs/sql/functions/agg/","title":"Aggregation &amp; Window Functions","text":"<p>An aggregation function can be used to combine data across many rows to form a single answer. Aggregations can be done with a <code>GROUP BY</code> clause, in which case the combined value is calculated once per unique combination of groupbing keys. Aggregations can also be done without the <code>GROUP BY</code> clause, in which case a single value is outputted by calculating the aggregation across all rows.</p> <p>For example:</p> <pre><code>SELECT AVG(A) FROM table1 GROUP BY B\n\nSELECT COUNT(Distinct A) FROM table1\n</code></pre> <p>Window functions can be used to compute an aggregation across a row and its surrounding rows. Most window functions have the following syntax:</p> <p><pre><code>SELECT WINDOW_FN(ARG1, ..., ARGN) OVER (PARTITION BY PARTITION_COLUMN_1, ..., PARTITION_COLUMN_N ORDER BY SORT_COLUMN_1, ..., SORT_COLUMN_N ROWS BETWEEN &lt;LOWER_BOUND&gt; AND &lt;UPPER_BOUND&gt;) FROM table_name\n</code></pre> The <code>ROWS BETWEEN ROWS BETWEEN &lt;LOWER_BOUND&gt; AND &lt;UPPER_BOUND&gt;</code> section is the window's \"frame\" used to specify the window over which to compute the function. A bound can can come before the current row, using <code>PRECEDING</code> or after the current row, using <code>FOLLOWING</code>. The bounds can be relative (i.e. <code>N PRECEDING</code> or <code>N FOLLOWING</code>), where <code>N</code> is a positive integer, or they can be absolute (i.e. <code>UNBOUNDED PRECEDING</code> or <code>UNBOUNDED FOLLOWING</code>).</p> <p>For example, consider the following window function calls:</p> <p><pre><code>SELECT\n    SUM(A) OVER () as S1,\n    SUM(A) OVER (PARTITION BY B) as S2,\n    SUM(A) OVER (PARTITION BY B ORDER BY C ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as S3,\n    SUM(A) OVER (ORDER BY C ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as S4,\nFROM table1\n</code></pre> This query computes 4 sums, and returns 1 row for every row in the original table:</p> <ul> <li><code>S1</code>: The sum of each row of <code>A</code> across the entire table. </li> <li><code>S2</code>: The sum of all values of <code>A</code> within each partition of <code>B</code>.</li> <li><code>S3</code>: The cumulative sum of each row and all rows before it when the rows are partitioned by B the sorted by C.</li> <li><code>S4</code>: The the sum each row with the row before &amp; after it in the entire table when ordered by C.</li> </ul> <p>Note</p> <p>For most window functions, BodoSQL returns <code>NULL</code> if the specified window frame is empty or all <code>NULL</code>. Exceptions to this behavior are noted.</p> <p>All window functions optionally allow <code>PARTITION BY</code>. Some window functions optionally allow <code>ORDER BY</code>, and some may actually require it. Some window functions optionally allow window frames, and others ban it. If a function supports window frames but one is not provided, the default frame behavior depends on the function.</p> <p>Note</p> <p><code>RANGE BETWEEN</code> is not currently supported.</p> <p>Note</p> <p>If a window frame contains <code>NaN</code> values, the output may diverge from Snowflake's behavior. When a <code>NaN</code> value enters a window, any window function that combines the results with arithmetic (e.g. <code>SUM</code>, <code>AVG</code>, <code>VARIANCE</code>, etc.) will output <code>NaN</code> until the <code>NaN</code> value has exited the window.</p> <p>BodoSQL Currently supports the following Aggregation &amp; Window functions:</p> Function Supported with GROUP BY? Supported without GROUP BY? Supported as window function? (WINDOW) Requires ORDER BY? (WINDOW) Allows frame? <code>ANY_VALUE</code> Y Y Y N N <code>APPROX_PERCENTILE</code> N Y Y N N <code>ARRAY_AGG</code> Y N N N/A N/A <code>ARRAY_UNIQUE_AGG</code> Y N N N/A N/A <code>AVG</code> Y Y Y N Y <code>BITAND_AGG</code> Y Y Y N N <code>BITOR_AGG</code> Y Y Y N N <code>BITXOR_AGG</code> Y Y Y N N <code>BOOLAND_AGG</code> Y Y Y N N <code>BOOLOR_AGG</code> Y Y Y N N <code>BOOLXOR_AGG</code> Y Y Y N N <code>CONDITIONAL_CHANGE_EVENT</code> N N Y Y N <code>CONDITIONAL_TRUE_EVENT</code> N N Y Y N <code>CORR</code> N N Y N N <code>COUNT</code> Y Y Y N Y <code>COUNT(*)</code> Y Y Y N Y <code>COUNT_IF</code> Y Y Y N Y <code>COVAR_POP</code> N N Y N N <code>COVAR_SAMP</code> N N Y N N <code>CUME_DIST</code> N N Y Y N <code>DENSE_RANK</code> N N Y Y N <code>FIRST_VALUE</code> N N Y N Y <code>KURTOSIS</code> Y Y Y N N <code>LEAD</code> N N Y Y N <code>LAST_VALUE</code> N N Y N Y <code>LAG</code> N N Y Y N <code>LISTAGG</code> Y Y N N/A N/A <code>MAX</code> Y Y Y N Y <code>MEDIAN</code> Y Y Y N N <code>MIN</code> Y Y Y N Y <code>MODE</code> Y N Y N N <code>NTH_VALUE</code> N N Y N Y <code>NTILE</code> N N Y Y N <code>OBJECT_AGG</code> Y N Y N N <code>PERCENTILE_CONT</code> Y Y N N/A N/A <code>PERCENTILE_DISC</code> Y Y N N/A N/A <code>PERCENT_RANK</code> N N Y Y N <code>RANK</code> N N Y Y N <code>RATIO_TO_REPORT</code> N N Y N N <code>ROW_NUMBER</code> N N Y Y N <code>SKEW</code> Y Y Y N N <code>STDDEV</code> Y Y Y N Y <code>STDDEV_POP</code> Y Y Y N Y <code>STDDEV_SAMP</code> Y Y Y N Y <code>SUM</code> Y Y Y N Y <code>VARIANCE</code> Y Y Y N Y <code>VARIANCE_POP</code> Y Y Y N Y <code>VARIANCE_SAMP</code> Y Y Y N Y <code>VAR_POP</code> Y Y Y N Y <code>VAR_SAMP</code> Y Y Y N Y"},{"location":"api_docs/sql/functions/agg/any_value/","title":"ANY_VALUE","text":"<p><code>ANY_VALUE</code></p> <p>Select an arbitrary value from the column/group/window. Supported on all types.</p> <p>Note</p> <p>Currently, BodoSQL always selects the first value, but this is subject to change at any time.</p>"},{"location":"api_docs/sql/functions/agg/approx_percentile/","title":"APPROX_PERCENTILE","text":"<p><code>APPROX_PERCENTILE(A, q)</code></p> <p>Returns the approximate value of the <code>q</code>-th percentile of column <code>A</code> (e.g. 0.5 = median, or 0.9 = the 90<sup>th</sup> percentile). <code>A</code> can be any numeric column, and <code>q</code> can be any scalar float between zero and one.</p> <p>The approximation is calculated using the t-digest algorithm.</p>"},{"location":"api_docs/sql/functions/agg/array_agg/","title":"ARRAY_AGG","text":"<p><code>ARRAY_AGG([DISTINCT] A) [WITHIN GROUP(ORDER BY orderby_terms)]</code></p> <p>Combines all the values in column <code>A</code> within each group into a single array.</p> <p>Optionally allows using a <code>WITHIN GROUP</code> clause to specify how the values should be ordered before being combined into an array. If no clause is specified, then the ordering is unpredictable. Nulls will not be included in the arrays.</p> <p>If the <code>DISTINCT</code> keyword is provided, then duplicate elements are removed from each of the arrays. However, if this keyword is provied and a <code>WITHIN GROUP</code> clause is also provided, then the <code>WITHIN GROUP</code> clause can only refer to the same column as the aggregation input.</p>"},{"location":"api_docs/sql/functions/agg/array_unique_agg/","title":"ARRAY_UNIQUE_AGG","text":"<p><code>ARRAY_UNIQUE_AGG(A)</code></p> <p>Equivalent to <code>ARRAY_AGG(DISTINCT A)</code></p>"},{"location":"api_docs/sql/functions/agg/avg/","title":"AVG","text":"<p><code>AVG</code></p> <p>Compute the mean of the the column/group/window. Supported on all numeric types.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/bitand_agg/","title":"BITAND_AGG","text":"<p><code>BITAND_AGG</code></p> <p>Compute the bitwise AND of every input in a column/group/window, returning <code>NULL</code> if there are no non-<code>NULL</code> entries. Accepts floating point values, integer values, and strings. Strings are interpreted directly as numbers, converting to 64-bit floating point numbers.</p>"},{"location":"api_docs/sql/functions/agg/bitor_agg/","title":"BITOR_AGG","text":"<p><code>BITOR_AGG</code></p> <p>Compute the bitwise OR of every input in a column/group/window, returning <code>NULL</code> if there are no non-<code>NULL</code> entries. Accepts floating point values, integer values, and strings. Strings are interpreted directly as numbers, converting to 64-bit floating point numbers.</p>"},{"location":"api_docs/sql/functions/agg/bitxor_agg/","title":"BITXOR_AGG","text":"<p><code>BITXOR_AGG</code></p> <p>Compute the bitwise XOR of every input in a column/group/window, returning <code>NULL</code> if there are no non-<code>NULL</code> entries. Accepts floating point values, integer values, and strings. Strings are interpreted directly as numbers, converting to 64-bit floating point numbers.</p>"},{"location":"api_docs/sql/functions/agg/booland_agg/","title":"BOOLAND_AGG","text":"<p><code>BOOLAND_AGG</code></p> <p>Compute the logical AND of the boolean value of every input in a column/group/window, returning <code>NULL</code> if there are no non-<code>NULL</code> entries, otherwise returning True if all non-<code>NULL</code> entries are also non-zero. This is supported for numeric and boolean types.</p>"},{"location":"api_docs/sql/functions/agg/boolor_agg/","title":"BOOLOR_AGG","text":"<p><code>BOOLOR_AGG</code></p> <p>Compute the logical OR of the boolean value of every input in a column/group/window, returning <code>NULL</code> if there are no non-<code>NULL</code> entries, otherwise returning True if there is at least 1 non-zero entry. This is supported for numeric and boolean types.</p>"},{"location":"api_docs/sql/functions/agg/boolxor_agg/","title":"BOOLXOR_AGG","text":"<p><code>BOOLXOR_AGG</code></p> <p>Returns <code>NULL</code> if there are no non-<code>NULL</code> entries, otherwise returning True if exactly one non-<code>NULL</code> entry is also non-zero (this is counterintuitive to how the logical XOR is normally thought of). This is supported for numeric and boolean types.</p>"},{"location":"api_docs/sql/functions/agg/conditional_change_event/","title":"CONDITIONAL_CHANGE_EVENT","text":"<p><code>CONDITIONAL_CHANGE_EVENT(COLUMN_EXPRESSION)</code></p> <p>Computes a counter within each partition that starts at zero and increases by 1 each time the value inside the window changes. <code>NULL</code> does not count as a new/changed value. <code>ORDER BY</code> is required for this function.</p>"},{"location":"api_docs/sql/functions/agg/conditional_true_event/","title":"CONDITIONAL_TRUE_EVENT","text":"<p><code>CONDITIONAL_TRUE_EVENT(BOOLEAN_COLUMN_EXPRESSION)</code></p> <p>Computes a counter within each partition that starts at zero and increases by 1 each time the boolean column's value is <code>true</code>. <code>ORDER BY</code> is required for this function.</p>"},{"location":"api_docs/sql/functions/agg/corr/","title":"CORR","text":"<p><code>CORR(Y, X)</code></p> <p>Compute the correlation over the window of both inputs, or <code>NULL</code> if the window is empty. Equivalent to <code>COVAR(Y, X) / (STDDEV_POP(Y) * STDDEV_POP(X))</code></p>"},{"location":"api_docs/sql/functions/agg/count/","title":"COUNT","text":"<p><code>COUNT</code></p> <p>Count the number of non-null elements in the column/group/window. Supported on all types. If used with the syntax <code>COUNT(*)</code> returns the total number of rows instead of non-null rows.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/count_if/","title":"COUNT_IF","text":"<p><code>COUNT_IF</code></p> <p>Compute the total number of occurrences of <code>true</code> in a column/group/window of booleans. For example:</p> <pre><code>SELECT COUNT_IF(A) FROM table1\n</code></pre> <p>Is equivalent to <pre><code>SELECT SUM(CASE WHEN A THEN 1 ELSE 0 END) FROM table1\n</code></pre></p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/covar_pop/","title":"COVAR_POP","text":"<p><code>COVAR_POP(Y, X)</code></p> <p>Compute the population covariance over the window of both inputs, or <code>NULL</code> if the window is empty. Supported on all numeric types.</p>"},{"location":"api_docs/sql/functions/agg/covar_samp/","title":"COVAR_SAMP","text":"<p><code>COVAR_SAMP(Y, X)</code></p> <p>Compute the sample covariance over the window of both inputs, or <code>NULL</code> if the window is empty. Supported on all numeric types.</p>"},{"location":"api_docs/sql/functions/agg/cume_dist/","title":"CUME_DIST","text":"<p><code>CUME_DIST()</code></p> <p>Compute the cumulative distribution of the value(s) in each row based on the value(s) relative to all value(s) within the window partition. <code>ORDER BY</code> is required for this function.</p>"},{"location":"api_docs/sql/functions/agg/dense_rank/","title":"DENSE_RANK","text":"<p><code>DENSE_RANK()</code></p> <p>Compute the rank of each row based on the value(s) in the row relative to all value(s) within the partition without producing gaps in the rank (compare with <code>RANK</code>). The rank begins with 1 and increments by one for each succeeding value. Rows with the same value(s) produce the same rank. <code>ORDER BY</code> is required for this function.</p> <p>Note</p> <p>To compare <code>RANK</code> and <code>DENSE_RANK</code>, on input array <code>['a', 'b', 'b', 'c']</code>, <code>RANK</code> will output <code>[1, 2, 2, 4]</code> while <code>DENSE_RANK</code> outputs <code>[1, 2, 2, 3]</code>.</p>"},{"location":"api_docs/sql/functions/agg/first_value/","title":"FIRST_VALUE","text":"<p><code>FIRST_VALUE(COLUMN_EXPRESSION)</code></p> <p>Select the first value in the window or <code>NULL</code> if the window is empty. Supported on all non-semi-structured types.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/kurtosis/","title":"KURTOSIS","text":"<p><code>KURTOSIS</code></p> <p>Compute the kurtosis of a column or <code>NULL</code> if the window contains fewer than 4 non-<code>NULL</code> entries. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p>"},{"location":"api_docs/sql/functions/agg/lag/","title":"LAG","text":"<p><code>LAG(COLUMN_EXPRESSION, [N], [FILL_VALUE])</code></p> <p>Returns the row that precedes the current row by N. If N is not specified, defaults to 1. If FILL_VALUE is not specified, defaults to <code>NULL</code>. If there are fewer than N rows the follow the current row in the window, it returns FILL_VALUE. N must be a literal integer if specified. FILL_VALUE must be a scalar if specified. Supported on all non-semi-structured types.</p>"},{"location":"api_docs/sql/functions/agg/last_value/","title":"LAST_VALUE","text":"<p><code>LAST_VALUE(COLUMN_EXPRESSION)</code></p> <p>Select the last value in the window or <code>NULL</code> if the window is empty. Supported on all non-semi-structured types.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/lead/","title":"LEAD","text":"<p><code>LEAD(COLUMN_EXPRESSION, [, N[, FILL_VALUE]])</code></p> <p>Equivalent to  <code>LEAD(COLUMN_EXPRESSION, -N, FILL_VALUE)</code>, in other words, returns the row following the current row by N.</p>"},{"location":"api_docs/sql/functions/agg/listagg/","title":"LISTAGG","text":"<p><code>LISTAGG(str_col[, delimeter]) [WITHIN GROUP (ORDER BY order_col)]</code></p> <p>Concatenates all the strings in <code>str_col</code> within each group into a single string seperated by the characters in the string <code>delimiter</code>. If no delimiter is provided, an empty string is used by default.</p> <p>Optionally allows using a <code>WITHIN GROUP</code> clause to specify how the strings should be ordered before being concatenated. If no clause is specified, then the ordering is unpredictable.</p> <p>Returns <code>''</code> if the input is all <code>NULL</code> or empty.</p>"},{"location":"api_docs/sql/functions/agg/max/","title":"MAX","text":"<p><code>MAX</code></p> <p>Compute the maximum value in the column/group/window. Supported on all non-semi-structured types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/median/","title":"MEDIAN","text":"<p><code>MEDIAN(COLUMN_EXPRESSION)</code></p> <p>Compute the median over the  column/group/window. Supported on all numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p>"},{"location":"api_docs/sql/functions/agg/min/","title":"MIN","text":"<p><code>MIN</code></p> <p>Compute the minimum value in the column/group/window. Supported on all types. Supported on all non-semi-structured types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/mode/","title":"MODE","text":"<p><code>MODE</code></p> <p>Returns the most frequent element in a column/group/window, including <code>#sql NULL</code> if that is the element that appears the most. Supported on all non-semi-structured types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>In case of a tie, BodoSQL will choose a value arbitrarily based on performance considerations.</p>"},{"location":"api_docs/sql/functions/agg/nth_value/","title":"NTH_VALUE","text":"<p><code>NTH_VALUE(COLUMN_EXPRESSION, N)</code></p> <p>Select the last value in the window or <code>NULL</code> if the window does not have <code>N</code> elements. Uses 1-indexing. Requires <code>N</code> to be a positive integer literal.  Supported on all non-semi-structured types.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/ntile/","title":"NTILE","text":"<p><code>NTILE(N)</code></p> <p>Divides the partitioned groups into N buckets based on ordering. For example if N=3 and there are 30 rows in a partition, the first 10 are assigned 1, the next 10 are assigned 2, and the final 10 are assigned 3. In cases where the number of rows cannot be evenly divided by the number of buckets, the first buckets will have one more value than the last bucket. For example, if N=4 and there are 22 rows in a partition, the first 6 are assigned 1, the next 6 are assigned 2, the next 5 are assigned 3, and the last 5 are assigned 4.</p>"},{"location":"api_docs/sql/functions/agg/object_agg/","title":"OBJECT_AGG","text":"<p><code>OBJECT_AGG(K, V)</code></p> <p>Combines the data from columns <code>K</code> and <code>V</code> into a JSON object where the rows of column <code>K</code> are the field names and the rows of column <code>V</code> are the values. Any row where <code>K</code> or <code>V</code> is <code>NULL</code> is not included in the final object. If the group is empty or all the rows are not included, an empty object is returned.</p>"},{"location":"api_docs/sql/functions/agg/percent_rank/","title":"PERCENT_RANK","text":"<p><code>PERCENT_RANK()</code></p> <p>Compute the percentage ranking of the value(s) in each row based on the value(s) relative to all value(s) within the window partition. Ranking calculated using <code>RANK()</code> divided by the number of rows in the window partition minus one. Partitions with one row have <code>PERCENT_RANK()</code> of 0. <code>ORDER BY</code> is required for this function.</p>"},{"location":"api_docs/sql/functions/agg/percentile_cont/","title":"APPROX_PEPERCENTILE_CONTRCENTILE","text":"<p><code>APPROX_PEPERCENTILE_CONTRCENTILE(q) WITHIN GROUP (ORDER BY A)</code></p> <p>Computes the exact value of the <code>q</code>-th percentile of column <code>A</code> (e.g. 0.5 = median, or 0.9 = the 90<sup>th</sup> percentile). <code>A</code> can be any numeric column, and <code>q</code> can be any scalar float between zero and one.</p> <p>If no value lies exactly at the desired percentile, the two nearest values are linearly interpolated. For example, consider the dataset <code>[2, 8, 25, 40]</code>. If we sought the percentile <code>q=0.25</code> we would be looking for the value at index 0.75. There is no value at index 0.75, so we linearly interpolate between 2 and 8 to get 6.5.</p>"},{"location":"api_docs/sql/functions/agg/percentile_disc/","title":"PERCENTILE_DISC","text":"<p><code>PERCENTILE_DISC(q) WITHIN GROUP (ORDER BY A)</code></p> <p>Computes the exact value of the <code>q</code>-th percentile of column <code>A</code> (e.g. 0.5 = median, or 0.9 = the 90<sup>th</sup> percentile). <code>A</code> can be any numeric column, and <code>q</code> can be any scalar float between zero and one.</p> <p>This function differs from <code>PERCENTILE_CONT</code> in that it always outputs a value from the original array. The value it chooses is the smallest value in <code>A</code> such that the <code>CUME_DIST</code> of all values in the column <code>A</code> is greater than or equal to <code>q</code>. For example, consider the dataset <code>[2, 8, 8, 40]</code>. The <code>CUME_DIST</code> of each of these values is <code>[0.25, 0.75, 0.75, 1.0]</code>. If we sought the percentile <code>q=0.6</code> we would output 8 since it has the smallest <code>CUME_DIST</code> that is <code>&gt;=0.6</code>.</p>"},{"location":"api_docs/sql/functions/agg/rank/","title":"RANK","text":"<p><code>RANK()</code></p> <p>Compute the rank of each row based on the value(s) in the row relative to all value(s) within the partition. The rank begins with 1 and increments by one for each succeeding value. Duplicate value(s) will produce the same rank, producing gaps in the rank (compare with <code>DENSE_RANK</code>). <code>ORDER BY</code> is required for this function.</p>"},{"location":"api_docs/sql/functions/agg/ratio_to_report/","title":"RATIO_TO_REPORT","text":"<p><code>RATIO_TO_REPORT(COLUMN_EXPRESSION)</code></p> <p>Returns an element in the window frame divided by the sum of all elements in the same window frame, or <code>NULL</code> if the window frame has a sum of zero. For example, if calculating <code>RATIO_TO_REPORT</code> on <code>[2, 5, NULL, 10, 3]</code> where the window is the entire partition, the answer is <code>[0.1, 0.25, NULL, 0.5, 0.15]</code>.</p>"},{"location":"api_docs/sql/functions/agg/row_number/","title":"ROW_NUMBER","text":"<p><code>ROW_NUMBER()</code></p> <p>Compute an increasing row number (starting at 1) for each row. This function cannot be used with <code>ROWS BETWEEN</code>.</p> <p>Note</p> <p>This window function is supported without a partition.</p>"},{"location":"api_docs/sql/functions/agg/skew/","title":"SKEW","text":"<p><code>SKEW</code></p> <p>Compute the skew of a column/group/window or <code>NULL</code> if the window contains fewer than 3 non-<code>NULL</code> entries. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p>"},{"location":"api_docs/sql/functions/agg/stddev/","title":"STDDEV","text":"<p><code>STDDEV</code></p> <p>Compute the standard deviation of the column/group/window with N - 1 degrees of freedom. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/stddev_pop/","title":"STDDEV_POP","text":"<p><code>STDDEV_POP</code></p> <p>Compute the standard deviation of the column/group/window with N degrees of freedom. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/stddev_samp/","title":"STDDEV_SAMP","text":"<p><code>STDDEV_SAMP</code></p> <p>Compute the standard deviation of the column/group/window with N - 1 degrees of freedom. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/sum/","title":"SUM","text":"<p><code>SUM</code></p> <p>Compute the sum of the column/group/window. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/var_pop/","title":"VAR_POP","text":"<p><code>VAR_POP</code></p> <p>Compute the variance for a column/group/window with N degrees of freedom. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/var_samp/","title":"VAR_SAMP","text":"<p><code>VAR_SAMP</code></p> <p>Compute the variance for a column/group/window with N - 1 degrees of freedom. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/variance/","title":"VARIANCE","text":"<p><code>VARIANCE</code></p> <p>Compute the variance for a column/group/window with N - 1 degrees of freedom. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/variance_pop/","title":"VARIANCE_POP","text":"<p><code>VARIANCE_POP</code></p> <p>Compute the variance for a column/group/window with N degrees of freedom. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/agg/variance_samp/","title":"VARIANCE_SAMP","text":"<p><code>VARIANCE_SAMP</code></p> <p>Compute the variance for a column/group/window with N - 1 degrees of freedom. Supported on numeric types.</p> <p>Returns <code>NULL</code> if the input is all <code>NULL</code> or empty.</p> <p>Note</p> <p>When used as a window function with an <code>ORDER BY</code> clause but no window frame, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> is used by default.</p>"},{"location":"api_docs/sql/functions/array/","title":"Array Functions","text":"<p>Array functions are used to manipulate arrays. BodoSQL supports the following array functions:</p> <ul> <li><code>!# sql ARRAY_CAT</code></li> <li><code>!# sql ARRAY_COMPACT</code></li> <li><code>!# sql ARRAY_CONSTRUCT</code></li> <li><code>!# sql ARRAY_CONSTRUCT_COMPACT</code></li> <li><code>!# sql ARRAY_CONTAINS</code></li> <li><code>!# sql ARRAY_EXCEPT</code></li> <li><code>!# sql ARRAY_INTERSECTION</code></li> <li><code>!# sql ARRAY_POSITION</code></li> <li><code>!# sql ARRAY_REMOVE</code></li> <li><code>!# sql ARRAY_REMOVE_AT</code></li> <li><code>!# sql ARRAY_SIZE</code></li> <li><code>!# sql ARRAY_SLICE</code></li> <li><code>!# sql ARRAY_TO_STRING</code></li> <li><code>!# sql ARRAYS_OVERLAP</code></li> <li><code>!# sql GET</code></li> <li><code>!# sql GET_IGNORE_CASE</code></li> </ul>"},{"location":"api_docs/sql/functions/array/array_cat/","title":"ARRAY_CAT","text":"<p><code>ARRAY_CAT(A, B)</code></p> <p>Takes in two arrays and returns an array of all the elements from the first array followed by all the elements in the second array.</p>"},{"location":"api_docs/sql/functions/array/array_compact/","title":"ARRAY_COMPACT","text":"<p><code>ARRAY_COMPACT(arr)</code></p> <p>Returns a compacted array with missing and null values removed from <code>arr</code>, effectively converting sparse arrays into dense arrays. Return <code>NULL</code> when <code>arr</code> is <code>NULL</code>.</p>"},{"location":"api_docs/sql/functions/array/array_construct/","title":"ARRAY_CONSTRUCT","text":"<p><code>ARRAY_CONSTRUCT(A, B, C, ...)</code></p> <p>Takes in a variable number of arguments and produces an array containing all of those values (including any null values).</p> <p>Note</p> <p>Snowflake allows any number of arguments (even zero arguments) of any type. BodoSQL currently requires 1+ arguments, and requires all arguments to be easily reconciled into a common type.</p>"},{"location":"api_docs/sql/functions/array/array_construct_compact/","title":"ARRAY_CONSTRUCT_COMPACT","text":"<p><code>ARRAY_CONSTRUCT_COMPACT(A, B, C, ...)</code></p> <p>Equivalent to  <code>ARRAY_COMPACT(ARRAY_CONSTRUCT(A, B, C, ...))</code></p>"},{"location":"api_docs/sql/functions/array/array_contains/","title":"ARRAY_CONTAINS","text":"<p><code>ARRAY_CONTAINS(elem, arr)</code></p> <p>Returns true if <code>elem</code> is an element of <code>arr</code>, or <code>NULL</code> if <code>arr</code> is <code>NULL</code>. The input <code>elem</code> can be <code>NULL</code>, in which case the funciton will check if <code>arr</code> contains <code>NULL</code>.</p>"},{"location":"api_docs/sql/functions/array/array_except/","title":"ARRAY_EXCEPT","text":"<p><code>ARRAY_EXCEPT(A, B)</code></p> <p>Takes in two arrays and returns a copy of the first array but with all of the elements from the second array dropped. If an element appears in the first array more than once, that element is only dropped as many times as it appears in the second array. For instance, if the first array contains three 1s and four 6s, and the second array contains two 1s and one 6, then the output will have one 1 and three 6s.</p>"},{"location":"api_docs/sql/functions/array/array_intersection/","title":"ARRAY_INTERSECTION","text":"<p><code>ARRAY_INTERSECTION(A, B)</code></p> <p>Takes in two arrays and returns an arary of all the elements from the first array that also appear in the second. If an element appears in either array more than once, that element is kept the minimum of the number of times it appears in either array. For instance, if the first array contains three 1s and four 6s, and the second array contains two 1s and five 6s, then the output will have two 1s and three 6s.</p>"},{"location":"api_docs/sql/functions/array/array_position/","title":"ARRAY_POSITION","text":"<p><code>ARRAY_POSITION(elem, arr)</code></p> <p>Returns the index of the first occurrence of <code>elem</code> in <code>arr</code> (using zero indexing), or <code>NULL</code> if there are no occurrences. The input <code>elem</code> can be <code>NULL</code>, in which case the funciton will look for the first <code>NULL</code> in the array input.</p>"},{"location":"api_docs/sql/functions/array/array_remove/","title":"ARRAY_REMOVE","text":"<p><code>ARRAY_REMOVE(array, to_remove)</code></p> <p>Given a source <code>array</code>, returns an array with all elements equal to the specified value <code>to_remove</code> removed. Returns <code>NULL</code> if <code>array</code> or <code>to_remove</code> is <code>NULL</code>.</p>"},{"location":"api_docs/sql/functions/array/array_remove_at/","title":"ARRAY_REMOVE_AT","text":"<p><code>ARRAY_REMOVE_AT(array, pos)</code></p> <p>Given a source <code>array</code>, returns an array with the element at the specified position <code>pos</code> removed. Returns <code>NULL</code> if <code>array</code> or <code>pos</code> is <code>NULL</code>. Negative indexing is supported. No element is removed of the index <code>pos</code> is out of bound.</p>"},{"location":"api_docs/sql/functions/array/array_size/","title":"ARRAY_SIZE","text":"<p><code>ARRAY_SIZE(array)</code></p> <p>Returns the size of array, size includes inner elements that are <code>NULL``. Returns</code>NULL<code>if array is</code>NULL`. See here for Snowflake documentation.</p>"},{"location":"api_docs/sql/functions/array/array_slice/","title":"ARRAY_SLICE","text":"<p><code>ARRAY_SLICE(arr, from, to)</code></p> <p>Returns an array constructed from a specified subset of elements of the input array <code>arr[from:to]</code>. Returns <code>NULL</code> if one of <code>arr</code>, <code>from</code> and <code>to</code> is <code>NULL</code>.</p>"},{"location":"api_docs/sql/functions/array/array_to_string/","title":"ARRAY_TO_STRING","text":"<p><code>ARRAY_TO_STRING(arr, sep)</code></p> <p>Converted the input array <code>arr</code> to a string by casting all values to strings (using <code>TO_VARCHAR</code>) and concatenating them (using <code>sep</code> to separate the elements).</p>"},{"location":"api_docs/sql/functions/array/arrays_overlap/","title":"ARRAYS_OVERLAP","text":"<p><code>ARRAYS_OVERLAP(arr0, arr1)</code></p> <p>Returns true if the two array arguments <code>arr0</code> and <code>arr1</code> have at least 1 element in common (including <code>NULL</code>).</p>"},{"location":"api_docs/sql/functions/array/get/","title":"GET","text":"<ul> <li><code>GET(arr, idx)</code></li> <li><code>GET(object, field)</code></li> <li><code>arr[idx]</code></li> <li><code>object[field]</code></li> </ul> <p>Returns the element found at the specified index in the array, or the specified field of an object.</p> <p>When indexing into an array: indexing is 0 based, not 1 based. Returns <code>NULL</code> if the index is outside of the boundaries of the array. The index must be an integer.</p> <p>When retrieving a field from an object: the field name must be a string. If the object is a struct, the field name must be a string literal. If the object is a map, it can be a non-constant string. Returns <code>NULL</code> if the field name is not found. Field name matching is case-sensitive.</p>"},{"location":"api_docs/sql/functions/array/get_ignore_case/","title":"GET_IGNORE_CASE","text":"<p><code>GET_IGNORE_CASE(object, idx)</code></p> <p>Returns the value for the key in the map/struct, ignoring case. Returns NULL if the key does not exist inside the map/struct.</p>"},{"location":"api_docs/sql/functions/casting/","title":"Casting / Conversion Functions","text":"<p>BodoSQL currently supports the following casting/conversion functions:</p> <ul> <li><code>TO_ARRAY</code></li> <li><code>TO_BINARY</code></li> <li><code>TO_BOOLEAN</code></li> <li><code>TO_CHAR</code></li> <li><code>TO_DATE</code></li> <li><code>TO_DECIMAL</code></li> <li><code>TO_DOUBLE</code></li> <li><code>TO_NUMBER</code></li> <li><code>TO_NUMERIC</code></li> <li><code>TO_OBJECT</code></li> <li><code>TO_TIME</code></li> <li><code>TO_TIMESTAMP</code></li> <li><code>TO_TIMESTAMP_LTZ</code></li> <li><code>TO_TIMESTAMP_NTZ</code></li> <li><code>TO_TIMESTAMP_TZ</code></li> <li><code>TO_VARCHAR</code></li> <li><code>TRY_TO_BINARY</code></li> <li><code>TRY_TO_BOOLEAN</code></li> <li><code>TRY_TO_DATE</code></li> <li><code>TRY_TO_DECIMAL</code></li> <li><code>TRY_TO_DOUBLE</code></li> <li><code>TRY_TO_NUMBER</code></li> <li><code>TRY_TO_NUMERIC</code></li> <li><code>TRY_TO_TIME</code></li> <li><code>TRY_TO_TIMESTAMP</code></li> <li><code>TRY_TO_TIMESTAMP_LTZ</code></li> <li><code>TRY_TO_TIMESTAMP_NTZ</code></li> </ul>"},{"location":"api_docs/sql/functions/casting/to_array/","title":"TO_ARRAY","text":"<p><code>TO_ARRAY(EXPR)</code></p> <p>Converts the input expression to a single-element array containing this value. If the input is an ARRAY, or VARIANT containing an array value, the result is unchanged. Returns <code>NULL</code> for <code>NULL</code> or a JSON null input.</p>"},{"location":"api_docs/sql/functions/casting/to_binary/","title":"TO_BINARY","text":"<ul> <li><code>TO_BINARY(COLUMN_EXPRESSION)</code></li> </ul> <p>Casts the input string to binary data. Currently only supports the <code>HEX</code> format. Raises an exception if the input is not a valid hex string: - Must have an even number of characters - All characters must be hexedecimal digits (0-9, a-f case insensitive)</p> <p>Example:</p> <p>We are given <code>table1</code> with columns <code>a</code> and <code>b</code>: <pre><code>table1 = pd.DataFrame({\n    'a': [\"AB\", \"626f646f\", \"4a2F3132\"],\n    'b': [\"ABC\", \"ZETA\", \"#fizz\"],\n})\n</code></pre> upon query <pre><code>SELECT TO_BINARY(a),\nFROM table1\n</code></pre> we will get the following output: <pre><code>    TO_BINARY(a)\n0   b'\\xab'             -- Binary encoding of the character '\u00bc'\n1   b'\\x62\\x6f\\x64\\x6f' -- Binary encoding of the string 'bodo'\n2   b'\\x4a\\x2f\\x31\\x32' -- Binary encoding of the string 'J/12'\n</code></pre> Upon query <pre><code>SELECT TO_BINARY(b)\nFROM table1\n</code></pre> we will see a value error because all of the values in column b are not valid hex strings: - <code>'ABC'</code> is 3 characters, which is not an even number - <code>'ZETA'</code> contains non-hex characters <code>Z</code> and <code>T</code> - <code>'#fizz'</code> is 5 characters, which is not an even number and contains non-hex characters <code>#</code>, <code>i</code> and <code>z</code></p>"},{"location":"api_docs/sql/functions/casting/to_boolean/","title":"TO_BOOLEAN","text":"<ul> <li><code>TO_BOOLEAN(COLUMN_EXPRESSION)</code></li> </ul> <p>Casts the input to a boolean value. If the input is a string, it will be cast to <code>true</code> if it is <code>'true'</code>, <code>'t'</code>, <code>'yes'</code>, <code>'y'</code>, <code>'1'</code>, cast to <code>false</code> if it is <code>'false'</code>, <code>'f'</code>, <code>'no'</code>, <code>'n'</code>, <code>'0'</code>, and throw an error otherwise. If the input is an integer, it will be cast to <code>true</code> if it is non-zero and <code>false</code> if it is zero. If the input is a float, it will be cast to <code>true</code> if it is non-zero, <code>false</code> if it is zero, and throw an error on other inputs (e.g. <code>inf</code>) input. If the input is <code>NULL</code>, the output will be <code>NULL</code>.</p> <p>Example:</p> <p>We are given <code>table1</code> with columns <code>a</code> and <code>b</code> and <code>c</code> <pre><code>table1 = pd.DataFrame({\n    'a': [1.1, 0, 2],\n    'b': ['t', 'f', 'YES'],\n    'c': [None, 1, 0]\n})\n</code></pre> upon query <pre><code>SELECT\n    TO_BOOLEAN(a) AS a,\n    TO_BOOLEAN(b) AS b,\n    TO_BOOLEAN(c) AS c\nFROM table1;\n</code></pre> we will get the following output: <pre><code>       a      b      c\n0   True   True   &lt;NA&gt;\n1  False  False   True\n2   True   True  False\n</code></pre> Upon query <pre><code>SELECT TO_BOOLEAN('other')\n</code></pre> we see the following error: <pre><code>ValueError: invalid value for boolean conversion: string must be one of {'true', 't', 'yes', 'y', 'on', '1'} or {'false', 'f', 'no', 'n', 'off', '0'}\n</code></pre></p> <p>Note</p> <p>BodoSQL will read <code>NaN</code>s as <code>NULL</code> and thus will not produce errors on <code>NaN</code> input.</p>"},{"location":"api_docs/sql/functions/casting/to_char/","title":"TO_CHAR","text":"<ul> <li><code>TO_CHAR(COLUMN_EXPRESSION)</code></li> </ul> <p>Casts the input to a string value. If the input is a boolean, it will be cast to <code>'true'</code> if it is <code>true</code> and <code>'false'</code> if it is <code>false</code>. If the input is <code>NULL</code>, the output will be <code>NULL</code>.</p> <p>Example:</p> <p>We are given <code>table1</code> with columns <code>a</code> and <code>b</code> and <code>c</code> <pre><code>table1 = pd.DataFrame({\n    'a': [1.1, 0, 2],\n    'b': [True, False, True],\n    'c': [None, 1, 0]\n})\n</code></pre> upon query <pre><code>SELECT\n    TO_CHAR(a) AS a,\n    TO_CHAR(b) AS b,\n    TO_CHAR(c) AS c\nFROM table1;\n</code></pre> we will get the following output: <pre><code>    a      b      c\n0  1.1   true   &lt;NA&gt;\n1    0  false      1\n2    2   true      0\n</code></pre></p> <p>Note that if the input is a <code>TIMESTAMP_TZ</code> the only currently supported output format that includes <code>TZH</code> or <code>TZM</code> is <code>YYYY-MM-DD HH:MM:SS.SSSSSSSSS +TZH:TZM</code> (where <code>+</code> represents <code>+</code> or <code>-</code>). Formats that do not include those identifiers are supported.</p>"},{"location":"api_docs/sql/functions/casting/to_date/","title":"TO_DATE","text":"<p><code>TO_DATE(EXPR)</code></p> <p>Converts an input expression to a <code>DATE</code> type. The input can be one of the following:</p> <ul> <li><code>TO_DATE(timestamp_expr)</code> truncates the timestamp to its date value.</li> <li><code>TO_DATE(timestamptz_expr)</code> truncates the TIMESTAMPTZ to its date value based on it's local timestamp (not UTC).</li> <li><code>TO_DATE(string_expr)</code> if the string is in date format (e.g. <code>\"1999-01-01\"</code>) then it is convrted to a corresponding date. If the string represents an integer (e.g. <code>\"123456\"</code>) then it is interpreted as the number of seconds/milliseconds/microseconds/nanoseconds since <code>1970-01-1</code>. Which unit it is interpreted as depends on the magnitude of the number, in accordance with the semantics used by Snowflake.</li> <li><code>TO_DATE(string_expr, format_expr)</code> uses the format string to specify how to parse the string expression as a date. Uses the format string rules as specified by Snowflake.</li> <li>If the input is <code>NULL</code>, outputs <code>NULL</code>.</li> </ul> <p>Raises an error if the input expression does not match one of these formats.</p>"},{"location":"api_docs/sql/functions/casting/to_decimal/","title":"TO_DECIMAL","text":"<p><code>TO_DECIMAL(EXPR [, PRECICION [, SCALE]])</code></p> <p>Equivalent to <code>TO_NUMBER(EXPR, PRECICION, SCALE)</code>.</p>"},{"location":"api_docs/sql/functions/casting/to_double/","title":"TO_DOUBLE","text":"<ul> <li><code>TO_DOUBLE(COLUMN_EXPRESSION)</code></li> </ul> <p>Converts a numeric or string expression to a double-precision floating-point number. For <code>NULL</code> input, the result is <code>NULL</code>. Fixed-point numbers are converted to floating point; the conversion cannot fail, but might result in loss of precision. Strings are converted as decimal or integer numbers. Scientific notation and special values (nan, inf, infinity) are accepted, case insensitive.</p> <p>Example:</p> <p>We are given <code>table1</code> with columns <code>a</code> and <code>b</code> <pre><code>table1 = pd.DataFrame({\n    'a': [1, 0, 2],\n    'b': ['3.7', '-2.2e-1', 'nan'],\n})\n</code></pre> upon query <pre><code>SELECT\n    TO_DOUBLE(a) AS a,\n    TO_DOUBLE(b) AS b,\nFROM table1;\n</code></pre> we will get the following output: <pre><code>       a      b\n0    1.0    3.7\n1    0.0  -0.22\n2    2.0   &lt;NA&gt;\n</code></pre></p>"},{"location":"api_docs/sql/functions/casting/to_number/","title":"TO_NUMBER","text":"<p><code>TO_NUMBER(EXPR [, PRECICION [, SCALE]])</code></p> <p>Converts an input expression to a fixed-point number with the specified precicion and scale. Precicon and scale must be constant integer literals if provided. Precicion must be between 1 and 38. Scale must be between 0 and prec - 1. Precicion and scale default to 38 and 0 if not provided. For <code>NULL</code> input, the output is <code>NULL</code>.</p>"},{"location":"api_docs/sql/functions/casting/to_numeric/","title":"TO_NUMERIC","text":"<p><code>TO_NUMERIC(EXPR [, PRECICION [, SCALE]])</code></p> <p>Equivalent to <code>TO_NUMBER(EXPR, PRECICION, SCALE)</code>.</p>"},{"location":"api_docs/sql/functions/casting/to_object/","title":"TO_OBJECT","text":"<p><code>TO_OBJECT(EXPR)</code></p> <p>If the input is an object type or a variant containing an object type, returns the input unmodified (except that its type is now <code>OBJECT</code> if it was <code>VARIANT</code>). For all other types, raises an error.  Returns <code>NULL</code> for <code>NULL</code> or a JSON null input.</p>"},{"location":"api_docs/sql/functions/casting/to_time/","title":"TO_TIME","text":"<p><code>TO_TIME(EXPR)</code></p> <p>Converts an input expression to a <code>TIME</code> type. The input can be one of the following:</p> <ul> <li><code>TO_TIME(timestamp_expr)</code> extracts the time component from a timestamp.</li> <li><code>TO_TIME(string_expr)</code> if the string is in date format (e.g. <code>\"12:30:15\"</code>) then it is convrted to a corresponding time.</li> <li><code>TO_TIME(string_expr, format_expr)</code> uses the format string to specify how to parse the string expression as a time. Uses the format string rules as specified by Snowflake.</li> <li>If the input is <code>NULL</code>, outputs <code>NULL</code></li> </ul> <p>Raises an error if the input expression does not match one of these formats.</p>"},{"location":"api_docs/sql/functions/casting/to_timestamp/","title":"TO_TIMESTAMP","text":"<p><code>TO_TIMESTAMP(EXPR)</code></p> <p>Converts an input expression to a <code>TIMESTAMP</code> type without a timezone. The input can be one of the following:</p> <ul> <li><code>TO_TIMESTAMP(date_expr)</code> upcasts a <code>DATE</code> to a <code>TIMESTAMP</code>.</li> <li><code>TO_TIMESTAMP(integer)</code> creates a timestamp using the integer as the number of seconds/milliseconds/microseconds/nanoseconds since <code>1970-01-1</code>. Which unit it is interpreted as depends on the magnitude of the number, in accordance with the semantics used by Snowflake.</li> <li><code>TO_TIMESTAMP(integer, scale)</code> the same as the integer case except that the scale provided specifes which unit is used. THe scale can be an integer constant between 0 and 9, where 0 means seconds and 9 means nanoseconds.</li> <li><code>TO_TIMESTAMP(string_expr)</code> if the string is in timestamp format (e.g. <code>\"1999-12-31 23:59:30\"</code>) then it is convrted to a corresponding timestamp. If the string represents an integer (e.g. <code>\"123456\"</code>) then it uses the same rule as the corresponding input integer.</li> <li><code>TO_TIMESTAMP(string_expr, format_expr)</code> uses the format string to specify how to parse the string expression as a timestamp. Uses the format string rules as specified by Snowflake.</li> <li><code>TO_TIMESTAMP(timestamp_exr)</code> returns a timestamp expression representing the same moment in time, but changing the timezone or offset if necessary to be timezone-naive.</li> <li>If the input is <code>NULL</code>, outputs <code>NULL</code></li> </ul> <p>Raises an error if the input expression does not match one of these formats.</p>"},{"location":"api_docs/sql/functions/casting/to_timestamp_ltz/","title":"TO_TIMESTAMP_LTZ","text":"<p><code>TO_TIMESTAMP_LTZ(EXPR)</code></p> <p>Equivalent to <code>TO_TIMESTAMP</code> except that it uses the local time zone. If <code>EXPR</code> evaluates to a <code>TIMESTAMP_TZ</code>, then the offset will be removed (local with respect to the offset), and the timezone will be set the local time zone. Note that the output will be converted to a <code>TIMESTAMP_LTZ</code> (output will have a defined timezone instead of a constant offset).</p>"},{"location":"api_docs/sql/functions/casting/to_timestamp_ntz/","title":"TO_TIMESTAMP_NTZ","text":"<p><code>TO_TIMESTAMP_NTZ(EXPR)</code></p> <p>Equivalent to <code>TO_TIMESTAMP</code>.</p>"},{"location":"api_docs/sql/functions/casting/to_timestamp_tz/","title":"TO_TIMESTAMP_TZ","text":"<p><code>TO_TIMESTAMP_TZ(EXPR)</code></p> <p>Equivalent to <code>TO_TIMESTAMP</code> except that if the input is a timezone-aware timestamp, then the timezone's offset for at the value specified by the timestamp is used as the <code>TIMESTAMPTZ</code> UTC offset, otherwise the local time zone's UTC offset at the value specified by <code>TO_TIMESTAMP(EXPR)</code> is used. For example, in the <code>America/Los Angeles</code> timezone, then the following would be true:</p> <pre><code>TO_TIMESTAMP_TZ('2024-03-10 00:00:00'::timestampltz) = '2024-03-10 00:00:00 -0800'::timestamptz\n\nTO_TIMESTAMP_TZ('2024-03-11 00:00:00'::timestampltz) = '2024-03-11 00:00:00 -0700'::timestamptz\n</code></pre> <p>Additionally, if <code>EXPR</code> evaluates to a string, if an offset is not explicitly specified, the offset of the timestamp in the session's timezone is used. The following formats for offset are supported:</p> <ul> <li><code>z</code> or <code>Z</code> for the zero offset</li> <li><code>[+-]H:M</code></li> <li><code>[+-]HH:M</code></li> <li><code>[+-]H:MM</code></li> <li><code>[+-]HH:MM</code></li> <li><code>[+-]HHMM</code></li> <li><code>[+-]HMM</code></li> <li><code>[+-]HH</code></li> </ul>"},{"location":"api_docs/sql/functions/casting/to_varchar/","title":"TO_VARCHAR","text":"<ul> <li><code>TO_VARCHAR(COLUMN_EXPRESSION)</code></li> </ul> <p>Alias for <code>TO_CHAR</code>.</p>"},{"location":"api_docs/sql/functions/casting/try_to_binary/","title":"TRY_TO_BINARY","text":"<ul> <li><code>TRY_TO_BINARY(COLUMN_EXPRESSION)</code></li> </ul> <p>See <code>TO_BINARY</code>. The only difference is that <code>TRY_TO_BINARY</code> will return <code>NULL</code> upon encountering an invalid expression instead of raising an exception.</p>"},{"location":"api_docs/sql/functions/casting/try_to_boolean/","title":"TRY_TO_BOOLEAN","text":"<ul> <li><code>TRY_TO_BOOLEAN(COLUMN_EXPRESSION)</code></li> </ul> <p>This is similar to <code>TO_BOOLEAN</code> except that it will return <code>NULL</code> instead of throwing an error invalid inputs.</p> <p>Example:</p> <p>We are given <code>table1</code> with columns <code>a</code> and <code>b</code> and <code>c</code> <pre><code>table1 = pd.DataFrame({\n    'a': [1.1, 0, np.inf],\n    'b': ['t', 'f', 'YES'],\n    'c': [None, 1, 0]\n})\n</code></pre> upon query <pre><code>SELECT\n    TRY_TO_BOOLEAN(a) AS a,\n    TRY_TO_BOOLEAN(b) AS b,\n    TRY_TO_BOOLEAN(c) AS c\nFROM table1;\n</code></pre> we will get the following output: <pre><code>    a      b      c\n0   True   True   &lt;NA&gt;\n1  False  False   True\n2   &lt;NA&gt;   True  False\n</code></pre></p>"},{"location":"api_docs/sql/functions/casting/try_to_date/","title":"TRY_TO_DATE","text":"<p><code>TRY_TO_DATE(EXPR)</code></p> <p>A special version of <code>TO_DATE</code> that performs the same operation but returns <code>NULL</code> instead of raising an error if something goes wrong during the conversion.</p>"},{"location":"api_docs/sql/functions/casting/try_to_decimal/","title":"TRY_TO_DECIMAL","text":"<p><code>TRY_TO_DECIMAL(EXPR [, PRECICION [, SCALE]])</code></p> <p>Equivalent to <code>TRY_TO_NUMBER(EXPR, PRECICION, SCALE)</code>.</p>"},{"location":"api_docs/sql/functions/casting/try_to_double/","title":"TRY_TO_DOUBLE","text":"<ul> <li><code>TRY_TO_DOUBLE(COLUMN_EXPRESSION)</code></li> </ul> <p>This is similar to <code>TO_DOUBLE</code> except that it will return <code>NULL</code> instead of throwing an error invalid inputs.</p>"},{"location":"api_docs/sql/functions/casting/try_to_number/","title":"TRY_TO_NUMBER","text":"<p><code>TRY_TO_NUMBER(EXPR [, PRECICION [, SCALE]])</code></p> <p>A special version of <code>TO_NUMBER</code> that performs the same operation (i.e. converts an input expression to a fixed-point number), but with error-handling support (i.e. if the conversion cannot be performed, it returns a <code>NULL</code> value instead of raising an error).</p>"},{"location":"api_docs/sql/functions/casting/try_to_numeric/","title":"TRY_TO_NUMERIC","text":"<p><code>TRY_TO_NUMERIC(EXPR [, PRECICION [, SCALE]])</code></p> <p>Equivalent to <code>TRY_TO_NUMBER(EXPR, PRECICION, SCALE)</code>.</p>"},{"location":"api_docs/sql/functions/casting/try_to_time/","title":"TRY_TO_TIME","text":"<p><code>TRY_TO_TIME(EXPR)</code></p> <p>A special version of <code>TO_TIME</code> that performs the same operation but returns <code>NULL</code> instead of raising an error if something goes wrong during the conversion.</p>"},{"location":"api_docs/sql/functions/casting/try_to_timestamp/","title":"TRY_TO_TIMESTAMP","text":"<p><code>TRY_TO_TIMESTAMP(EXPR)</code></p> <p>A special version of <code>TO_TIMESTAMP</code> that performs the same operation but returns <code>NULL</code> instead of raising an error if something goes wrong during the conversion.</p>"},{"location":"api_docs/sql/functions/casting/try_to_timestamp_ltz/","title":"TRY_TO_TIMESTAMP_LTZ","text":"<p><code>TRY_TO_TIMESTAMP_NTZ(EXPR)</code></p> <p>Equivalent to <code>TRY_TO_TIMESTAMP</code> except that it uses the local time zone.</p>"},{"location":"api_docs/sql/functions/casting/try_to_timestamp_ntz/","title":"TRY_TO_TIMESTAMP_NTZ","text":"<p><code>TRY_TO_TIMESTAMP_NTZ(EXPR)</code></p> <p>Equivalent to <code>TRY_TO_TIMESTAMP</code>.</p>"},{"location":"api_docs/sql/functions/casting/try_to_timestamp_tz/","title":"TRY_TO_TIMESTAMP_TZ","text":"<p><code>TRY_TO_TIMESTAMP_NTZ(EXPR)</code></p> <p>Equivalent to <code>TRY_TO_TIMESTAMP</code> except that it uses the local time zone, or keeps the original timezone if the input is a timezone-aware timestamp.</p>"},{"location":"api_docs/sql/functions/context/","title":"Context Functions (Session Object)","text":"<p>Context functions are used to access session information and settings. BodoSQL currently supports the following context functions:</p> <ul> <li><code>CURRENT_DATABASE</code></li> <li><code>CURRENT_ACCOUNT</code></li> </ul>"},{"location":"api_docs/sql/functions/context/current_account/","title":"CURRENT_ACCOUNT","text":"<p><code>CURRENT_ACCOUNT()</code></p> <p>Returns the name of the account in use for the current session.</p>"},{"location":"api_docs/sql/functions/context/current_account/#current_account_name","title":"CURRENT_ACCOUNT_NAME","text":"<p><code>CURRENT_ACCOUNT_NAME()</code></p> <p>An alias for <code>CURRENT_ACCOUNT</code></p>"},{"location":"api_docs/sql/functions/context/current_database/","title":"CURRENT_DATABASE","text":"<p><code>CURRENT_DATABASE()</code></p> <p>Returns the name of the database in use for the current session.</p>"},{"location":"api_docs/sql/functions/control/","title":"Control Flow Functions","text":"<p>Control flow functions are used to control the flow of execution in a SQL query. BodoSQL currently supports the following control flow functions:</p> <ul> <li><code>COALESCE</code></li> <li><code>DECODE</code></li> <li><code>EQUAL_NULL</code></li> <li><code>IF</code></li> <li><code>IFF</code></li> <li><code>IFNULL</code></li> <li><code>NULLIF</code></li> <li><code>NULLIFZERO</code></li> <li><code>NVL</code></li> <li><code>NVL2</code></li> <li><code>ZEROIFNULL</code></li> </ul>"},{"location":"api_docs/sql/functions/control/coalesce/","title":"COALESCE","text":"<p><code>COALESCE(A, B, C, ...)</code></p> <p>Returns the first non-<code>NULL</code> argument, or <code>NULL</code> if no non-<code>NULL</code> argument is found. Requires at least two arguments. If Arguments do not have the same type, BodoSQL will attempt to cast them to a common data type, which is currently undefined behavior.</p>"},{"location":"api_docs/sql/functions/control/decode/","title":"DECODE","text":"<p><code>DECODE(Arg0, Arg1, Arg2, ...)</code></p> <p>When <code>Arg0</code> is <code>Arg1</code>, outputs <code>Arg2</code>. When <code>Arg0</code> is <code>Arg3</code>, outputs <code>Arg4</code>. Repeats until it runs out of pairs of arguments. At this point, if there is one remaining argument, this is used as a default value. If not, then the output is <code>NULL</code>.</p> <p>Note</p> <p>Treats <code>NULL</code> as a literal value that can be matched on.</p> <p>Therefore, the following:</p> <pre><code>DECODE(A, NULL, 0, 'x', 1, 'y', 2, 'z', 3, -1)\n</code></pre> <p>Is logically equivalent to:</p> <pre><code>CASE WHEN A IS NULL THEN 0\n     WHEN A = 'x' THEN 1\n     WHEN A = 'y' THEN 2\n     WHEN A = 'z' THEN 3\n     ELSE -1 END\n</code></pre>"},{"location":"api_docs/sql/functions/control/equal_null/","title":"EQUAL_NULL","text":"<p><code>EQUAL_NULL(A, B)</code></p> <p>Returns true if A and B are both <code>NULL</code>, or both non-null and equal to each other.</p>"},{"location":"api_docs/sql/functions/control/if/","title":"IF","text":"<p><code>IF(Cond, TrueValue, FalseValue)</code></p> <p>Returns <code>TrueValue</code> if cond is true, and <code>FalseValue</code> if cond is false. Logically equivalent to:</p> <pre><code>CASE WHEN Cond THEN TrueValue ELSE FalseValue END\n</code></pre>"},{"location":"api_docs/sql/functions/control/iff/","title":"IFF","text":"<p><code>IFF(Cond, TrueValue, FalseValue)</code></p> <p>Equivalent to <code>IF</code></p>"},{"location":"api_docs/sql/functions/control/ifnull/","title":"IFNULL","text":"<p><code>IFNULL(Arg0, Arg1)</code></p> <p>Equivalent to <code>COALESCE(Arg0, Arg1)</code></p>"},{"location":"api_docs/sql/functions/control/nullif/","title":"NULLIF","text":"<p><code>NULLIF(Arg0, Arg1)</code></p> <p>Returns <code>NULL</code> if <code>Arg0</code> is equal to <code>Arg1</code>, and otherwise returns <code>Arg0</code>.</p>"},{"location":"api_docs/sql/functions/control/nullifzero/","title":"NULLIFZERO","text":"<p><code>NULLIFZERO(Arg0)</code></p> <p>Equivalent to <code>NULLIF(Arg0, 0)</code></p>"},{"location":"api_docs/sql/functions/control/nvl/","title":"NVL","text":"<p><code>NVL(Arg0, Arg1)</code></p> <p>Equivalent to <code>COALESCE(Arg0, Arg1)</code></p>"},{"location":"api_docs/sql/functions/control/nvl2/","title":"NVL2","text":"<p><code>NVL2(Arg0, Arg1, Arg2)</code></p> <p>Equivalent to <code>IF(Arg0 IS NOT NULL, Arg1, Arg2)</code></p>"},{"location":"api_docs/sql/functions/control/zeroifnull/","title":"ZEROIFNULL","text":"<p><code>ZEROIFNULL(Arg0, Arg1)</code></p> <p>Equivalent to <code>COALESCE(Arg0, 0)</code></p>"},{"location":"api_docs/sql/functions/data_gen/","title":"Data Generation Functions","text":"<p>BodoSQL Currently supports the following data generation functions:</p> <ul> <li><code>RANDOM</code></li> <li><code>UNIFORM</code></li> <li><code>UUID_STRING</code></li> </ul>"},{"location":"api_docs/sql/functions/data_gen/random/","title":"RANDOM","text":"<p><code>RANDOM()</code></p> <p>Outputs a random 64-bit integer. If used inside of a select statement with a table, the number of random values will match the number of rows in the input table (and each value should be randomly and independently generated). Note that running with multiple processors may affect the randomization results.</p> <p>Note</p> <p>Currently, BodoSQL does not support the format of <code>RANDOM()</code> that takes in a seed value.</p> <p>Note</p> <p>At present, aliases to <code>RANDOM</code> calls occasionally produce unexpected behavior. For certain SQL operations, calling <code>RANDOM</code> and storing the result with an alias, then later re-using that alias may result in another call to <code>RANDOM</code>. This behavior is somewhat rare.</p>"},{"location":"api_docs/sql/functions/data_gen/uniform/","title":"UNIFORM","text":"<p><code>UNIFORM(lo, hi, gen)</code></p> <p>Outputs a random number uniformly distributed in the interval <code>[lo, hi]</code>. If <code>lo</code> and <code>hi</code> are both integers, then the output is an integer between <code>lo</code> and <code>hi</code> (including both endpoints). If either <code>lo</code> or <code>hi</code> is a float, the output is a random float between them. The values of <code>gen</code> are used to seed the randomness, so if <code>gen</code> is all distinct values (or is randomly generated) then the output of <code>UNIFORM</code> should be random. However, if 2 rows have the same <code>gen</code> value they will produce the same output value.</p>"},{"location":"api_docs/sql/functions/data_gen/uuid_string/","title":"UUID_STRING","text":"<p><code>UUID_STRING([uuid, name])</code></p> <p>Outputs a UUID (as defined by RFC 4122) formatted as a string. If no arguments are provdied then a version 4 (randomly generated) UUID is returned, otherwise a version 5 (generated by name) UUID is returned.</p>"},{"location":"api_docs/sql/functions/numeric/","title":"Numeric Functions","text":"<p>Except where otherwise specified, the inputs to each of these functions can be any numeric type, column or scalar. </p> <p>BodoSQL supports the following numeric functions:</p> <ul> <li><code>ABS</code></li> <li><code>ACOS</code></li> <li><code>ASIN</code></li> <li><code>ATAN</code></li> <li><code>ATAN2</code></li> <li><code>BITAND</code></li> <li><code>BITNOT</code></li> <li><code>BITOR</code></li> <li><code>BITSHIFTLEFT</code></li> <li><code>BITSHIFTRIGHT</code></li> <li><code>BITXOR</code></li> <li><code>BOOLAND</code></li> <li><code>BOOLNOT</code></li> <li><code>BOOLOR</code></li> <li><code>BOOLXOR</code></li> <li><code>CEIL</code></li> <li><code>CEILING</code></li> <li><code>CONV</code></li> <li><code>COS</code></li> <li><code>COTAN</code></li> <li><code>DEGREES</code></li> <li><code>EXP</code></li> <li><code>FLOOR</code></li> <li><code>GETBIT</code></li> <li><code>HASH</code></li> <li><code>LN</code></li> <li><code>LOG</code></li> <li><code>LOG10</code></li> <li><code>MOD</code></li> <li><code>PI</code></li> <li><code>POW</code></li> <li><code>POWER</code></li> <li><code>RADIANS</code></li> <li><code>REGR_VALX</code></li> <li><code>REGR_VALY</code></li> <li><code>ROUND</code></li> <li><code>SIGN</code></li> <li><code>SIN</code></li> <li><code>SQRT</code></li> <li><code>TAN</code></li> <li><code>TRUNC</code></li> <li><code>TRUNCATE</code></li> </ul>"},{"location":"api_docs/sql/functions/numeric/abs/","title":"ABS","text":"<p><code>ABS(n)</code></p> <p>Returns the absolute value of n</p>"},{"location":"api_docs/sql/functions/numeric/acos/","title":"ACOS","text":"<p><code>ACOS(n)</code></p> <p>Calculates the Arccosine of n</p>"},{"location":"api_docs/sql/functions/numeric/asin/","title":"ASIN","text":"<p><code>AIN(n)</code></p> <p>Calculates the Arcsine of n</p>"},{"location":"api_docs/sql/functions/numeric/atan/","title":"ATAN","text":"<p><code>ATAN(n)</code></p> <p>Calculates the Arctangent of n</p>"},{"location":"api_docs/sql/functions/numeric/atan2/","title":"ATAN2","text":"<p><code>ATAN2(A, B)</code></p> <p>Calculates the Arctangent of <code>A</code> divided by <code>B</code></p>"},{"location":"api_docs/sql/functions/numeric/bitand/","title":"BITAND","text":"<p><code>BITAND(A, B)</code></p> <p>Returns the bitwise-and of its inputs.</p>"},{"location":"api_docs/sql/functions/numeric/bitnot/","title":"BITNOT","text":"<p><code>BITNOT(A)</code></p> <p>Returns the bitwise-negation of its input.</p>"},{"location":"api_docs/sql/functions/numeric/bitor/","title":"BITOR","text":"<p><code>BITOR(A, B)</code></p> <p>Returns the bitwise-or of its inputs.</p>"},{"location":"api_docs/sql/functions/numeric/bitshiftleft/","title":"BITSHIFTLEFT","text":"<p><code>BITSHIFTLEFT(A, B)</code></p> <p>Returns the bitwise-leftshift of its inputs.</p> <p>Note</p> <ul> <li>The output is always of type int64.</li> <li>Undefined behavior when B is negative or too large.</li> </ul>"},{"location":"api_docs/sql/functions/numeric/bitshiftright/","title":"BITSHIFTRIGHT","text":"<p><code>BITSHIFTRIGHT(A, B)</code></p> <p>Returns the bitwise-rightshift of its inputs. Undefined behavior when B is negative or too large.</p>"},{"location":"api_docs/sql/functions/numeric/bitxor/","title":"BITXOR","text":"<p><code>BITOR(A, B)</code></p> <p>Returns the bitwise-xor of its inputs.</p>"},{"location":"api_docs/sql/functions/numeric/booland/","title":"BOOLAND","text":"<p><code>BOOLAND(A, B)</code></p> <p>Returns true when <code>A</code> and <code>B</code> are both non-null non-zero. Returns false when one of the arguments is zero and the other is either zero or <code>NULL</code>. Returns <code>NULL</code> otherwise.</p>"},{"location":"api_docs/sql/functions/numeric/boolnot/","title":"BOOLNOT","text":"<p><code>BOOLNOT(A)</code></p> <p>Returns true if <code>A</code> is zero. Returns false if <code>A</code> is non-zero. Returns <code>NULL</code> if <code>A</code> is <code>NULL</code>.</p>"},{"location":"api_docs/sql/functions/numeric/boolor/","title":"BOOLOR","text":"<p><code>BOOLOR(A, B)</code></p> <p>Returns true if either <code>A</code> or <code>B</code> is non-null and non-zero. Returns false if both <code>A</code> and <code>B</code> are zero. Returns <code>NULL</code> otherwise.</p>"},{"location":"api_docs/sql/functions/numeric/boolxor/","title":"BOOLXOR","text":"<p><code>BOOLXOR(A, B)</code></p> <p>Returns true if one of <code>A</code> and <code>B</code> is zero and the other is non-zero. Returns false if <code>A</code> and <code>B</code> are both zero or both non-zero. Returns <code>NULL</code> if either <code>A</code> or <code>B</code> is <code>NULL</code>.</p>"},{"location":"api_docs/sql/functions/numeric/ceil/","title":"CEIL","text":"<p><code>CEIL(X[, scale])</code></p> <p>Converts X to the specified scale, rounding towards positive infinity. For example, <code>scale=0</code> rounds up to the nearest integer, <code>scale=2</code> rounds up to the nearest <code>0.01</code>, and <code>scale=-1</code> rounds up to the nearest multiple of 10.</p>"},{"location":"api_docs/sql/functions/numeric/ceiling/","title":"CEILING","text":"<p><code>CEILING(X)</code></p> <p>Equivalent to <code>CEIL</code></p>"},{"location":"api_docs/sql/functions/numeric/conv/","title":"CONV","text":"<p><code>CONV(X, current_base, new_base)</code></p> <p><code>CONV</code> takes a string representation of an integer value, it's current_base, and the base to convert that argument to. <code>CONV</code> returns a new string, that represents the value in the new base. <code>CONV</code> is only supported for converting to/from base 2, 8, 10, and 16.</p> <p>For example:</p> <pre><code>CONV('10', 10, 2) =='1010'\nCONV('10', 2, 10) =='2'\nCONV('FA', 16, 10) =='250'\n</code></pre>"},{"location":"api_docs/sql/functions/numeric/cos/","title":"COS","text":"<p><code>COS(n)</code></p> <p>Calculates the Cosine of n</p>"},{"location":"api_docs/sql/functions/numeric/cotan/","title":"COTAN","text":"<p><code>COTAN(X)</code></p> <p>Calculates the Cotangent of <code>X</code></p>"},{"location":"api_docs/sql/functions/numeric/degrees/","title":"DEGREES","text":"<p><code>DEGREES(X)</code></p> <p>Converts a value in radians to the corresponding value in degrees</p>"},{"location":"api_docs/sql/functions/numeric/exp/","title":"EXP","text":"<p><code>EXP(X)</code></p> <p>Returns e to the power of X</p>"},{"location":"api_docs/sql/functions/numeric/floor/","title":"FLOOR","text":"<p><code>FLOOR(X[, scale])</code></p> <p>Converts X to the specified scale, rounding towards negative infinity. For example, <code>scale=0</code> down up to the nearest integer, <code>scale=2</code> rounds down to the nearest <code>0.01</code>, and <code>scale=-1</code> rounds down to the nearest multiple of 10.</p>"},{"location":"api_docs/sql/functions/numeric/getbit/","title":"GETBIT","text":"<p><code>GETBIT(A, B)</code></p> <p>Returns the bit of A corresponding to location B, where 0 is the rightmost bit. Undefined behavior when B is negative or too large.</p>"},{"location":"api_docs/sql/functions/numeric/hash/","title":"HASH","text":"<p><code>HASH(A, B, C, ...)</code></p> <p>Takes in a variable number of arguments of any type and returns a hash value that considers the values in each column. The hash function is deterministic across multiple ranks or multiple sessions.</p> <p>Also supports the syntactic sugar forms <code>HASH(*)</code> and <code>HASH(T.*)</code> as shortcuts for referencing all of the columns in a table, or multiple tables. For example, if <code>T1</code> has columns <code>A</code> and <code>B</code>, and <code>T2</code> has columns <code>A</code>, <code>E</code> and <code>I</code>, then the following query:</p> <p><code>SELECT HASH(*), HASH(T1.*) FROM T1 INNER JOIN T2 ON T1.A=T2.I</code></p> <p>Would be syntactic sugar for the following:</p> <p><code>SELECT HASH(T1.A, T1.B, T2.A, T2.E, T2.I), HASH(T1.A, T1.B) FROM T1 INNER JOIN T2 ON T1.A=T2.I</code></p>"},{"location":"api_docs/sql/functions/numeric/ln/","title":"LN","text":"<p><code>LN(X)</code></p> <p>Computes the natural log of x. Returns <code>NaN</code> for negative inputs, and <code>-inf</code> for 0 inputs.</p>"},{"location":"api_docs/sql/functions/numeric/log/","title":"LOG","text":"<p><code>LOG(X)</code></p> <p>Equivalent to <code>LOG10(x)</code></p>"},{"location":"api_docs/sql/functions/numeric/log10/","title":"LOG10","text":"<p><code>LOG10(X)</code></p> <p>Computes Log base 10 of x. Returns NaN for negative inputs, and -inf for 0 inputs.</p>"},{"location":"api_docs/sql/functions/numeric/mod/","title":"MOD","text":"<p><code>MOD(A,B)</code></p> <p>Computes A modulo B (behavior analogous to the C library function <code>fmod</code>). Returns <code>NaN</code> if B is 0 or if A is inf.</p>"},{"location":"api_docs/sql/functions/numeric/pi/","title":"PI","text":"<p><code>PI()</code></p> <p>Returns the value of <code>PI</code></p>"},{"location":"api_docs/sql/functions/numeric/power/","title":"POWER","text":"<p>Can also be called using <code>POW</code></p> <p><code>POW(A, B), POWER(A, B)</code></p> <p>Returns A to the power of B. Returns <code>NaN</code> if A is negative, and B is a float. <code>POW(0,0)</code> is 1</p>"},{"location":"api_docs/sql/functions/numeric/radians/","title":"RADIANS","text":"<p><code>RADIANS(X)</code></p> <p>Converts a value in radians to the corresponding value in degrees</p>"},{"location":"api_docs/sql/functions/numeric/regr_valx/","title":"REGR_VALX","text":"<p><code>REGR_VALX(Y, X)</code></p> <p>Returns <code>NULL</code> if either input is <code>NULL</code>, otherwise <code>X</code></p>"},{"location":"api_docs/sql/functions/numeric/regr_valy/","title":"REGR_VALY","text":"<p><code>REGR_VALY(Y, X)</code></p> <p>Returns <code>NULL</code> if either input is <code>NULL</code>, otherwise <code>Y</code></p>"},{"location":"api_docs/sql/functions/numeric/round/","title":"ROUND","text":"<p><code>ROUND(X[, num_decimal_places])</code></p> <p>Rounds X to the specified number of decimal places.  By default, rounds to 0 decimal places.</p> <p>For fixed-point decimals, usage follows that of Snowflake's <code>half_away_from_zero</code> rounding mode. See the Snowflake documentation for more details.</p>"},{"location":"api_docs/sql/functions/numeric/sign/","title":"SIGN","text":"<p><code>SIGN(X)</code></p> <p>Returns 1 if X &gt; 0, -1 if X &lt; 0, and 0 if X = 0</p>"},{"location":"api_docs/sql/functions/numeric/sin/","title":"SIN","text":"<p><code>SIN(n)</code></p> <p>Calculates the Sine of n</p>"},{"location":"api_docs/sql/functions/numeric/sqrt/","title":"SQRT","text":"<p><code>SQRT(X)</code></p> <p>Computes the square root of x. Returns <code>NaN</code> for negative inputs, and <code>-inf</code> for 0 inputs.</p>"},{"location":"api_docs/sql/functions/numeric/tan/","title":"TAN","text":"<p><code>TAN(n)</code></p> <p>Calculates the Tangent of n</p>"},{"location":"api_docs/sql/functions/numeric/trunc/","title":"TRUNC","text":"<p><code>TRUNC(X[, num_decimal_places])</code></p> <p>Equivalent to <code>TRUNC(X[, num_decimal_places])</code> if <code>X</code> is numeric. Note that <code>TRUNC</code> is overloaded and may invoke the timestamp function <code>TRUNC</code> if <code>X</code> is a date or time expression.</p>"},{"location":"api_docs/sql/functions/numeric/truncate/","title":"TRUNCATE","text":"<p><code>TRUNCATE(X[, num_decimal_places])</code></p> <p>Equivalent to <code>ROUND(X, num_decimal_places)</code>. If <code>num_decimal_places</code> is not supplied, it defaults to 0.</p>"},{"location":"api_docs/sql/functions/object/","title":"Object Functions","text":"<p>BodoSQL currently supports the following Object functions:</p> <p>GET_PATH JSON_EXTRACT_PATH_TEXT OBJECT_CONSTRUCT OBJECT_CONSTRUCT_KEEP_NULL OBJECT_DELETE OBJECT_INSERT OBJECT_KEYS OBJECT_PICK PARSE_JSON</p>"},{"location":"api_docs/sql/functions/object/get_path/","title":"GET_PATH","text":"<p><code>GET_PATH(data, path_string)</code></p> <p>Extracts an entry from a semi-structured data expression based on the path string. Obeys the specification described here.</p>"},{"location":"api_docs/sql/functions/object/json_extract_path_text/","title":"JSON_EXTRACT_PATH_TEXT","text":"<p><code>JSON_EXTRACT_PATH_TEXT(data, path)</code></p> <p>Parses the string <code>data</code> as if it were JSON data, then extracts values from within (possibly multiple times if the data is nested) using the string <code>path</code>.</p> <p>Obeys the specification described here.</p>"},{"location":"api_docs/sql/functions/object/object_construct/","title":"OBJECT_CONSTRUCT","text":"<p><code>OBJECT_CONSTRUCT(key1, value1[, key2, value2, ...])</code></p> <p>The same as <code>OBJECT_CONSTRUCT_KEEP_NULL</code> except that for any rows where any input value (e.g. <code>value1</code>, <code>value2</code>, ...) is null have that key-value pair dropped from the row's final JSON output.</p> <p>Note</p> <p>BodoSQL only supports this function under narrow conditions where all of the values are either of the same type or of easily reconciled types.</p> <p>The full Snowflake specification.</p> <p>BodoSQL supports the syntactic sugar <code>OBJECT_CONSTRUCT(*)</code> which indicates that all columns should be used as key-value pairs, where the column is the value and its column name is the key. For example, if we have the table <code>T</code> as defined below:</p> First Middle Last \"George\" NULL \"WASHINGTON\" \"John\" \"Quincy\" \"Adams\" \"Lyndon\" \"Baines\" \"Johnson\" \"James\" NULL \"Madison\" <p>Then <code>SELECT OBJECT_CONSTRUCT(*) as name FROM T</code> returns the following table:</p> name {\"First\": \"George\", \"Last\": \"Washington\"} {\"First\": \"John\", \"Middle\": \"Quincy\", \"Last\": \"Adams\"} {\"First\": \"Lyndon\", \"Middle\":\"Baines\", \"Last\": \"Johnson\"} {\"First\": \"Thomas\", \"Last\": \"Jefferson\"}"},{"location":"api_docs/sql/functions/object/object_construct_keep_null/","title":"OBJECT_CONSTRUCT_KEEP_NULL","text":"<p><code>OBJECT_CONSTRUCT_KEEP_NULL(key1, value1[, key2, value2, ...])</code></p> <p>Takes in a variable number of key-value pairs and combines them into JSON data. BodoSQL currently requires all <code>key</code> arguments to be string literals.</p> <p>The full Snowflake specification.</p> <p>BodoSQL supports the syntactic sugar <code>OBJECT_CONSTRUCT_KEEP_NULL(*)</code> which indicates that all columns should be used as key-value pairs, where the column is the value and its column name is the key. For example, if we have the table <code>T</code> as defined below:</p> First Middle Last \"George\" NULL \"WASHINGTON\" \"John\" \"Quincy\" \"Adams\" \"Lyndon\" \"Baines\" \"Johnson\" \"James\" NULL \"Madison\" <p>Then <code>SELECT OBJECT_CONSTRUCT_KEEP_NULL(*) as name FROM T</code> returns the following table:</p> name {\"First\": \"George\", \"Middle\": NULL, \"Last\": \"Washington\"} {\"First\": \"John\", \"Middle\": \"Quincy\", \"Last\": \"Adams\"} {\"First\": \"Lyndon\", \"Middle\":\"Baines\", \"Last\": \"Johnson\"} {\"First\": \"Thomas\", \"Middle\": NULL, \"Last\": \"Jefferson\"}"},{"location":"api_docs/sql/functions/object/object_delete/","title":"OBJECT_DELETE","text":"<p><code>OBJECT_DELETE(data, key1[, key2, ...])</code></p> <p>Takes in a column of JSON data and 1+ keys and returns the same JSON data but with all of those keys removed. If a specified key is not present in <code>data</code>, it is ignored.</p> <p>!!! note: BodoSQL supports when the keys are passed in as string literals, but only supports when they are passed in as columns of strings if the object is a map instead of struct.</p>"},{"location":"api_docs/sql/functions/object/object_insert/","title":"OBJECT_INSERT","text":"<p><code>OBJECT_INSERT(data, key, value[, update])</code></p> <p>Takes a columns of JSON data, a column of string keys, and a columns of values and inserts the keys and values into the data. If the key is already present in the data, an error will be thrown, unless an additional argument (<code>update</code>) of type boolean is supplied, which will update existing keys to hold the new value only if the value is true.</p>"},{"location":"api_docs/sql/functions/object/object_keys/","title":"OBJECT_KEYS","text":"<p><code>OBJECT_KEYS(data)</code></p> <p>Extracts all of the field names from the object <code>data</code> and returns them as an array of strings.</p>"},{"location":"api_docs/sql/functions/object/object_pick/","title":"OBJECT_PICK","text":"<p><code>OBJECT_PICK(data, key1[, key2, ...])</code></p> <p>Takes in a column of object data and 1+ keys and returns the object data only containing the keys specified. If a specified key is not present in <code>data</code>, it is ignored.</p> <p>!!! note: BodoSQL supports when the keys are passed in as string literals, but only supports when they are passed in as columns of strings if the object is a map instead of struct.</p>"},{"location":"api_docs/sql/functions/object/parse_json/","title":"PARSE_JSON","text":"<p><code>PARSE_JSON(str)</code></p> <p>Takes in a string representing a json document and parses it to the corresponding value as a variant. For example:</p> <ul> <li> <p><code>PARSE_JSON('42')</code> is equivalent to <code>TO_VARIANT(42)</code></p> </li> <li> <p><code>PARSE_JSON('{\"A\": 0, \"B\": 3.1}')</code> is equivalent to <code>TO_VARIANT({\"A\": 0, \"B\": 3.1})</code></p> </li> </ul> <p>Note</p> <p>Currently only supported under limited conditions where it is possible to rewrite the call to <code>PARSE_JSON</code> as a sequence of Parse-Extract-Cast operations, where the output of <code>PARSE_JSON</code> immediately has an extraction operation like GET/GET_PATH called on it, and the result is casted to a non-semi-structured type. For example, <code>PARSE_JSON(S):fizz::integer</code> can be rewritten, as can <code>GET_PATH(TO_OBJECT(TO_ARRAY(PARSE_JSON(S))[0]), 'foo.bar')::varchar</code>.</p>"},{"location":"api_docs/sql/functions/string/","title":"String Functions","text":"<p>BodoSQL currently supports the following string functions:</p> <ul> <li>BASE64_DECODE_BINARY</li> <li>BASE64_DECODE_STRING</li> <li>BASE64_ENCODE</li> <li>CHAR</li> <li>CHARINDEX</li> <li>CONCAT</li> <li>CONCAT_WS</li> <li>EDITDISTANCE</li> <li>ENDSWITH</li> <li>HEX_DECODE_BINARY</li> <li>HEX_DECODE_STRING</li> <li>HEX_ENCODE</li> <li>INDEX</li> <li>INSERT</li> <li>JAROWINKLER_SIMILARITY</li> <li>LCASE</li> <li>LEFT</li> <li>LENGTH</li> <li>LOWER</li> <li>LPAD</li> <li>LTRIM</li> <li>MD5</li> <li>MD5_HEX</li> <li>MID</li> <li>ORD</li> <li>POSITION</li> <li>REPEAT</li> <li>REPLACE</li> <li>REVERSE</li> <li>RIGHT</li> <li>RPAD</li> <li>RTRIM</li> <li>SHA2</li> <li>SHA2_HEX</li> <li>SPACE</li> <li>SPLIT_PART</li> <li>STARTSWITH</li> <li>STRCMP</li> <li>STRTOK</li> <li>SUBSTR</li> <li>SUBSTRING</li> <li>SUBSTRING_INDEX</li> <li>TRIM</li> <li>TRY_BASE64_DECODE_BINARY</li> <li>TRY_BASE64_DECODE_STRING</li> <li>TRY_HEX_DECODE_BINARY</li> <li>TRY_HEX_DECODE_STRING</li> <li>UCASE</li> <li>UPPER</li> </ul>"},{"location":"api_docs/sql/functions/string/base64_decode_binary/","title":"BASE64_DECODE_BINARY","text":"<p><code>BASE64_DECODE_BINARY(msg[, alphabet])</code></p> <p>The same as <code>BASE64_DECODE_STRING</code> except that the output is binary instead of a string.</p>"},{"location":"api_docs/sql/functions/string/base64_decode_string/","title":"BASE64_DECODE_STRING","text":"<p><code>BASE64_DECODE_STRING(msg[, alphabet])</code></p> <p>Reverses the process of calling <code>BASE64_ENCODE</code> on a string with the given alphabet, ignoring any newline characters produced by the <code>max_line_length</code> argument. Raises an exception if the string is malformed in any way. See here for Snowflake documentation.</p>"},{"location":"api_docs/sql/functions/string/base64_encode/","title":"BASE64_ENCODE","text":"<p><code>BASE64_ENCODE(msg[, max_line_length[, alphabet]])</code></p> <p>Encodes the <code>msg</code> string into a string using the base64 encoding scheme as if it were binary data (or directly encodes binary data). If <code>max_line_length</code> (default zero) is greater than zero, then newline characters will be inserted after that many characters to effectively add \"text wrapping\". If <code>alphabet</code> is provided, it specifies substitutes for the usual encoding characters for index 62, index 63, and the padding character. See here for Snowflake documentation.</p>"},{"location":"api_docs/sql/functions/string/char/","title":"CHAR","text":"<p><code>CHAR(int)</code></p> <p>Returns the character of the corresponding unicode value. Currently only supported for ASCII characters (0 to 127, inclusive)</p>"},{"location":"api_docs/sql/functions/string/charindex/","title":"CHARINDEX","text":"<p><code>CHARINDEX(str1, str2[, start_position])</code></p> <p>Equivalent to <code>POSITION(str1, str2)</code> when 2 arguments are provided. When the optional third argument is provided, it only starts searching at that index.</p> <p>Note</p> <p>Not currently supported on binary data.</p>"},{"location":"api_docs/sql/functions/string/concat/","title":"CONCAT","text":"<p><code>CONCAT(str_0, str_1, ...)</code></p> <p>Concatenates the strings together. Requires at least one argument.</p>"},{"location":"api_docs/sql/functions/string/concat_ws/","title":"CONCAT_WS","text":"<p><code>CONCAT_WS(str_separator, str_0, str_1, ...)</code></p> <p>Concatenates the strings together, with the specified separator. Requires at least two arguments.</p>"},{"location":"api_docs/sql/functions/string/editdistance/","title":"EDITDISTANCE","text":"<p><code>EDITDISTANCE(string0, string1[, max_distance])</code></p> <p>Returns the minimum edit distance between <code>string0</code> and <code>string1</code> according to Levenshtein distance. Optionally accepts a third argument specifying a maximum distance value. If the minimum edit distance between the two strings exceeds this value, then this value is returned instead. If it is negative, zero is returned.</p>"},{"location":"api_docs/sql/functions/string/endswith/","title":"ENDSWITH","text":"<p><code>ENDSWITH(str1, str2)</code></p> <p>Returns whether <code>str2</code> is a suffix of <code>str1</code>.</p>"},{"location":"api_docs/sql/functions/string/hex_decode_binary/","title":"HEX_DECODE_BINARY","text":"<p><code>HEX_DECODE_BINARY(msg)</code></p> <p>The same as <code>HEX_DECODE_STRING</code> except that the output is binary instead of a string.</p>"},{"location":"api_docs/sql/functions/string/hex_decode_string/","title":"HEX_DECODE_STRING","text":"<p><code>HEX_DECODE_STRING(msg)</code></p> <p>Reverses the process of calling <code>HEX_ENCODE</code> on a string with either capitalization. Raises an exception if the string is malformed in any way. See here for Snowflake documentation.</p>"},{"location":"api_docs/sql/functions/string/hex_encode/","title":"HEX_ENCODE","text":"<p><code>HEX_ENCODE(msg[, case])</code></p> <p>Encodes the <code>msg</code> string into a string using the hex encoding scheme as if it were binary data (or directly encodes binary data). If <code>case</code> (default one) is zero then the alphabetical hex characters are lowercase, if it is one then they are uppercase. See here for Snowflake documentation.</p>"},{"location":"api_docs/sql/functions/string/insert/","title":"INSERT","text":"<p><code>INSERT(str1, pos, len, str2)</code></p> <p>Inserts <code>str2</code> into <code>str1</code> at position <code>pos</code> (1-indexed), replacing the first <code>len</code> characters after <code>pos</code> in the process. If <code>len</code> is zero, inserts <code>str2</code> into <code>str1</code> without deleting any characters. If <code>pos</code> is one, prepends <code>str2</code> to <code>str1</code>. If <code>pos</code> is larger than the length of <code>str1</code>, appends <code>str2</code> to <code>str1</code>.</p> <p>Note</p> <p>Behavior when <code>pos</code> or <code>len</code> are negative is not well-defined at this time.</p>"},{"location":"api_docs/sql/functions/string/jarowinkler_similarity/","title":"JAROWINKLER_SIMILARITY","text":"<p><code>JAROWINKLER_SIMILARITY(string0, string1)</code></p> <p>Computes the Jaro-Winkler similarity between <code>string0</code> and <code>string1</code> as an integer between 0 and 100 (with 0 being no similarity and 100 being an exact match). The computation is not case-sensitive, but is sensitive to spaces or formatting characters. A scaling factor of 0.1 is used for the computation. For the definition of Jaro-Winkler similarity, see here.</p>"},{"location":"api_docs/sql/functions/string/lcase/","title":"LCASE","text":"<p><code>LCASE(str)</code></p> <p>Same as <code>LOWER</code>.</p>"},{"location":"api_docs/sql/functions/string/left/","title":"LEFT","text":"<p><code>LEFT(str, n)</code></p> <p>Takes a substring of the specified string consisting of the leftmost n characters</p>"},{"location":"api_docs/sql/functions/string/length/","title":"LENGTH","text":"<p><code>LENGTH(string)</code></p> <p>Returns the number of characters in the given string.</p>"},{"location":"api_docs/sql/functions/string/lower/","title":"LOWER","text":"<p><code>LOWER(str)</code></p> <p>Converts the string scalar/column to lower case.</p>"},{"location":"api_docs/sql/functions/string/lpad/","title":"LPAD","text":"<p><code>LPAD(string, len, padstring)</code></p> <p>Extends the input string to the specified length, by appending copies of the padstring to the left of the string. If the input string's length is less than the len argument, it will truncate the input string.</p> <p>For example: <pre><code>LPAD('hello', 10, 'abc') =='abcabhello'\nLPAD('hello', 1, 'abc') =='h'\n</code></pre></p>"},{"location":"api_docs/sql/functions/string/ltrim/","title":"LTRIM","text":"<p><code>LTRIM(str[, chars])</code></p> <p>Removes leading characters from a string column/literal str. These characters are specified by chars or are whitespace.</p>"},{"location":"api_docs/sql/functions/string/md5/","title":"MD5","text":"<p><code>MD5(msg)</code></p> <p>Encodes the <code>msg</code> string using the <code>MD5</code> algorithm. Outputs the result as a hex-encoded string.</p>"},{"location":"api_docs/sql/functions/string/md5_hex/","title":"MD5_HEX","text":"<p><code>MD5_HEX(msg)</code></p> <p>Equivalent to <code>MD5_HEX(msg)</code></p>"},{"location":"api_docs/sql/functions/string/mid/","title":"MID","text":"<p><code>MID(str, start_index, len)</code></p> <p>Equivalent to <code>SUBSTRING</code></p>"},{"location":"api_docs/sql/functions/string/ord/","title":"ORD","text":"<p><code>ORD(str)</code></p> <p>Returns the integer value of the unicode representation of the first character of the input string. returns 0 when passed the empty string</p>"},{"location":"api_docs/sql/functions/string/position/","title":"POSITION","text":"<p><code>POSITION(str1, str2)</code></p> <p>Returns the 1-indexed location where <code>str1</code> first occurs in <code>str2</code>, or 0 if there is no occurrences of <code>str1</code> in <code>str2</code>.</p> <p>Note</p> <p>BodoSQL oes not currently support alternate syntax <code>POSITION(str1, str2)</code>, or binary data.</p>"},{"location":"api_docs/sql/functions/string/repeat/","title":"REPEAT","text":"<p><code>REPEAT(str, len)</code></p> <p>Extends the specified string to the specified length by repeating the string. Will truncate the string If the string's length is less than the len argument</p> <p>For example:</p> <pre><code>REPEAT('abc', 7) =='abcabca'\nREPEAT('hello world', 5) =='hello'\n</code></pre>"},{"location":"api_docs/sql/functions/string/replace/","title":"REPLACE","text":"<p><code>REPLACE(base_string, substring_to_remove, string_to_substitute)</code></p> <p>Replaces all occurrences of the specified substring with the substitute string.</p> <p>For example: <pre><code>REPLACE('hello world', 'hello' 'hi') =='hi world'\n</code></pre></p>"},{"location":"api_docs/sql/functions/string/reverse/","title":"REVERSE","text":"<p><code>REVERSE(str)</code></p> <p>Returns the reversed string.</p>"},{"location":"api_docs/sql/functions/string/right/","title":"RIGHT","text":"<p><code>RIGHT(str, n)</code></p> <p>Takes a substring of the specified string consisting of the rightmost n characters</p>"},{"location":"api_docs/sql/functions/string/rpad/","title":"RPAD","text":"<p><code>RPAD(string, len, padstring)</code></p> <p>Extends the input string to the specified length, by appending copies of the padstring to the right of the string. If the input string's length is less than the len argument, it will truncate the input string.</p> <p>For example: <pre><code>RPAD('hello', 10, 'abc') =='helloabcab'\nRPAD('hello', 1, 'abc') =='h'\n</code></pre></p>"},{"location":"api_docs/sql/functions/string/rtrim/","title":"RTRIM","text":"<p><code>RTRIM(str[, chars])</code></p> <p>Removes trailing characters from a string column/literal str. These characters are specified by chars or are whitespace.</p>"},{"location":"api_docs/sql/functions/string/sha2/","title":"SHA2","text":"<p><code>SHA2(msg[, digest_size])</code></p> <p>Encodes the <code>msg</code> string using the <code>SHA-2</code> algorithm with the specified digest size (only values supported are, 224, 256, 384 and 512). Outputs the result as a hex-encoded string.</p>"},{"location":"api_docs/sql/functions/string/sha2_hex/","title":"SHA2_HEX","text":"<p><code>SHA2_HEX(msg[, digest_size])</code></p> <p>Equivalent to <code>SHA2(msg[, digest_size])</code></p>"},{"location":"api_docs/sql/functions/string/space/","title":"SPACE","text":"<p><code>SPACE(int)</code></p> <p>Returns a string containing the specified number of spaces.</p>"},{"location":"api_docs/sql/functions/string/split_part/","title":"SPLIT_PART","text":"<p><code>SPLIT_PART(source, delimiter, part)</code></p> <p>Returns the substring of the source between certain occurrence of the delimiter string, the occurrence being specified by the part. I.e. if part=1, returns the substring before the first occurrence, and if part=2, returns the substring between the first and second occurrence. Zero is treated like 1. Negative indices are allowed. If the delimiter is empty, the source is treated like a single token. If the part is out of bounds, '' is returned.</p>"},{"location":"api_docs/sql/functions/string/startswith/","title":"STARTSWITH","text":"<p><code>STARTSWITH(str1, str2)</code></p> <p>Returns whether <code>str2</code> is a prefix of <code>str1</code>.</p>"},{"location":"api_docs/sql/functions/string/strcmp/","title":"STRCMP","text":"<p><code>STRCMP(str1, str2)</code></p> <p>Compares the two strings lexicographically. If <code>str1 &gt; str2</code>, return 1. If <code>str1 &lt; str2</code>, returns -1. If <code>str1 == str2</code>, returns 0.</p>"},{"location":"api_docs/sql/functions/string/strtok/","title":"STRTOK","text":"<p><code>STRTOK(source[, delimiter[, part]])</code></p> <p>Tokenizes the source string by occurrences of any character in the delimiter string and returns the occurrence specified by the part. I.e. if part=1, returns the substring before the first occurrence, and if part=2, returns the substring between the first and second occurrence. Zero and negative indices are not allowed. Empty tokens are always skipped in favor of the next non-empty token. In any case where the only possible output is '', the output is <code>NULL</code>. The delimiter is optional and defaults to ' '. The part is optional and defaults to 1.</p>"},{"location":"api_docs/sql/functions/string/substr/","title":"SUBSTR","text":"<p><code>SUBSTR(str, start_index, len)</code></p> <p>Equivalent to <code>SUBSTRING</code></p>"},{"location":"api_docs/sql/functions/string/substring/","title":"SUBSTRING","text":"<p><code>SUBSTRING(str, start_index, len)</code></p> <p>Takes a substring of the specified string, starting at the specified index, of the specified length. <code>start_index = 1</code> specifies the first character of the string, <code>start_index = -1</code> specifies the last character of the string. <code>start_index = 0</code> causes the function to return empty string. If <code>start_index</code> is positive and greater than the length of the string, returns an empty string. If <code>start_index</code> is negative, and has an absolute value greater than the length of the string, the behavior is equivalent to <code>start_index = 1</code>.</p> <p>For example:</p> <pre><code>SUBSTRING('hello world', 1, 5) =='hello'\nSUBSTRING('hello world', -5, 7) =='world'\nSUBSTRING('hello world', -20, 8) =='hello wo'\nSUBSTRING('hello world', 0, 10) ==''\n</code></pre>"},{"location":"api_docs/sql/functions/string/substring_index/","title":"SUBSTRING_INDEX","text":"<p><code>SUBSTRING_INDEX(str, delimiter_str, n)</code></p> <p>Returns a substring of the input string, which contains all characters that occur before n occurrences of the delimiter string. if n is negative, it will return all characters that occur after the last n occurrences of the delimiter string. If <code>num_occurrences</code> is 0, it will return the empty string</p> <p>For example: <pre><code>SUBSTRING_INDEX('1,2,3,4,5', ',', 2) =='1,2'\nSUBSTRING_INDEX('1,2,3,4,5', ',', -2) =='4,5'\nSUBSTRING_INDEX('1,2,3,4,5', ',', 0) ==''\n</code></pre></p>"},{"location":"api_docs/sql/functions/string/trim/","title":"TRIM","text":"<p><code>TRIM(str[, chars])</code></p> <p>Returns the input string, will remove all spaces from the left and right of the string</p>"},{"location":"api_docs/sql/functions/string/try_base64_decode_binary/","title":"TRY_BASE64_DECODE_BINARY","text":"<p><code>TRY_BASE64_DECODE_BINARY(msg[, alphabet])</code></p> <p>Equivalent to <code>BASE64_DECODE_BINARY</code> except that it will return null instead of raising an exception if the string is malformed in any way.</p>"},{"location":"api_docs/sql/functions/string/try_base64_decode_string/","title":"TRY_BASE64_DECODE_STRING","text":"<p><code>TRY_BASE64_DECODE_STRING(msg[, alphabet])</code></p> <p>Equivalent to <code>BASE64_DECODE_STRING</code> except that it will return null instead of raising an exception if the string is malformed in any way.</p>"},{"location":"api_docs/sql/functions/string/try_hex_decode_binary/","title":"TRY_HEX_DECODE_BINARY","text":"<p><code>TRY_HEX_DECODE_BINARY(msg)</code></p> <p>Equivalent to <code>HEX_DECODE_BINARY</code> except that it will return null instead of raising an exception if the string is malformed in any way.</p>"},{"location":"api_docs/sql/functions/string/try_hex_decode_string/","title":"TRY_HEX_DECODE_STRING","text":"<p><code>TRY_HEX_DECODE_STRING(msg)</code></p> <p>Equivalent to <code>HEX_DECODE_STRING</code> except that it will return null instead of raising an exception if the string is malformed in any way.</p>"},{"location":"api_docs/sql/functions/string/ucase/","title":"UCASE","text":"<p><code>UCASE(str)</code></p> <p>Same as <code>UPPER</code>.</p>"},{"location":"api_docs/sql/functions/string/upper/","title":"UPPER","text":"<p><code>UPPER(str)</code></p> <p>Converts the string scalar/column to upper case.</p>"},{"location":"api_docs/sql/functions/table/","title":"Table Functions","text":"<p>Bodo currently supports the following functions that produce tables:</p> <p><code>EXTERNAL_TABLE_FILES</code> <code>FLATTEN</code> <code>GENERATOR</code> <code>INDEX</code> <code>SPLIT_TO_TABLE</code></p>"},{"location":"api_docs/sql/functions/table/external_table_files/","title":"EXTERNAL_TABLE_FILES","text":"<p><code>EXTERNAL_TABLE_FILES(TABLE_NAME=&gt;string_literal)</code></p> <p>See here for the Snowflake documentation.</p>"},{"location":"api_docs/sql/functions/table/flatten/","title":"FLATTEN","text":"<p><code>FLATTEN([INPUT=&gt;]expr[, PATH=&gt;path_epxr][, OUTER=&gt;outer_expr][, RECURSIVE=&gt;recursive_expr][, MODE=&gt;mode_epxr])</code></p> <p>Takes in a column of semi-structured data and produces a table by \"exploding\" the data into multiple rows, producing the following columns:</p> <ul> <li><code>SEQ</code>: not currently supported by BodoSQL.</li> <li><code>KEY</code>: the individual values from the json data.</li> <li><code>PATH</code>: not currently supported by BodoSQL.</li> <li><code>INDEX</code>: the index within the array that the value came from.</li> <li><code>VALUE</code>: the individual values from the array or json data.</li> <li><code>THIS</code>: a copy of the input data.</li> </ul> <p>The function has the following named arguments:</p> <ul> <li><code>INPUT</code> (required): the expression of semi-structured data to flatten. Also allowed to be passed in as a positional argument without the <code>INPUT</code> keyword.</li> <li><code>PATH</code> (optional): a constant expression referencing how to access the semi-structured data to flatten from the input expression. BodoSQL currently only supports when this argument is omitted or is an empty string (indicating that the expression itself is the array to flatten).</li> <li><code>OUTER</code> (optional): a boolean indicating if a row should be generated even if the input data is an empty/null array/struct/map. The default is false. If provided, the <code>KEY</code>, <code>PATH</code>, <code>INDEX</code> and <code>VALUE</code> outputs will be null in the generated row.</li> <li><code>RECURSIVE</code> (optional): a boolean indicating if flattening should occur recursively, as opposed to just on the data referenced by <code>PATH</code>. BodoSQL currently only supports when this argument is omitted or is false (which is the default).</li> <li><code>MODE</code> (optional): a string literal that can be either <code>'OBJECT'</code>, <code>'ARRAY'</code> or <code>'BOTH'</code>, indicating what type of flattening rule should be done. BodoSQL currently only supports when this argument is omitted or is <code>'BOTH'</code> (which is the default).</li> </ul> <p>Note</p> <p>BodoSQL supports the input argument being an array, json or variant so long as the values are of the same type (with limited support for JSON when the values are also JSON).</p> <p>Below is an example of a query using the <code>FLATTEN</code> function with the <code>LATERAL</code> keyword to explode an array column while also replicating another column.</p> <pre><code>SELECT id, lat.index as idx, lat.value as val FROM table1, lateral flatten(tags) lat\n</code></pre> <p>If the input data was as follows:</p> id tags 10 [\"A\", \"B\"] 16 [] 72 [\"C\", \"A\", \"B\", \"D\", \"C\"] <p>Then the query would produce the following data:</p> id idx val 10 0 \"A\" 10 1 \"B\" 72 0 \"C\" 72 1 \"A\" 72 2 \"B\" 72 3 \"D\" 72 4 \"C\" <p>Below is an example of a query using the <code>FLATTEN</code> function with the <code>LATERAL</code> keyword to explode an JSON column while also replicating another column.</p> <pre><code>SELECT id, lat.key as key, lat.value as val FROM table1, lateral flatten(attributes) lat\n</code></pre> <p>If the input data was as follows:</p> id attributes 42 {\"A\": 0} 50 {} 64 {\"B\": 1, \"C\": 2} <p>Then the query would produce the following data:</p> id key value 42 \"A\" 0 64 \"B\" 1 64 \"C\" 2"},{"location":"api_docs/sql/functions/table/generator/","title":"GENERATOR","text":"<p><code>GENERATOR([ROWCOUNT=&gt;count][, TIMELIMIT=&gt;sec])</code></p> <p>Generates a table with a certain number of rows, specified by the <code>ROWCOUNT</code> argument. Currently only supports when the <code>ROW_COUNT</code> argument is provided and when it is a non-negative integer. Does not support when the <code>TIMELIMIT</code> argument is provided, neither argument is provided, or both are provided.</p>"},{"location":"api_docs/sql/functions/table/split_to_table/","title":"SPLIT_TO_TABLE","text":"<p><code>SPLIT_TO_TABLE(str, delim)</code></p> <p>Takes in a string column and a delimeter and produces a table by \"exploding\" the string into multiple rows based on the delimeter, producing the following columns:</p> <ul> <li><code>SEQ</code>: not currently supported by BodoSQL.</li> <li><code>INDEX</code>: which index in the splitted string did the current seciton come from.</li> <li><code>VALUE</code>: the current section of the splitted string.</li> </ul> <p>Note</p> <p>Currently, BodoSQL supports this function as an alias for <code>FLATTEN(SPLIT(str, delim))</code>.</p> <p>Below is an example of a query using the <code>SPLIT_TO_TABLE</code> function with the <code>LATERAL</code> keyword to explode an string column while also replicating another column.</p> <pre><code>SELECT id, lat.index as idx, lat.value as val FROM table1, lateral split_to_table(colors, ' ') lat\n</code></pre> <p>If the input data was as follows:</p> id colors 50 \"red orange yellow\" 75 \"green blue\" <p>Then the query would produce the following data:</p> id idx val 50 0 \"red\" 50 1 \"orange\" 50 2 \"yellow\" 75 0 \"green\" 75 1 \"blue\""},{"location":"api_docs/sql/functions/timestamp/","title":"Timestamp Functions","text":"<p>BodoSQL currently supports the following Timestamp functions:</p> <ul> <li><code>ADDDATE</code></li> <li><code>CURDATE</code></li> <li><code>CURRENT_DATE</code></li> <li><code>CURRENT_TIME</code></li> <li><code>CURRENT_TIMESTAMP</code></li> <li><code>DATE_ADD</code></li> <li><code>DATE_FORMAT</code></li> <li><code>DATE_FROM_PARTS</code></li> <li><code>DATE_PART</code></li> <li><code>DATE_SUB</code></li> <li><code>DATE_TRUNC</code></li> <li><code>DATEADD</code></li> <li><code>DATEDIFF</code></li> <li><code>DATEFROMPARTS</code></li> <li><code>DAYNAME</code></li> <li><code>EXTRACT</code></li> <li><code>FROM_DAYS</code></li> <li><code>FROM_UNIXTIME</code></li> <li><code>GETDATE</code></li> <li><code>HOUR</code></li> <li><code>LAST_DAY</code></li> <li><code>LOCALTIME</code></li> <li><code>LOCALTIMESTAMP</code></li> <li><code>MAKEDATE</code></li> <li><code>MICROSECOND</code></li> <li><code>MINUTE</code></li> <li><code>MONTH</code></li> <li><code>MONTH_NAME</code></li> <li><code>MONTHNAME</code></li> <li><code>NOW</code></li> <li><code>QUARTER</code></li> <li><code>SECOND</code></li> <li><code>STR_TO_DATE</code></li> <li><code>SUBDATE</code></li> <li><code>SYSDATE</code></li> <li><code>SYSTIMESTAMP</code></li> <li><code>TIME_FROM_PARTS</code></li> <li><code>TIME_SLICE</code></li> <li><code>TIMEADD</code></li> <li><code>TIMEFROMPARTS</code></li> <li><code>TIMESTAMP_FROM_PARTS</code></li> <li><code>TIMESTAMP_LTZ_FROM_PARTS</code></li> <li><code>TIMESTAMP_NTZ_FROM_PARTS</code></li> <li><code>TIMESTAMP_TZ_FROM_PARTS</code></li> <li><code>TIMESTAMPADD</code></li> <li><code>TIMESTAMPDIFF</code></li> <li><code>TIMESTAMPFROMPARTS</code></li> <li><code>TIMESTAMPLTZFROMPARTS</code></li> <li><code>TIMESTAMPNTZFROMPARTS</code></li> <li><code>TIMESTAMPTZFROMPARTS</code></li> <li><code>TO_DAYS</code></li> <li><code>TO_SECONDS</code></li> <li><code>TRUNC</code></li> <li><code>UNIX_TIMESTAMP</code></li> <li><code>UTC_DATE</code></li> <li><code>UTC_TIMESTAMP</code></li> <li><code>WEEK</code></li> <li><code>WEEKDAY</code></li> <li><code>WEEKISO</code></li> <li><code>WEEKOFYEAR</code></li> <li><code>YEAR</code></li> <li><code>YEAROFWEEKISO</code></li> <li><code>YEARWEEK</code></li> </ul>"},{"location":"api_docs/sql/functions/timestamp/adddate/","title":"ADDDATE","text":"<p><code>ADDDATE(timestamp_val, interval)</code></p> <p>Same as <code>DATE_ADD</code></p>"},{"location":"api_docs/sql/functions/timestamp/curdate/","title":"CURDATE","text":"<p><code>CURDATE()</code></p> <p>Computes a timestamp equal to the current system time, excluding the time information</p>"},{"location":"api_docs/sql/functions/timestamp/current_date/","title":"CURRENT_DATE","text":"<p><code>CURRENT_DATE()</code></p> <p>Equivalent to <code>CURDATE</code></p>"},{"location":"api_docs/sql/functions/timestamp/current_time/","title":"CURRENT_TIME","text":"<p><code>CURRENT_TIME()</code></p> <p>Equivalent to <code>LOCALTIME</code></p>"},{"location":"api_docs/sql/functions/timestamp/current_timestamp/","title":"CURRENT_TIMESTAMP","text":"<p><code>CURRENT_TIMESTAMP()</code></p> <p>Equivalent to <code>NOW</code></p>"},{"location":"api_docs/sql/functions/timestamp/date_add/","title":"DATE_ADD","text":"<p><code>DATE_ADD(timestamp_val, interval)</code></p> <p>Computes a timestamp column by adding an interval column/scalar to a timestamp value. If the first argument is a string representation of a timestamp, Bodo will cast the value to a timestamp.</p> <p><code>DATE_ADD(timestamp_val, amount)</code></p> <p>Equivalent to <code>DATE_ADD('day', amount, timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/date_format/","title":"DATE_FORMAT","text":"<p><code>DATE_FORMAT(timestamp_val, literal_format_string)</code></p> <p>Converts a timestamp value to a String value given a scalar format string.</p> <p>Recognized formatting characters:</p> <p><code>%i</code> Minutes, zero padded (00 to 59) <code>%M</code> Full month name (January to December) <code>%r</code> Time in format in the format (hh:mm:ss AM/PM) <code>%s</code> Seconds, zero padded (00 to 59) <code>%T</code> Time in format in the format (hh:mm:ss) <code>%T</code> Time in format in the format (hh:mm:ss) <code>%u</code> week of year, where monday is the first day of the week(00 to 53) <code>%a</code> Abbreviated weekday name (sun-sat) <code>%b</code> Abbreviated month name (jan-dec) <code>%f</code> Microseconds, left padded with 0's, (000000 to 999999) <code>%H</code> Hour, zero padded (00 to 23) <code>%j</code> Day Of Year, left padded with 0's (001 to 366) <code>%m</code> Month number (00 to 12) <code>%p</code> AM or PM, depending on the time of day <code>%d</code> Day of month, zero padded (01 to 31) <code>%Y</code> Year as a 4 digit value <code>%y</code> Year as a 2 digit value, zero padded (00 to 99) <code>%U</code> Week of year, where Sunday is the first day of the week     (00 to 53) <code>%S</code> Seconds, zero padded (00 to 59)</p> <p>For example:</p> <pre><code>DATE_FORMAT(Timestamp '2020-01-12', '%Y %m %d') =='2020 01 12'\nDATE_FORMAT(Timestamp '2020-01-12 13:39:12', 'The time was %T %p. It was a %u') =='The time was 13:39:12 PM. It was a Sunday'\n</code></pre>"},{"location":"api_docs/sql/functions/timestamp/date_from_parts/","title":"DATE_FROM_PARTS","text":"<ul> <li><code>DATE_FROM_PARTS(year, month, day)</code></li> </ul> <p>Constructs a date from the integer inputs specified, e.g. <code>(2020, 7, 4)</code> will output July 4<sup>th</sup>, 2020.</p> <p>Note</p> <p>Month does not have to be in the 1-12 range, and day does not have to be in the 1-31 range. Values out of bounds are overflowed logically, e.g. <code>(2020, 14, -1)</code> will output January 31<sup>st</sup>, 2021.</p>"},{"location":"api_docs/sql/functions/timestamp/date_part/","title":"DATE_PART","text":"<p><code>DATE_PART(unit, timestamp_val)</code></p> <p>Equivalent to <code>EXTRACT(unit FROM timestamp_val)</code> with the following unit string literals:</p> <ul> <li>YEAR: <code>year</code>, <code>years</code>, <code>yr</code>, <code>yrs</code>, <code>y</code>, <code>yy</code>, <code>yyy</code>, <code>yyyy</code></li> <li>QUARTER: <code>quarter</code>, <code>quarters</code>, <code>q</code>, <code>qtr</code>, <code>qtrs</code></li> <li>MONTH: <code>month</code>, <code>months</code>, <code>mm</code>, <code>mon</code>, <code>mons</code></li> <li>WEEK: <code>week</code>, <code>weeks</code>, <code>weekofyear</code>, <code>w</code>, <code>wk</code>, <code>woy</code>, <code>wy</code></li> <li>DAY: <code>day</code>, <code>days</code>, <code>dayofmonth</code>, <code>d</code>, <code>dd</code></li> <li>HOUR: <code>hour</code>, <code>hours</code>, <code>hrs</code>, <code>h</code>, <code>hr</code>, <code>hrs</code></li> <li>MINUTE: <code>minute</code>, <code>minutes</code>, <code>m</code>, <code>mi</code>, <code>min</code>, <code>mins</code></li> <li>SECOND: <code>second</code>, <code>seconds</code>, <code>s</code>, <code>sec</code>, <code>secs</code></li> <li>MILLISECOND: <code>millisecond</code>, <code>milliseconds</code>, <code>ms</code>, <code>msecs</code></li> <li>MICROSECOND: <code>microsecond</code>, <code>microseconds</code>, <code>us</code>, <code>usec</code></li> <li>NANOSECOND: <code>nanosecond</code>, <code>nanoseconds</code>, <code>nanosec</code>, <code>nsec</code>, <code>nsecs</code>, <code>nsecond</code>, <code>ns</code>, <code>nanonsecs</code></li> <li>TZH</li> <li>TZM</li> </ul> <p>Supported with timezone-aware data. Note that <code>TZH</code>/<code>TZM</code> are only supported for <code>TIMESTAMP_TZ</code> inputs and extracts the offset hours and offset minutes respectively.</p>"},{"location":"api_docs/sql/functions/timestamp/date_sub/","title":"DATE_SUB","text":"<p><code>DATE_SUB(timestamp_val, interval)</code></p> <p>Computes a timestamp column by subtracting an interval column/scalar to a timestamp value. If the first argument is a string representation of a timestamp, Bodo will cast the value to a timestamp.</p>"},{"location":"api_docs/sql/functions/timestamp/date_trunc/","title":"DATE_TRUNC","text":"<p><code>DATE_TRUNC(str_literal, timestamp_val)</code></p> <p>Truncates a timestamp to the provided str_literal field. str_literal must be a compile time constant and one of:</p> <ul> <li>\"MONTH\"</li> <li>\"WEEK\"</li> <li>\"DAY\"</li> <li>\"HOUR\"</li> <li>\"MINUTE\"</li> <li>\"SECOND\"</li> <li>\"MILLISECOND\"</li> <li>\"MICROSECOND\"</li> <li>\"NANOSECOND\"</li> </ul>"},{"location":"api_docs/sql/functions/timestamp/dateadd/","title":"DATEADD","text":"<p><code>DATEADD(unit, amount, timestamp_val)</code></p> <p>Computes a timestamp column by adding the amount of the specified unit to the timestamp val. For example, <code>DATEADD('day', 3, T)</code> adds 3 days to column <code>T</code>. Allows the following units, with the specified abbreviations as string literals:</p> <ul> <li>YEAR: <code>year</code>, <code>years</code>, <code>yr</code>, <code>yrs</code>, <code>y</code>, <code>yy</code>, <code>yyy</code>, <code>yyyy</code></li> <li>QUARTER: <code>quarter</code>, <code>quarters</code>, <code>q</code>, <code>qtr</code>, <code>qtrs</code></li> <li>MONTH: <code>month</code>, <code>months</code>, <code>mm</code>, <code>mon</code>, <code>mons</code></li> <li>WEEK: <code>week</code>, <code>weeks</code>, <code>weekofyear</code>, <code>w</code>, <code>wk</code>, <code>woy</code>, <code>wy</code></li> <li>DAY: <code>day</code>, <code>days</code>, <code>dayofmonth</code>, <code>d</code>, <code>dd</code></li> <li>HOUR: <code>hour</code>, <code>hours</code>, <code>hrs</code>, <code>h</code>, <code>hr</code>, <code>hrs</code></li> <li>MINUTE: <code>minute</code>, <code>minutes</code>, <code>m</code>, <code>mi</code>, <code>min</code>, <code>mins</code></li> <li>SECOND: <code>second</code>, <code>seconds</code>, <code>s</code>, <code>sec</code>, <code>secs</code></li> <li>MILLISECOND: <code>millisecond</code>, <code>milliseconds</code>, <code>ms</code>, <code>msecs</code></li> <li>MICROSECOND: <code>microsecond</code>, <code>microseconds</code>, <code>us</code>, <code>usec</code></li> <li>NANOSECOND: <code>nanosecond</code>, <code>nanoseconds</code>, <code>nanosec</code>, <code>nsec</code>, <code>nsecs</code>, <code>nsecond</code>, <code>ns</code>, <code>nanonsecs</code></li> </ul> <p>Supported with timezone-aware data.</p> <p><code>DATEADD(timestamp_val, amount)</code></p> <p>Equivalent to <code>DATEADD('day', amount, timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/datediff/","title":"DATEDIFF","text":"<p><code>DATEDIFF(timestamp_val1, timestamp_val2)</code></p> <p>Computes the difference in days between two Timestamp values (timestamp_val1 - timestamp_val2)</p> <p><code>DATEDIFF(unit, timestamp_val1, timestamp_val2)</code></p> <p>Computes the difference between two Timestamp values (timestamp_val2 - timestamp_val1) in terms of unit</p> <p>Allows the following units, with the specified abbreviations as string literals:</p> <ul> <li>YEAR: <code>year</code>, <code>years</code>, <code>yr</code>, <code>yrs</code>, <code>y</code>, <code>yy</code>, <code>yyy</code>, <code>yyyy</code></li> <li>QUARTER: <code>quarter</code>, <code>quarters</code>, <code>q</code>, <code>qtr</code>, <code>qtrs</code></li> <li>MONTH: <code>month</code>, <code>months</code>, <code>mm</code>, <code>mon</code>, <code>mons</code></li> <li>WEEK: <code>week</code>, <code>weeks</code>, <code>weekofyear</code>, <code>w</code>, <code>wk</code>, <code>woy</code>, <code>wy</code></li> <li>DAY: <code>day</code>, <code>days</code>, <code>dayofmonth</code>, <code>d</code>, <code>dd</code></li> <li>HOUR: <code>hour</code>, <code>hours</code>, <code>hrs</code>, <code>h</code>, <code>hr</code>, <code>hrs</code></li> <li>MINUTE: <code>minute</code>, <code>minutes</code>, <code>m</code>, <code>mi</code>, <code>min</code>, <code>mins</code></li> <li>SECOND: <code>second</code>, <code>seconds</code>, <code>s</code>, <code>sec</code>, <code>secs</code></li> <li>MILLISECOND: <code>millisecond</code>, <code>milliseconds</code>, <code>ms</code>, <code>msecs</code></li> <li>MICROSECOND: <code>microsecond</code>, <code>microseconds</code>, <code>us</code>, <code>usec</code></li> <li>NANOSECOND: <code>nanosecond</code>, <code>nanoseconds</code>, <code>nanosec</code>, <code>nsec</code>, <code>nsecs</code>, <code>nsecond</code>, <code>ns</code>, <code>nanonsecs</code></li> </ul> <p>Note that if <code>timestamp1</code> or <code>timestamp2</code> are <code>TIMESTAMP_TZ</code> they will first be converted their to <code>UTC</code> timestamps (offset is subtracted from the local timestamp value).</p>"},{"location":"api_docs/sql/functions/timestamp/datefromparts/","title":"DATEFROMPARTS","text":"<ul> <li><code>DATEFROMPARTS(year, month, day)</code></li> </ul> <p>Equivalent to <code>DATE_FROM_PARTS</code></p>"},{"location":"api_docs/sql/functions/timestamp/dayname/","title":"DAYNAME","text":"<p><code>DAYNAME(timestamp_val)</code></p> <p>Computes the 3 letter abreviation for the day of the timestamp value.</p>"},{"location":"api_docs/sql/functions/timestamp/extract/","title":"EXTRACT","text":"<p><code>EXTRACT(TimeUnit from timestamp_val)</code></p> <p>Extracts the specified TimeUnit from the supplied date.</p> <p>Allowed TimeUnits are:</p> <ul> <li><code>MICROSECOND</code></li> <li><code>MINUTE</code></li> <li><code>HOUR</code></li> <li><code>DAY</code> (Day of Month)</li> <li><code>DOY</code> (Day of Year)</li> <li><code>DOW</code> (Day of week)</li> <li><code>WEEK</code></li> <li><code>MONTH</code></li> <li><code>QUARTER</code></li> <li><code>YEAR</code></li> </ul> <p>TimeUnits are not case-sensitive.</p>"},{"location":"api_docs/sql/functions/timestamp/from_days/","title":"FROM_DAYS","text":"<p><code>FROM_DAYS(n)</code></p> <p>Returns a timestamp values that is n days after year 0 of the Gregorian calendar</p>"},{"location":"api_docs/sql/functions/timestamp/from_unixtime/","title":"FROM_UNIXTIME","text":"<p><code>FROM_UNIXTIME(n)</code></p> <p>Returns a Timestamp value that is n seconds after the unix epoch</p>"},{"location":"api_docs/sql/functions/timestamp/getdate/","title":"GETDATE","text":"<p><code>GETDATE()</code></p> <p>Equivalent to <code>NOW</code></p>"},{"location":"api_docs/sql/functions/timestamp/hour/","title":"HOUR","text":"<p><code>HOUR(timestamp_val)</code></p> <p>Equivalent to <code>EXTRACT(HOUR from timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/last_day/","title":"LAST_DAY","text":"<p><code>LAST_DAY(timestamp_val)</code></p> <p>Given a timestamp value, returns a timestamp value that is the last day in the same month as timestamp_val.</p>"},{"location":"api_docs/sql/functions/timestamp/localtime/","title":"LOCALTIME","text":"<p><code>LOCALTIME()</code></p> <p>Computes a time equal to the current time in the session's timezone. By default the current time is in local time, and it can be updated as a parameter when using the Snowflake Catalog.</p>"},{"location":"api_docs/sql/functions/timestamp/localtimestamp/","title":"LOCALTIMESTAMP","text":"<p><code>LOCALTIMESTAMP()</code></p> <p>Equivalent to <code>NOW</code></p>"},{"location":"api_docs/sql/functions/timestamp/makedate/","title":"MAKEDATE","text":"<p><code>MAKEDATE(integer_years_val, integer_days_val)</code></p> <p>Computes a timestamp value that is the specified number of days after the specified year.</p>"},{"location":"api_docs/sql/functions/timestamp/microsecond/","title":"MICROSECOND","text":"<p><code>MICROSECOND(timestamp_val)</code></p> <p>Equivalent to <code>EXTRACT(MICROSECOND from timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/minute/","title":"MINUTE","text":"<p><code>MINUTE(timestamp_val)</code></p> <p>Equivalent to <code>EXTRACT(MINUTE from timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/month/","title":"MONTH","text":"<p><code>MONTH(timestamp_val)</code></p> <p>Equivalent to <code>EXTRACT(MONTH from timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/month_name/","title":"MONTH_NAME","text":"<p><code>MONTH_NAME(timestamp_val)</code></p> <p>Computes the 3 letter abreviation for the month of the timestamp value.</p>"},{"location":"api_docs/sql/functions/timestamp/monthname/","title":"MONTHNAME","text":"<p><code>MONTHNAME(timestamp_val)</code></p> <p>Computes the 3 letter abreviation for the month of the timestamp value.</p>"},{"location":"api_docs/sql/functions/timestamp/now/","title":"NOW","text":"<p><code>NOW()</code></p> <p>Computes a timestamp equal to the current time in the session's timezone. By default, the current timezone is UTC, and it can be updated as a parameter when using the Snowflake Catalog.</p>"},{"location":"api_docs/sql/functions/timestamp/quarter/","title":"QUARTER","text":"<p><code>QUARTER(timestamp_val)</code></p> <p>Equivalent to <code>EXTRACT(QUARTER from timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/second/","title":"SECOND","text":"<p><code>SECOND(timestamp_val)</code></p> <p>Equivalent to <code>EXTRACT(SECOND from timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/str_to_date/","title":"STR_TO_DATE","text":"<p><code>STR_TO_DATE(str_val, literal_format_string)</code></p> <p>Converts a string value to a Timestamp value given a literal format string. If a year, month, and day value is not specified, they default to 1900, 01, and 01 respectively. Will throw a runtime error if the string cannot be parsed into the expected values. See <code>DATE_FORMAT</code> for recognized formatting characters.</p> <p>For example:</p> <pre><code>STR_TO_DATE('2020 01 12', '%Y %m %d') ==Timestamp '2020-01-12'\nSTR_TO_DATE('01 12', '%m %d') ==Timestamp '1900-01-12'\nSTR_TO_DATE('hello world', '%Y %m %d') ==RUNTIME ERROR\n</code></pre>"},{"location":"api_docs/sql/functions/timestamp/subdate/","title":"SUBDATE","text":"<p><code>SUBDATE(timestamp_val, interval)</code></p> <p>Same as <code>DATE_SUB</code></p>"},{"location":"api_docs/sql/functions/timestamp/sysdate/","title":"SYSDATE","text":"<ul> <li><code>SYSDATE()</code></li> </ul> <p>Equivalent to <code>UTC_TIMESTAMP</code></p>"},{"location":"api_docs/sql/functions/timestamp/systimestamp/","title":"SYSTIMESTAMP","text":"<p><code>SYSTIMESTAMP()</code></p> <p>Equivalent to <code>NOW</code></p>"},{"location":"api_docs/sql/functions/timestamp/time_from_parts/","title":"TIME_FROM_PARTS","text":"<p><code>TIME_FROM_PARTS(integer_hour_val, integer_minute_val, integer_second_val [, integer_nanoseconds_val])</code></p> <p>Creates a time from individual numeric components. Usually, <code>integer_hour_val</code> is in the 0-23 range, <code>integer_minute_val</code> is in the 0-59 range, <code>integer_second_val</code> is in the 0-59 range, and <code>integer_nanoseconds_val</code> (if provided) is a 9-digit integer. <pre><code>TIMEFROMPARTS(12, 34, 56, 987654321)\n12:34:56.987654321\n</code></pre></p>"},{"location":"api_docs/sql/functions/timestamp/time_slice/","title":"TIME_SLICE","text":"<p><code>TIME_SLICE(date_or_time_expr, slice_length, unit[, start_or_end])</code></p> <p>Calculates one of the endpoints of a \"slice\" of time containing the date specified by <code>date_or_time_expr</code> where each slice has length of time corresponding to <code>slice_length</code> times the date/time unit specified by <code>unit</code>. The slice start/ends are always aligned to the unix epoch <code>1970-01-1</code> (at midnight). The fourth argument specifies whether to return the begining or the end of the slice (<code>'START'</code> for begining, <code>'END'</code> for end), where the default is <code>'START'</code>.</p> <p>For example, <code>TIME_SLICE(T, 3, 'YEAR')</code> would return the timestamp corresponding to the begining of the first 3-year window (aligned with 1970) that contains timestamp <code>T</code>. So <code>T = 1995-7-4 12:30:00</code> would output <code>1994-1-1</code> for <code>'START'</code> or <code>1997-1-1</code> for <code>'END'</code>.</p>"},{"location":"api_docs/sql/functions/timestamp/timeadd/","title":"TIMEADD","text":"<p><code>TIMEADD(unit, amount, timestamp_val)</code></p> <p>Equivalent to <code>DATEADD</code>.</p>"},{"location":"api_docs/sql/functions/timestamp/timefromparts/","title":"TIMEFROMPARTS","text":"<p><code>TIMEFROMPARTS(integer_hour_val, integer_minute_val, integer_second_val [, integer_nanoseconds_val])</code></p> <p>See TIME_FROM_PARTS.</p> <pre><code>TIMEFROMPARTS(12, 34, 56, 987654321)\n12:34:56.987654321\n</code></pre>"},{"location":"api_docs/sql/functions/timestamp/timestamp_from_parts/","title":"TIMESTAMP_FROM_PARTS","text":"<p><code>TIMESTAMP_FROM_PARTS(year, month, day, hour, minute, second[, nanosecond[, timezone]])</code> <code>TIMESTAMP_FROM_PARTS(date_expr, time_expr)</code> The first overload is equivalent to <code>DATE_FROM_PARTS</code> but also takes in an hour, minute and second (which can be out of bounds just like the month/day). Optionally takes in a nanosecond value, and a timezone value for the output. If the timezone is not specified, the output is timezone-naive. Note that if any numeric argument cannot be converted to an int64, then it will become NULL.</p> <p>Note</p> <p>Timezone argument is not supported at this time.</p> <p>The second overload constructs the timestamp by combining the date and time arguments. The output of this function is always timestamp-naive.</p>"},{"location":"api_docs/sql/functions/timestamp/timestamp_ltz_from_parts/","title":"TIMESTAMP_LTZ_FROM_PARTS","text":"<ul> <li><code>TIMESTAMP_LTZ_FROM_PARTS(year, month, day, hour, minute, second[, nanosecond])</code></li> </ul> <p>Equivalent to <code>TIMESTAMP_FROM_PARTS(year, month, day, hour, minute, second[, nanosecond])</code> but without the optional timezone argument in the first overload. The output is always timezone-aware using the local timezone.</p>"},{"location":"api_docs/sql/functions/timestamp/timestamp_ntz_from_parts/","title":"TIMESTAMP_NTZ_FROM_PARTS","text":"<ul> <li><code>TIMESTAMP_NTZ_FROM_PARTS(year, month, day, hour, minute, second[, nanosecond])</code></li> <li><code>TIMESTAMP_NTZ_FROM_PARTS(date_expr, time_expr)</code></li> </ul> <p>Equivalent to <code>TIMESTAMP_FROM_PARTS</code> but without the optional timezone argument in the first overload. The output is always timezone-naive.</p>"},{"location":"api_docs/sql/functions/timestamp/timestamp_tz_from_parts/","title":"TIMESTAMP_TZ_FROM_PARTS","text":"<ul> <li><code>TIMESTAMP_TZ_FROM_PARTS(year, month, day, hour, minute, second[, nanosecond[, timezone]])</code></li> </ul> <p>Returns a TIMESTAMP_TZ constructed with the specified date/time components using the offset of the provided timezone at that time of year. If no timezone is provided, the session timezone is used.</p>"},{"location":"api_docs/sql/functions/timestamp/timestampadd/","title":"TIMESTAMPADD","text":"<p><code>TIMESTAMPADD(unit, amount, timestamp_val)</code></p> <p>Equivalent to <code>DATEADD</code>.</p>"},{"location":"api_docs/sql/functions/timestamp/timestampdiff/","title":"TIMESTAMPDIFF","text":"<p><code>TIMESTAMPDIFF(unit, timestamp_val1, timestamp_val2)</code></p> <p>Returns the amount of time that has passed since <code>timestamp_val1</code> until <code>timestamp_val2</code> in terms of the unit specified, ignoring all smaller units. E.g., December 31 of 2020 and January 1 of 2021 count as 1 year apart.</p> <p>Note</p> <p>For all units larger than <code>NANOSECOND</code>, the output type is <code>INTEGER</code> instead of <code>BIGINT</code>, so any difference values that cannot be stored as signed 32-bit integers might not be returned correct.</p>"},{"location":"api_docs/sql/functions/timestamp/timestampfromparts/","title":"TIMESTAMPFROMPARTS","text":"<ul> <li><code>TIMESTAMPFROMPARTS(year, month, day, hour, minute, second[, nanosecond[, timezone]])</code></li> <li><code>TIMESTAMPFROMPARTS(date_expr, time_expr)</code></li> </ul> <p>Equivalent to <code>TIMESTAMP_FROM_PARTS</code></p>"},{"location":"api_docs/sql/functions/timestamp/timestampltzfromparts/","title":"TIMESTAMPLTZFROMPARTS","text":"<ul> <li><code>TIMESTAMP_LTZ_FROM_PARTS(year, month, day, hour, minute, second[, nanosecond])</code></li> </ul> <p>Equivalent to <code>TIMESTAMP_LTZ_FROM_PARTS</code></p>"},{"location":"api_docs/sql/functions/timestamp/timestampntzfromparts/","title":"TIMESTAMPNTZFROMPARTS","text":"<ul> <li><code>TIMESTAMP_NTZ_FROM_PARTS(year, month, day, hour, minute, second[, nanosecond])</code></li> <li><code>TIMESTAMP_NTZ_FROM_PARTS(date_expr, time_expr)</code></li> </ul> <p>Equivalent to <code>TIMESTAMP_NTZ_FROM_PARTS</code></p>"},{"location":"api_docs/sql/functions/timestamp/timestamptzfromparts/","title":"TIMESTAMPTZFROMPARTS","text":"<ul> <li><code>TIMESTAMPTZFROMPARTS(year, month, day, hour, minute, second[, nanosecond[, timezone]])</code></li> </ul> <p>Equivalent to <code>TIMESTAMP_TZ_FROM_PARTS</code></p>"},{"location":"api_docs/sql/functions/timestamp/to_days/","title":"TO_DAYS","text":"<p><code>TO_DAYS(timestamp_val)</code></p> <p>Computes the difference in days between the input timestamp, and year 0 of the Gregorian calendar</p>"},{"location":"api_docs/sql/functions/timestamp/to_seconds/","title":"TO_SECONDS","text":"<p><code>TO_SECONDS(timestamp_val)</code></p> <p>Computes the number of seconds since year 0 of the Gregorian calendar</p>"},{"location":"api_docs/sql/functions/timestamp/trunc/","title":"TRUNC","text":"<p><code>TRUNC(timestamp_val, str_literal)</code></p> <p>Equivalent to <code>DATE_TRUNC(str_literal, timestamp_val)</code>. The argument order is reversed when compared to <code>DATE_TRUNC</code>. Note that <code>TRUNC</code> is overloaded, and may invoke the numeric function <code>TRUNCATE</code> if the arguments are numeric.</p>"},{"location":"api_docs/sql/functions/timestamp/unix_timestamp/","title":"UNIX_TIMESTAMP","text":"<p><code>UNIX_TIMESTAMP()</code></p> <p>Computes the number of seconds since the unix epoch</p>"},{"location":"api_docs/sql/functions/timestamp/utc_date/","title":"UTC_DATE","text":"<p><code>UTC_DATE()</code></p> <p>Returns the current UTC date as a Timestamp value.</p>"},{"location":"api_docs/sql/functions/timestamp/utc_timestamp/","title":"UTC_TIMESTAMP","text":"<p><code>UTC_TIMESTAMP()</code></p> <p>Returns the current UTC date and time as a timestamp value.</p>"},{"location":"api_docs/sql/functions/timestamp/week/","title":"WEEK","text":"<p><code>WEEK(timestamp_val)</code></p> <p>Equivalent to <code>EXTRACT(WEEK from timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/weekday/","title":"WEEKDAY","text":"<p><code>WEEKDAY(timestamp_val)</code></p> <p>Returns the weekday number for timestamp_val.</p> <p>Note</p> <p><code>Monday = 0</code>, <code>Sunday=6</code></p>"},{"location":"api_docs/sql/functions/timestamp/weekiso/","title":"WEEKISO","text":"<p><code>WEEKISO(timestamp_val)</code></p> <p>Computes the ISO week for the provided timestamp value.</p>"},{"location":"api_docs/sql/functions/timestamp/weekofyear/","title":"WEEKOFYEAR","text":"<p><code>WEEKOFYEAR(timestamp_val)</code></p> <p>Equivalent to <code>EXTRACT(WEEK from timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/year/","title":"YEAR","text":"<p><code>YEAR(timestamp_val)</code></p> <p>Equivalent to <code>EXTRACT(YEAR from timestamp_val)</code></p>"},{"location":"api_docs/sql/functions/timestamp/yearofweekiso/","title":"YEAROFWEEKISO","text":"<p><code>YEAROFWEEKISO(timestamp_val)</code></p> <p>Computes the ISO year for the provided timestamp value.</p>"},{"location":"api_docs/sql/functions/timestamp/yearweek/","title":"YEARWEEK","text":"<p><code>YEARWEEK(timestamp_val)</code></p> <p>Returns the year and week number for the provided timestamp_val concatenated as a single number. For example: <pre><code>YEARWEEK(TIMESTAMP '2021-08-30::00:00:00')\n202135\n</code></pre></p>"},{"location":"api_docs/sql/functions/type/","title":"Type Predicates","text":"<p>BodoSQL supports checking the type of a given value using the following type predicates:</p> <ul> <li><code>IS_ARRAY</code></li> <li><code>IS_OBJECT</code></li> </ul>"},{"location":"api_docs/sql/functions/type/is_array/","title":"IS_ARRAY","text":"<p><code>IS_ARRAY(variant_expr)</code></p> <p>Returns <code>TRUE</code> for all rows where <code>variant_expr</code> is an array, <code>NULL</code> if the row is <code>NULL</code>, and <code>FALSE</code> otherwise.</p>"},{"location":"api_docs/sql/functions/type/is_object/","title":"IS_OBJECT","text":"<p><code>IS_OBJECT(variant_expr)</code></p> <p>Returns <code>TRUE</code> for all rows where <code>variant_expr</code> is an object, <code>NULL</code> if the row is <code>NULL</code>, and <code>FALSE</code> otherwise.</p>"},{"location":"api_docs/sql/query_syntax/","title":"Query Syntax","text":"<p>We currently support the following SQL query clauses with BodoSQL, and are continuously adding support towards completeness. Note that BodoSQL ignores casing of keywords, and column and table names, except for the final output column name. Therefore, <code>select a from table1</code> is treated the same as <code>SELECT A FROM Table1</code>, except for the names of the final output columns (<code>a</code> vs <code>A</code>).</p> <ul> <li>Aliasing</li> <li><code>CASE</code></li> <li><code>CAST</code></li> <li><code>GREATEST</code></li> <li><code>GROUP BY</code></li> <li><code>HAVING</code></li> <li><code>INDEX</code></li> <li><code>::</code></li> <li><code>INTERSECT</code></li> <li><code>JOIN</code></li> <li><code>LEAST</code></li> <li><code>LIKE</code></li> <li><code>LIMIT</code></li> <li><code>NATURAL JOIN</code></li> <li><code>NOT BETWEEN</code></li> <li><code>NOT IN</code></li> <li><code>ORDER BY</code></li> <li><code>PIVOT</code></li> <li><code>QUALIFY</code></li> <li><code>SELECT</code></li> <li><code>SELECT DISTINCT</code></li> </ul>"},{"location":"api_docs/sql/query_syntax/aliasing/","title":"Aliasing","text":"<p>SQL aliases are used to give a table, or a column in a table, a temporary name:</p> <pre><code>SELECT &lt;COLUMN_NAME&gt; AS &lt;ALIAS&gt;\nFROM &lt;TABLE_NAME&gt;\n</code></pre> <p>For example: <pre><code>Select SUM(A) as total FROM table1\n</code></pre></p> <p>We strongly recommend using aliases for the final outputs of any queries to ensure all column names are predictable.</p>"},{"location":"api_docs/sql/query_syntax/case/","title":"CASE","text":"<p>The <code>CASE</code> statement goes through conditions and returns a value when the first condition is met: <pre><code>SELECT CASE WHEN cond1 THEN value1 WHEN cond2 THEN value2 ... ELSE valueN END\n</code></pre> For example: <pre><code>SELECT (CASE WHEN A 1 THEN A ELSE B END) as mycol FROM table1\n</code></pre> If the types of the possible return values are different, BodoSQL will attempt to cast them all to a common type, which is currently undefined behavior. The last else clause can optionally be excluded, in which case, the <code>CASE</code> statement will return null if none of the conditions are met. For example: <pre><code>SELECT (CASE WHEN A &lt; 0 THEN 0 END) as mycol FROM table1\n</code></pre> is equivalent to: <pre><code>SELECT (CASE WHEN A &lt; 0 THEN 0 ELSE NULL END) as mycol FROM table1\n</code></pre></p>"},{"location":"api_docs/sql/query_syntax/cast/","title":"CAST","text":"<p>THE <code>CAST</code> operator converts an input from one type to another. In many cases casts are created implicitly, but this operator can be used to force a type conversion.</p> <p>The following casts are currently supported. Please refer to <code>supported_dataframe_data_types</code> for the Python types for each type keyword:</p> From To Notes <code>VARCHAR</code> <code>VARCHAR</code> <code>VARCHAR</code> <code>TINYINT/SMALLINT/INTEGER/BIGINT</code> <code>VARCHAR</code> <code>FLOAT/DOUBLE</code> <code>VARCHAR</code> <code>DECIMAL</code> Equivalent to <code>DOUBLE</code>. This may change in the future. <code>VARCHAR</code> <code>TIMESTAMP</code> <code>VARCHAR</code> <code>DATE</code> Truncates to date but is still Timestamp type. This may change in the future. <code>TINYINT/SMALLINT/INTEGER/BIGINT</code> <code>VARCHAR</code> <code>TINYINT/SMALLINT/INTEGER/BIGINT</code> <code>TINYINT/SMALLINT/INTEGER/BIGINT</code> <code>TINYINT/SMALLINT/INTEGER/BIGINT</code> <code>FLOAT/DOUBLE</code> <code>TINYINT/SMALLINT/INTEGER/BIGINT</code> <code>DECIMAL</code> Equivalent to <code>DOUBLE</code>. This may change in the future. <code>TINYINT/SMALLINT/INTEGER/BIGINT</code> <code>TIMESTAMP</code> <code>FLOAT/DOUBLE</code> <code>VARCHAR</code> <code>FLOAT/DOUBLE</code> <code>TINYINT/SMALLINT/INTEGER/BIGINT</code> <code>FLOAT/DOUBLE</code> <code>FLOAT/DOUBLE</code> <code>FLOAT/DOUBLE</code> <code>DECIMAL</code> Equivalent to <code>DOUBLE</code>. This may change in the future <code>TIMESTAMP</code> <code>VARCHAR</code> <code>TIMESTAMP</code> <code>TINYINT/SMALLINT/INTEGER/BIGINT</code> <code>TIMESTAMP</code> <code>TIMESTAMP</code> <code>TIMESTAMP</code> <code>DATE</code> Truncates to date but is still Timestamp type. This may change in the future. <p>Note</p> <p><code>CAST</code> correctness can often not be determined at compile time. Users are responsible for ensuring that conversion is possible (e.g. <code>CAST(str_col as INTEGER)</code>).</p>"},{"location":"api_docs/sql/query_syntax/greatest/","title":"GREATEST","text":"<p>The <code>GREATEST</code> clause is used to return the largest value from a list of columns: <pre><code>SELECT GREATEST(col1, col2, ..., colN) FROM table_name\n</code></pre> For example: <pre><code>SELECT GREATEST(A, B, C) FROM table1\n</code></pre></p>"},{"location":"api_docs/sql/query_syntax/group_by/","title":"GROUP BY","text":"<p>The <code>GROUP BY</code> statement groups rows that have the same values into summary rows, like \"find the number of customers in each country\". The <code>GROUP BY</code> statement is often used with aggregate functions to group the result-set by one or more columns: <pre><code>SELECT &lt;COLUMN_NAMES&gt;\nFROM &lt;TABLE_NAME&gt;\nWHERE &lt;CONDITION&gt;\nGROUP BY &lt;GROUP_EXPRESSION&gt;\nORDER BY &lt;COLUMN_NAMES&gt;\n</code></pre></p> <p>For example: <pre><code>SELECT MAX(A) FROM table1 GROUP BY B\n</code></pre> <code>GROUP BY</code> statements also referring to columns by alias or column number: <pre><code>SELECT MAX(A), B - 1 as val FROM table1 GROUP BY val\nSELECT MAX(A), B FROM table1 GROUP BY 2\n</code></pre></p> <p>BodoSQL supports several subclauses that enable grouping by multiple different sets of columns in the same <code>SELECT</code> statement. <code>GROUPING SETS</code> is the first. It is equivalent to performing a group by for each specified set (setting each column not present in the grouping set to null), and unioning the results. For example:</p> <pre><code>SELECT MAX(A), B, C FROM table1 GROUP BY GROUPING SETS (B, B, (B, C), ())\n</code></pre> <p>This is equivalent to:</p> <pre><code>SELECT * FROM\n    (SELECT MAX(A), B, null FROM table1 GROUP BY B)\nUNION\n    (SELECT MAX(A), B, null FROM table1 GROUP BY B)\nUNION\n    (SELECT MAX(A), B, C FROM table1 GROUP BY B, C)\nUNION\n    (SELECT MAX(A), null, null FROM table1)\n</code></pre> <p>Note</p> <p>The above example is not valid BodoSQL code, as we do not support null literals. It is used only to show the null filling behavior.</p> <p><code>CUBE</code> is equivalent to grouping by all possible permutations of the specified set. For example:</p> <pre><code>SELECT MAX(A), B, C FROM table1 GROUP BY CUBE(B, C)\n</code></pre> <p>Is equivalent to</p> <pre><code>SELECT MAX(A), B, C FROM table1 GROUP BY GROUPING SETS ((B, C), (B), (C), ())\n</code></pre> <p><code>ROLLUP</code> is equivalent to grouping by n + 1 grouping sets, where each set is constructed by dropping the rightmost element from the previous set, until no elements remain in the grouping set. For example:</p> <pre><code>SELECT MAX(A), B, C FROM table1 GROUP BY ROLLUP(B, C, D)\n</code></pre> <p>Is equivalent to</p> <pre><code>SELECT MAX(A), B, C FROM table1 GROUP BY GROUPING SETS ((B, C, D), (B, C), (B), ())\n</code></pre> <p><code>CUBE</code> and <code>ROLLUP</code> can be nested into a <code>GROUPING SETS</code> clause. For example:</p> <pre><code>SELECT MAX(A), B, C GROUP BY GROUPING SETS (ROLLUP(B, C, D), CUBE(B, C), (A))\n</code></pre> <p>Which is equivalent to</p> <pre><code>SELECT MAX(A), B, C GROUP BY GROUPING SETS ((B, C, D), (B, C), (B), (), (B, C), (B), (C), (), (A))\n</code></pre>"},{"location":"api_docs/sql/query_syntax/having/","title":"HAVING","text":"<p>The <code>HAVING</code> clause is used for filtering with <code>GROUP BY</code>. <code>HAVING</code> applies the filter after generating the groups, whereas <code>WHERE</code> applies the filter before generating any groups:</p> <pre><code>SELECT column_name(s)\nFROM table_name\nWHERE condition\nGROUP BY column_name(s)\nHAVING condition\n</code></pre> <p>For example: <pre><code>SELECT MAX(A) FROM table1 GROUP BY B HAVING C &lt; 0\n</code></pre> <code>HAVING</code> statements also referring to columns by aliases used in the <code>GROUP BY</code>: <pre><code>SELECT MAX(A), B - 1 as val FROM table1 GROUP BY val HAVING val 5\n</code></pre></p>"},{"location":"api_docs/sql/query_syntax/infix/","title":"::","text":"<p>Infix cast operator. Equivalent to cast, but the format is <code>value::Typename</code></p>"},{"location":"api_docs/sql/query_syntax/intersect/","title":"INTERSECT","text":"<p>The <code>INTERSECT</code> operator is used to calculate the intersection of two <code>SELECT</code> statements:</p> <pre><code>SELECT &lt;COLUMN_NAMES&gt; FROM &lt;TABLE1&gt;\nINTERSECT\nSELECT &lt;COLUMN_NAMES&gt; FROM &lt;TABLE2&gt;\n</code></pre> <p>Each <code>SELECT</code> statement within the <code>INTERSECT</code> clause must have the same number of columns. The columns must also have similar data types. The output of the <code>INTERSECT</code> is the set of rows which are present in both of the input SELECT statements. The <code>INTERSECT</code> operator selects only the distinct values from the inputs.</p>"},{"location":"api_docs/sql/query_syntax/join/","title":"JOIN","text":"<p>A <code>JOIN</code> clause is used to combine rows from two or more tables, based on a related column between them: <pre><code>SELECT &lt;COLUMN_NAMES&gt;\n  FROM &lt;LEFT_TABLE_NAME&gt;\n  &lt;JOIN_TYPE&gt; &lt;RIGHT_TABLE_NAME&gt;\n  ON &lt;LEFT_TABLE_COLUMN_NAME&gt; OP &lt;RIGHT_TABLE_COLUMN_NAME&gt;\n</code></pre> For example: <pre><code>SELECT table1.A, table1.B FROM table1 JOIN table2 on table1.A = table2.C\n</code></pre> Here are the different types of the joins in SQL:</p> <ul> <li><code>(INNER) JOIN</code>: returns records that have matching values in both tables</li> <li><code>LEFT (OUTER) JOIN</code>: returns all records from the left table, and the matched records from the right table</li> <li><code>RIGHT (OUTER) JOIN</code>: returns all records from the right table, and the matched records from the left table</li> <li><code>FULL (OUTER) JOIN</code>: returns all records when there is a match in either left or right table</li> </ul>"},{"location":"api_docs/sql/query_syntax/join/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt;@bodo.jit\n... def g1(df1, df2):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df1, \"PAYMENTS\":df2})\n...    query = \"SELECT name, \\\"paymentType\\\" FROM customers JOIN payments ON customers.customerID = payments.customerID\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;@bodo.jit\n... def g2(df1, df2):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df1, \"PAYMENTS\":df2})\n...    query = \"SELECT name, paymentType FROM customers FULL JOIN payments ON customers.customerID = payments.customerID\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;customer_df = pd.DataFrame({\n...    \"CUSTOMERID\": [0, 2, 4, 5, 7,],\n...    \"NAME\": [\"Deangelo Todd\",\"Nikolai Kent\",\"Eden Heath\", \"Taliyah Martinez\",\"Demetrius Chavez\",],\n...    \"ADDRESS\": [\"223 Iroquois LanenWest New York, NJ 07093\",\"37 Depot StreetnTaunton, MA 02780\",\n...                \"639 Maple St.nNorth Kingstown, RI 02852\",\"93 Bowman Rd.nChester, PA 19013\",\n...                \"513 Manchester Ave.nWindsor, CT 06095\",],\n...    \"BALANCE\": [1123.34, 2133.43, 23.58, 8345.15, 943.43,]\n... })\n&gt;&gt;&gt;payment_df = pd.DataFrame({\n...     \"CUSTOMERID\": [0, 1, 4, 6, 7],\n...     \"paymentType\": [\"VISA\", \"VISA\", \"AMEX\", \"VISA\", \"WIRE\",],\n... })\n\n&gt;&gt;&gt;g1(customer_df, payment_df) # INNER JOIN\n               NAME paymentType\n0     Deangelo Todd        VISA\n1        Eden Heath        AMEX\n2  Demetrius Chavez        WIRE\n\n&gt;&gt;&gt;g2(customer_df, payment_df) # OUTER JOIN\n               NAME paymentType\n0     Deangelo Todd        VISA\n1      Nikolai Kent         NaN\n2        Eden Heath        AMEX\n3  Taliyah Martinez         NaN\n4  Demetrius Chavez        WIRE\n5               NaN        VISA\n6               NaN        VISA\n</code></pre>"},{"location":"api_docs/sql/query_syntax/least/","title":"LEAST","text":"<p>The <code>LEAST</code> clause is used to return the smallest value from a list of columns: <pre><code>SELECT LEAST(col1, col2, ..., colN) FROM table_name\n</code></pre> For example: <pre><code>SELECT LEAST(A, B, C) FROM table1\n</code></pre></p>"},{"location":"api_docs/sql/query_syntax/like/","title":"LIKE","text":"<p>The <code>LIKE</code> clause is used to filter the strings in a column to those that match a pattern: <pre><code>SELECT column_name(s) FROM table_name WHERE column LIKE pattern\n</code></pre> In the pattern we support the wildcards <code>%</code> and <code>_</code>. For example: <pre><code>SELECT A FROM table1 WHERE B LIKE '%py'\n</code></pre></p>"},{"location":"api_docs/sql/query_syntax/limit/","title":"LIMIT","text":"<p>BodoSQL supports the <code>LIMIT</code> keyword to select a limited number of rows. This keyword can optionally include an offset:</p> <p><pre><code>SELECT &lt;COLUMN_NAMES&gt;\nFROM &lt;TABLE_NAME&gt;\nWHERE &lt;CONDITION&gt;\nLIMIT &lt;LIMIT_NUMBER&gt; OFFSET &lt;OFFSET_NUMBER&gt;\n</code></pre> For Example:</p> <p><pre><code>SELECT A FROM table1 LIMIT 5\n\nSELECT B FROM table2 LIMIT 8 OFFSET 3\n</code></pre> Specifying a limit and offset can be also be written as:</p> <p><pre><code>LIMIT &lt;OFFSET_NUMBER&gt;, &lt;LIMIT_NUMBER&gt;\n</code></pre> For Example:</p> <pre><code>SELECT B FROM table2 LIMIT 3, 8\n</code></pre>"},{"location":"api_docs/sql/query_syntax/limit/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt;@bodo.jit\n... def g1(df):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df})\n...    query = \"SELECT name FROM customers LIMIT 4\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;@bodo.jit\n... def g2(df):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df})\n...    query = \"SELECT name FROM customers LIMIT 4 OFFSET 2\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;customers_df = pd.DataFrame({\n...     \"CUSTOMERID\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n...     \"NAME\": [\"Deangelo Todd\",\"Nikolai Kent\",\"Eden Heath\", \"Taliyah Martinez\",\n...                 \"Demetrius Chavez\",\"Weston Jefferson\",\"Jonathon Middleton\",\n...                 \"Shawn Winters\",\"Keely Hutchinson\", \"Darryl Rosales\",],\n...     \"BALANCE\": [1123.34, 2133.43, 23.58, 8345.15, 943.43, 68.34, 12764.50, 3489.25, 654.24, 25645.39]\n... })\n\n&gt;&gt;&gt;g1(customers_df) # LIMIT 4\n               NAME\n0     Deangelo Todd\n1      Nikolai Kent\n2        Eden Heath\n3  Taliyah Martinez\n\n&gt;&gt;&gt;g2(customers_df) # LIMIT 4 OFFSET 2\n               NAME\n2        Eden Heath\n3  Taliyah Martinez\n4  Demetrius Chavez\n5  Weston Jefferson\n</code></pre>"},{"location":"api_docs/sql/query_syntax/natural_join/","title":"NATURAL JOIN","text":"<p>A natural join is a type of join that provides an equality condition on all columns with the same name and only returns 1 column for the keys. On cannot be provided because it is implied but all join types can be provided.</p> <p><pre><code>SELECT &lt;COLUMN_NAMES&gt;\n  FROM &lt;LEFT_TABLE_NAME&gt;\n  NATURAL &lt;JOIN_TYPE&gt; &lt;RIGHT_TABLE_NAME&gt;\n</code></pre> For example: <pre><code>SELECT table1.A, table1.B FROM table1 NATURAL JOIN table2\n</code></pre> Here are the different types of the joins in SQL:</p> <ul> <li><code>(INNER) JOIN</code>: returns records that have matching values in both tables</li> <li><code>LEFT (OUTER) JOIN</code>: returns all records from the left table, and the matched records from the right table</li> <li><code>RIGHT (OUTER) JOIN</code>: returns all records from the right table, and the matched records from the left table</li> <li><code>FULL (OUTER) JOIN</code>: returns all records when there is a match in either left or right table</li> </ul>"},{"location":"api_docs/sql/query_syntax/natural_join/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt;@bodo.jit\n... def g1(df1, df2):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df1, \"PAYMENTS\":df2})\n...    query = \"SELECT payments.* FROM customers NATURAL JOIN payments\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;@bodo.jit\n... def g2(df1, df2):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df1, \"PAYMENTS\":df2})\n...    query = \"SELECT payments.* FROM customers NATURAL FULL JOIN payments\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;customer_df = pd.DataFrame({\n...    \"CUSTOMERID\": [0, 2, 4, 5, 7,],\n...    \"NAME\": [\"Deangelo Todd\",\"Nikolai Kent\",\"Eden Heath\", \"Taliyah Martinez\",\"Demetrius Chavez\",],\n...    \"ADDRESS\": [\"223 Iroquois LanenWest New York, NJ 07093\",\"37 Depot StreetnTaunton, MA 02780\",\n...                \"639 Maple St.nNorth Kingstown, RI 02852\",\"93 Bowman Rd.nChester, PA 19013\",\n...                \"513 Manchester Ave.nWindsor, CT 06095\",],\n...    \"BALANCE\": [1123.34, 2133.43, 23.58, 8345.15, 943.43,]\n... })\n&gt;&gt;&gt;payment_df = pd.DataFrame({\n...     \"CUSTOMERID\": [0, 1, 4, 6, 7],\n...     \"paymentType\": [\"VISA\", \"VISA\", \"AMEX\", \"VISA\", \"WIRE\",],\n... })\n\n&gt;&gt;&gt;g1(customer_df, payment_df) # INNER JOIN\n   CUSTOMERID paymentType\n0           0        VISA\n1           4        AMEX\n2           7        WIRE\n\n&gt;&gt;&gt;g2(customer_df, payment_df) # OUTER JOIN\n   CUSTOMERID paymentType\n0           0        VISA\n1        &lt;NA&gt;        &lt;NA&gt;\n2           4        AMEX\n3        &lt;NA&gt;        &lt;NA&gt;\n4           7        WIRE\n5           1        VISA\n6           6        VISA\n</code></pre>"},{"location":"api_docs/sql/query_syntax/not_between/","title":"NOT BETWEEN","text":"<p>The <code>BETWEEN</code> operator selects values within a given range. The values can be numbers, text, or datetimes. The <code>BETWEEN</code> operator is inclusive: begin and end values are included: <pre><code>SELECT &lt;COLUMN_NAMES&gt;\nFROM &lt;TABLE_NAME&gt;\nWHERE &lt;COLUMN_NAME&gt; BETWEEN &lt;VALUE1&gt; AND &lt;VALUE2&gt;\n</code></pre> For example: <pre><code>SELECT A FROM table1 WHERE A BETWEEN 10 AND 100\n</code></pre></p>"},{"location":"api_docs/sql/query_syntax/not_between/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt;@bodo.jit\n... def g(df):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df})\n...    query = \"SELECT name, balance FROM customers WHERE balance BETWEEN 1000 and 5000\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;@bodo.jit\n... def g2(df):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df})\n...    query = \"SELECT name, balance FROM customers WHERE balance NOT BETWEEN 100 and 10000\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;customers_df = pd.DataFrame({\n...     \"CUSTOMERID\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n...     \"NAME\": [\"Deangelo Todd\",\"Nikolai Kent\",\"Eden Heath\", \"Taliyah Martinez\",\n...                 \"Demetrius Chavez\",\"Weston Jefferson\",\"Jonathon Middleton\",\n...                 \"Shawn Winters\",\"Keely Hutchinson\", \"Darryl Rosales\",],\n...     \"BALANCE\": [1123.34, 2133.43, 23.58, 8345.15, 943.43, 68.34, 12764.50, 3489.25, 654.24, 25645.39]\n... })\n\n&gt;&gt;&gt;g1(payment_df) # BETWEEN\n            NAME  BALANCE\n0  Deangelo Todd  1123.34\n1   Nikolai Kent  2133.43\n7  Shawn Winters  3489.25\n\n&gt;&gt;&gt;g2(payment_df) # NOT BETWEEN\n                 NAME   BALANCE\n2          Eden Heath     23.58\n5    Weston Jefferson     68.34\n6  Jonathon Middleton  12764.50\n9      Darryl Rosales  25645.39\n</code></pre>"},{"location":"api_docs/sql/query_syntax/not_in/","title":"NOT IN","text":"<p>The <code>IN</code> determines if a value can be chosen a list of options. Currently, we support lists of literals or columns with matching types: <pre><code>SELECT &lt;COLUMN_NAMES&gt;\nFROM &lt;TABLE_NAME&gt;\nWHERE &lt;COLUMN_NAME&gt; IN (&lt;val1&gt;, &lt;val2&gt;, ... &lt;valN&gt;)\n</code></pre> For example: <pre><code>SELECT A FROM table1 WHERE A IN (5, 10, 15, 20, 25)\n</code></pre></p>"},{"location":"api_docs/sql/query_syntax/not_in/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt;@bodo.jit\n... def g1(df):\n...    bc = bodosql.BodoSQLContext({\"PAYMENTS\":df})\n...    query = \"SELECT customerID FROM payments WHERE \\\"paymentType\\\" IN ('AMEX', 'WIRE')\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;@bodo.jit\n... def g2(df):\n...    bc = bodosql.BodoSQLContext({\"PAYMENTS\":df})\n...    query = \"SELECT customerID FROM payments WHERE \\\"paymentType\\\" NOT IN ('AMEX', 'VISA')\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;payment_df = pd.DataFrame({\n...     \"CUSTOMERID\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n...     \"paymentType\": [\"VISA\", \"VISA\", \"AMEX\", \"VISA\", \"WIRE\", \"VISA\", \"VISA\", \"WIRE\", \"VISA\", \"AMEX\"],\n... })\n\n&gt;&gt;&gt;g1(payment_df) # IN\n   CUSTOMERID\n2           2\n4           4\n7           7\n9           9\n\n&gt;&gt;&gt;g2(payment_df) # NOT IN\n   CUSTOMERID\n4           4\n7           7\n</code></pre>"},{"location":"api_docs/sql/query_syntax/order_by/","title":"ORDER BY","text":"<p>The <code>ORDER BY</code> keyword sorts the resulting DataFrame in ascending or descending order. By default, it sorts the records in ascending order. NULLs are sorted in accordance with the optional <code>NULLS FIRST</code> or <code>NULLS LAST</code> keywords.</p> <p>BodoSQL's default NULLS FIRST and NULLS LAST behavior is controlled by an environment variable <code>BODO_SQL_STYLE</code> which has two currently supported values:</p> <ul> <li><code>SNOWFLAKE</code> (the default)</li> <li><code>SPARK</code></li> </ul> <p>If <code>BODO_SQL_STYLE</code> is set to <code>SNOWFLAKE</code> then the default behavior is <code>NULLS LAST</code> for ascending order and <code>NULLS FIRST</code> for descending order. If <code>BODO_SQL_STYLE</code> is set to <code>SPARK</code> then the default behavior is <code>NULLS FIRST</code> for ascending order and <code>NULLS LAST</code> for descending order. If you are transitioning a query from any other system we strongly recommend manually specifying <code>NULLS FIRST</code> or <code>NULLS LAST</code> to ensure the correct behavior.</p> <pre><code>SELECT &lt;COLUMN_NAMES&gt;\nFROM &lt;TABLE_NAME&gt;\nORDER BY &lt;ORDERED_COLUMN_NAMES&gt; [ASC|DESC] [NULLS FIRST|LAST]\n</code></pre> <p>For Example: <pre><code>SELECT A, B FROM table1 ORDER BY B, A DESC NULLS LAST\n</code></pre></p>"},{"location":"api_docs/sql/query_syntax/order_by/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt;@bodo.jit\n... def g(df):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df})\n...    query = \"SELECT name, balance FROM customers ORDER BY balance\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;customers_df = pd.DataFrame({\n...     \"CUSTOMERID\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n...     \"NAME\": [\"Deangelo Todd\",\"Nikolai Kent\",\"Eden Heath\", \"Taliyah Martinez\",\n...                 \"Demetrius Chavez\",\"Weston Jefferson\",\"Jonathon Middleton\",\n...                 \"Shawn Winters\",\"Keely Hutchinson\", \"Darryl Rosales\",],\n...     \"BALANCE\": [1123.34, 2133.43, 23.58, 8345.15, 943.43, 68.34, 12764.50, 3489.25, 654.24, 25645.39]\n... })\n\n&gt;&gt;&gt;g(customers_df)\n                NAME   BALANCE\n2          Eden Heath     23.58\n5    Weston Jefferson     68.34\n8    Keely Hutchinson    654.24\n4    Demetrius Chavez    943.43\n0       Deangelo Todd   1123.34\n1        Nikolai Kent   2133.43\n7       Shawn Winters   3489.25\n3    Taliyah Martinez   8345.15\n6  Jonathon Middleton  12764.50\n9      Darryl Rosales  25645.39\n</code></pre>"},{"location":"api_docs/sql/query_syntax/pivot/","title":"PIVOT","text":"<p>The <code>PIVOT</code> clause is used to transpose specific data rows in one or more columns into a set of columns in a new DataFrame: <pre><code>SELECT col1, ..., colN FROM table_name PIVOT (\n    AGG_FUNC_1(colName or pivotVar) AS alias1, ...,  AGG_FUNC_N(colName or pivotVar) as aliasN\n    FOR pivotVar IN (ROW_VALUE_1 as row_alias_1, ..., ROW_VALUE_N as row_alias_N)\n)\n</code></pre> <code>PIVOT</code> produces a new column for each pair of pivotVar and aggregation functions.</p> <p>For example: <pre><code>SELECT single_sum_a, single_avg_c, triple_sum_a, triple_avg_c FROM table1 PIVOT (\n    SUM(A) AS sum_a, AVG(C) AS avg_c\n    FOR A IN (1 as single, 3 as triple)\n)\n</code></pre> Here <code>single_sum_a</code> will contain sum(A) where <code>A = 1</code>, single_avg_c will contain AVG(C) where <code>A = 1</code> etc.</p> <p>If you explicitly specify other columns as the output, those columns will be used to group the pivot columns. For example: <pre><code>SELECT B, single_sum_a, single_avg_c, triple_sum_a, triple_avg_c FROM table1 PIVOT (\n    SUM(A) AS sum_a, AVG(C) AS avg_c\n    FOR A IN (1 as single, 3 as triple)\n)\n</code></pre> Contains 1 row for each unique group in B. The pivotVar can also require values to match in multiple columns. For example: <pre><code>SELECT * FROM table1 PIVOT (\n    SUM(A) AS sum_a, AVG(C) AS avg_c\n    FOR (A, B) IN ((1, 4) as col1, (2, 5) as col2)\n)\n</code></pre></p>"},{"location":"api_docs/sql/query_syntax/qualify/","title":"QUALIFY","text":"<p><code>QUALIFY</code> is similar to <code>HAVING</code>, except it applies filters after computing the results of at least one window function. <code>QUALIFY</code> is used after using <code>WHERE</code> and <code>HAVING</code>.</p> <p>For example:</p> <pre><code>SELECT column_name(s),\nFROM table_name\nWHERE condition\nGROUP BY column_name(s)\nHAVING condition\nQUALIFY MAX(A) OVER (PARTITION BY B ORDER BY C ROWS BETWEEN 1 FOLLOWING AND 1 PRECEDING) &gt; 1\n</code></pre> <p>Is equivalent to</p> <pre><code>SELECT column_name(s) FROM\n    (SELECT column_name(s), MAX(A) OVER (PARTITION BY B ORDER BY C ROWS BETWEEN 1 FOLLOWING AND 1 PRECEDING) as window_output\n    FROM table_name\n    WHERE condition\n    GROUP BY column_name(s)\n    HAVING condition)\nWHERE window_output &gt; 1\n</code></pre>"},{"location":"api_docs/sql/query_syntax/select/","title":"SELECT","text":"<p>The <code>SELECT</code> statement is used to select data in the form of columns. The data returned from BodoSQL is stored in a dataframe.</p> <pre><code>SELECT &lt;COLUMN_NAMES&gt; FROM &lt;TABLE_NAME&gt;\n</code></pre> <p>For Instance:</p> <pre><code>SELECT A FROM customers\n</code></pre>"},{"location":"api_docs/sql/query_syntax/select/#example-usage","title":"Example Usage:","text":"<pre><code>&gt;&gt;&gt;@bodo.jit\n... def g(df):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df})\n...    query = \"SELECT name FROM customers\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;customers_df = pd.DataFrame({\n...     \"CUSTOMERID\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n...     \"NAME\": [\"Deangelo Todd\",\"Nikolai Kent\",\"Eden Heath\", \"Taliyah Martinez\",\n...                 \"Demetrius Chavez\",\"Weston Jefferson\",\"Jonathon Middleton\",\n...                 \"Shawn Winters\",\"Keely Hutchinson\", \"Darryl Rosales\",],\n...     \"BALANCE\": [1123.34, 2133.43, 23.58, 8345.15, 943.43, 68.34, 12764.50, 3489.25, 654.24, 25645.39]\n... })\n\n&gt;&gt;&gt;g(customers_df)\n                NAME\n0       Deangelo Todd\n1        Nikolai Kent\n2          Eden Heath\n3    Taliyah Martinez\n4    Demetrius Chavez\n5    Weston Jefferson\n6  Jonathon Middleton\n7       Shawn Winters\n8    Keely Hutchinson\n9      Darryl Rosales\n</code></pre> <p>The <code>SELECT</code> also has some special syntactic forms. The <code>*</code> term is used as a shortcut for specifying all columns. The clause <code>* EXCLUDE col</code> or <code>* EXCLUDE (col1, col2, col3...)</code> is a shortcut for specifying every column except the ones after the EXCLUDE keyword.</p> <p>For example, suppose we have a table The <code>T</code> with columns named The <code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>, <code>E</code>. Consider the following queries</p> <pre><code>SELECT * FROM T\n\nSELECT * EXCLUDE (A, E) FROM T\n</code></pre> <p>These two are syntactic sugar for the following:</p> <pre><code>SELECT A, B, C, D, E FROM T\n\nSELECT B, C, D FROM T\n</code></pre>"},{"location":"api_docs/sql/query_syntax/select_distinct/","title":"SELECT DISTINCT","text":"<p>The <code>SELECT DISTINCT</code> statement is used to return only distinct (different) values:</p> <pre><code>SELECT DISTINCT &lt;COLUMN_NAMES&gt; FROM &lt;TABLE_NAME&gt;\n</code></pre> <p><code>DISTINCT</code> can be used in a SELECT statement or inside an aggregate function. For example:</p> <pre><code>SELECT DISTINCT A FROM table1\n\nSELECT COUNT DISTINCT A FROM table1\n</code></pre>"},{"location":"api_docs/sql/query_syntax/select_distinct/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt;@bodo.jit\n... def g(df):\n...    bc = bodosql.BodoSQLContext({\"PAYMENTS\":df})\n...    query = \"SELECT DISTINCT \\\"paymentType\\\" FROM payments\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;payment_df = pd.DataFrame({\n...     \"CUSTOMERID\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n...     \"paymentType\": [\"VISA\", \"VISA\", \"AMEX\", \"VISA\", \"WIRE\", \"VISA\", \"VISA\", \"WIRE\", \"VISA\", \"AMEX\"],\n... })\n\n&gt;&gt;&gt;g(payment_df) # inside SELECT\npaymentType\n0        VISA\n2        AMEX\n4        WIRE\n\n&gt;&gt;&gt;def g(df):\n...    bc = bodosql.BodoSQLContext({\"PAYMENTS\":df})\n...    query = \"SELECT COUNT(DISTINCT \\\"paymentType\\\") as num_payment_types FROM payments\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;g(payment_df) # inside aggregate\nNUM_PAYMENT_TYPES\n0          3\n</code></pre>"},{"location":"api_docs/sql/query_syntax/union/","title":"UNION","text":"<p>The <code>UNION</code> operator is used to combine the result-set of two <code>SELECT</code> statements: <pre><code>SELECT &lt;COLUMN_NAMES&gt; FROM &lt;TABLE1&gt;\nUNION\nSELECT &lt;COLUMN_NAMES&gt; FROM &lt;TABLE2&gt;\n</code></pre> Each <code>SELECT</code> statement within the <code>UNION</code> clause must have the same number of columns. The columns must also have similar data types. The output of the <code>UNION</code> is the set of rows which are present in either of the input <code>SELECT</code> statements.</p> <p>The <code>UNION</code> operator selects only the distinct values from the inputs by default. To allow duplicate values, use <code>UNION ALL</code>:</p> <pre><code>SELECT &lt;COLUMN_NAMES&gt; FROM &lt;TABLE1&gt;\nUNION ALL\nSELECT &lt;COLUMN_NAMES&gt; FROM &lt;TABLE2&gt;\n</code></pre>"},{"location":"api_docs/sql/query_syntax/union/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt;@bodo.jit\n... def g1(df):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df1, \"PAYMENTS\":df2})\n...    query = \"SELECT name, \\\"paymentType\\\" FROM customers JOIN payments ON customers.customerID = payments.customerID WHERE \\\"paymentType\\\" in ('WIRE')\n...             UNION SELECT name, paymentType FROM customers JOIN payments ON customers.customerID = payments.customerID WHERE balance &lt; 1000\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;@bodo.jit\n... def g2(df):\n...    bc = bodosql.BodoSQLContext({\"customers\":df1, \"payments\":df2})\n...    query = \"SELECT name, \\\"paymentType\\\" FROM customers JOIN payments ON customers.customerID = payments.customerID WHERE \\\"paymentType\\\" in ('WIRE')\n...             UNION ALL SELECT name, paymentType FROM customers JOIN payments ON customers.customerID = payments.customerID WHERE balance &lt; 1000\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;customer_df = pd.DataFrame({\n...    \"CUSTOMERID\": [0, 2, 4, 5, 7,],\n...    \"NAME\": [\"Deangelo Todd\",\"Nikolai Kent\",\"Eden Heath\", \"Taliyah Martinez\",\"Demetrius Chavez\",],\n...    \"ADDRESS\": [\"223 Iroquois LanenWest New York, NJ 07093\",\"37 Depot StreetnTaunton, MA 02780\",\n...                \"639 Maple St.nNorth Kingstown, RI 02852\",\"93 Bowman Rd.nChester, PA 19013\",\n...                \"513 Manchester Ave.nWindsor, CT 06095\",],\n...    \"BALANCE\": [1123.34, 2133.43, 23.58, 8345.15, 943.43,]\n... })\n&gt;&gt;&gt;payment_df = pd.DataFrame({\n...     \"CUSTOMERID\": [0, 1, 4, 6, 7],\n...     \"paymentType\": [\"VISA\", \"VISA\", \"AMEX\", \"VISA\", \"WIRE\",],\n... })\n\n&gt;&gt;&gt;g1(customer_df, payment_df) # UNION\n           NAME paymentType  BALANCE\n0  Demetrius Chavez        WIRE   943.43\n0        Eden Heath        AMEX    23.58\n\n&gt;&gt;&gt;g2(customer_df, payment_df) # UNION ALL\n            NAME paymentType  BALANCE\n0  Demetrius Chavez        WIRE   943.43\n0        Eden Heath        AMEX    23.58\n1  Demetrius Chavez        WIRE   943.43\n</code></pre>"},{"location":"api_docs/sql/query_syntax/where/","title":"WHERE","text":"<p>The <code>WHERE</code> clause on columns can be used to filter records that satisfy specific conditions:</p> <pre><code>SELECT &lt;COLUMN_NAMES&gt; FROM &lt;TABLE_NAME&gt; WHERE &lt;CONDITION&gt;\n</code></pre> <p>For Example: <pre><code>SELECT A FROM table1 WHERE B &gt; 4\n</code></pre></p>"},{"location":"api_docs/sql/query_syntax/where/#example-usage","title":"Example Usage","text":"<pre><code>&gt;&gt;&gt;@bodo.jit\n... def g(df):\n...    bc = bodosql.BodoSQLContext({\"CUSTOMERS\":df})\n...    query = \"SELECT name FROM customers WHERE balance 3000\"\n...    res = bc.sql(query)\n...    return res\n\n&gt;&gt;&gt;customers_df = pd.DataFrame({\n...     \"CUSTOMERID\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n...     \"NAME\": [\"Deangelo Todd\",\"Nikolai Kent\",\"Eden Heath\", \"Taliyah Martinez\",\n...                 \"Demetrius Chavez\",\"Weston Jefferson\",\"Jonathon Middleton\",\n...                 \"Shawn Winters\",\"Keely Hutchinson\", \"Darryl Rosales\",],\n...     \"BALANCE\": [1123.34, 2133.43, 23.58, 8345.15, 943.43, 68.34, 12764.50, 3489.25, 654.24, 25645.39]\n... })\n\n&gt;&gt;&gt;g(customers_df)\n                NAME\n3    Taliyah Martinez\n6  Jonathon Middleton\n7       Shawn Winters\n9      Darryl Rosales\n</code></pre>"},{"location":"api_docs/sql/query_syntax/with/","title":"WITH","text":"<p>The <code>WITH</code> clause can be used to name sub-queries: <pre><code>WITH sub_table AS (SELECT column_name(s) FROM table_name)\nSELECT column_name(s) FROM sub_table\n</code></pre> For example: <pre><code>WITH subtable as (SELECT MAX(A) as max_al FROM table1 GROUP BY B)\nSELECT MAX(max_val) FROM subtable\n</code></pre></p>"},{"location":"bodo_parallelism/advanced/","title":"Advanced Parallelism Topics","text":"<p>This page discusses parallelism topics that are useful for performance tuning and advanced use cases.</p>"},{"location":"bodo_parallelism/advanced/#dist-flags","title":"Distributed Flags For JIT Functions","text":"<p>Bodo infers data distributions for inputs and outputs of JIT functions automatically. For example, all dataframe arguments and return values are distributed in this code:</p> <pre><code>@bodo.jit\ndef f():\n    df = pd.read_parquet(\"pd_example.pq\")\n    return df\n\n\n@bodo.jit\ndef h(df):\n    df2 = df.groupby(\"A\").sum()\n    return df2\n\n\n@bodo.jit\ndef g(df):\n    df3 = h(df)\n    return df3\n\n\ndf = f()\ndf3 = g(df)\n</code></pre> <p>Bodo tracks distributions across JIT functions and between JIT and regular Python code (by setting metadata in regular Pandas dataframes). However, the user can specify distributions manunally as well. The above code is equivalent to:</p> <pre><code>@bodo.jit(distributed=[\"df\"])\ndef f():\n    df = pd.read_parquet(\"pd_example.pq\")\n    return df\n\n\n@bodo.jit(distributed=[\"df\", \"df2\"])\ndef h(df):\n    df2 = df.groupby(\"A\").sum()\n    return df2\n\n\n@bodo.jit(distributed=[\"df\", \"df3\"])\ndef g(df):\n    df3 = h(df)\n    return df3\n\n\ndf = f()\ndf3 = g(df)\n</code></pre> <p>Generally, Bodo can handle distributions of most use cases automatically and we do not recommend setting distributions manually due to the possibility of human error. However, there are some advanced use cases where setting these flags may be desirable or necessary. For example, when a small dataframe is an input to a join, setting its distribution to replicated can improve parallel performance. In the example below, a small dataframe <code>df2</code> is an argument to a join on a large dataframe <code>df1</code>, and we specify <code>df2</code> as replicated for better parallel performance.  <pre><code>@bodo.jit(distributed=[\"df1\"], replicated=[\"df2\"])\ndef load_data():\n    df1 = pd.read_parquet(\"my_large_data.pq\")\n    df2 = pd.read_parquet(\"my_tiny_data.pq\")\n    return df1, df2\n\n\n@bodo.jit\ndef merge_data():\n    df1, df2 = load_data()\n    df3 = df1.merge(df2, on=\"id\")\n    df3.to_parquet(\"my_merged_data.pq\")\n\n\nmerge_data()\n</code></pre></p> <p>Another potential use case is when we want to parallelize computation without distributing data, for applications such as parameter tuning and simulations. The example below creates some parameters, distributes them manually using <code>bodo.scatterv</code>, and performs some computation on each one using a <code>bodo.prange</code> parallel loop. The input dataframe <code>df</code> is replicated across processors since all of its values are needed for computations on each parameter. Functions <code>create_params</code> and <code>load_data</code> have <code>distributed=False</code> set, which makes all of their data structures and computations replicated across processors.</p> <p>See Also</p> <p>API Docs for <code>bodo.scatterv</code></p> <pre><code>@bodo.jit(distributed=False)\ndef create_params():\n    params = [1, 3, 4, 5, 7, 8, 11, 15, 17, 21]\n    params2 = [a * 2 for a in params]\n    return np.array(params + params2)\n\n\n@bodo.jit(distributed=False)\ndef load_data():\n    df = pd.read_parquet(\"my_large_data.pq\")\n    return df\n\n\n@bodo.jit\ndef run_params():\n    params = create_params()\n    df = load_data()\n    params_dist = bodo.scatterv(params)\n    n = len(params_dist)\n    res = np.zeros(n)\n    for i in bodo.prange(n):\n        p = params_dist[i]\n        res[i] = df.apply(lambda x, a: x.B % a, axis=1, a=p).sum()\n    print(res.max())\n\n\nrun_params()\n</code></pre> <p>A similar flag is <code>distributed_block</code> which informs bodo that the data is distributed in equal chunks across cores (as done and expected by Bodo). Typically, this is used when output of <code>bodo.scatterv</code> is passed to a JIT function to allow for optimization and parallelization of more complex code. (This example assumes SPMD launch mode)</p> <pre><code>@bodo.jit(spawn=False, distributed_block=[\"A\"])\ndef f(A):\n    ...\n\ndata = bodo.scatterv(...)\nf(data)\n</code></pre>"},{"location":"bodo_parallelism/advanced/#indexing-operations-on-distributed-data","title":"Indexing Operations on Distributed Data","text":"<p>Distributed data is usually accessed and modified through high-level Pandas and Numpy APIs. However, in many cases, Bodo allows indexing operations on distributed data without code modification. Here are such cases that Bodo currently supports:</p> <ol> <li> <p>Getting values using boolean array indexing, e.g. <code>B = A[A &gt; 3]</code>.     The output can be distributed, but may be imbalanced     (<code>bodo.rebalance()</code> can be used if necessary).</p> </li> <li> <p>Getting values using a slice, e.g. <code>B = A[::2]</code>. The output can be     distributed, but may be imbalanced      (<code>bodo.rebalance()</code> can be used if necessary).</p> </li> <li> <p>Getting a value using a scalar index, e.g. <code>a = A[m]</code>. The output     can be replicated.</p> </li> <li> <p>Setting values using boolean array indexing, e.g. <code>A[A &gt; 3] = a</code>.     Only supports setting a scalar or lower-dimension value currently.</p> </li> <li> <p>Setting values using a slice, e.g. <code>A[::2] = a</code>. Only supports     setting a scalar or lower-dimension value currently.</p> </li> <li> <p>Setting a value using a scalar index, e.g. <code>A[m] = a</code>.</p> </li> </ol>"},{"location":"bodo_parallelism/advanced/#concatenation-reduction","title":"Concatenation Reduction","text":"<p>Some algorithms require generating variable-length output data per input data element. Bodo supports parallelizing this pattern, which we refer to as concatenation reduction. For example:</p> <pre><code>@bodo.jit\ndef impl(n):\n   df = pd.DataFrame()\n   for i in bodo.prange(n):\n      df = pd.concat([df, pd.DataFrame({\"A\": np.arange(i)})], ignore_index=True)\n\n   return df\n</code></pre> <p>A common use case is simulation applications that generate possible outcomes based on parameters. For example:</p> <pre><code>@bodo.jit\ndef impl():\n   params = np.array([0.1, 0.2, 0.5, 1.0, 1.2, 1.5, ..., 100])\n   params = bodo.scatterv(params)\n   df = pd.DataFrame()\n   for i in bodo.prange(len(params)):\n      df = pd.concat([df, get_result(params[i])], ignore_index=True)\n\n   return df\n</code></pre> <p>In this example, we chose to manually parallelize the parameter array for simplicity, since the workload is compute-heavy and the parameter data is relatively small.</p>"},{"location":"bodo_parallelism/advanced/#load-balancing-distributed-data","title":"Load Balancing Distributed Data","text":"<p>Some computations such as <code>filter</code>, <code>join</code> or <code>groupby</code> can result in imbalanced data chunks across cores for distributed data. This may result in some cores operating on nearly empty dataframes, and others on relatively large ones.</p> <p>Bodo provides <code>bodo.rebalance</code> to allow manual load balance if necessary. For example:</p> <pre><code>@bodo.jit(distributed={\"df\"})\ndef rebalance_example(df):\n    df = df[df[\"A\"] &gt; 3]\n    df = bodo.rebalance(df)\n    return df.sum()\n</code></pre> <p>In this case, we use <code>bodo.rebalance</code> to make sure the filtered dataframe has near-equal data chunk sizes across cores, which would accelerate later computations (<code>sum</code> in this case).</p> <p>We can also use the <code>dests</code> keyword to specify a subset of ranks to which bodo should distribute the data from all ranks.</p> <p>Example usage:</p> <pre><code>@bodo.jit(distributed={\"df\"})\ndef rebalance_example(df):\n    df = df[df[\"A\"] &gt; 3]\n    df = bodo.rebalance(df, dests=[0, 1])\n    return df.sum()\n</code></pre>"},{"location":"bodo_parallelism/advanced/#explicit-parallel-loops","title":"Explicit Parallel Loops","text":"<p>Sometimes explicit parallel loops are required since a program cannot be written in terms of data-parallel operators easily. In this case, one can use Bodo's <code>prange</code> in place of <code>range</code> to specify that a loop can be parallelized. The user is required to make sure the loop does not have cross-iteration dependencies except for supported reductions. Currently, reductions using <code>+=</code>, <code>*=</code>, <code>min</code>, and <code>max</code> operators are supported. Iterations are simply divided between processes and executed in parallel, but reductions are handled using data exchange.</p> <p>The example below demonstrates a parallel loop with a reduction:</p> <pre><code>import bodo\nfrom bodo import prange\nimport numpy as np\n\n@bodo.jit\ndef prange_test(n):\n    A = np.random.ranf(n)\n    s = 0\n    B = np.empty(n)\n    for i in prange(len(A)):\n        bodo.parallel_print(\"rank\", bodo.get_rank())\n        # A[i]: distributed data access with loop index\n        # s: a supported sum reduction\n        s += A[i]\n        # write array with loop index\n        B[i] = 2 * A[i]\n    return s + B.sum()\n\nres = prange_test(10)\nprint(res)\n</code></pre> <p>Output: </p> <pre><code>[stdout:0]\nrank 0\nrank 0\nrank 0\n13.077183553245497\n[stdout:1]\nrank 1\nrank 1\nrank 1\n13.077183553245497\n[stdout:2]\nrank 2\nrank 2\n13.077183553245497\n[stdout:3]\nrank 3\nrank 3\n13.077183553245497\n</code></pre> <p>The user is also responsible for ensuring that control flow doesn't prevent the loop from being reduced. This can occur when operations are potentially applied unevenly or when the order the operation occurs in matters. This means that mixing reductions and control flow breaks such as <code>break</code> or <code>raise</code> are not supported.</p> <p>The below example shows what happens when control flow prevents a reduction from being parallelized:</p> <pre><code>import bodo\nfrom bodo import prange\nimport numpy as np\n\n@bodo.jit\ndef prange_test(n):\n    A = np.random.ranf(n)\n    s = 0\n    for i in prange(len(A)):\n        if A[i] % 2 == 0:\n            s *= 2\n        else:\n            s += A[i]\n    return s\n\nres = prange_test(10)\nprint(res)\n</code></pre> <p>Output: </p> <pre><code>numba.core.errors.UnsupportedRewriteError: Failed in bodo mode pipeline (step: convert to parfors)\nReduction variable s has multiple conflicting reduction operators.\n</code></pre>"},{"location":"bodo_parallelism/advanced/#integration-with-non-bodo-apis","title":"Integration with non-Bodo APIs","text":"<p>There are multiple methods for integration with APIs that Bodo does not support natively:</p> <ol> <li>Switch to regular Python inside JIT functions with @bodo.wrap_python</li> <li>Pass data in and out of JIT functions</li> </ol>"},{"location":"bodo_parallelism/advanced/#passing-distributed-data","title":"Passing Distributed Data","text":"<p>By default, Bodo will transparently handle distributing inputs across all processes and will lazily collect output back onto the main process as the data is accessed. In other words, programs that access data outside of a JIT context will incur some overhead as the data is collected back onto a single process, while programs that pass data between JIT functions will run faster. Note that peeking at the first few rows of data will also be fast and efficient but operations that require the full table (e.g. printing out the entire table) will trigger collection of values.</p>"},{"location":"bodo_parallelism/advanced/#passing-distributed-data-in-spmd-launch-mode","title":"Passing Distributed Data in SPMD launch mode","text":"<p>Bodo can receive or return chunks of distributed data to allow flexible integration with any non-Bodo Python code. The following example passes chunks of data to interpolate with Scipy, and returns interpolation results back to jit function.</p> <pre><code>import scipy.interpolate\n\n@bodo.jit(distributed=[\"X\", \"Y\", \"X2\"])\ndef dist_pass_test(n):\n    X = np.arange(n)\n    Y = np.exp(-X/3.0)\n    X2 = np.arange(0, n, 0.5)\n    return X, Y, X2\n\nX, Y, X2 = dist_pass_test(100)\n# clip potential out-of-range values\nX2 = np.minimum(np.maximum(X2, X[0]), X[-1])\nf = scipy.interpolate.interp1d(X, Y)\nY2 = f(X2)\n\n@bodo.jit(distributed={\"Y2\"})\ndef dist_pass_res(Y2):\n    return Y2.sum()\n\nres = dist_pass_res(Y2)\nprint(res)\n</code></pre> <pre><code>[stdout:0] 6.555500504321469 \n[stdout:1] 6.555500504321469\n[stdout:2] 6.555500504321469 \n[stdout:3] 6.555500504321469\n</code></pre>"},{"location":"bodo_parallelism/advanced/#collections-of-distributed-data","title":"Collections of Distributed Data","text":"<p>List and dictionary collections can be used to hold distributed data structures:</p> <pre><code>@bodo.jit(distributed=[\"df\"])\ndef f():\n    to_concat = []\n    for i in range(10):\n        to_concat.append(pd.DataFrame({'A': np.arange(100), 'B': np.random.random(100)}))\n        df = pd.concat(to_concat)\n    return df\n\nf()\n</code></pre> <p></p>"},{"location":"bodo_parallelism/advanced/#run_on_single_rank","title":"Run code on a single rank","text":"<p>By default, all non-JIT code will only be run on a single rank. Within a JIT function, if there's some code you want to only run from a single rank, you can do so as follows: <pre><code>@bodo.wrap_python(bodo.none)\ndef rm_dir():\n    # Remove directory\n    import os, shutil\n    if os.path.exists(\"data/data.pq\"):\n        shutil.rmtree(\"data/data.pq\")\n\n\n@bodo.jit\ndef f():\n    if bodo.get_rank() == 0:\n        rm_dir()\n\n    # To synchronize all ranks before proceeding\n    bodo.barrier()\n\n    ...\n</code></pre></p> <p>This is similar in SPMD launch mode (where the whole script is launched as parallel MPI processes), except you will need to ensure that code that must only run on a single rank is protected even outside of JIT functions: </p> <pre><code>if bodo.get_rank() == 0:\n    # Remove directory\n    import os, shutil\n    if os.path.exists(\"data/data.pq\"):\n        shutil.rmtree(\"data/data.pq\")\n\n# To synchronize all ranks before proceeding\nbodo.barrier()\n</code></pre>"},{"location":"bodo_parallelism/advanced/#run_on_each_node","title":"Run code once on each node","text":"<p>In cases where some code needs to be run once on each node in a multi-node cluster, such as a file system operation, installing packages, etc., it can be done as follows from inside a JIT function:</p> <pre><code>if bodo.get_rank() in bodo.get_nodes_first_ranks():\n    # Remove directory on all nodes\n    import os, shutil\n    if os.path.exists(\"data/data.pq\"):\n        shutil.rmtree(\"data/data.pq\")\n\n# To synchronize all ranks before proceeding\nbodo.barrier()\n</code></pre> <p>In SPMD launch mode the above can also be run outside of JIT functions.</p> <p>Warning</p> <p>Running code on a single rank or a subset of ranks can lead to deadlocks. Ensure that your code doesn't include any MPI or Bodo functions.</p>"},{"location":"bodo_parallelism/bodo_parallelism_basics/","title":"Bodo Parallelism Basics","text":"<p>This page discusses Bodo's JIT compilation workflow and the parallelism model and APIs provided by Bodo.</p>"},{"location":"bodo_parallelism/bodo_parallelism_basics/#jit","title":"JIT (Just-in-time) Compilation Workflow","text":"<p>Bodo provides a just-in-time (JIT) compilation workflow using the <code>@bodo.jit</code> decorator, which replaces a Python function with a so-called <code>Dispatcher</code> object. Bodo compiles the function the first time a Dispatcher object is called and reuses the compiled version afterwards. The function is recompiled only if the same function is called with different argument types (not often in practice). All of this is completely transparent to the caller, and does not affect any Python code calling the function.</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; @bodo.jit\n... def f(n, a):\n...   df = pd.DataFrame({\"A\": np.arange(n) + a})\n...   return df.head(3)\n... \n&gt;&gt;&gt; print(f)\nCPUDispatcher(&lt;function f at 0x100bec310&gt;)\n&gt;&gt;&gt; print(f(8, 1)) # compiles for (int, int) input types\n   A\n0  1\n1  2\n2  3\n&gt;&gt;&gt; print(f(8, 2)) # same input types, no need to compile\n   A\n0  2\n1  3\n2  4\n&gt;&gt;&gt; print(f(8, 2.2)) # compiles for (int, float) input types\n     A\n0  2.2\n1  3.2\n2  4.2\n</code></pre> <p>Note</p> <p>In many cases, the binary that Bodo generates when compiling a function can be saved to disk and reused across program executions. See caching for more information.</p>"},{"location":"bodo_parallelism/bodo_parallelism_basics/#parallel-execution-model","title":"Parallel Execution Model","text":"<p>As we saw in the \"Getting Started\" tutorial, Bodo transforms functions for parallel execution. Under the hood, the Bodo dispatcher spawns processes on the fly, running compiled code in parallel and transparently distributing inputs and lazily collecting outputs onto the main process on demand.</p> <p>Bodo parallelizes functions with the <code>bodo.jit</code> decorator by distributing the data across the processes. Each rank runs the same code on a chunk of the data, and Bodo automatically communicates the data between the ranks (as needed).</p> <p>For example, save the following code in a<code>test_bodo.py</code> and use 4 processes as follows:</p> <pre><code>import numpy as np\nimport pandas as pd\nimport bodo\n\n\n@bodo.jit\ndef f(n, a):\n    df = pd.DataFrame({\"A\": np.arange(n) + a})\n    print(df)\n    return df\n\n\nres = f(8, 1)\nprint(\"RESULT\")\nprint(res)\n</code></pre> <pre><code>BODO_NUM_WORKERS=4 python test_bodo.py\n</code></pre> <p>Output:</p> <pre><code>   A\n2  3\n3  4\n   A\n6  7\n7  8\n   A\n4  5\n5  6\n   A\n0  1\n1  2\n\nRESULT\n   A\n0  1\n1  2\n2  3\n3  4\n4  5\n5  6\n6  7\n7  8\n</code></pre> <p>In this example, the <code>bodo.jit</code> decorator informs <code>Bodo</code> to run the function <code>f</code>  on 4 processes. Execution is parallelized by Bodo and each process generates a chunk of the data in <code>np.arange</code>.</p> <p>Note how the prints within the compiled code occur once per process, while regular prints occur only once. Within the parallel context, each process operates on a chunk of the full data and will communicate when necessary to operate on data that isn't locally available. Outside of the JIT function, the returned data will only be collected onto the main process if it is accessed. In cases where the full data is never accessed on the main thread and simply passed to another JIT function, there is no overhead.</p> <p>Warning</p> <ul> <li>Bodo functions run in parallel assuming that Bodo is able to parallelize them. Otherwise, Bodo prints the following warning and runs sequentially on every process.</li> </ul> <pre><code>BodoWarning: No parallelism found for function\n</code></pre> <p>On Jupyter notebook, parallel execution happens in very much the same way.</p> <p>See Also</p> <p>Parallel APIs</p>"},{"location":"bodo_parallelism/bodo_parallelism_basics/#data-distribution","title":"Data Distribution","text":"<p>Bodo parallelizes computation by dividing data into separate chunks across processes. However, some data handled by a Bodo function may not be divided into chunks. There are are two main data distribution schemes:</p> <ul> <li>Replicated (REP): the data associated with the variable is the     same on every process.</li> <li>One-dimensional (1D): the data is divided into chunks, split along     one dimension (rows of a dataframe or first dimension of an array).</li> </ul> <p>Bodo determines distribution of variables automatically, using the nature of the computation that produces them. Let's see an example:</p> <pre><code>import bodo\nimport pandas as pd\n@bodo.jit\ndef mean_power_speed():\n    df = pd.read_parquet(\"data/cycling_dataset.pq\")\n    m = df[[\"power\", \"speed\"]].mean()\n    print(m)\n    return m\n\nres = mean_power_speed()\n</code></pre> <p>Save code in mean_power_speed.py and run it as follows:</p> <pre><code>BODO_NUM_WORKERS=4 python mean_power_speed.py\n</code></pre> <pre><code>[stdout:0]\npower    102.078421\nspeed      5.656851\ndtype: float64\n[stdout:1]\npower    102.078421\nspeed      5.656851\ndtype: float64\n[stdout:2]\npower    102.078421\nspeed      5.656851\ndtype: float64\n[stdout:3]\npower    102.078421\nspeed      5.656851\ndtype: float64\n</code></pre> <p>In this example, <code>df</code> is parallelized (each process reads a different chunk) but <code>m</code> is replicated, even though it is a Series. Semantically, it makes sense for the output of <code>mean</code> operation to be replicated on all processors, since it is a reduction and produces \"small\" data.</p>"},{"location":"bodo_parallelism/bodo_parallelism_basics/#collecting-results-from-distributed-execution","title":"Collecting Results from Distributed Execution","text":"<p>Data that is returned from JIT functions is not immediately collected onto the main process. Output data is collected lazily only if necessary. This has two benefits. First, in cases where the size of the data exceeds the amount of available memory on a single host, the program will not crash and downstream JIT functions can still be called to continue processing data. Second, there is no overhead incurred by needing to collect results until the full results are accessed in the main process. For example, in cases where the output of a JIT function is obtained to peek at the first few rows (<code>df.head()</code>) before being consumed as an input to another JIT function, there will be no data that needs to be collected.</p> <p>Lazy data collection is done completely transparently to the user. There is no visible difference between, say, a distributed lazy Bodo DataFrame versus a regular Pandas DataFrame. As the DataFrame is accessed outside of a JIT context, data is collected from other processes back onto the main process to allow regular Python execution. Other data types are also supported in this way such as pandas Series. </p>"},{"location":"bodo_parallelism/bodo_parallelism_basics/#distributed-diagnostics","title":"Distributed Diagnostics","text":"<p>The distributions found by Bodo can be printed either by setting the environment variable <code>BODO_DISTRIBUTED_DIAGNOSTICS=1</code> or calling <code>distributed_diagnostics()</code> on the compiled function. Let's examine the previous example's distributions by adding following line to <code>mean_power_speed</code> script:</p> <pre><code>mean_power_speed.distributed_diagnostics()\n</code></pre> <pre><code>BODO_NUM_WORKERS=1 python mean_power_speed.py\n</code></pre> <pre><code>Distributed analysis replicated return variable $30return_value.12. Set distributed flag for the original variable if distributed partitions should be returned.\n[stdout:0]\npython mean_power_speed.py             \npower    102.078421\nspeed      5.656851\ndtype: float64\nDistributed diagnostics for function mean_power_speed, /Users/mean_power_speed.py (3)\n\nData distributions:\n    pq_table.0                                                              1D_Block\n    pq_index.1                                                              1D_Block\n    data_74                                                                 REP\n\n    Parfor distributions:\n       0                    1D_Block\n       1                    1D_Block\n\n    Distributed listing for function mean_power_speed, /Users/hadia/Bodo/testing/mean_power_speed.py (3)\n    ---------------------------------------------------------------------| parfor_id/variable: distribution\n    @bodo.jit                                                            | \n    def mean_power_speed():                                              | \n        df = pd.read_parquet(\"Bodo-tutorial/data/cycling_dataset.pq\")----| pq_table.0: 1D_Block, pq_index.1: 1D_Block\n        m = df[[\"power\", \"speed\"]].mean()--------------------------------| #0: 1D_Block, #1: 1D_Block, data_74: REP\n        return m                                                         | \n\n    Setting distribution of variable 'impl_v48_data_74' to REP: output of np.asarray() call on non-array is REP\n</code></pre> <p>Bodo compiler optimizations rename the variables.  The output shows that <code>power</code> and <code>speed</code> columns of <code>df</code> are distributed (<code>1D_Block</code>), but <code>m</code> is replicated (<code>REP</code>).  This is because <code>df</code> is the output from <code>read_parquet</code> and input to <code>mean</code>, both of which can be distributed by Bodo.  <code>m</code> is the output from <code>mean</code>, which is replicated (available on every process).</p>"},{"location":"bodo_parallelism/bodo_parallelism_basics/#spmd","title":"Avoiding Spawn Overheads (SPMD launch mode)","text":"<p>By default, Bodo spawns MPI processes the first time a JIT function is called. In addition, for each top level JIT call, Bodo sends  the execution function and its arguments and receives the output in the main process when execution finished. This workflow fits existing sequential Python workflows seamlessly but has some overheads that may be significant for small computations.</p> <p>The user can launch the program using <code>mpiexec</code> to avoid spawn overheads. In this mode, the whole program runs in Single Program Multiple Data (SPMD) fashion and the dispatcher does not launch processes on the fly. Instead, all processes are launched at the beginning and run the same file using the <code>mpiexec</code> command. The user code has to be updated to make sure it is valid to run the non-JIT parts (which are not managed by Bodo) on all processes in parallel.</p> <p>For example, save the following code in <code>test_bodo.py</code> and use <code>mpiexec</code> to launch 4 processes as follows:</p> <pre><code>import numpy as np\nimport pandas as pd\nimport bodo\n\n\n@bodo.jit(spawn=False)\ndef f(n, a):\n    df = pd.DataFrame({\"A\": np.arange(n) + a})\n    return df\n\n\nprint(f(8, 1))\n</code></pre> <pre><code>mpiexec -n 4 python test_bodo.py\n</code></pre> <p>Output:</p> <pre><code>   A\n2  3\n3  4\n   A\n6  7\n7  8\n   A\n4  5\n5  6\n   A\n0  1\n1  2\n</code></pre> <p>In this example, <code>mpiexec</code> launches 4 Python processes, each  executing the same <code>test_bodo.py</code> file. Since the function <code>f</code> is decorated with <code>bodo.jit</code> and Bodo can parallelize it, each process generates a chunk of the data in <code>np.arange</code>.</p> <p>An advantage of SPMD launch mode is that operations outside of JIT functions that do not need access to the full data are inherently parallelized. Since each core only has a chunk of the data when returned from JIT, regular Python operations happen on data chunks of the JIT output. However, this can be error-prone, as operations that require access to all rows will need explicit communication. The main challenge is that most Python libraries are usually not aware of the MPI runtime, and may not expect to be run on every core. A common workaround is to use <code>bodo.get_rank()</code> to get a unique ID per core (integers from <code>0</code> to <code>num processes</code>), and conditionally execute code only when the rank is <code>0</code>.</p> <p>Warning</p> <ul> <li>Python code outside of Bodo functions executes sequentially on every process.</li> </ul>"},{"location":"bodo_parallelism/not_supported/","title":"Unsupported Python Programs","text":"<p>Bodo compiles functions into efficient native parallel binaries, which requires all the operations used in the code to be supported by Bodo. This excludes the Python features discussed on this page.</p>"},{"location":"bodo_parallelism/not_supported/#typestability","title":"Type Stability","text":"<p>To enable type inference, the program should be type stable, which means Bodo should be able to assign a single type to every variable.</p>"},{"location":"bodo_parallelism/not_supported/#schemastability","title":"DataFrame Schema","text":"<p>Deterministic dataframe schemas, which are required in most data systems, is key for type stability. For example, variable <code>df</code> in example below could be either a single column dataframe or a two column one -- Bodo cannot determine it at compilation time:</p> <pre><code>@bodo.jit\ndef f(a):\n    df = pd.DataFrame({\"A\": [1, 2, 3]})\n    df2 = pd.DataFrame({\"A\": [1, 3, 4], \"C\": [-1, -2, -3]})\n    if len(a) &gt; 3:\n        df = df.merge(df2)\n\n    return df.mean()\n\nprint(f([2, 3]))\n# TypeError: Cannot unify dataframe((array(int64, 1d, C),), RangeIndexType(none), ('A',), False)\n# and dataframe((array(int64, 1d, C), array(int64, 1d, C)), RangeIndexType(none), ('A', 'C'), False) for 'df'\n</code></pre> <p>The error message means that Bodo cannot find a type that can unify the two types into a single type. This code can be refactored so that the <code>if</code> control flow is executed in regular Python context, but the rest of computation is in Bodo functions. For example, one could use two versions of the function:</p> <pre><code>@bodo.jit\ndef f1():\n    df = pd.DataFrame({\"A\": [1, 2, 3]})\n    return df.mean()\n\n@bodo.jit\ndef f2():\n    df = pd.DataFrame({\"A\": [1, 2, 3]})\n    df2 = pd.DataFrame({\"A\": [1, 3, 4], \"C\": [-1, -2, -3]})\n    df = df.merge(df2)\n    return df.mean()\n\na = [2, 3]\nif len(a) &gt; 3:\n    print(f1())\nelse:\n    print(f2())\n</code></pre> <p>Another common place where schema stability may be compromised is in passing non-constant list of key column names to dataframe operations such as <code>groupby</code>, <code>merge</code> and <code>sort_values</code>. In these operations, Bodo should be able to deduce the list of key column names at compile time in order to determine the output dataframe schema. For example, the program below is potentially type unstable since Bodo may not be able to infer <code>column_list</code> during compilation:</p> <pre><code>@bodo.jit\ndef f(a, i):\n    column_list = a[:i]  # some computation that cannot be inferred statically\n    df = pd.DataFrame({\"A\": [1, 2, 1], \"B\": [4, 5, 6]})\n    return df.groupby(column_list).sum()\n\na = [\"A\", \"B\"]\ni = 1\nf(a, i)\n# BodoError: groupby(): 'by' parameter only supports a constant column label or column labels.\n</code></pre> <p>This code can be refactored so that the computation for <code>column_list</code> is performed in regular Python context, and the result is passed as a function argument:</p> <pre><code>@bodo.jit\ndef f(column_list):\n    df = pd.DataFrame({\"A\": [1, 2, 1], \"B\": [4, 5, 6]})\n    return df.groupby(column_list).sum()\n\na = [\"A\", \"B\"]\ni = 1\ncolumn_list = a[:i]\nf(column_list)\n</code></pre> <p>In general, Bodo can infer constants from function arguments, global variables, and constant values in the program. Furthermore, Bodo supports implicitly inferring constant lists automatically for list addition and set difference operations such as:</p> <pre><code>df.groupby([\"A\"] + [\"B\"]).sum()\ndf.groupby(list(set(df.columns) - set([\"A\", \"C\"]))).sum()\n</code></pre> <p>Bodo will support inferring more implicit constant cases in the future (e.g. more list and set operations).</p> <p>Referring to dataframe columns (e.g. <code>[df[\"A\"]]</code>) requires constants for schema stability as well. <code>for</code> loops over dataframe column names such as below is not supported yet:</p> <pre><code>@bodo.jit\ndef f(df):\n    s = 0\n    for c in df.columns:\n        s += df[c].sum()\n    return s\n\nf(pd.DataFrame({\"A\": [1, 2, 1], \"B\": [4, 5, 6]}))\n# BodoError: df[] getitem selecting a subset of columns requires providing constant column names. For more information, see https://docs.bodo.ai/latest/programming_with_bodo/require_constants.html\n</code></pre>"},{"location":"bodo_parallelism/not_supported/#variable-types-and-functions","title":"Variable Types and Functions","text":"<p>The example below is not type stable since variable <code>a</code> can be both a float and an array of floats:</p> <pre><code>if flag:\n    a = 1.0\nelse:\n    a = np.ones(10)\n</code></pre> <p>The use of <code>isinstance</code> operator of Python often means type instability and is not supported.</p> <p>Similarly, function calls should also be deterministic. The below example is not supported since the function <code>f</code> is not known in advance:</p> <pre><code>if flag:\n    f = np.zeros\nelse:\n    f = np.random.ranf\nA = f(10)\n</code></pre> <p>One can usually avoid these cases in analytics codes without significant effort.</p>"},{"location":"bodo_parallelism/not_supported/#accessing-individual-values-of-nullable-data","title":"Accessing individual values of nullable data","text":"<p>The type of null (NA) value for most nullable data arrays is different than regular values (except float data which stores <code>np.nan</code>). Therefore, accessing individual values (i.e. using <code>[[]]</code> with an integer index) may not be type stable. In these cases, Bodo assumes the value is not NA and returns an \"neutral\" value:</p> <pre><code>@bodo.jit\ndef f(S, i):\n    return S.iloc[i]  # not type stable\nS = pd.Series([\"A\", None, \"CC\"])\nf(S, 1)  # returns \"\"\n</code></pre> <p>The solution is to check for NA values using <code>pd.isna</code> to handle NA values appropriately:</p> <pre><code>@bodo.jit\ndef f(S, i):\n    if pd.isna(S.iloc[i]):\n        return \"NA\"\n    return S.iloc[i]\nS = pd.Series([\"A\", None, \"CC\"])\nf(S, 1)  # returns \"NA\"\n</code></pre> <p>We are working on making it possible to avoid stability issues automatically in most practical cases.</p>"},{"location":"bodo_parallelism/not_supported/#notsupportedpython","title":"Unsupported Python Constructs","text":"<p>Bodo relies on Numba for supporting basic Python features. Therefore, Python constructs that are not supported by Numba should be avoided in Bodo programs.</p> <p>Generally, these Python features are not supported:</p> <ul> <li>exceptions: <code>try .. except</code>, <code>raise</code></li> <li>context manager: <code>with</code></li> <li>list, set, dict and generator comprehensions</li> <li>async features</li> <li>class definition: <code>class</code></li> <li>jit functions cannot have <code>**kwargs</code></li> <li>functions can be passed as arguments but not returned</li> <li>lists of lists cannot be passed as arguments unless Numba typed-lists     are used.</li> <li>Numba typed-dicts     are currently required for passing dictionaries as argument to jit     functions.</li> </ul>"},{"location":"bodo_parallelism/not_supported/#heterogeneousdtype","title":"Heterogeneous types inside a data structure","text":"<ul> <li> <p><code>List</code> containing values of heterogeneous type:      <pre><code>myList = [1, \"a\", 0.1]\n</code></pre></p> </li> <li> <p><code>Dictionary</code> containing values of heterogeneous type</p> <pre><code>myDict = {\"A\": 1, \"B\": \"a\", \"C\": 0.1}\n</code></pre> </li> </ul>"},{"location":"bodo_parallelism/typing_considerations/","title":"Typing Considerations","text":"<p>This section discusses some supported Pandas datatypes, potential typing related issues, and ways to resolve them.</p>"},{"location":"bodo_parallelism/typing_considerations/#pandas-dtype","title":"Supported Pandas Data Types","text":"<p>Bodo supports the following data types as values in Pandas Dataframe and Series data structures. This represents all Pandas data types except <code>TZ-aware datetime</code>, <code>Period</code>, <code>Interval</code>, and <code>Sparse</code> (which will be supported in the future). Comparing to Spark, equivalents of all Spark data types are supported.</p> <ul> <li>Numpy booleans: <code>np.bool_</code>.</li> <li>Numpy integer data types: <code>np.int8</code>, <code>np.int16</code>, <code>np.int32</code>, <code>np.int64</code>, <code>np.uint8</code>, <code>np.uint16</code>, <code>np.uint32</code>, <code>np.uint64</code>.</li> <li>Numpy floating point data types: <code>np.float32</code>, <code>np.float64</code>.</li> <li>Numpy datetime data types: <code>np.dtype(\"datetime64[ns]\")</code> and <code>np.dtype(\"timedelta[ns]\")</code>. The resolution has to be <code>ns</code> currently, which covers most practical use cases.</li> <li>Numpy complex data types: <code>np.complex64</code> and <code>np.complex128</code>.</li> <li>Strings (including nulls).</li> <li><code>datetime.date</code> values (including nulls).</li> <li><code>datetime.timedelta</code> values (including nulls).</li> <li>Pandas nullable integers.</li> <li>Pandas nullable booleans.</li> <li>Pandas Categoricals.</li> <li>Lists of other data types.</li> <li>Tuples of other data types.</li> <li>Structs of other data types.</li> <li>Maps of other data types (each map is a set of key-value pairs). All keys should have the same type to ensure type stability. All values should have the same type as well.</li> <li><code>decimal.Decimal</code> values (including nulls). The decimal values are stored as fixed-precision Apache Arrow Decimal128 format, which is also similar to PySpark decimals. The decimal type has a <code>precision</code> (the maximum total number of digits) and a <code>scale</code> (the number of digits on the right of dot) attribute, specifying how the stored data is interpreted. For example, the (4, 2) case can store from -999.99 to 999.99. The precision can be up to 38, and the scale must be less or equal to precision. Arbitrary-precision Python <code>decimal.Decimal</code> values are converted with precision of 38 and scale of 18.</li> </ul> <p>In addition, it may be desirable to specify type annotations in some cases (e.g., file I/O array input types). Typically these types are array types and they all can be accessed directly from the <code>bodo</code> module. The following table can be used to select the necessary Bodo Type based upon the desired Python, Numpy, or Pandas type.</p> Bodo Type Name Equivalent Python, Numpy, or Pandas type <code>bodo.bool_[:]</code>, <code>bodo.int8[:]</code>, ..., <code>bodo.int64[:]</code>, <code>bodo.uint8[:]</code>, ..., <code>bodo.uint64[:]</code>, <code>bodo.float32[:]</code>, <code>bodo.float64[:]</code> One-dimensional Numpy array of the given type. A full list of supported Numpy types can be found here. A multidimensional can be specified by adding additional colons (e.g., <code>bodo.int32[:, :, :]</code> for a three-dimensional array). <code>bodo.string_array_type</code> Array of nullable strings <code>bodo.IntegerArrayType(integer_type)</code> Array of Pandas nullable integers of the given integer type.  e.g., <code>bodo.IntegerArrayType(bodo.int64)</code> <code>bodo.boolean_array_type</code> Array of Pandas nullable booleans <code>bodo.datetime64ns[:]</code> Array of Numpy datetime64 values <code>bodo.timedelta64ns[:]</code> Array of Numpy timedelta64 values <code>bodo.datetime_date_array_type</code> Array of datetime.date types <code>bodo.timedelta_array_type</code> Array of datetime.timedelta types <code>bodo.DecimalArrayType(precision, scale)</code> Array of Apache Arrow Decimal128 values with the given precision and scale.  e.g., <code>bodo.DecimalArrayType(38, 18)</code> <code>bodo.binary_array_type</code> Array of nullable bytes values <code>bodo.StructArrayType(data_types, field_names)</code> Array of a user defined struct with the given tuple of data types and field names.  e.g., <code>bodo.StructArrayType((bodo.int32[:], bodo.datetime64ns[:]), (\"a\", \"b\"))</code> <code>bodo.TupleArrayType(data_types)</code> Array of a user defined tuple with the given tuple of data types.  e.g., <code>bodo.TupleArrayType((bodo.int32[:], bodo.datetime64ns[:]))</code> <code>bodo.MapArrayType(key_arr_type, value_arr_type)</code> Array of Python dictionaries with the given key and value array types.  e.g., <code>bodo.MapArrayType(bodo.uint16[:], bodo.string_array_type)</code> <code>bodo.PDCategoricalDtype(cat_tuple, cat_elem_type, is_ordered_cat)</code> Pandas categorical type with the possible categories, each category's type, and if the categories are ordered.  e.g., <code>bodo.PDCategoricalDtype((\"A\", \"B\", \"AA\"), bodo.string_type, True)</code> <code>bodo.CategoricalArrayType(categorical_type)</code> Array of Pandas categorical values.  e.g., <code>bodo.CategoricalArrayType(bodo.PDCategoricalDtype((\"A\", \"B\", \"AA\"), bodo.string_type, True))</code> <code>bodo.DatetimeIndexType(name_type)</code> Index of datetime64 values with a given name type.  e.g., <code>bodo.DatetimeIndexType(bodo.string_type)</code> <code>bodo.NumericIndexType(data_type, name_type)</code> Index of <code>pd.Int64</code>, <code>pd.Uint64</code>, or <code>Float64</code> objects, based upon the given data_type and name type.  e.g., <code>bodo.NumericIndexType(bodo.float64, bodo.string_type)</code> <code>bodo.PeriodIndexType(freq, name_type)</code> pd.PeriodIndex with a given frequency and name type.  e.g., <code>bodo.PeriodIndexType('A', bodo.string_type)</code> <code>bodo.RangeIndexType(name_type)</code> RangeIndex with a given name type.  e.g., <code>bodo.RangeIndexType(bodo.string_type)</code> <code>bodo.StringIndexType(name_type)</code> Index of strings with a given name type.  e.g., <code>bodo.StringIndexType(bodo.string_type)</code> <code>bodo.BinaryIndexType(name_type)</code> Index of binary values with a given name type.  e.g., <code>bodo.BinaryIndexType(bodo.string_type)</code> <code>bodo.TimedeltaIndexType(name_type)</code> Index of timedelta64 values with a given name type. e.g., <code>bodo.TimedeltaIndexType(bodo.string_type)</code> <code>bodo.SeriesType(dtype=data_type, index=index_type, name_typ=name_type)</code> Series with a given data type, index type, and name type.  e.g., <code>bodo.SeriesType(bodo.float32, bodo.DatetimeIndexType(bodo.string_type), bodo.string_type)</code> <code>bodo.DataFrameType(data_types_tuple, index_type, column_names)</code> DataFrame with a tuple of data types, an index type, and the names of the columns.  e.g., <code>bodo.DataFrameType((bodo.int64[::1], bodo.float64[::1]), bodo.RangeIndexType(bodo.none), (\"A\", \"B\"))</code>"},{"location":"bodo_parallelism/typing_considerations/#require_constants","title":"Compile Time Constants","text":"<p>Unlike regular Python, which is dynamically typed, Bodo needs to be able to type all functions at compile time. While in most cases, the output types depend solely on the input types, some APIs require knowing exact values in order to produce accurate types.</p> <p>As an example, consider the <code>iloc</code> DataFrame API. This API can be used to selected a subset of rows and columns by passing integers or slices of integers. A Bodo JIT version of a function calling this API might look like:</p> <pre><code>import numpy as np\nimport pandas as pd\nimport bodo\n\n@bodo.jit\ndef df_iloc(df, rows, columns):\n   return df.iloc[rows, columns]\n\ndf = pd.DataFrame({'A': np.arange(100), 'B': [\"A\", \"B\", \"C\", \"D\"]* 25})\nprint(df_iloc(df, slice(1, 4), 0))\n</code></pre> <p>If we try to run this file, we will get an error message:</p> <pre><code>$ python iloc_example.py\nTraceback (most recent call last):\nFile \"iloc_example.py\", line 10, in &lt;module&gt;\n   df_iloc(df, slice(1, 4), 0)\nFile \"/my_path/bodo/numba_compat.py\", line 1195, in _compile_for_args\n   raise error\nbodo.utils.typing.BodoError: idx2 in df.iloc[idx1, idx2] should be a constant integer or constant list of integers\n\nFile \"iloc_example.py\", line 7:\ndef df_iloc(df, rows, columns):\n   return df.iloc[rows, columns]\n</code></pre> <p>The relevant part of the error message is <code>idx2 in df.iloc[idx1, idx2] should be a constant integer or constant list of integers</code>.</p> <p>This error is thrown because depending on the value of <code>columns</code>, Bodo selects different columns with different types. When <code>columns=0</code> Bodo will need to compile code for numeric values, but when <code>columns=1</code> Bodo needs to compile code for strings, so it cannot properly type this function.</p> <p>To resolve this issue, you will need to replace <code>columns</code> with a literal integer. If instead the Bodo function is written as:</p> <pre><code>import numpy as np\nimport pandas as pd\nimport bodo\n\n@bodo.jit\ndef df_iloc(df, rows):\n   return df.iloc[rows, 0]\n\ndf = pd.DataFrame({'A': np.arange(100), 'B': [\"A\", \"B\", \"C\", \"D\"]* 25})\nprint(df_iloc(df, slice(1, 4)))\n</code></pre> <p>Bodo now can see that the output DataFrame should have a single <code>int64</code> column and it is able to compile the code.</p> <p>Whenever a value needs to be known for typing purposes, Bodo will throw an error that indicates some argument requires <code>a constant value</code>. All of these can be resolved by making this value a literal. Alternatively, some APIs support other ways of specifying the output types, which will be indicated in the error message.</p>"},{"location":"bodo_parallelism/typing_considerations/#integer-na-issue-pandas","title":"Integer NA issue in Pandas","text":"<p>DataFrame and Series objects with integer data need special care due to integer NA issues in Pandas. By default, Pandas dynamically converts integer columns to floating point when missing values (NAs) are needed (which can result in loss of precision). This is because Pandas uses the NaN floating point value as NA, and Numpy does not support NaN values for integers. Bodo does not perform this conversion unless enough information is available at compilation time.</p> <p>Pandas introduced a new nullable integer data type that can solve this issue, which is also supported by Bodo. For example, this code reads column <code>A</code> into a nullable integer array (the capital <code>\"I\"</code> denotes nullable integer type):</p> <pre><code>@bodo.jit\ndef example(fname):\n  dtype = {'A': 'Int64', 'B': 'float64'}\n  df = pd.read_csv(fname,\n      names=dtype.keys(),\n      dtype=dtype,\n  )\n  ...\n</code></pre>"},{"location":"bodo_parallelism/typing_considerations/#type-inference-for-object-data","title":"Type Inference for Object Data","text":"<p>Pandas stores some data types (e.g. strings) as object arrays which are untyped. Therefore, Bodo needs to infer the actual data type of object arrays when dataframes or series values are passed to JIT functions from regular Python. Bodo uses the first non-null value of the array to determine the type, and throws a warning if the array is empty or all nulls:</p> <pre><code>BodoWarning: Empty object array passed to Bodo, which causes ambiguity in typing. This can cause errors in parallel execution.\n</code></pre> <p>In this case, Bodo assumes the array is a string array which is the most common. However, this can cause errors if a distributed dataset is passed to Bodo, and some other processor has non-string data. This corner case can usually be avoided by load balancing the data across processors to avoid empty arrays.</p>"},{"location":"dataframe_library/","title":"bodo.pandas","text":"<p>Bodo.pandas is an optimized and distributed dataframe library that is a drop-in replacement for the Pandas library.  The library lazily collects dataframe and series operations and determines the optimum point to execute the collected operations.  This allows the library to apply advanced query optimizations that in some cases can reduce the runtime  of your application by hundreds of times.  Eagerly executed dataframe libraries such as Pandas cannot apply most of these optimizations.</p> <p>Dataframes and series generated by this library are compatible with those used by Bodo's JIT decorator.  Thus, a user may seemlessly transition back-and-forth between accessing dataframes through library mode and manipulating them through Bodo's JIT decorator.  </p> <p>This dataframe library exposes the full Pandas API and for cases where support for a function (or specific modes of a function) from that API is lacking, the library can fallback to attempting to execute that operation with Pandas.  However, if the cumulative data for the relevant dataframes is larger than the memory of the node on which you started the application then this fallback may fail.  In either case, this fallback process requires the copying of all the distributed data for the dataframe back to the starting node which can be a very time consuming process.  The Bodo dataframe library can thus operate in one of three modes: 1) automatic fallback is disabled and an error is generated if fallback would be necessary to execute the operation, 2) automatic fallback is enabled but a performance warning is generated each time a dataframe has to be copied back to the start node in order to be executed, and 3) automatic fallback is enabled but the performance warning is disabled.  This fallback is controlled by the environment variable BODO_PANDAS_FALLBACK which when not present or having the value 0 the automatic fallback is disabled.  When present and having the value 1 then automatic fallback with performance warning is enabled and when having the value 2 automatic fallback without performance warning is enabled.</p>"},{"location":"diagnostics_and_troubleshooting/Bodoerrors/","title":"Bodo Error Messages","text":"<p>This page lists some of the compilation error messages you may encounter with your jitted functions, reasons for them and suggestions on how to proceed with resolving them.</p>"},{"location":"diagnostics_and_troubleshooting/Bodoerrors/#unsupported-bodo-functionality","title":"Unsupported Bodo Functionality","text":"<ul> <li> <p><code>BodoError: &lt;functionality&gt; not supported yet</code></p> <p>As the error states, this message is encountered when you are attempting to call an as yet unsupported API within a jit function. For example :</p> <pre><code>@bodo.jit\ndef unsupported_func(pd_str_series):\n    return pd_str_series.str.casefold()\n</code></pre> <p>would result in an unsupported <code>BodoError</code> as follows:</p> <pre><code>BodoError: Series.str.casefold not supported yet\n</code></pre> <p>Please submit a request for us to support your required functionality here. Also consider joining our community slack, where you can interact directly with fellow Bodo users to find a workaround for your requirements. For longer and more detailed discussions, please join our discourse.</p> <p>See Also</p> <p>@bodo.wrap_python can be used to switch to Python interpreted context to be able to run your workload, but we strongly recommend trying to find a Bodo-native workaround.</p> </li> <li> <p><code>BodoError: &lt;operation&gt; : &lt;parameter_name&gt; parameter only supports default value</code></p> <p>Certain methods only support default parameter values for some of their parameters. Please see supported Pandas API for a list of supported pandas functionality and their respective parameters. We also have a list of supported Numpy , as well as ML operations.</p> </li> </ul>"},{"location":"diagnostics_and_troubleshooting/Bodoerrors/#typing-errors","title":"Typing Errors","text":"<ul> <li> <p><code>BodoError: &lt;operation&gt;: &lt;operand&gt; must be a compile time constant</code></p> <p>Bodo needs certain arguments to be known at compile time to produce an optimized binary. Please refer to the documentation on compile time constants for more details.</p> </li> <li> <p><code>BodoError: dtype &lt;DataType&gt; cannot be stored in arrays</code></p> <p>This error message is encountered when Bodo is unable to assign a supported type to elements of an array.</p> <p>Example:</p> <pre><code>@bodo.jit\ndef obj_in_array():\n    df = pd.DataFrame({'col1': [\"1\", \"2\"], 'col2': [3, 4]})\n    return df.select_dtypes(include='object')\n\na = obj_in_array()\nprint(a)\n</code></pre> <p>Error: <pre><code>BodoError: dtype pyobject cannot be stored in arrays\n</code></pre></p> <p>In this example, we get this error because we attempted to get Bodo to recognize <code>col1</code> as a column with the datatype <code>object</code>, and the <code>object</code> type is too generic for Bodo. A workaround for this specific example would be to return <code>df.select_dtypes(exclude='int')</code>.</p> </li> <li> <p><code>Invalid Series.dt/Series.cat/Series.str, cannot handle conditional yet</code></p> <p>This error is encountered when there are conditional assignments of series functions <code>Series.dt</code>, <code>Series.cat</code> or <code>Series.str</code>, which Bodo cannot handle yet.</p> <p>Example:</p> <pre><code>@bodo.jit\ndef conditional_series_str(flag):\n    s = pd.Series([\"Str_Series\"])\n    s1 = pd.Series([\"Str_Series_1\"]).str\n    if flag:\n        s1 = s.str\n    else:\n        s1 = s1\n    return s1.split(\"_\")\n</code></pre> <p>Error:</p> <pre><code>BodoError: ...\n          Invalid Series.str, cannot handle conditional yet\n</code></pre> <p>When using these operations, you need to include the function and accessor together inside the control flow if it is absolutely necessary. For this specific case, we simply compute the <code>str.split</code> within the conditional:</p> <pre><code>@bodo.jit\ndef test_category(flag):\n    s = pd.Series([\"A_Str_Series\"])\n    s1 = pd.Series([\"test_series\"]).str\n    s2 = None\n    if flag:\n        s2 = s.str.split(\"_\")\n    else:\n        s2 = s1.split(\"_\")\n    return s2\n</code></pre> </li> </ul>"},{"location":"diagnostics_and_troubleshooting/Bodoerrors/#unsupported-numba-errors","title":"Unsupported Numba Errors","text":"<ul> <li> <p><code>numba.core.errors.TypingError: Compilation error</code></p> <p>This is likely due to unsupported functionality. If you encounter this error, please provide us a minimum reproducer for this error here.</p> </li> <li> <p><code>numba.core.errors.TypingError: Unknown attribute &lt;attribute&gt; of type</code></p> <p>This is an uncaught error due to unsupported functionality. If you encounter this error, please provide us a minimum reproducer for this error here.</p> </li> </ul>"},{"location":"diagnostics_and_troubleshooting/compilation/","title":"Compilation Tips and Troubleshooting","text":""},{"location":"diagnostics_and_troubleshooting/compilation/#what-code-to-jit-compile","title":"What Code to JIT Compile","text":"<p>The general recommendation is to use Bodo JIT compilation only for code that is data and/or compute intensive (e.g. Pandas code on large dataframes). In other words:</p> <ul> <li>Only use Bodo for data processing and analytics code such as Pandas, Numpy, and Scikit-Learn      (see Bodo API reference for analytics APIs with JIT support).</li> <li>Refactor code that sets up infrastructure or performs initializations out of JIT functions.</li> </ul> <p>This reduces the risk of encountering unsupported features and also reduces compilation time. For example, the program below finds the input file name in regular Python, and uses Bodo JIT only for data load and processing:</p> <pre><code>import bodo\nimport pandas as pd\nimport os\n\ndef get_filename():\n    if os.path.exists(\"input.parquet\"):\n        return \"input.parquet\"\n    if \"INPUT_FILE\" in os.environ:\n        return os.environ[\"INPUT_FILE\"]\n    raise Exception(\"Input file name not found\")\n\n@bodo.jit\ndef f(fname):\n    df = pd.read_parquet(fname)\n    print(df.sum())\n\nfname = get_filename()\nf(fname)\n</code></pre> <p>This recommendation is similar to Numba's What to compile.</p>"},{"location":"diagnostics_and_troubleshooting/compilation/#whycompilationerror","title":"Compilation Errors","text":"<p>First of all, let us understand why the code may fail to compile. There are three main kinds of issues:</p> <ol> <li>Some API is used that is not supported in Bodo JIT yet (see Bodo API Reference).</li> <li>Some Python construct or data structure is used that cannot be JIT compiled     (see Unsupported Python APIs).</li> <li>The code has type stability issues (see type stability).</li> </ol> <p>Below are some examples of the type of errors you may see due to these issues.</p>"},{"location":"diagnostics_and_troubleshooting/compilation/#unsupported-functions-or-methods","title":"Unsupported Functions or Methods","text":"<p>If a JIT function uses an unsupported function or method (e.g. in Pandas APIs), Bodo raises <code>BodoError</code> explaining that the method is not supported yet:</p> <pre><code>BodoError: &lt;method&gt; not supported yet\n</code></pre> <p>For example:</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.swapaxes(0, 1)\n...\n&gt;&gt;&gt; f(df)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/miniconda3/envs/Bodo/lib/python3.12/site-packages/bodo/numba_compat.py\", line 874, in _compile_for_args\n    raise error\nbodo.utils.typing.BodoError: DataFrame.swapaxes() not supported yet\n</code></pre>"},{"location":"diagnostics_and_troubleshooting/compilation/#unsupported-attributes","title":"Unsupported Attributes","text":"<p>Attempting to access an unsupported attribute in Bodo JIT functions will result in a <code>BodoError</code> as follows:</p> <pre><code>BodoError: &lt;attribute&gt; not supported yet\n</code></pre> <p>For example:</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.flags\n...\n&gt;&gt;&gt; f(df)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/miniconda3/envs/Bodo/lib/python3.12/site-packages/bodo/numba_compat.py\", line 874, in _compile_for_args\n    raise error\nbodo.utils.typing.BodoError: DataFrame.flags not supported yet\n</code></pre>"},{"location":"diagnostics_and_troubleshooting/compilation/#unsupported-arguments","title":"Unsupported Arguments","text":"<p>Supported APIs may not support all optional arguments. Supplying an unsupported argument will result in a <code>BodoError</code>:</p> <pre><code>BodoError: &lt;method&gt;: &lt;keyword&gt; argument not supported yet\n</code></pre> <p>For example:</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.sort_index(key=lambda x: x.str.lower())\n...\n&gt;&gt;&gt; f(df)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/miniconda3/envs/Bodo/lib/python3.12/site-packages/bodo/numba_compat.py\", line 874, in _compile_for_args\n    raise error\nbodo.utils.typing.BodoError: DataFrame.sort_index(): key parameter only supports default value None\nPlease check supported Pandas operations here (https://docs.bodo.ai/latest/api_docs/pandas/dataframe/).\n</code></pre>"},{"location":"diagnostics_and_troubleshooting/compilation/#type-stability-errors","title":"Type Stability Errors","text":"<p>Bodo needs to infer data types for all program variables for successful JIT compilation. A type stability issue arises when different program control flow paths assign values with different types to a variable. For example, variable <code>a</code> below could either be an integer or a string:</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(flag):\n...     if flag:\n...         a = 3\n...     else:\n...         a = \"A\"\n...     return a\n...\n&gt;&gt;&gt; f(True)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/miniconda3/envs/Bodo/lib/python3.12/site-packages/bodo/numba_compat.py\", line 874, in _compile_for_args\n    raise error\nbodo.utils.typing.BodoError: Unable to unify the following function return types: [Literal[int](3), Literal[str](A)]\n</code></pre> <p>The error <code>TypingError: Cannot unify &lt;type1&gt; and &lt;type2&gt;</code> means that the two possible data types cannot be combined and therefore, the variable cannot have a single data type.</p> <p>Dataframe variables require their schema (column names and their types) to be consistent for type stability (see dataframe schema stability). For example, the dataframe variable <code>df</code> below could either have a single column (\"A\": integer) or two columns (\"A\": integer, \"B\": float) depending on the runtime value of <code>flag</code>, which results in a type stability error:</p> <pre><code>&gt;&gt;&gt; @bodo.jit\n... def f(flag):\n...     df = pd.DataFrame({\"A\": [1, 2, 3, 4]})\n...     if flag:\n...         df[\"B\"] = [1.2, 0.4, 0.7, 121.9]\n...     print(df)\n...\n&gt;&gt;&gt; f(True)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/miniconda3/envs/Bodo/lib/python3.12/site-packages/bodo/numba_compat.py\", line 854, in _compile_for_args\n    error_rewrite(e, 'typing')\n  File \"/opt/miniconda3/envs/Bodo/lib/python3.12/site-packages/bodo/numba_compat.py\", line 763, in error_rewrite\n    raise e.with_traceback(None)\nnumba.core.errors.TypingError: Cannot unify dataframe((Array(int64, 1, 'C', False, aligned=True),), RangeIndexType(none), ('A',), 1D_Block_Var, False, False) and dataframe((Array(int64, 1, 'C', False, aligned=True), Array(float64, 1, 'C', False, aligned=True)), RangeIndexType(none), ('A', 'B'), 1D_Block_Var, False, False) for 'df', defined at &lt;stdin&gt; (3)\n</code></pre> <p>Additionally, some function arguments need to be constant to ensure type stability. In certain cases where it is possible, Bodo may infer the constant values. In other cases, it may throw an error indicating that the argument should be constant. For instance, <code>axis</code> argument in <code>pd.concat</code> determines whether the output is a Series type or a dataframe type in the example below. Therefore, Bodo needs to know the value at compilation time for type inference. Otherwise, an error is thrown (passing <code>axis</code> as argument to the JIT function fixes the error in this case):</p> <p><pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import bodo\n&gt;&gt;&gt; @bodo.jit\n... def f(S1, S2, flag):\n...     axis = 0\n...     if flag:\n...         axis = 1\n...     return pd.concat([S1, S2], axis=axis)\n...\n&gt;&gt;&gt; S1 = pd.Series([1, 2, 3], name=\"A\")\n&gt;&gt;&gt; S2 = pd.Series([3, 4, 5], name=\"B\")\n&gt;&gt;&gt; f(S1, S2, False)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/miniconda3/envs/Bodo/lib/python3.12/site-packages/bodo/numba_compat.py\", line 874, in _compile_for_args\n    raise error\nbodo.utils.typing.BodoError: pd.concat(): 'axis' should be a constant integer\n\n\n&gt;&gt;&gt; @bodo.jit\n... def f(S1, S2, axis):\n...     return pd.concat([S1, S2], axis=axis)\n...\n&gt;&gt;&gt; print(f(S1, S2, 0))\n\n    0    1\n    1    2\n    2    3\n    0    3\n    1    4\n    2    5\n    dtype: int64\n</code></pre> See Bodo API reference for more details on argument requirements.</p>"},{"location":"diagnostics_and_troubleshooting/compilation/#troubleshooting-compilation-errors","title":"Troubleshooting Compilation Errors","text":"<p>Now that we understand what causes the error, let's fix it!</p> <p>For potential unsupported APIs, Python feature gaps or type stability issues try the following:</p> <ol> <li> <p>Make sure your code works in Python. In a lot of cases, a Bodo     decorated function does not compile, but it does not compile in     Python either.</p> </li> <li> <p>Refactor your code with supported operations if possible. For     instance, the <code>sort_index(key=lambda ...)</code> examble above can be     replaced with regular <code>sort_values</code>:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     return df.sort_index(key=lambda x: x.str.lower())\n...\n&gt;&gt;&gt; f(df)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/miniconda3/envs/Bodo/lib/python3.12/site-packages/bodo/numba_compat.py\", line 874, in _compile_for_args\n    raise error\nbodo.utils.typing.BodoError: DataFrame.sort_index(): key parameter only supports default value None\n\n&gt;&gt;&gt; @bodo.jit\n... def f(df):\n...     df[\"key\"] = df.index.map(lambda a: a.lower())\n...     return df.sort_values(\"key\").drop(columns=\"key\")\n...\n&gt;&gt;&gt; f(df)\n    a\n    A  1\n    b  2\n    C  3\n    d  4\n</code></pre> </li> <li> <p>Refactor your code and use regular Python for unsupported     features.</p> <p>a.  Move the code causing issues to regular Python and pass     necessary data to JIT functions.</p> <p>b.  Use <code>@bodo.wrap_python</code> to perform some computation within JIT     functions in regular Python if necessary (see @bodo.wrap_python).</p> </li> <li> <p>Refactor your code to make it type stable (see     type stability). For example:</p> <pre><code>&gt;&gt;&gt; flag = True\n&gt;&gt;&gt; @bodo.jit\n... def f(flag):\n...     df = pd.read_parquet(\"in.parquet\")\n...     if flag:\n...             df[\"C\"] = 1\n...     df.to_parquet(\"out.parquet\")\n...\n&gt;&gt;&gt; f(flag)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/miniconda3/envs/Bodo/lib/python3.12/site-packages/bodo/numba_compat.py\", line 854, in _compile_for_args\n    error_rewrite(e, 'typing')\n  File \"/opt/miniconda3/envs/Bodo/lib/python3.12/site-packages/bodo/numba_compat.py\", line 763, in error_rewrite\n    raise e.with_traceback(None)\nnumba.core.errors.TypingError: Cannot unify dataframe((Array(datetime64[ns], 1, 'C', False, aligned=True), Array(int64, 1, 'C', False, aligned=True)), RangeIndexType(none), ('A', 'B'), 1D_Block_Var, True, False) and dataframe((Array(datetime64[ns], 1, 'C', False, aligned=True), Array(int64, 1, 'C', False, aligned=True), Array(int64, 1, 'C', False, aligned=True)), RangeIndexType(none), ('A', 'B', 'C'), 1D_Block_Var, True, False) for 'df', defined at &lt;stdin&gt; (3)\n\n\n\n&gt;&gt;&gt; @bodo.jit\n... def f1():\n...     df = pd.read_parquet(\"in.parquet\")\n...     return df\n...\n&gt;&gt;&gt; @bodo.jit\n... def f2(df):\n...     df[\"C\"] = 1\n...     return df\n...\n&gt;&gt;&gt; @bodo.jit\n... def f3(df):\n...     df.to_parquet(\"out.parquet\")\n...\n&gt;&gt;&gt; df = f1()\n&gt;&gt;&gt; if flag:\n...     df = f2(df)\n...\n&gt;&gt;&gt; f3(df)\n</code></pre> </li> </ol>"},{"location":"diagnostics_and_troubleshooting/compilation/#disabling-python-output-buffering","title":"Disabling Python Output Buffering","text":"<p>Sometimes standard output prints may not appear when the program fails, due to Python's I/O buffering. Therefore, setting <code>PYTHONUNBUFFERED</code> environment variable is recommended for debugging:</p> <pre><code>export PYTHONUNBUFFERED=1\n</code></pre>"},{"location":"diagnostics_and_troubleshooting/compilation/#requesting-unsupported-functionality-and-reporting-errors","title":"Requesting Unsupported Functionality and Reporting Errors","text":"<p>If you want to request a new feature, or report a bug you have found, please create an issue in our GitHub repository. If you encounter an error which is not covered on this page, please report it to our repository as well.</p>"},{"location":"diagnostics_and_troubleshooting/verbose_mode/","title":"Verbose Mode","text":"<p>When compiling functions, Bodo introduces various optimizations to improve runtime performance. Since the success of certain optimizations can be essential, we provide the option to run Bodo in <code>verbose mode</code> and track certain optimizations at compile time. The information provided by <code>verbose mode</code> can help you better understand you workload's performance as well as how to debug the workload. Additionally, using Python's <code>logging</code> module alongside Bodo's <code>verbose mode</code> can be used to track optimizations across frequently running jobs.</p> <p>Note</p> <p>Currently all of our optimizations are tracked at compile time. This information is not stored if a function is cached.</p>"},{"location":"diagnostics_and_troubleshooting/verbose_mode/#example-usage","title":"Example Usage","text":"<p>To detect important optimizations, all you need to do is set a verbose level in the global scope of a Python file using <code>bodo.set_verbose_level(level)</code>. The verbose level is a positive integer, with greater values outputting more detailed information. The optimizations that are expected to be the most impactful are tracked at level 1, so in most situations you can just do <code>bodo. set_verbose_level(1)</code>. More information on the optimizations that are displayed is found in the <code>set_verbose_level</code> API reference. Now when Bodo compiles a function, <code>rank 0</code> will log important optimizations to <code>stderr</code> using Python's <code>logging</code> package.</p> <p>Below is an example using the <code>verbose mode</code> to verify that Bodo is only loading the 1 column from a parquet file that is actually needed as opposed to any additional columns.</p> <pre><code>bodo.set_verbose_level(1)\n\n@bodo.jit\ndef load_data(filename):\n    df = pd.read_parquet(filename)\n    return df.id\n\nload_data(\"my_file.pq\")\n</code></pre> <pre><code>2024-03-24 10:44:21,023 - Bodo Default Logger - INFO - \n================================================================================\n--------------------------------Filter Pushdown---------------------------------\nArrow filters pushed down:\nNone\nNone\n\n\n================================================================================\n2024-03-24 10:44:21,023 - Bodo Default Logger - INFO - \n================================================================================\n---------------------------------Column Pruning---------------------------------\nFinish column pruning on read_parquet node:\n\nFile \"objmode.py\", line 10:\ndef load_data(filename):\n    df = pd.read_parquet(filename)\n    ^\nColumns loaded ['id']\n\n================================================================================\n</code></pre> <p>You can also log this information to a valid <code>logging.Logger</code> instance with Bodo.</p> <p>Important</p> <p>The logger should be a variable set in a global scope.</p> <pre><code>bodo.set_verbose_level(1)\nlogger = logging.getLogger(\"myLogger\")\nlogger.setLevel(logging.INFO)\nlogger.addHandler(logging.FileHandler(\"example.log\"))\nbodo.set_bodo_verbose_logger(logger)\n\n@bodo.jit\ndef load_data(filename):\n    df = pd.read_parquet(filename)\n    return df.id\n\nload_data(\"my_file.pq\")\n</code></pre> <p>The output will be written to <code>example.log</code> in the current working directory.</p> <pre><code>## Leveraging Optimizations for Debugging\n\nOptimzation logging can be useful for diagnosing possible performance issues. Below is an example that shows the impact of printing a section of a DataFrame to inspect the result of `read_parquet`.\n\n```py\nbodo.set_verbose_level(1)\n\n@bodo.jit\ndef load_data(filename):\n    df = pd.read_parquet(filename)\n    print(df.head(10))\n    return df.id\n\nload_data(\"my_file.pq\")\n</code></pre> <pre><code>2024-03-24 10:48:19,046 - Bodo Default Logger - INFO - \n================================================================================\n--------------------------------Filter Pushdown---------------------------------\nArrow filters pushed down:\nNone\nNone\n\n\n================================================================================\n2024-03-24 10:48:19,047 - Bodo Default Logger - INFO - \n================================================================================\n---------------------------------Column Pruning---------------------------------\nFinish column pruning on read_parquet node:\n\nFile \"objmode.py\", line 11:\ndef load_data(filename):\n    df = pd.read_parquet(filename)\n    ^\nColumns loaded ['id', 'Hectare', 'Date', 'Age', 'Primary Fur Color', 'Highlight Fur Color', 'Location', 'Specific Location', 'Running', 'Chasing', 'Climbing', 'Eating', 'Foraging', 'Other Activities', 'Kuks', 'Quaas', 'Moans', 'Tail flags', 'Tail twitches', 'Approaches', 'Indifferent', 'Runs from', 'Other Interactions']\n</code></pre> <p>Printing <code>df.head()</code> prints every column in the DataFrame, so Bodo must load all of the columns. In contrast, without this print, Bodo can load just a single column from the parquet file, so this increases both memory usage and execution time.</p> <p>In some situations the reason for an optimization failure may not be as straightforward as the example above. Even when the code is more complicated, the success/failure of optimizations can be an extremely useful first step to determine why performance is worse than expected.</p>"},{"location":"diagnostics_and_troubleshooting/verbose_mode/#user-apis","title":"User APIs","text":""},{"location":"diagnostics_and_troubleshooting/verbose_mode/#set_verbose_level","title":"set_verbose_level","text":"<ul> <li> <p><code>bodo.set_verbose_level(level)</code> </p> <p>Determines if compiled JIT functions should output logging information. Level 0 disables optimization logging and level 1 contains all of the most important operations.</p> <p>The optimizations currently displayed at each level are:</p> Verbose Level Optimizations 1 <ul><li>Column Pruning</li><li>Filter Pushdown</li><li>Dictionary Encoding</li><li>Limit Pushdown</li><li>BodoSQL generated IO time</li></ul> 2 <ul><li>Join column pruning</li></ul> <p>Arguments</p> <ul> <li>level: A non-negative integer for the logging granularity.</li> </ul> <p>Note: <code>bodo.set_verbose_level()</code> should not be used inside a JIT function.</p> </li> </ul>"},{"location":"diagnostics_and_troubleshooting/verbose_mode/#set_bodo_verbose_logger","title":"set_bodo_verbose_logger","text":"<ul> <li> <p><code>bodo.set_bodo_verbose_logger(logger)</code> </p> <p>Sets the logging location for Bodo verbose messages. Bodo will write to this logger on <code>rank 0</code> only, to prevent possible conflicts when writing to an output file. All messages are given with <code>logging.info</code>, so the logger should have an appropriate effect level.</p> <p>Arguments</p> <ul> <li><code>logger</code>: An instance of type <code>logging.Logger</code>.</li> </ul> <p>Note: <code>bodo.set_bodo_verbose_logger()</code> should not be used inside a JIT function.</p> </li> </ul>"},{"location":"guides/using_bodo_platform/","title":"Bodo Cloud Platform","text":"<p>This set of guides explains the basics of using the Bodo cloud platform and associated concepts.</p>"},{"location":"guides/using_bodo_platform/#organization-basics","title":"Organization Basics","text":"<p>This page describes the fundamental components of the Bodo Cloud Platform and how they are organized, </p>"},{"location":"guides/using_bodo_platform/#creating-a-cluster","title":"Creating a Cluster","text":"<p>This guide describes how to create a cluster, and walks through various configuration parameters used in the process.</p>"},{"location":"guides/using_bodo_platform/#using-notebooks","title":"Using Notebooks","text":"<p>This guide describes how to attach a notebook to a cluster to perform computation on the cluster. </p>"},{"location":"guides/using_bodo_platform/#running-jobs","title":"Running Jobs","text":"<p>This guide shows how to run jobs using the Bodo Cloud Platform.</p>"},{"location":"guides/using_bodo_platform/#native-sql-with-catalogs","title":"Native SQL with Catalogs","text":"<p>This guide provides an overview of using native SQL with Notebooks and how to use Catalogs to access data.</p>"},{"location":"guides/using_bodo_platform/#platform-sdk-guide","title":"Platform SDK Guide","text":"<p>This guide provides a walkthrough of the Bodo Platform SDK that provides a simple way to interact with the Bodo Platform API.</p>"},{"location":"guides/using_bodo_platform/#instance-role-for-a-cluster","title":"Instance Role for a Cluster","text":"<p>This guide explains how to use your own instance role for a cluster on the Bodo Cloud Platform.</p>"},{"location":"guides/using_bodo_platform/#managing-packages-on-the-cluster-using-jupyter-magics-conda-and-pip","title":"Managing Packages on the cluster using Jupyter magics - Conda and Pip","text":"<p>This guide includes instructions on how to manage packages on the cluster using Jupyter magics.</p>"},{"location":"guides/using_bodo_platform/#running-shell-commands-on-the-cluster-using-jupyter-magics","title":"Running shell commands on the cluster using Jupyter magics","text":"<p>This guide explains how to run shell commands on the cluster using Jupyter magics.</p>"},{"location":"guides/using_bodo_platform/#connecting-to-a-cluster","title":"Connecting to a Cluster","text":"<p>This guide explains how to interact with Bodo clusters outside of Jobs and Notebooks.</p>"},{"location":"guides/using_bodo_platform/#creating-network-configuration","title":"Creating Network Configuration","text":"<p>This guide explains how to create a network configuration for the Bodo Cloud Platform.</p>"},{"location":"guides/using_bodo_platform/#configuring-customer-managed-vpc","title":"Configuring Customer Managed VPC","text":"<p>This guide explains how to configure a Customer Managed VPC for the Bodo Cloud Platform.</p>"},{"location":"guides/using_bodo_platform/#configuring-aws-privatelink","title":"Configuring AWS PrivateLink","text":"<p>This guide explains how to configure an AWS PrivateLink in Customer Managed VPC for the Bodo Cloud Platform.</p>"},{"location":"guides/using_bodo_platform/#troubleshooting","title":"Troubleshooting","text":"<p>This guide offers some troubleshooting tips for potential issues encountered while using the Bodo Cloud Platform.</p>"},{"location":"guides/using_bodo_platform/aws_private_link/","title":"AWS PrivateLink","text":"<p> Supported on AWS \u00b7</p> <p>AWS PrivateLink is a service provided by Amazon Web Services (AWS) that enables secure, private connectivity between your virtual private cloud (VPC) and on-premises networks to AWS services. It ensures that traffic between these resources never traverses the public internet, enhancing security, reducing exposure to internet-based threats, and providing low-latency connectivity.</p>"},{"location":"guides/using_bodo_platform/aws_private_link/#bodo-cluster-aws-privatelink","title":"Bodo Cluster AWS PrivateLink","text":"<p>AWS PrivateLink for Bodo Platform enables private connectivity between Bodo Platform and client clusters. With this feature, you do not need to use an internet gateway or NAT to allow communication with the Bodo Platform from Bodo clusters in your VPC subnets.</p>"},{"location":"guides/using_bodo_platform/aws_private_link/#bodo-cluster-endpoint-services","title":"Bodo Cluster Endpoint Services","text":"<p>List of supported endpoint services:</p> Region Service Name Supported AZs us-east-1 com.amazonaws.vpce.us-east-1.vpce-svc-0b0c6643c1f764b62 az1, az4, az6 us-east-2 com.amazonaws.vpce.us-east-2.vpce-svc-0c49909796ff87d5b az1, az2, az3 us-west-1 com.amazonaws.vpce.us-west-1.vpce-svc-0ce34162b3b8f1eaa az1, az3 us-west-2 com.amazonaws.vpce.us-west-2.vpce-svc-026e1758e07ba65d5 az1, az2, az3 eu-west-1 com.amazonaws.vpce.eu-west-1.vpce-svc-05352b1056a782d85 az1, az2, az3"},{"location":"guides/using_bodo_platform/aws_private_link/#configure-bodo-workspace-with-privatelink","title":"Configure Bodo Workspace with PrivateLink","text":"<p>This section explains how to configure AWS Private link,  so the connection between the Bodo Platform and Bodo clusters will be made in the AWS internal network.</p> <p>This can be done in two ways:</p>"},{"location":"guides/using_bodo_platform/aws_private_link/#using-cloud-formation-template","title":"Using Cloud Formation Template","text":"<p>You can use the following CloudFormation template. You can find the template here</p>"},{"location":"guides/using_bodo_platform/aws_private_link/#manual-configuration","title":"Manual Configuration","text":"<ol> <li> <p>To use AWS PrivateLink, you need to create a workspace with Customer Managed VPC without     an internet gateway or NAT gateway.      Make sure that the VPC has DNS resolution and DNS hostnames enabled.</p> </li> <li> <p>Create an interface endpoint that points to the specific Bodo Cluster Endpoint Service, depending on the region: </p> </li> <li> <p>Once the endpoint is available, modify the private DNS name: </p> </li> <li> <p>Create an S3 gateway endpoint if it does not already exist in the VPC (required for access workspace S3 storage): </p> </li> <li> <p>Create an SSM interface endpoint if it does not already exist in the VPC (required for Bodo clusters to read workspace SSM parameters): </p> </li> <li> <p>Once you have created all the above resources, you can create a [network configuration] (network_configuration.md).</p> </li> </ol> <p>Important</p> <p>For interface endpoints, you don't need to select all the subnets used by the workers; you just need to select at least one. For the S3 gateway, you need to select all route tables associated with subnets used by Bodo clusters.</p>"},{"location":"guides/using_bodo_platform/bodo_jobs/","title":"Running Jobs","text":"<p> Supported on AWS \u00b7  Supported on Azure \u00b7</p> <p>Bodo supports Python and SQL jobs. Jobs are useful for running data processing tasks, such as ETL, ELT, data preparation, and data analysis.  Bodo Cloud Platform allows users to create Job Template and then submit Job Runs for execution.</p> <p></p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#job-template","title":"Job Template","text":"<p>Job templates are stored objects which can be used to run your data processing and analysis applications in a Bodo Platform workspace. You need to have an available workspace before creating a job templates. </p> <p></p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#job-template-form-fields","title":"Job Template Form Fields","text":"<p>Here are the fields you need to provide when creating a job template. </p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#1-basic","title":"1) Basic:","text":"<ul> <li> <p>Name:  The job template name.</p> </li> <li> <p>Type:  We currently support three types of sources: <code>WORKSPACE</code>, <code>GIT</code>, <code>URL</code> and <code>S3</code>.</p> <p>Note</p> <p>The job template name must be unique within the workspace. Each job template is assigned a unique id (UUID).  If a job template is deleted from the workspace, the name can be reused. However, the UUID of the new job template will be different.</p> </li> </ul>"},{"location":"guides/using_bodo_platform/bodo_jobs/#2-source","title":"2) Source:","text":"<ul> <li> <p>Type : The type of the job. Currently, we support three types of jobs: Python(<code>PYTHON</code>) SQL(<code>SQL</code>), Notebook(<code>IPYNB</code>).</p> </li> <li> <p>File Name : The relative path from the job source to the <code>.py</code>, <code>.sql</code> and <code>ipynb</code> file that contains the job script.</p> </li> <li> <p>Timeout: The timeout for the job in minutes. The default value is 60 minutes.  Note that the timeout applies to each individual retry attempt, and not the total execution time of a job run with potentially multiple retries.</p> </li> </ul>"},{"location":"guides/using_bodo_platform/bodo_jobs/#workspace","title":"Workspace","text":"<p>The job source is a file in the workspace (<code>bodofs</code>). You need to specify the path and file name in the workspace.   </p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#git","title":"GIT","text":"<p>The job source is a file in a Git repository. You need to specify the Git repository URL and the file path in the repository. You also need to provide a Git username and an access token for accessing the repository. If you want to check out a specific branch or commit, you can specify the branch or commit <code>reference</code>. Otherwise, the default branch will be used.   </p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#url","title":"URL","text":"<p>The job source is a file under specific url. You need to specify the base url and file name.   </p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#s3","title":"S3","text":"<p>The job source is a file in an S3 bucket. You need to specify the file path including the bucket name. You also need to provide a bucket region.   </p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#3-advanced-options","title":"3) Advanced Options","text":""},{"location":"guides/using_bodo_platform/bodo_jobs/#arguments","title":"Arguments","text":"<p>The arguments to the job. The arguments are passed to the job script as command line arguments for <code>.py</code> files.  Example arguments<pre><code>{\n    \"args\": \"--arg1 value1 --arg2 value2\"\n}\n</code></pre></p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#environment-variables","title":"Environment Variables","text":"<pre><code>Key-value pairs of environment variables for the job. Default value is an empty dictionary.\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_jobs/#catalog","title":"Catalog","text":"<pre><code>Catalog is configuration object that grant BodoSQL access to load tables from a database.\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_jobs/#retry-strategy","title":"Retry Strategy","text":"<p>The retry strategy for the job. The retry strategy is a JSON object that specifies the retry policy for the job. It includes the following fields:</p> <ul> <li>Number of Retries: The number of retries for the job. The default value is 0.</li> <li>Retry Delay: The retry interval in minutes. The default value is one minute.</li> <li>Auto Retry on Timeout: Whether to retry on job timeout. The default value is <code>false</code>.</li> </ul>"},{"location":"guides/using_bodo_platform/bodo_jobs/#description","title":"Description","text":"<p>The description of the job template.</p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#4-job-cluster","title":"4) Job Cluster","text":"<p>The job cluster configuration specifies the default cluster configuration for the job.  This is optional, as you can always choose a cluster to submit the job run. It includes the following fields:</p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#cluster-name","title":"Cluster Name","text":"<p>The cluster name that will be created.</p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#instance-type","title":"Instance Type","text":"<pre><code>The cluster instance type depending on the cloud service provider.\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_jobs/#number-of-workers","title":"Number of Workers","text":"<pre><code>The number of workers in the cluster.\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_jobs/#bodo-version","title":"Bodo Version","text":"<pre><code>The Bodo version to use for the cluster.\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_jobs/#running-a-job","title":"Running a Job","text":"<p>Once you've created a job template, you can run it whenever you want.</p> <p></p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#create-job-run-form","title":"Create Job Run Form","text":"<p>To submit a new job from template, you need to select:</p> <p>Note</p> <ul> <li>If you don't provide a cluster UUID, the job will run on a new cluster with the default cluster configuration provided by the associated definition.</li> <li>If neither a cluster UUID nor a cluster configuration is provided, an error will be thrown.</li> </ul>"},{"location":"guides/using_bodo_platform/bodo_jobs/#create-new-cluster-from-job-template","title":"Create New Cluster from Job Template","text":"<p>Option only enabled if in job template definition we have defined a cluster default configuration.</p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#pause-cluster-on-finish","title":"Pause Cluster on finish","text":"<pre><code>Pause a cluster after job finish.\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_jobs/#cluster-uuid","title":"Cluster UUID","text":"<pre><code>The UUID of the cluster to run the job on.\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_jobs/#job-logs-and-status","title":"Job Logs and Status","text":"<p>Jobs can have one of the following statuses. </p> <ul> <li><code>PENDING</code> - The job is pending.</li> <li><code>RUNNING</code> - The job is running.</li> <li><code>SUCCEEDED</code> - The job run succeeded.</li> <li><code>FAILED</code> - The job run failed.</li> <li><code>CANCELLED</code> - The job run was cancelled.</li> </ul> <p>Each job that is not <code>SUCCEEDED</code> also has a status reason associated with it. When the status is one of <code>PENDING</code>, <code>FAILED</code> or <code>CANCELLED</code>, the reason could be one of the following:</p> <ul> <li><code>Cancelled by user</code> - The job run was <code>CANCELLED</code> by the user.</li> <li><code>In queue</code> - The job is <code>PENDING</code> in the queue because there's potentially another job running on the same cluster. </li> <li><code>Recently submitted</code> - The job is <code>PENDING</code> because it was recently submitted. </li> <li><code>NonZeroExitCode</code> - The job run <code>FAILED</code> because the job script exited with a non-zero exit code.</li> <li><code>Timeout</code> - The job run <code>FAILED</code> because it timed out.</li> </ul> <p>You can access the logs of a job run from the UI as well.</p> <p></p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#sql-job","title":"Running a SQL Query as a Job","text":"<p>Bodo supports running SQL queries as jobs without explicitly writing a job template.  See Bodo Platform SDK for usage details.</p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#queuing-job-runs","title":"Queuing Job Runs","text":"<p>If you submit a job run while there's another job running on the same cluster,  the new job will automatically be queued. Currently, at most 100 job runs can be queued on a cluster at a time. Note that you can queue job runs for different job templates on the same cluster.</p>"},{"location":"guides/using_bodo_platform/bodo_jobs/#submitting-job-to-a-paused-cluster","title":"Submitting Job to a Paused Cluster","text":"<p>You can choose whether to allow a cluster to resume on submission of a job run. This is enabled by default. If you submit a job to a paused cluster with auto-resume enabled, the cluster will be resumed automatically. If auto-resume is not enabled, then the job run submission will fail. </p>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/","title":"Bodo Platform SDK Guide","text":"<p>Bodo Platform SDK is a Python library that provides a simple way to interact with the Bodo Platform API. It allows you to create, manage, and monitor resources such as clusters, jobs, and workspaces.</p>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#getting-started","title":"Getting Started","text":""},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#installation","title":"Installation","text":"<pre><code>pip install bodosdk\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#create-a-workspace-client","title":"Create a workspace client","text":"<p>First you need to access your workspace in <code>https://platform.bodo.ai/</code> and create an API Token in the Bodo Platform. This token is used to authenticate your client with the Bodo Platform API.</p> <p>Navigate to API Tokens in the Admin Console to generate a token. Copy and save the token's Client ID and Secret Key and use them to define a client (<code>BodoClient</code>) that can interact with the Bodo Platform.</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient(\n    client_id=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    secret_key=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n)\n</code></pre> <p>Alternatively, set <code>BODO_CLIENT_ID</code> and <code>BODO_SECRET_KEY</code> environment variables to avoid storing keys in your code.</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\n</code></pre> <p>You can access the <code>workspace_data</code> attribute of the client to get more information about your workspace:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nprint(my_workspace.workspace_data)\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#additional-configuration-options-for-bodoclient","title":"Additional Configuration Options for <code>BodoClient</code>","text":"<ul> <li><code>print_logs</code>: defaults to False. All API requests and responses are printed to the console if set to True.</li> </ul> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient(print_logs=True)\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#create-a-cluster","title":"Create a cluster","text":"<p>This example creates a simple one node cluster in your workspace with the latest available bodo version. It returns a cluster object.</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_cluster = my_workspace.ClusterClient.create(\n    name='My first cluster',\n    instance_type='c5.large',\n    workers_quantity=1\n)\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#wait-for-status","title":"Wait for status","text":"<p>You can use the <code>wait_for_status</code> method to wait until the cluster ready to use.</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_cluster = my_workspace.ClusterClient.create(\n    name='My first cluster',\n    instance_type='c5.large',\n    workers_quantity=1\n)\nmy_cluster.wait_for_status(['RUNNING'])\n</code></pre> <p>This method will wait until either the cluster reaches any of the provided status, or fails (goes into the <code>FAILED</code> status) for any reason.</p>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#update-your-cluster","title":"Update your cluster","text":"<p>Now let's update our cluster. While the cluster is <code>RUNNING</code>, you can update the <code>name</code>, <code>description</code>, <code>auto_pause</code>, <code>auto_stop</code>, and <code>workers_quantity</code>(this will trigger scaling) fields only:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_cluster = my_workspace.ClusterClient.create(\n    name='My first cluster',\n    instance_type='c5.large',\n    workers_quantity=1\n)\n\nmy_cluster.wait_for_status(['RUNNING'])\nmy_cluster.update(\n    description='My description',\n    name=\"My updated cluster\",\n    auto_pause=15,\n    auto_stop=30\n)\n</code></pre> <p>All other modifications like <code>instance_type</code>, <code>bodo_version</code> etc. need the cluster to be <code>STOPPED</code> first.</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_cluster = my_workspace.ClusterClient.get(\"cluster_id\")\nif my_cluster.status != 'STOPPED':\n    my_cluster.stop(wait=True)\nmy_cluster.update(instance_type='c5.2xlarge', workers_quantity=2)\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#create-a-job","title":"Create a Job","text":"<p>Lets now create a job on our <code>RUNNING</code> cluster. First, access <code>https://platform.bodo.ai</code> and navigate to the Jupyter notebook in your workspace. Then create the following <code>test.py</code> file in your main directory:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport bodo\nimport time\n\nNUM_GROUPS = 30\nNUM_ROWS = 20_000_000\n\ndf = pd.DataFrame({\n    \"A\": np.arange(NUM_ROWS) % NUM_GROUPS,\n    \"B\": np.arange(NUM_ROWS)\n})\ndf.to_parquet(\"my_data.pq\")\ntime.sleep(1)  # wait till file will be available on all nodes\n\n\n@bodo.jit(cache=True)\ndef computation():\n    t1 = time.time()\n    df = pd.read_parquet(\"my_data.pq\")\n    df2 = pd.DataFrame({\"A\": df.apply(lambda r: 0 if r.A == 0 else (r.B // r.A), axis=1)})\n    df2.to_parquet(\"out.pq\")\n    print(\"Execution time:\", time.time() - t1)\n\ncomputation()\n</code></pre> <p>Now you can define a job on cluster through SDK, wait till it has <code>SUCCEEDED</code> and check its logs as follows:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_cluster = my_workspace.ClusterClient.get(\"cluster_id\")\nmy_job = my_cluster.run_job(\n    code_type='PYTHON',\n    source={'type': 'WORKSPACE', 'path': '/'},\n    exec_file='test.py'\n)\nprint(my_job.wait_for_status(['SUCCEEDED']).get_stdout())\n</code></pre> <p>You can use almost the same configuration to run a SQL file. All you need is to define your <code>test.sql</code> file and a Catalog on <code>https://platform.bodo.ai</code>:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_cluster = my_workspace.ClusterClient.get(\"cluster_id\")\nmy_job = my_cluster.run_job(\n    code_type='SQL',\n    source={'type': 'WORKSPACE', 'path': '/'},\n    exec_file='test.sql',\n    catalog=\"MyCatalog\"\n)\nprint(my_job.wait_for_status(['SUCCEEDED']).get_stdout())\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#cluster-list-and-executing-jobs-on-multiple-clusters","title":"Cluster List and executing jobs on multiple clusters","text":"<p>Now let's try to run same job on different clusters:</p> <pre><code>from bodosdk import BodoWorkspaceClient\nimport random\n\nmy_workspace = BodoWorkspaceClient()\n\nrandom_val = random.random()  # just to avoid conflicts on name\nclusters_conf = [('c5.large', 8), ('c5.xlarge', 4), ('c5.2xlarge', 2)]\nfor i, conf in enumerate(clusters_conf):\n    my_workspace.ClusterClient.create(\n        name=f'Test {i}',\n        instance_type=conf[0],\n        workers_quantity=conf[1],\n        custom_tags={'test_tag': f'perf_test{random_val}'}  # let's add tag to easy filter our clusters\n    )\n\n# get list by tag\nclusters = my_workspace.ClusterClient.list(filters={\n    'tags': {'test_tag': f'perf_test{random_val}'}\n})\n\n# run same job 3 times, once per each cluster\njobs = clusters.run_job(\n    code_type='PYTHON',\n    source={'type': 'WORKSPACE', 'path': '/'},\n    exec_file='test.py'\n)\n\n# wait for jobs to finish and print results\nfor job in jobs.wait_for_status(['SUCCEEDED']):\n    print(job.name, job.cluster.name)\n    print(job.get_stdout())\n\n# remove our clusters\njobs.clusters.delete()  # or clusters.delete()\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#execute-sql-query","title":"Execute SQL query","text":"<p>You can also execute SQL queries by passing just query text like following:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_sql_job = my_workspace.JobClient.run_sql_query(\n    sql_query=\"SELECT 1\", \n    catalog=\"MyCatalog\", \n    cluster={\n        \"name\": 'Temporary cluster',\n        \"instance_type\": 'c5.large',\n        \"workers_quantity\": 1\n    }\n)\nprint(my_sql_job.wait_for_status(['SUCCEEDED']).get_stdout())\n</code></pre> <p>In this case, when you provide a cluster configuration rather than an existing cluster, the created cluster will be terminated as soon as SQL job finishes.</p> <p>If you want to execute a SQL job on existing cluster, you can use <code>run_sql_query</code> on cluster:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_cluster = my_workspace.ClusterClient.create(\n    name='My cluster',\n    instance_type='c5.large',\n    workers_quantity=1\n)\nmy_sql_job = my_cluster.run_sql_query(sql_query=\"SELECT 1\", catalog=\"MyCatalog\")\nprint(my_sql_job.wait_for_status(['SUCCEEDED']).get_stdout())\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#connector","title":"Connector","text":"<p>You can also execute SQL queries using a cluster Connector via a Cursor:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_cluster = my_workspace.ClusterClient.create(\n    name='My cluster',\n    instance_type='c5.large',\n    workers_quantity=1\n)\nconnection = my_cluster.connect(\n    'MyCatalog')  # or connection = my_workspace.ClusterClient.connect('MyCatalog', 'cluster_id')\nprint(connection.cursor().execute(\"SELECT 1\").fetchone())\nmy_cluster.delete()\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#job-templates","title":"Job Templates","text":"<p>Rather than defining jobs from scratch every time, you can create a template for your jobs.</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\ntpl = my_workspace.JobTemplateClient.create(\n    name='My template',\n    cluster={\n        'instance_type': 'c5.xlarge',\n        'workers_quantity': 1\n    },\n    code_type=\"SQL\",\n    catalog=\"MyCatalog\",\n    exec_text=\"SELECT 1\"\n)\njob1 = tpl.run()  # you can simply run it\njob2 = tpl.run(exec_text=\"SELECT 2\")  # or run it with overriding template values\njob3 = tpl.run(cluster={'instance_type': 'c5.large'})  # you can override even part of cluster configuration\n\njobs = my_workspace.JobClient.list(filters={'template_ids': [tpl.id]})  # you can filter jobs by its template_id\nfor job in jobs.wait_for_status(['SUCCEEDED']):\n    print(job.name, job.cluster.instance_type, job.get_stdout())\n</code></pre> <p>You can also run your template on specific clusters:</p> <pre><code>from bodosdk import BodoWorkspaceClient\nfrom bodosdk.models import JobTemplateFilter\n\nmy_workspace = BodoWorkspaceClient()\ntpls = my_workspace.JobTemplateClient.list(filters=JobTemplateFilter(names=['My template']))\nmy_cluster = my_workspace.ClusterClient.create(\n    name='My cluster',\n    instance_type='c5.large',\n    workers_quantity=1\n)\nprint(my_cluster.run_job(template_id=tpls[0].id).wait_for_status(['SUCCEEDED']).get_stdout())\nmy_cluster.delete()\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#statuses","title":"Statuses","text":"<p>Each resource, Cluster, Job or Workspace has own set of statuses which are as follows:</p>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#cluster","title":"Cluster","text":"<ul> <li>NEW</li> <li>INPROGRESS</li> <li>PAUSING</li> <li>PAUSED</li> <li>STOPPING</li> <li>STOPPED</li> <li>INITIALIZING</li> <li>RUNNING</li> <li>FAILED</li> <li>TERMINATED</li> </ul>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#job","title":"Job","text":"<ul> <li>PENDING</li> <li>RUNNING</li> <li>SUCCEEDED</li> <li>FAILED</li> <li>CANCELLED</li> <li>CANCELLING</li> <li>TIMEOUT</li> </ul>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#workspace","title":"Workspace","text":"<ul> <li>NEW</li> <li>INPROGRESS</li> <li>READY</li> <li>FAILED</li> <li>TERMINATING</li> <li>TERMINATED</li> </ul>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#organization-client-and-workspaces","title":"Organization client and workspaces","text":"<p>To manage workspaces, you need different keys (generated for the organization) and different SDK clients. Let's list all our workspaces:</p> <pre><code>from bodosdk import BodoOrganizationClient\n\nmy_org = BodoOrganizationClient(\n    client_id=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    secret_key=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n)  # or BodoOrganizationClient() if `BODO_ORG_CLIENT_ID` and `BODO_ORG_SECRET_KEY` are exported\nfor w in my_org.list_workspaces():\n    print(w.name)\n</code></pre> <p>You can filter workspaces with valid filters:</p> <pre><code>from bodosdk import BodoOrganizationClient\nfrom bodosdk.models import WorkspaceFilter\n\nmy_org = BodoOrganizationClient()\n\nfor w in my_org.list_workspaces(filters=WorkspaceFilter(statuses=['READY'])):\n    print(w.name)\n</code></pre> <p>You can provide filters such as <code>WorkspaceFilter</code> imported from <code>bodosdk.models</code> or as a dictionary:</p> <pre><code>from bodosdk import BodoOrganizationClient\n\nmy_org = BodoOrganizationClient()\n\nfor w in my_org.list_workspaces(filters={\"statuses\": ['READY']}):\n    print(w.name)\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#create-new-workspace","title":"Create new Workspace","text":"<pre><code>from bodosdk import BodoOrganizationClient\n\nmy_org = BodoOrganizationClient()\nmy_workspace = my_org.create_workspace(\n    name=\"SDK test\",\n    region='us-east-2',\n    cloud_config_id=\"a0d1242c-3091-42de-94d9-548e2ae33b73\",\n    storage_endpoint_enabled=True\n).wait_for_status(['READY'])\nassert my_workspace.id == my_org.list_workspaces(filters={\"names\": ['SDK test'], \"statuses\": ['READY']})[0].id\nmy_workspace.delete()  # remove workspace at the end\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#upgrade-workspace-infra","title":"Upgrade workspace infra","text":"<p>In some cases when you have a workspace created a while ago, you may want to re-run terraform to apply fresh changes to workspace infrastructure. You can do it as follows:</p> <pre><code>from bodosdk import BodoOrganizationClient\n\nmy_org = BodoOrganizationClient()\nmy_org.list_workspaces(filters={'ids': ['workspace_to_update1_id', 'workspace_to_update2_id']}).update_infra()\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#advanced","title":"Advanced","text":"<p>In this section we will present more examples of bodosdk usage.</p>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#workspace-created-in-existing-vpc","title":"Workspace created in existing VPC","text":"<p>Its possible to create workspace on existing infrastructure. The only requirement is that the VPC needs access to the Internet, either NAT or IGW. Bodo platform needs this to allow clusters to be authorized via an external authorization service.</p> <pre><code>from bodosdk import BodoOrganizationClient\n\nmy_org = BodoOrganizationClient()\nmy_workspace = my_org.create_workspace(\n    cloud_config_id=\"cloudConfigId\",\n    name=\"My workspace\",\n    region=\"us-east-1\",\n    storage_endpoint_enabled=True,\n    vpc_id=\"existing-vpc-id\",\n    private_subnets_ids=['subnet1', 'subnet2'],\n    public_subnets_ids=['subnet3']\n)\nmy_workspace.wait_for_status(['READY'])\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#spot-instances-and-auto-az","title":"Spot instances and Auto-AZ","text":"<p>You can create a cluster using spot instances, to reduce cost of usage. A limitation of using spot instances is that you cannot pause this type of cluster. Further cluster may be occasionally become unavailable (when spot instance is released).</p> <p>Auto-AZ is a mechanism which retries cluster creation in another Availability zone, when current availability zone does not have enough instances of desired type.</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_cluster = my_workspace.ClusterClient.create(\n    name='Spot cluster',\n    instance_type='c5.large',\n    workers_quantity=1,\n    use_spot_instance=True,\n    auto_az=True,\n)\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#accelerated-networking","title":"Accelerated networking","text":"<p>Accelerated networking is enabled by default for instances that support it.</p> <p>You can get a list of all supported instances using the <code>ClusterClient.get_instances</code> function. This returns a list of InstanceType objects. The field <code>accelerated_networking</code> tells you whether network acceleration is enabled.</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\n\naccelerated_networking_instances = [x for x in my_workspace.ClusterClient.get_instances() if x.accelerated_networking]\n\nmy_cluster = my_workspace.ClusterClient.create(\n    name='Spot cluster',\n    instance_type=accelerated_networking_instances[0].name,\n    workers_quantity=1,\n)\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#preparing-clusters-for-future-use","title":"Preparing clusters for future use","text":"<p>A cluster may be suspended in two states: <code>PAUSED</code> and <code>STOPPED</code>. Spot clusters cannot be <code>PAUSED</code>. There are 3 differences between those states: cost, start up time, error rate.</p>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#costs","title":"Costs","text":"<p><code>PAUSED</code> &gt; <code>STOPPED</code> - The<code>PAUSED</code> state incurs disk costs while the <code>STOPPED</code> state doesn't.</p>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#start-up-time","title":"Start up time","text":"<p><code>STOPPED</code> &gt; <code>PAUSED</code> - Resuming machines in the <code>PAUSED</code> state is much faster than restarting <code>STOPPED</code> clusters, as those machines are already defined in the cloud provider.</p>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#error-rate","title":"Error rate","text":"<p><code>PAUSED</code> &gt; <code>STOPPED</code> - The error rate is the likelihood of the situation where number of available instances of desired types is lower than number of requested workers.</p> <p>In <code>PAUSED</code> state, instance entities are already defined. Upon resuming a <code>PAUSED</code> cluster, since we request for the cluster resources all at once, it's more likely to run into capacity issues than in a <code>STOPPED</code> state. In a <code>STOPPED</code> state, instance creation is managed by ASG, which will try to create instances in a staggered manner.</p> <p>In the following example, we prepare a cluster for future use by pausing it once its created:</p> <pre><code>from bodosdk import BodoWorkspaceClient\nfrom bodosdk.models import ClusterFilter\n\nmy_workspace = BodoWorkspaceClient()\n\nclusters_conf = {\n    'Team A': {\n        'instance_type': 'c5.2xlarge',\n        'workers': 4,\n    },\n    'Team b': {\n        'instance_type': 'c5.xlarge',\n        'workers': 2,\n    },\n    'Team C': {\n        'instance_type': 'c5.16xlarge',\n        'workers': 2,\n    }\n}\nfor owner, conf in clusters_conf.items():\n    my_workspace.ClusterClient.create(\n        name=f\"{owner} Cluster\",\n        instance_type=conf['instance_type'],\n        workers_quantity=conf['workers'],\n        custom_tags={'owner': owner, 'purpose': 'test'}\n    )\n\nmy_workspace.ClusterClient.list(\n    filters=ClusterFilter(tags={'purpose': 'test'})\n).wait_for_status(\n    ['RUNNING', 'INITIALIZING']\n).pause().wait_for_status(['PAUSED'])\n</code></pre>"},{"location":"guides/using_bodo_platform/bodo_platform_sdk_guide/#run-a-job-using-a-cluster-as-template","title":"Run a job using a cluster as template","text":"<p>Let's imagine that you have a cluster (in any state) and you want to run job on the same specification, but you don't want to use the previously defined cluster. You can use the existing cluster as a template for the cluster to run the job:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient()\nmy_cluster = my_workspace.ClusterClient.get('existing_cluster')\ncluster_conf = my_cluster.dict()\ndel cluster_conf['uuid']\nmy_sql_job = my_workspace.JobClient.run_sql_query(sql_query=\"SELECT 1\", catalog=\"MyCatalog\", cluster=cluster_conf)\n</code></pre> <p>A new cluster will be created with the same configuration as the existing cluster, and the job will be run on it. The new cluster will be terminated as soon as the job finishes.</p> <p>See Also</p> <p>BodoSDK Reference</p>"},{"location":"guides/using_bodo_platform/clusters/","title":"Creating a Cluster","text":"<p>Clusters are the compute resources that you can use to run your Bodo code in your workspace. This guide explains how to create a cluster on the Bodo Cloud Platform. In the left bar, click on Clusters (or click on the second step in the Onboarding list). This action will take you to the Clusters page. At the top right corner, click <code>Create Cluster,</code> which opens the cluster creation form.</p> <p></p>"},{"location":"guides/using_bodo_platform/clusters/#cluster-basic-configuration","title":"Cluster Basic Configuration","text":""},{"location":"guides/using_bodo_platform/clusters/#cluster-name","title":"Cluster Name","text":"<p>Choose a name for your cluster.</p>"},{"location":"guides/using_bodo_platform/clusters/#instance-type","title":"Instance type","text":"<p>Select the type of nodes in the cluster to be created from the dropdown list. EFA will be used if the instance type supports it.</p> <p></p> <p>Note</p> <p>If the Instance type dropdown list does not populate, either the credentials are not entered properly or they are not valid. Please see how to set your AWS or Azure credentials and make sure your credentials are valid.</p>"},{"location":"guides/using_bodo_platform/clusters/#use-spot-instances","title":"Use Spot Instances","text":"<p>This option enables spot instances in the cluster. Use this option to reduce the cost of VMs.</p> <p></p> <p>Note</p> <p>However, it's important to note that selecting this option can also have some drawbacks.  For further insights, please refer to the breakdowns associated with AWS and Azure spot instances. Azure Spot, AWS Spot</p>"},{"location":"guides/using_bodo_platform/clusters/#number-of-instances","title":"Number of Instances","text":"<p>This option specifies the number of nodes in your cluster.</p>"},{"location":"guides/using_bodo_platform/clusters/#automatic-version-upgrade","title":"Automatic Version Upgrade","text":"<p>This option enables automatic Bodo version upgrades.  The cluster will automatically select the latest Bodo version during each cluster creation or restart.  If you want to use a specific Bodo version, please deselect this option and specify the version in the <code>advanced</code> tab. Typically, the three latest Bodo Releases are available.</p> <p></p>"},{"location":"guides/using_bodo_platform/clusters/#cluster-auto-pause-auto-stop","title":"Cluster Auto Pause / Auto Stop","text":"<p>Cluster Auto Pause is the period of inactivity after which the platform will automatically pause the cluster.  Activity is based on attached notebooks and jobs.</p> <p>Cluster Auto Stop is the period of inactivity after which the platform will automatically stop the cluster.  This will remove VMs with storage but leave a reference in the Bodo Platform, allowing the cluster to be restarted.</p> <p>Note</p> <p>In both cases, activity is determined by attached notebooks and jobs.  If you don\u2019t plan to attach a notebook or job (and will use SSH instead), it\u2019s recommended to set this to \u2018Never.\u2019  Otherwise, the cluster will be stopped or paused after the specified time</p> <p></p>"},{"location":"guides/using_bodo_platform/clusters/#cluster-tags","title":"Cluster Tags","text":"<p>This option allows the user to specify additional tags that can be used to find resources in your cloud.</p> <p></p>"},{"location":"guides/using_bodo_platform/clusters/#cluster-advanced-configuration","title":"Cluster Advanced Configuration","text":"<p>Additionally, you can specify the following advanced configuration options for cluster.</p> <p></p>"},{"location":"guides/using_bodo_platform/clusters/#availability-zone","title":"Availability Zone","text":"<p> On AWS only</p> <p>Select the availability zone where you want to deploy your cluster. By default, this is set to <code>Auto Select</code>.</p>"},{"location":"guides/using_bodo_platform/clusters/#instance-role","title":"Instance Role","text":"<p> On AWS only </p> <p>Is the instance role that should be attached to the cluster instances.  You can define these in Settings. By default, a new role will be created and attached.</p>"},{"location":"guides/using_bodo_platform/clusters/#bodo-version","title":"Bodo Version","text":"<p>This option specifies the Bodo version to be installed on your cluster.</p>"},{"location":"guides/using_bodo_platform/clusters/#cluster-description","title":"Cluster description","text":"<p>Description for the cluster.</p>"},{"location":"guides/using_bodo_platform/clusters/#cluster-instance-type-and-size-recommendations","title":"Cluster Instance Type and Size Recommendations","text":"<p>If you were previously running a query on a Snowflake Warehouse this table provides a starting point for what instance type and size you can use to run the query using Bodo. Since this is only a starting point you should experiment to find the best configuration for your specific use case.</p> Snowflake Warehouse Size Bodo Cluster Spec 2X-Small 1 x i4i.xlarge X-Small 1 x i4i.2xlarge Small 1 x i4i.4xlarge Medium 1 x i4i.8xlarge Large 1 x i4i.16xlarge X-Large 1 x i4i.32xlarge 2X-Large 2 x i4i.32xlarge 3X-Large 4 x i4i.32xlarge 4X-Large 8 x i4i.32xlarge 5X-Large 16 x i4i.32xlarge 6X-Large 32 x i4i.32xlarge"},{"location":"guides/using_bodo_platform/clusters/#monitoring-the-cluster-creation-status","title":"Monitoring the Cluster Creation Status","text":"<p>Once you have filled in all the required click on <code>CREATE</code>. You will see that a new task for creating the cluster has been created. The status is updated to INPROGRESS when the task starts executing and cluster creation is in progress.</p> <p></p> <p>Once the cluster is successfully created and ready to use, its status will be updated to RUNNING.  You can click on the Details drop-down to view more information.</p> <p></p>"},{"location":"guides/using_bodo_platform/connect/","title":"Connecting to a Cluster","text":"<p>We recommend interacting with clusters primarily through Jupyter Notebooks and Jobs. However, it may be necessary to connect directly to a cluster in some cases. In that case, you can connect through a notebook terminal.</p>"},{"location":"guides/using_bodo_platform/connect/#connecting-with-a-notebook-terminal","title":"Connecting with a Notebook Terminal","text":"<p>If your cluster has more than one node , you can connect to any of the cluster nodes by running the following command in the terminal:</p> <pre><code>ssh &lt;NODE-IP&gt;\n</code></pre> <p></p> <p>Through this terminal, you can access the <code>/bodofs</code> folder, which is shared by all the instances in the cluster and the Notebook instance. Verify your connection to interact directly with your cluster.</p>"},{"location":"guides/using_bodo_platform/connect/#verify_your_connection","title":"Verify your Connection","text":"<p>Once you have connected to a node in your cluster, you should verify that you can run operations across all the instances in the cluster.</p> <ol> <li>Verify the path to the hostfile for your cluster. You can find it by     running:     <pre><code>ls -la /home/bodo/hostfile\n</code></pre></li> <li> <p>Check that you can run a command across you cluster. To do this,     run:</p> <pre><code>mpiexec -n &lt;TOTAL_CORE_COUNT&gt; -f /home/bodo/hostfile hostname\n</code></pre> <p>This will print one line per each core in the cluster, with one unique hostname per cluster node.</p> <p>Your cluster's <code>TOTAL_CORE_COUNT</code> is usually half the number of vCPUs on each instance times the number of instances in your cluster. For example, if you have a 4 instance cluster of c5.4xlarge, then your <code>TOTAL_CORE_COUNT</code> is 32.</p> </li> <li> <p>Verify that you can run a python command across your cluster. For     example, run:</p> <pre><code>mpiexec -n &lt;TOTAL_CORE_COUNT&gt; -f /home/bodo/hostfile python --version\n</code></pre> </li> </ol> <p>If all commands succeed, you should be able to execute workloads across your cluster. You can place scripts and data that are shared across cluster nodes in <code>/bodofs</code>.</p>"},{"location":"guides/using_bodo_platform/customer_managed_vpc/","title":"Configuring Customer Managed VPC","text":""},{"location":"guides/using_bodo_platform/customer_managed_vpc/#overview","title":"Overview","text":"<p>In a Bodo Managed VPC, Bodo creates and manages the VPC in your AWS account.  You can optionally create your Bodo workspaces in your own VPC, a feature known as customer-managed VPC.  You can use a customer-managed VPC to have more control over your network configurations to comply with specific cloud security and governance standards your organization may require. Your workspace must use a customer-managed VPC to configure it to use AWS PrivateLink for any type of connection.</p> <p>A customer-managed VPC is good solution if you have:</p> <ul> <li> <p>Security policies that prevent PaaS providers from creating VPCs in your own AWS account.</p> </li> <li> <p>An approval process to create a new VPC, in which the VPC is configured and secured in a well-documented way by internal information security or cloud engineering teams.</p> </li> </ul>"},{"location":"guides/using_bodo_platform/customer_managed_vpc/#creating-a-workspace-with-customer-managed-vpc","title":"Creating a Workspace with Customer Managed VPC","text":"<p>To create a workspace, go to the \"Workspaces\" section in the sidebar and click \"Create Workspace.\"</p> <p>Important</p> <p>Make sure you have a cloud configuration created before creating a workspace. Refer to the Setting AWS Credentials.</p> <p>In the creation form: </p> <ol> <li> <p>Enter the name of the workspace</p> </li> <li> <p>Select the Cloud Configuration from the dropdown</p> </li> <li> <p>Select region to deploy the resources </p> </li> <li> <p>In the Optional section, Select the Network Configuration from the dropdown</p> </li> <li> <p>Click on \"Create Workspace\" to create the workspace.</p> </li> </ol> <p>Important</p> <p>Please refer to the following link for creating a network configuration: Creating Network Configuration</p>"},{"location":"guides/using_bodo_platform/instance_role/","title":"Using your own Instance Role for a Cluster","text":"<p> Supported on AWS only\u00b7 In cases where you want to access additional AWS resources from Bodo clusters e.g., S3 buckets, you can create an IAM Role in your AWS account and then register it as an Instance Role on the Bodo Platform, which will allow you to access those resources from Bodo clusters without using AWS keys.</p> <p></p> <p>Note that, by default, Bodo creates an IAM role with the necessary policies for each cluster. When you register your own role with the Bodo Platform, it will automatically attach the other required policies to this role.</p> <p>Here, we walk through setting up an IAM Role in AWS and then registering it as an Instance Role on the Bodo Platform. For this example, we will be creating a role with access to an S3 bucket in your AWS account:</p> <p>Step 1: Create an AWS IAM Role on the AWS Management Console: 1. Go to the IAM service.</p> <p></p> <ol> <li>In the left sidebar click on Roles.</li> </ol> <p></p> <ol> <li>Click on the button <code>Create role</code>, then select:</li> <li>Trusted entity type: AWS service</li> <li>Common use cases: EC2</li> </ol> <p></p> <ol> <li>Click next, and then create a new policy that will be attached to this role:</li> <li> <p>json policy: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;private-s3-bucket-name&gt;\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;private-s3-bucket-name&gt;/*\"\n            ]\n        }\n    ]\n}\n</code></pre></p> </li> <li> <p>Go back to Create role, refresh the list of policies, and add the policy that you created.</p> </li> <li>Click Next, then in the Role name field, type a role name and click Create role.</li> <li>Copy the Role ARN from the role summary.</li> </ol> <p></p> <p>Step 2: Register your AWS IAM Role on the Bodo Platform as a new Instance Role:</p> <ol> <li>Click on the CREATE INSTANCE ROLE button and in the creation form, fill the following fields:</li> <li>Name: Name for the Instance Role </li> <li>Role ARN: AWS Role ARN from Step 1</li> <li>Description: Short description for Instance Role</li> </ol> <p></p> <ol> <li>Click on the Create button.</li> </ol> <p>The Instance Role will now be registered on the Platform. It can have one of two status-es: * Active: Instance Role is ready to use * Failed: Something went wrong while registering the Instance Role and it cannot be used. Some possible problems could be:    * The Platform wasn't able to find the specified role.    * The Platform was not able to attach additional Bodo polices that are required for normal cluster operations.</p>"},{"location":"guides/using_bodo_platform/network_configuration/","title":"Creating Network Configuration","text":""},{"location":"guides/using_bodo_platform/network_configuration/#overview","title":"Overview","text":"<p>In a Bodo Managed Workspace, Bodo creates and manages the VPC in your AWS account. You can optionally create your Bodo workspaces in your own VPC, a feature known as customer-managed VPC. You can use a customer-managed VPC to have more control over your network configurations to comply with specific cloud security and governance standards your organization may require. In order to configure your workspace to use AWS PrivateLink for any type of connection, it must use a customer managed VPC. </p> <p>To create a Customer Managed VPC or Configure PrivateLink, you must create a network configuration in the Bodo Platform.</p>"},{"location":"guides/using_bodo_platform/network_configuration/#network-configuration-requirements","title":"Network Configuration Requirements","text":""},{"location":"guides/using_bodo_platform/network_configuration/#vpc-requirements","title":"VPC Requirements","text":"<p>A Customer Managed VPC must meet the following requirements to be able to deploy Bodo workspaces</p>"},{"location":"guides/using_bodo_platform/network_configuration/#vpc-region","title":"VPC Region","text":"<p>Currently, Bodo supports us-east-1, us-east-2, us-west-1, us-west-2 and eu-west-1 regions for customer-managed VPCs.</p>"},{"location":"guides/using_bodo_platform/network_configuration/#vpc-sizing","title":"VPC sizing","text":"<p>A single VPC can be shared with multiple workspaces in the same AWS account.</p> <p>However, Bodo recommends having unique subnets and security groups for each workspace to avoid any potential conflicts. Make sure to size your VPC and subnets accordingly. Bodo assigns a single IP address for each node. The total number of instances for each subnet should not exceed the number of available IP addresses in the subnet.</p>"},{"location":"guides/using_bodo_platform/network_configuration/#vpc-ip-address-ranges","title":"VPC IP address Ranges","text":"<p>Bodo doesn't limit netmasks for the workspace VPC but each workspace subnet must have netmask between <code>/17</code> and <code>/26</code></p> <p>Important</p> <p>If you have configured secondary CIDR blocks for your VPC,  make sure that the subnets for the Bodo workspace are configured with the same VPC CIDR block.</p>"},{"location":"guides/using_bodo_platform/network_configuration/#dns","title":"DNS","text":"<p>The VPC must have DNS resolution and DNS hostnames enabled.</p>"},{"location":"guides/using_bodo_platform/network_configuration/#subnets","title":"Subnets","text":"<p>Bodo must have access to at least two subnets for each workspace, with each subnet in a different Availability Zone (AZ). Your network configuration could have more than one subnet per AZ, but Bodo requires at least two subnets in different AZs.</p>"},{"location":"guides/using_bodo_platform/network_configuration/#subnet-requirements","title":"Subnet Requirements","text":"<ul> <li> <p>Each subnet must have a netmask between <code>/17</code> and <code>/26</code>.</p> </li> <li> <p>Subnets must be private</p> </li> <li> <p>Subnets must have a route to the internet gateway or NAT gateway for internet access.</p> </li> <li> <p>The NAT gateway must be in separate subnet called Public Subnet that routes <code>0.0.0.0/0</code> traffic to an internet gateway.</p> </li> <li> <p>The route table for the workspace subnets must have <code>0.0.0.0/0</code> traffic that targets the appropriate network device.</p> </li> <li> <p><code>0.0.0.0/0</code> traffic must be routed to the NAT gateway or your own managed NAT device or proxy appliance.</p> </li> </ul> <p>Important</p> <p>When using PrivateLink, you don't need to have a route to the internet gateway or NAT gateway for internet access.</p>"},{"location":"guides/using_bodo_platform/network_configuration/#security-groups","title":"Security Groups","text":"<p>A Bodo workspace must have access to at least one AWS security group and no more than 5 security groups. You can reuse the same security group for multiple workspaces, but it is recommended to have a separate security group for each workspace.</p> <p>Security groups must have the following rules:</p> <p>Egress(Outbound):</p> <ul> <li> <p>Allow all TCP access to the workspace security group (for internal traffic)</p> </li> <li> <p>Allow TCP access to <code>0.0.0.0/0</code> for these ports</p> <ul> <li> <p>443: For Bodo Infrastructure access, cloud data sources etc.</p> </li> <li> <p>2049: For EFS access to the shared file system</p> </li> <li> <p>80(optional): Needed for Snowflake OCSP checks (Required for Snowflake customers)</p> </li> </ul> </li> </ul> <p>Ingress(Inbound):</p> <ul> <li>Allow all TCP access to the workspace security group</li> </ul> <p>Important</p> <p>Workspaces must have outbound access from the VPC to the public network.</p>"},{"location":"guides/using_bodo_platform/network_configuration/#subnet-network-acls","title":"Subnet Network ACLs","text":"<p>Subnet Network ACLs must not deny any traffic ingress or egress the workspace security group. If defined, the ACLs must have the following rules:</p> <p>Egress(Outbound):</p> <ul> <li> <p>Allow all TCP access to the workspace security group (for internal traffic)</p> </li> <li> <p>Allow TCP access to 0.0.0.0/0 for these ports</p> <ul> <li> <p>443: For Bodo Infrastructure access, cloud data sources etc.</p> </li> <li> <p>2049: For EFS access to the shared file system</p> </li> <li> <p>80(optional): Needed for Snowflake OCSP checks (Required for Snowflake customers)</p> </li> </ul> </li> </ul> <p>Ingress(Inbound):</p> <ul> <li>Allow all TCP access to the workspace security group</li> </ul>"},{"location":"guides/using_bodo_platform/network_configuration/#creating-network-configuration","title":"Creating Network Configuration","text":"<p>To create a network configuration, go to the \"Configurations\" section in the sidebar and click on \"Network Configurations\" tab  and click on \"Create Network Configuration\" on top-right corner.</p> <p>In the network configuration form:</p> <ol> <li>Enter the name of the network configuration</li> <li>Select the region</li> <li>Enter the VPC ID</li> <li>Enter the Subnet IDs (Minimum 2)</li> <li>Enter the Security Group IDs (Minimum 1)</li> <li>Select Enabled AWS PrivateLink if you want to enable PrivateLink for the workspace</li> <li>Enter the VPC endpoint IDs for the services you want to connect to using PrivateLink (S3, SSM, Bodo Cluster endpoints are mandatory for PrivateLink)</li> <li> <p>Click on Create Network Configuration to create the network configuration.</p> <p></p> </li> </ol>"},{"location":"guides/using_bodo_platform/notebooks/","title":"Using Notebooks","text":"<p>Jupyter servers act as your interface to your shared file system and compute clusters. Users can execute code from their notebooks on the compute cluster from the Jupyter interface. A Jupyter server is automatically provisioned when you first enter the workspace.</p> <p></p> <p>You can update/restart Jupyter servers in the \"Workspace Settings.\"</p> <p></p>"},{"location":"guides/using_bodo_platform/notebooks/#attaching_notebook_to_cluster","title":"Attaching a Notebook to a Cluster","text":"<p>To attach a notebook to a cluster, select the cluster from the drop-down in the top-left.</p> <p></p> <p>To execute your code across the attached cluster, select the Parallel Python cell type from the cell type selector dropdown.</p> <p></p> <p>To run a SQL query, first select the catalog you want to use, then select the SQL cell type from the cell type selector dropdown.  For more information on SQL catalogs, refer to the SQL Catalogs usage guide.</p> <p></p> <p>Note</p> <p>Execution is only allowed when the notebook is attached to a cluster. If you execute a cell without a cluster attached, the following warning will be shown:</p> <p></p>"},{"location":"guides/using_bodo_platform/organization/","title":"Organization Basics","text":"<p>This page describes the fundamental components of the Bodo Cloud Platform and how they are organized. The platform is designed to be a multi-tenant system, which has the entities described below.</p>"},{"location":"guides/using_bodo_platform/organization/#organizations","title":"Organizations","text":"<p>Organizations on the Bodo Cloud Platform are tenants for billing and cloud resource management purposes. An organization can have multiple workspaces and cloud configurations, and users can be part of multiple organizations.</p> <p></p>"},{"location":"guides/using_bodo_platform/organization/#cloud-configurations","title":"Cloud-Configurations","text":"<p>A cloud-configuration is an entity used to store information about your AWS or Azure account. It consists of:</p> <ol> <li>Details regarding the trust relationship between the platform and your cloud provider account.    For AWS accounts, this is done through a cross-account IAM role.    For Azure account, this is done through a service principal (scoped to a specific resource group)    for the Bodo Platform application.    This allows the platform to provision and manage cloud resources in your account.</li> <li>Details regarding metadata storage. The platform needs to store    specific metadata to carry out its functions, such as the state of your various cloud deployments, logs, etc.    On AWS, this data is stored in an S3 bucket and a DynamoDB table.    On Azure, this data is stored in a storage container.</li> </ol> <p></p>"},{"location":"guides/using_bodo_platform/organization/#workspaces","title":"Workspaces","text":"<p>A workspace on the Bodo Cloud Platform consists of:</p> <ol> <li>A shared filesystem where you can collaborate with your team on your projects.</li> <li>Networking infrastructure such as virtual networks, security groups, and subnets in which    your compute clusters and Jupyter servers will be securely deployed.</li> </ol> <p>A workspace is tied to a particular cloud configuration and has its own user management, i.e., you can have different subsets of users with different sets of roles and permissions in different workspaces within the same organization.</p> <p>Important</p> <p>If a user who is not part of the organization is invited to a workspace in the organization, they are automatically added to the organization with minimal permissions.</p> <p></p> <p>To create a workspace, go to the \"Workspaces\" section in the sidebar and click \"Create Workspace.\" In the creation form, enter the name of the workspace, select the cloud configuration to use for provisioning it and the region where it should be deployed, and click on \"Create Workspace.\"</p> <p></p> <p>This will start the workspace deployment. When the workspace is in the \"READY\" state, click on the button next to it to enter it.</p> <p></p>"},{"location":"guides/using_bodo_platform/packages/","title":"Managing Packages on the cluster using Jupyter magics - Conda and Pip","text":"<p>We recommend all packages to be installed using Conda as that is what we use in our environments. Any conda command can be run in parallel on all the nodes of your cluster using <code>%pconda</code>. To install a new package on all the nodes of your cluster you can use <code>%pconda install</code>. All conda install arguments work as expected, e.g., <code>-c conda-forge</code> to set the channel.</p> <pre><code>%pconda install -c conda-forge &lt;PACKAGE_NAME&gt;\n</code></pre> <p>To learn more about the packages installed on the cluster nodes <code>%pconda list</code>. <pre><code>%pconda list\n</code></pre></p> <p>To remove a conda package on all your cluster nodes, use <code>%pconda remove</code>.</p> <pre><code>%pconda remove &lt;PACKAGE_NAME&gt;\n</code></pre> <p></p> <p>Any pip command can be run in parallel on all the nodes of your cluster using <code>%ppip</code>.</p> <p>Example: <pre><code>%ppip install &lt;PACKAGE_NAME&gt;\n</code></pre></p> <p>To learn about the installed packages, you can use <code>%ppip show</code> to get the details of the package.</p> <pre><code>%ppip show &lt;PACKAGE_NAME&gt;\n</code></pre> <p>To remove the same package on all the nodes of your cluster, use <code>%ppip uninstall</code>.</p> <pre><code>%ppip uninstall &lt;PACKAGE_NAME&gt; -y\n</code></pre> <p></p>"},{"location":"guides/using_bodo_platform/shell/","title":"Running shell commands on the cluster using Jupyter magics","text":"<p>Shell commands can be run in parallel on the nodes of your cluster using <code>%psh &lt;shell_command&gt;</code>.</p> <pre><code>%psh echo \"Hello World\"\n</code></pre> <p></p>"},{"location":"guides/using_bodo_platform/troubleshooting/","title":"Troubleshooting Guide","text":"<p>Here are solutions to potential issues you may encounter while using the Bodo Cloud Platform. </p>"},{"location":"guides/using_bodo_platform/troubleshooting/#notebook-403-forbidden","title":"Notebook: 403 Forbidden","text":"<p>This error is typically caused by an incorrect token, often occurring when a token is cached from accessing a different workspace.  There are two recommended workarounds:</p> <p>First, attempt to resolve the issue by navigating back to the organization using the left menu <code>Back to Organization</code>.  Once there, re-enter the workspace.</p> <p>If the problem persists, try clearing your web browser's cache and then logging in to Bodo Platform once more.</p>"},{"location":"guides/using_bodo_platform/troubleshooting/#notebook-404-not-found-502-bad-gateway","title":"Notebook: 404 Not Found / 502 Bad Gateway","text":"<p>If for some reason <code>My Notebook</code> is displaying error code 404 / 502, you should try to update and then restart the notebook server using left menu. If this does not help, please contact us for further assistance.</p>"},{"location":"guides/using_bodo_platform/troubleshooting/#notebook-file-save-error","title":"Notebook: File Save Error","text":"<p>If you get a file save error with message <code>invalid response: 413</code> make sure your notebook (<code>.ipynb</code>) file is less than 16MB in size. Bodo Platform does not support notebook files larger than 16MB in size. To reduce file size don't print large sections of text and clear output cells by clicking <code>Edit</code> &gt; <code>Clear All Outputs</code> in the notebook interface.</p>"},{"location":"guides/using_bodo_platform/troubleshooting/#account-locked-error","title":"Account Locked Error","text":"<p>When you login to the platform, if you get an account locked error with message <code>User is locked out. To unlock user, please contact your administrators</code>, this means that your account has been dormant (no login in more than 90 days). Please contact us to unlock your account.</p>"},{"location":"help_and_reference/eula/","title":"End User License Agreement","text":"<p>THIS ONLINE END-USER LICENSE AGREEMENT (\"AGREEMENT\") IS A BINDING LEGAL CONTRACT BETWEEN YOU (THE USER) AND BODO INC. (\"WE\", \"US\", OR \"BODO\"). BY DOWNLOADING, INSTALLING, ACCESSING OR USING THE SOFTWARE, SERVICES, AND ANY OTHER MATERIALS MADE AVAILABLE BY BODO ON THIS SITE OR IN ANY OTHER FORMAT (COLLECTIVELY, THE \"SERVICES\"), YOU (A) AGREE TO BE BOUND BY THIS AGREEMENT; (B) ACKNOWLEDGE AND AGREE YOU HAVE INDEPENDENTLY EVALUATED THE DESIRABILITY OF USING THE SERVICES AND ARE NOT RELYING ON ANY REPRESENTATION, GUARANTEE, OR STATEMENT OTHER THAN AS EXPRESSLY PROVIDED IN THIS AGREEMENT; AND (C) REPRESENT YOU ARE LAWFULLY ABLE TO ENTER INTO CONTRACTS AND ARE OF THE LEGAL AGE OF MAJORITY IN THE JURISDICTION IN WHICH YOU RESIDE (AT LEAST EIGHTEEN YEARS OF AGE IN MANY COUNTRIES/JURISDICTIONS). IF THIS AGREEMENT IS BEING AGREED TO BY A COMPANY OR OTHER LEGAL ENTITY, THEN THE PERSON AGREEING TO THIS AGREEMENT ON BEHALF OF THAT COMPANY OR ENTITY REPRESENTS AND WARRANTS THAT HE OR SHE IS AUTHORIZED AND LAWFULLY ABLE TO BIND THAT COMPANY OR ENTITY TO THIS AGREEMENT.</p> <p>IF YOU DO NOT AGREE TO THIS AGREEMENT, YOU MAY NOT USE THE SERVICES.</p> <ol> <li> <p>Services. Subject to the terms and conditions of this Agreement     and, if applicable, your payment of all relevant fees, we grant you     a non-exclusive, non-transferable, limited license to access and use     our software services, content, and other materials provided by Bodo     or its third-party vendors through this Web site or in other format     (the \"Services\") for your internal use only. Certain     third-party services may have their own terms and conditions, which     will be presented to you in your use of the Services. Your use of     those third-party services will indicate your acceptance of the     additional terms and conditions. In connection with the Services, we     may afford you the ability to interface and interoperate with     certain third-party software and to upload data from that software.     This functionality is dependent on the operation of the third-party     software and is provided on an entirely as-is basis. We may change,     modify, or discontinue all or any portion of the Services at any     time, without prior notice.</p> </li> <li> <p>Restrictions. You may only use the Services as described in the     documentation we make generally available from time to time to our     customers for use of the Services (the \"Documentation\"). Any     breach of this Agreement by your employees or agents will constitute     a breach by you. Except as expressly authorized by this Agreement,     you will not (and will not allow any third-party to): (i) permit any     third-party to access and/or use the Services; (ii) decompile,     disassemble, or reverse engineer the Services, or attempt to derive     the source code, underlying ideas, algorithm or structure of     software provided to you in object code form; (iii) use the Services     or any of our Confidential Information (as defined below) to develop     a competing product or service; (iv) sell, transfer, assign,     distribute, rent, loan, lease, sublicense or otherwise make     available the software associated with the Services or its     functionality to third parties; (v) modify, translate or otherwise     create any derivative works of any software used and made available     by Bodo in connection with the Services; (vi) provide, lease, lend,     use for timesharing or service bureau purposes or otherwise use or     allow others to use the Services for the benefit of any third     party; (vii) use the Services, or allow the transfer, transmission,     export, or re-export of the Services, including by way of a \"deemed     export,\" in violation of any export control laws or regulations     administered by the U.S. Commerce Department or any other government     agency; or (viii) remove any copyright, trademark, proprietary     rights, disclaimer or warning notice included on or embedded in any     part of the Services or Documentation. Nothing in this Agreement     shall be construed to give you a right to use, or otherwise obtain     access to, any source code from which the software used in     connection with the Services or any portion thereof is compiled or     interpreted. Under no circumstances, will we be liable or     responsible for any use, or any results obtained by the use, of the     Services in conjunction with any other software or third-party     products. All such use will be at your sole risk.</p> </li> <li> <p>Proprietary Rights. You acknowledge that all Services are     protected by intellectual property rights of Bodo and its     vendors/licensors and that you have no rights to transfer or     reproduce the Services or prepare any derivative works with respect     to, or disclose Confidential Information pertaining to, the     Services. Under no circumstances will you be deemed to receive title     to any portion of any Services, title to which at all times will     vest exclusively in us and our licensors. This is not a \"work made     for hire\" agreement, as that term is defined in Section 101 of     Title 17 of the United States Code (\"the Copyright Act\"). You     will preserve all Services from any liens, encumbrances, and claims     of any individual or entity. You will not use any of our information     or data to contest the validity of any of our intellectual property     or our licensors. Any such use of our information and data will     constitute a material, non-curable breach of this Agreement. To the     extent you provide us with any content (e.g., graphics, logos,     artwork, text, data) for use in connection with the Services     (collectively, the \"Customer Content\"), you grant us a     non-exclusive, world-wide, royalty-free license to use the Customer     Content for purposes of performing this Agreement. You are     responsible for obtaining all rights, permissions, licenses, and     consents required to furnish the Customer Content to us for use as     described above. You are also responsible for preserving and making     adequate backups of the Customer Content and will not rely on us to     preserve or make adequate backups of data used in connection with     the Services, or to maintain a record of your usage of any part or     all of the Services. Your rights in and to the Services and related     software are limited to those expressly granted under this Agreement     and no other licenses are granted whether by implication, estoppel     or otherwise. Bodo reserves all rights, title and interest in and to     the Services and related software not expressly granted under this     Agreement.</p> </li> <li> <p>Third Party Software. The Services may come bundled with, or     otherwise include or be distributed with, third party software     licensed by a Bodo supplier and/or open source software provided     under an open source license (Open Source Software) (collectively,     \"Third Party Software\"). Notwithstanding anything to the     contrary herein, Third Party Software is licensed to you subject to     the terms and conditions of the software license agreement     accompanying such Third Party Software whether in the form of a     discrete agreement, click-through license, or electronic license     terms accepted at the time of installation and any additional terms     or agreements provided by the third party licensor (\"Third Party     License Terms\"). Use of the Third Party Software by you shall be     governed by such Third Party License Terms, or if no Third Party     License Terms apply, then the Third Party Software is provided to     you as-is, as available, for use in or with the Services and not     otherwise used separately. Copyright to Third Party Software is held     by the copyright holders indicated in the Third Party License Terms.</p> </li> <li> <p>Feedback. You may provide us with suggestions, comments or other     feedback (collectively, \"Feedback\") with respect to our     products and services, including the Services. Feedback is voluntary     and we are not required to hold it in confidence. We may use     Feedback for any purpose without obligation of any kind. To the     extent a license is required under your intellectual property rights     to make use of the Feedback, you grant us an irrevocable,     non-exclusive, perpetual, royalty-free license to use the Feedback     in connection with our business, products, and services, including     the enhancement of the Services.</p> </li> <li> <p>Aggregated Data. You grant us a non-exclusive, perpetual,     irrevocable, fully-paid-up, royalty free license to use data derived     from your use of the Services (the \"Aggregated Data\") for our     business purposes, including the provision of products and services     to our customers; provided the Aggregated Data is combined with     similar data from our other customers. \"Aggregated Data\" does     not include (directly or by inference) any information identifying     you or any identifiable individual. You further grant us the right     to (i) use the Aggregated Data in any aggregate or statistical     products or reports, (ii) transfer and/or disclose the Aggregated     Data upon a sale of our company or its assets or other form of     reorganization, (iii) disclose Aggregated Data in a summary report     that does not show, display or indicate customer specific or     customer identifying information, (iv) provide Aggregated Data to a     third party service provider, for analytical purposes, and (v) use     the Aggregated Data (without personally identifiable information) to     compare with other organizations within the same industry or group.     The Aggregated Data will not be considered your Confidential     Information.</p> </li> <li> <p>Fees. You will promptly pay Bodo all applicable fees and, as     described below, taxes associated with the Services. Except as     expressly provided otherwise in this Agreement, all fees (if any)     are non-refundable. Payments not made within such time period will     be subject to late charges equal to the lesser of (i) one and     one-half percent (1.5%) per month of the overdue amount or (ii) the     maximum amount permitted under applicable law. You are responsible     for paying all personal property, sales, use and other taxes     (excluding taxes based upon our net income) and license and     registration fees and other assessments or charges levied or imposed     by any governmental body or agency as a result of the execution or     performance of this Agreement, including your receipt of the     Services. On notice of not less than sixty (60) days, we may, in our     discretion, adjust any or all fees for the Services. You may     terminate this Agreement on written notice to us within thirty (30)     days of its receipt of our notice to adjust the fees; provided,     however, that if you do not object to the adjustment in writing     within the foregoing thirty (30) day period then you will be deemed     to have agreed to the adjustment.</p> </li> <li> <p>Your Warranties. You represent and warrant that (i) you have     full power, capacity, and authority to enter into this Agreement and     to grant the license in Section 4 (Proprietary Rights); and (ii)     your use of the Services will be in compliance with all applicable     local, state, and federal laws and regulations.</p> </li> <li> <p>Indemnification. You will defend and indemnify Bodo and hold it     and its affiliates, officers, directors, employees, and agents     harmless from any and all claims, actions, proceedings, losses,     deficiencies, damages, liabilities, costs, and expenses (including     but not limited to reasonable attorneys' fees and all related costs     and expenses) incurred by them as a result of any claim, judgment,     or adjudication related to or arising from any or all of the     following: (i) your use of the Services; (ii) breach of any of your     obligations, representations, or warranties in this Agreement;     or (iii) your failure to comply with applicable laws and     regulations.</p> </li> <li> <p>Beta Services. We may designate certain new functionality or     services to be made available in connection with the Services as     \"Beta Services.\" The Beta Services will not be ready for use     in a production environment. Because they will be at an early stage     of development, operation and use of the Beta Services may be     unpredictable and lead to erroneous results. You acknowledge and     agree that: (i) the Beta Services will be experimental and will not     have been fully tested; (ii) the Beta Services may not meet your     requirements; (iii) the use or operation of the Beta Services may     not be uninterrupted or error free; and (iv) your use of the Beta     Services will be for purposes of evaluating and testing the new     functionality and services and providing feedback to us. Your use of     the Beta Services will be subject to all of the terms and conditions     of this Agreement relating to the Services. You agree to promptly     report any errors, defects, or other deficiencies in the Beta     Services to us. NOTWITHSTANDING ANY OTHER PROVISION OF THIS     AGREEMENT, ALL BETA SERVICES ARE PROVIDED \"AS-IS\" AND     \"AS-AVAILABLE,\" WITHOUT WARRANTIES OF ANY KIND. You waive any and     all claims, now known or later discovered, that you may have against     us and our suppliers and licensors arising out of the Beta Services.</p> </li> <li> <p>Suspension or Termination of Services and Removal of Customer     Content. We may, in our sole discretion, suspend your access to     the Services for any of the following reasons: (i) to prevent     disruption of or damages to, or degradation of, the Services and our     systems; (ii) to comply with any law, regulation, court order, or     other governmental request; (iii) to otherwise protect us from     potential legal liability; (iv) to remove Customer Content that is     illegal, offensive, or otherwise inappropriate, in our sole     discretion, or (iv) in the event an invoice remains unpaid for more     than forty-five (45) or more days from the invoice date. We will     restore access to the Services as soon as the event giving rise to     suspension has been resolved. This Section will not be construed as     imposing any obligation or duty on us to monitor use of the     Services.</p> </li> <li> <p>Confidentiality. 12.1 \"Confidential Information\" means all     information or material which (i) gives a party some competitive     business advantage or the opportunity of obtaining such advantage or     the disclosure of which could be detrimental to the interests of     that party; or (ii) which from all the relevant circumstances should     reasonably be assumed to be confidential and proprietary. Each     party's Confidential Information will remain the sole and exclusive     property of that party. Confidential Information includes, but is     not limited to, the Services. Neither party will have any obligation     with respect to confidential information which: (i) is or becomes     generally known to the public by any means other than a breach of     the obligations of a receiving party; (ii) was previously known to     the receiving party or rightly received by the receiving party from     a third party; (iii) is independently developed by the receiving     party; or (iv) subject to disclosure under court order or other     lawful process.</p> <p>12.2 Treatment of Confidential Information. Each party recognizes the importance of the other party's Confidential Information. In particular, each party recognizes and agrees that the Confidential Information of the other is critical to their respective businesses and that neither party would enter into this Agreement without assurance that the information will be protected as provided in this Section 12 and elsewhere in this Agreement. Accordingly, each party agrees as follows:</p> <p>(a) Each party will hold any and all Confidential Information it     obtains in strictest confidence and will use and permit use of     Confidential Information solely as permitted under this     Agreement; and (b) Each party may disclose or provide access to its responsible     employees and agents or as otherwise permitted under this     Agreement, and may make copies, of Confidential Information only     to the extent permitted under this Agreement.</p> <p>12.3 Non-Exclusive Equitable Remedy. Each party acknowledges and agrees that due to the unique nature of the Confidential Information there can be no adequate remedy at law for any breach of its obligations hereunder, and therefore, that upon any such breach or any threat thereof, each party will be entitled to appropriate equitable relief from a court of competent jurisdiction in addition to whatever remedies either of them might have at law or equity.</p> <p>12.4 You agree not to use any Confidential Information of Bodo, and shall restrict your affiliates and sublicensees from using the Confidential Information of Bodo, for purposes of challenging the validity of such Confidential Information, or Bodo's ability to use and exploit such Confidential Information.</p> </li> <li> <p>Limited Warranty; Exclusive Remedy. During the Term, Bodo     warrants the Services will materially comply with the requirements     of this Agreement and Documentation. In the event of a breach of the     foregoing warranty, Bodo's sole and exclusive liability and your     sole and exclusive remedy will be to use reasonable efforts to     correct the non-conformity. In the event Bodo is unable through     reasonable efforts to correct the defective Service, you may elect     to terminate this Agreement and, if applicable, receive a prorated     refund of any pre-paid, unused recurring fees.</p> </li> <li> <p>Disclaimer of Warranties. EXCEPT AS PROVIDED IN SECTION 13     (LIMITED WARRANTY), THE SERVICES ARE PROVIDED \"AS IS\" AND     \"AS-AVAILABLE,\" WITH ALL FAULTS, AND WITHOUT WARRANTY OF ANY KIND.     BODO AND ITS VENDORS AND LICENSORS DISCLAIM ALL OTHER WARRANTIES,     EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED     WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE,     QUIET ENJOYMENT, QUALITY OF INFORMATION, OR TITLE/NON-INFRINGEMENT     AND ALL SUCH WARRANTIES ARE HEREBY SPECIFICALLY DISCLAIMED. YOU     EXPRESSLY AGREE AND ACKNOWLEDGE THAT USE OF SERVICES, IS AT YOUR     SOLE RISK. NO ORAL OR WRITTEN INFORMATION OR ADVICE GIVEN BY BODO OR     ITS AUTHORIZED REPRESENTATIVES WILL CREATE A WARRANTY OR IN ANY WAY     INCREASE THE SCOPE OF BODO'S OBLIGATIONS HEREUNDER.</p> <p>THE SERVICES MAY BE USED TO ACCESS AND TRANSFER INFORMATION OVER THE INTERNET. YOU ACKNOWLEDGE AND AGREE THAT BODO AND ITS VENDORS AND LICENSORS DO NOT OPERATE OR CONTROL THE INTERNET AND THAT: (I) VIRUSES, WORMS, TROJAN HORSES, OR OTHER UNDESIRABLE DATA OR SOFTWARE; OR (II) UNAUTHORIZED USERS (E.G., HACKERS) MAY ATTEMPT TO OBTAIN ACCESS TO AND DAMAGE THE CUSTOMER CONTENT, WEB-SITES, COMPUTERS, OR NETWORKS. WE WILL NOT BE RESPONSIBLE FOR THOSE ACTIVITIES.</p> </li> <li> <p>Limitation of Liability and Damages. NEITHER BODO NOR ITS     VENDORS AND LICENSORS WILL HAVE ANY LIABILITY TO YOU OR ANY THIRD     PARTY FOR ANY LOSS OF PROFITS, BUSINESS, DATA, OR OTHER INCIDENTAL,     CONSEQUENTIAL, OR SPECIAL LOSS OR DAMAGE, INCLUDING EXEMPLARY AND     PUNITIVE, OF ANY KIND OR NATURE RESULTING FROM OR ARISING OUT OF     THIS AGREEMENT, INCLUDING USE OF THE SERVICES EVEN IF BODO HAS BEEN     ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. THE TOTAL LIABILITY OF     BODO AND ITS VENDORS AND LICENSORS TO YOU OR ANY THIRD PARTY ARISING     OUT OF THIS AGREEMENT OR USE OF THE SERVICES IN CONNECTION WITH ANY     CLAIM OR TYPE OF DAMAGE (WHETHER IN CONTRACT OR TORT) WILL NOT     EXCEED THE TOTAL FEES YOU PAID, IF ANY, DURING THE SIX (6) MONTHS     IMMEDIATELY PRECEDING THE EVENT GIVING RISE TO THE LIABILITY. THIS     LIMITATION OF LIABILITY WILL APPLY EVEN IF THE EXPRESS WARRANTIES     PROVIDED ABOVE FAIL OF THEIR ESSENTIAL PURPOSE.</p> </li> <li> <p>Term and Termination. Unless otherwise agreed by the parties,     the Agreement shall be on-going until terminated by either party on     thirty (30) days prior notice to the other party. In the event we     terminate this Agreement for reasons other than breach of contract,     any prepaid but unused fees will be refunded.</p> </li> <li> <p>Government Restrictions. Any software or other programming     provided by us in connection with this Agreement is commercial     computer software as described in DFARS 252.227-7014(a)(1) and FAR     2.101. If acquired by or on behalf of the United States Department     of Defense or any component thereof, the United States Government     acquires this commercial computer software and commercial computer     software documentation subject to the terms of this Agreement as     specified in DFARS 227.7202-3, Rights in Commercial Computer     Software or Commercial Computer Software Documentation. If acquired     by or on behalf of any civilian agency, the United States Government     acquires this commercial computer software and commercial computer     software documentation subject to the terms of this Agreement as     specified in FAR 12.212, Computer Software.</p> </li> <li> <p>USA Patriot Act Notice. The U.S. federal USA Patriot Act     (\"USA Patriot Act\") provides generally for the operator of a     communication host and law enforcement to be able to monitor any     content, upon request of the operator. We anticipate fully complying     with our obligations and availing ourselves of all rights under the     USA Patriot Act.</p> </li> <li> <p>General. Except for the payment of fees, if applicable, neither     party will be liable for any failure or delay in performance under     this Agreement which is due to any event beyond the reasonable     control of such party, including without limitation, fire,     explosion, unavailability of utilities or raw materials, Internet     delays and failures, telecommunications failures, unavailability of     components, labor difficulties, war, riot, act of God, export     control regulation, laws, judgments or government instructions. This     Agreement provides the entire agreement between the parties with     regard to its subject matter. Except as provided below, this     Agreement may not be amended without a writing signed by both     parties. We may, at any time and from time-to-time, change the terms     of this Agreement. Any changes will be posted on our Web site. In     addition, we may also send you a notice about the amended terms via     email. If you do not accept the terms of any modification, your only     recourse is to terminate this Agreement by sending a termination     notice us before the effective date of the amendments. The     termination will be effective on the date we receive the notice. The     most current version of the Agreement will be available on our Web     site and will supersede all previous versions of the Agreement. Your     continued use of the Services will constitute your acceptance of the     changes. This Agreement will be construed according to, and the     rights of the parties will be governed by, the law of the State of     California, without reference to its conflict of laws rules. Any     action at law or in equity arising out of or directly or indirectly     relating to this Agreement may be instituted only in the Federal or     state courts located in San Francisco, California. You consent and     submit to the personal jurisdiction of those courts for the purposes     of any action related to this Agreement, and to extra-territorial     service of process. No action, regardless of form, arising out of     this Agreement, may be brought by either party more than one (1)     year after the cause of action has arisen. You may not assign this     Agreement without the prior written consent of Bodo. If any of the     provisions of this Agreement are found or deemed by a court to be     invalid or unenforceable, they will be severable from the remainder     of this Agreement and will not cause the invalidity or     unenforceability of the remainder of this Agreement. Neither party     will by mere lapse of time without giving notice or taking other     action hereunder be deemed to have waived any breach by the other     party of any of the provisions of this Agreement. The following     provisions will survive termination or expiration of this Agreement:     4 (Proprietary Rights), 9 (Indemnification), 12 (Confidentiality),     13 (Limited Warranty; Exclusive Remedy); 14 (Disclaimer of     Warranties), 15 (Limitation of Liability and Damages), 17     (Government Restrictions), 18 (USA Patriot Act Notice), and 19     (General Provisions). This Agreement may be accepted in electronic     form (e.g., by an electronic or other means of demonstrating assent)     and your acceptance will be deemed binding between us. Neither of us     will contest the validity or enforceability of this Agreement and     any related documents, including under any applicable statute of     frauds, because they were accepted or signed in electronic form.</p> </li> </ol>"},{"location":"help_and_reference/eula/#licenses-for-redistributed-software","title":"Licenses for redistributed software","text":""},{"location":"help_and_reference/eula/#mpi4py","title":"Mpi4py","text":"<p>Copyright \u00a9 2024, Lisandro Dalcin</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li>Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.</li> <li>Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.</li> <li>Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.</li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"help_and_reference/eula/#mpich","title":"Mpich","text":"<pre><code>              COPYRIGHT\n</code></pre> <p>The following is a notice of limited availability of the code, and disclaimer which must be included in the prologue of the code and in all source listings of the code.</p> <p>Copyright Notice 1998--2020, Argonne National Laboratory</p> <p>Permission is hereby granted to use, reproduce, prepare derivative works, and to redistribute to others.  This software was authored by:</p> <p>Mathematics and Computer Science Division Argonne National Laboratory, Argonne IL 60439</p> <p>(and)</p> <p>Department of Computer Science University of Illinois at Urbana-Champaign</p> <pre><code>              GOVERNMENT LICENSE\n</code></pre> <p>Portions of this material resulted from work developed under a U.S. Government Contract and are subject to the following license: the Government is granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable worldwide license in this computer software to reproduce, prepare derivative works, and perform publicly and display publicly.</p> <pre><code>              DISCLAIMER\n</code></pre> <p>This computer code material was prepared, in part, as an account of work sponsored by an agency of the United States Government.  Neither the United States, nor the University of Chicago, nor any of their employees, makes any warranty express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights.</p> <pre><code>           EXTERNAL CONTRIBUTIONS\n</code></pre> <p>Portions of this code have been contributed under the above license by:</p> <ul> <li>Intel Corporation</li> <li>Cray</li> <li>IBM Corporation</li> <li>Microsoft Corporation</li> <li>Mellanox Technologies Ltd.</li> <li>DataDirect Networks.</li> <li>Oak Ridge National Laboratory</li> <li>Sun Microsystems, Lustre group</li> <li>Dolphin Interconnect Solutions Inc.</li> <li>Institut Polytechnique de Bordeaux</li> <li>Quobyte Corporation</li> </ul>"},{"location":"iceberg/data_types/","title":"Supported Iceberg Data Types","text":"<p>Bodo supports most data types defined in the Apache Iceberg specification. This following table shows how Iceberg data types are represented in Python and SQL.</p> Iceberg Data Type Equivalent Python / Pandas Array Type Equivalent SQL Column Type boolean <code>bool[pyarrow]</code> BOOL int <code>int32[pyarrow]</code> INT long <code>int64[pyarrow]</code> BIGINT float <code>float32[pyarrow]</code> FLOAT double <code>float64[pyarrow]</code> DOUBLE decimal(P, S) <code>decimal128(P, S)[pyarrow]</code> DECIMAL(P, S) date <code>date32[pyarrow]</code> DATE time <code>time32[pyarrow]</code> TIME timestamp <code>timestamp[us][pyarrow]</code> TIMESTAMP timestamptz <code>timestamp[us, tz=UTC][pyarrow]</code> TIMESTAMPTZ string <code>large_string[pyarrow]</code> STRING binary <code>binary[pyarrow]</code> BINARY struct&lt;...&gt; <code>struct&lt;...&gt;[pyarrow]</code> STRUCT list <code>large_list&lt;E&gt;[pyarrow]</code> LIST map <code>map&lt;K, V&gt;[pyarrow]</code> MAP <p>Bodo does not support these Iceberg data types yet:</p> <ul> <li>fixed</li> <li>uuid</li> </ul>"},{"location":"iceberg/intro/","title":"Introduction to Iceberg in Bodo","text":"<p>Apache Iceberg is an open table format designed for storing large datasets as a lakehouse. With Iceberg, data stored in open-source file formats in a data lake (e.g. S3) can be used like a data warehouse. This solves many of the problems of traditional data lakes, such as:</p> <ul> <li>ACID (Atomicity, Consistency, Isolation, and Durability) transaction compliance</li> <li>Evolving table schemas</li> <li>Consistent metadata storage formats</li> <li>Scalable reads and writes at scale</li> <li>Time travel</li> </ul> <p>Bodo has first-class read and write support for Iceberg tables in both Python and SQL. Bodo supports Iceberg tables that use the Apache Parquet file format.</p> <p>Note</p> <p>Iceberg support is generally available as of v2024.4. If you are using a previous alpha version, we recommend that you upgrade.</p>"},{"location":"iceberg/intro/#getting-started","title":"Getting Started","text":"<p>The Iceberg Connector comes pre-installed on the Bodo Platform for immediate use.</p> <p>For a general introduction on how to use Iceberg in Bodo, take a look at our quickstart with Iceberg.</p> <p>Otherwise, start by installing the <code>bodo-iceberg-connector</code> package from Conda.</p> <pre><code>conda install -c bodo.ai bodo-iceberg-connector\n</code></pre>"},{"location":"iceberg/intro/#supported-iceberg-catalogs","title":"Supported Iceberg Catalogs","text":"<p>These are the Iceberg catalogs supported in Bodo Python and SQL:</p> Catalog Name Bodo Python Support BodoSQL Support Additional Notes HadoopCatalog Yes Yes, via the FileSystemCatalog Local and S3 Support Snowflake's Managed Iceberg Catalog Yes Yes, via the SnowflakeCatalog Integrated into BodoSQL's Snowflake support RESTCatalog Yes Yes, via the RESTCatalog GlueCatalog Yes Yes, via the GlueCatalog HiveCatalog Yes Yes, via TablePath S3 Tables Yes Yes, via the S3TablesCatalog"},{"location":"iceberg/intro/#limitations-and-considerations","title":"Limitations and Considerations","text":"<p>Here are the following limitations when working with Iceberg tables in Bodo:</p>"},{"location":"iceberg/intro/#iceberg-features","title":"Iceberg Features","text":"<ul> <li>Bodo only supports data files in the Parquet format. Avro and ORC is currently unsupported.</li> <li>Bodo can read from V1 tables, but not write to them.</li> <li>Bodo can't read or write Iceberg columns of type UUID and FIXED.</li> <li>Bodo does not support reading or writing delete files. Thus, it does not support Merge-on-Read yet.</li> </ul>"},{"location":"iceberg/intro/#sql-features","title":"SQL Features","text":"<ul> <li>Iceberg tables do not support the <code>TEMPORARY</code> or <code>TRANSIENT</code> options when creating tables.</li> <li>The Iceberg View spec is not supported right now. In the case of the:<ul> <li>Filesystem Catalog: View names will be undefined.</li> <li>Snowflake Catalog: View names will be defined if there is a definition in Snowflake. Otherwise, it will be undefined.</li> </ul> </li> </ul>"},{"location":"iceberg/puffin_files/","title":"Using Puffin Files and Theta Sketches with Bodo","text":""},{"location":"iceberg/puffin_files/#iceberg-theta-sketch","title":"Theta Sketches","text":"<p>A Theta Sketch is a data structure used to approximate the number of distinct values (NDV) in data columns, which is critical for SQL planner optimizations.  BodoSQL creates Theta Sketches as it writes data to an Iceberg table (either creating a table or inserting into one).  This way, future queries that are run on those tables will have access to the NDV values so the planner can make better decisions about how to optimize queries.</p> <p>If BodoSQL is used to insert into an existing Iceberg table, BodoSQL will attempt to union any existing Theta Sketches with the Theta Sketches created on the newly inserted data, thus obtaining NDV estimates for the entire table (both old and new data combined).</p> <p>Currently, BodoSQL uses the following rules to determine when it should create Theta Sketches for a column:</p> <ul> <li>If BodoSQL is creating a table with a <code>CREATE TABLE AS SELECT</code> (CTAS) query, then currently it creates Theta Sketches for all columns of the following BodoSQL types: <code>Int32</code>, <code>Int64</code>, <code>Date</code>, <code>Time</code>, <code>Timestamp</code>, <code>Timestamp_LTZ</code>, <code>String</code>, <code>Binary</code> and <code>Decimal</code>.</li> <li>If BodoSQL is creating a table with an <code>INSERT INTO</code> query, then currently it creates and unions Theta Sketches for all columns that already have a Theta Sketch and are of one of the data types above, as well as <code>FLOAT</code> and <code>DOUBLE</code>.</li> </ul> <p>Note</p> <p>If the environment variable <code>BODO_ENABLE_THETA_SKETCHES</code> is set to <code>0</code>, then Theta Sketches are disabled always, no matter what the column types are.</p> <p>Bodo uses the Apache DataSketches library to implement Theta Sketches.</p> <p>To learn more about Theta Sketches, see the documentation.</p>"},{"location":"iceberg/puffin_files/#iceberg-puffin-files","title":"Puffin Files","text":"<p>The way that Theta Sketches are stored when being written is via Puffin files.  A Puffin file is an Iceberg statistics file located in the metadata folder of an Iceberg table.  As of this writing, the only statistics it supports is a Theta Sketch.  Each Puffin file can contain one or more Theta Sketches for various columns in the table.  The Theta Sketches are serialized and stored in sections of the Puffin file referred to as \"blobs\". Each blob is associated with a specific snapshot-id and sequence number. </p> <p>When the BodoSQL planner is attempting to infer metadata about tables for the purposes of optimization, it will try to find any Puffin files that exist for the current snapshot, and will use the NDV values from the Theta Sketches whose snapshot id and sequence number indicate that they are fresh.  If other engines have inserted rows into the table without writing a new Puffin file since the last Puffin file was created, or rows have been dropped from the table, then the sketches and their NDV estimates are no longer reliable so BodoSQL cannot use them.</p> <p>To learn more about Puffin files, see the documentation.</p>"},{"location":"iceberg/read_write/","title":"Reading and Writing Iceberg in Bodo","text":""},{"location":"iceberg/read_write/#iceberg-sql","title":"SQL","text":"<p>BodoSQL can be used to read, create, or insert into an Iceberg table. Iceberg Tables are automatically detected by existing catalogs and are used during read:</p> <ul> <li>Snowflake Iceberg Tables are automatically detected when using the <code>SnowflakeCatalog</code>.</li> <li>Tables within the specified warehouse are automatically detected when using the <code>RESTCatalog</code>.</li> <li>Tables within the specified warehouse are automatically detected when using the <code>GlueCatalog</code>.</li> <li>Tables within the specified warehouse are automatically detected when using the <code>S3TablesCatalog</code>.</li> <li>Hadoop Iceberg Catalogs and Tables are detected when using the <code>FileSystemCatalog</code>.</li> <li>Other Catalogs supported in the Python APIs can be accessed via the <code>TablePath</code> API using the same connection string syntax.</li> </ul> <p>To query an Iceberg table, use the standard <code>SELECT</code> syntax. To learn more about supported SELECT syntax, see the SELECT API reference.</p> <pre><code>SELECT ... FROM &lt;... namespace_path ...&gt;.&lt;table_name&gt; ...\n</code></pre>"},{"location":"iceberg/read_write/#write-support","title":"Write Support","text":"<p>The <code>CREATE TABLE</code> syntax can be used to create Iceberg tables:</p> <pre><code>CREATE [OR REPLACE] [TRANSIENT | TEMPORARY] TABLE &lt;...namespace_path...&gt;.&lt;table_name&gt;\n</code></pre> <p>Inserting into existing Iceberg tables is supported via the <code>INSERT INTO</code> syntax:</p> <pre><code>INSERT INTO &lt;...namespace_path...&gt;.&lt;table_name&gt;\n</code></pre>"},{"location":"iceberg/read_write/#snowflake-iceberg-write-support","title":"Snowflake Iceberg Write Support","text":"<p>To create Iceberg tables in Snowflake, a Snowflake External Volume is required. The volume to use must be specified via the <code>exvol</code> argument to the <code>SnowflakeCatalog</code>:</p> <pre><code>catalog = bodosql.SnowflakeCatalog(\n    ...\n    exvol='&lt;... Snowflake Volume ...&gt;'\n)\n\nbc = bodosql.BodoSQLContext(catalog=catalog)\n</code></pre> <p>Warning</p> <ul> <li>Inserting into Snowflake Managed Iceberg Tables is not supported.</li> <li>When the <code>exvol</code> parameter is specified, all tables constructed via <code>CREATE TABLE</code> will be Snowflake Iceberg tables.</li> </ul>"},{"location":"iceberg/read_write/#iceberg-python","title":"Python","text":"<p>Bodo supports reading and writing to Iceberg tables from multiple catalogs and object stores (local, S3, and HDFS).</p> <ul> <li>Iceberg Reads are supported through the <code>pandas.read_sql_table</code> API.</li> <li>Iceberg Writes are supported through the <code>pandas.DataFrame.to_sql</code> API.</li> </ul>"},{"location":"iceberg/read_write/#iceberg-conn-str","title":"Connection String Syntax","text":"<p>To specify the Iceberg catalog in the Pandas APIs, the <code>conn</code> parameter must contain a connection string in one of the following formats.</p> <p>Iceberg connection strings vary by catalog, but in general are of the form <code>iceberg&lt;+conn&gt;://&lt;path&gt;&lt;?params&gt;</code> where - <code>&lt;conn&gt;://&lt;path&gt;</code> is the location of the catalog or Iceberg warehouse - <code>params</code> is a list of properties to pass to the catalog. Each parameter must be of the form <code>&lt;key&gt;=&lt;value&gt;</code> and separated with <code>&amp;</code>, similar to HTTP URLs.</p> <p>The following parameters are supported: - <code>type</code>: Type of catalog. The supported values are listed below. When the connection string is ambiguous, this parameter is used to determine the type of catalog implementation. - <code>warehouse</code>: Location of the warehouse. Required when creating a new table using a Glue or Hive catalog.</p> <p>The following catalogs are supported:</p> <ul> <li> <p>Hadoop Catalog on Local Filesystem:</p> <ul> <li>Used when <code>type=hadoop</code> is specified or when <code>&lt;conn&gt;</code> is <code>file</code> or empty</li> <li><code>&lt;path&gt;</code> is the absolute path to the warehouse (directory containing the database schema)</li> <li>Parameter <code>warehouse</code> will be ignored if specified</li> <li>E.g. <code>iceberg://&lt;ABSOLUTE PATH TO ICEBERG WAREHOUSE&gt;</code> or <code>iceberg+file://&lt;ABSOLUTE PATH TO ICEBERG WAREHOUSE&gt;</code></li> </ul> </li> <li> <p>Hadoop Catalog on S3</p> <ul> <li>Used when <code>type=hadoop-s3</code> is specified or when <code>&lt;conn&gt;</code> is <code>s3</code>.</li> <li><code>&lt;conn&gt;://&lt;path&gt;</code> is the S3 path to the warehouse (directory or bucket containing the database schema).</li> <li>Parameter <code>warehouse</code> will be ignored if specified.</li> <li>E.g. <code>iceberg+s3://&lt;S3 PATH TO ICEBERG WAREHOUSE&gt;</code></li> </ul> </li> <li> <p>AWS Glue Catalog</p> <ul> <li>Connection string must be of the form <code>iceberg+glue?&lt;params&gt;</code>.</li> <li>Parameter <code>type</code> will be ignored if specified.</li> <li>Parameter <code>warehouse</code> is required to create a table.</li> <li>E.g. <code>iceberg+glue</code> or <code>iceberg+glue?warehouse=s3://&lt;ICEBERG-BUCKET&gt;</code></li> </ul> </li> <li> <p>S3 Tables Catalog</p> <ul> <li>Connection string must be of the form <code>iceberg+arn:aws:s3tables:&lt;region&gt;:&lt;account_number&gt;:bucket/&lt;bucket&gt;</code></li> <li><code>params</code> is unused</li> <li>E.g. <code>iceberg+arn:aws:s3tables:&lt;region&gt;:&lt;account_number&gt;:bucket/&lt;bucket&gt;</code></li> </ul> </li> <li> <p>Hive / Thrift Catalog</p> <ul> <li>Used when <code>type=hive</code> is specified or when <code>&lt;conn&gt;</code> is <code>thrift</code>.</li> <li><code>&lt;conn&gt;://&lt;path&gt;</code> is the URL to the Thrift catalog, i.e. <code>thrift://localhost:9083</code>.</li> <li>Parameter <code>warehouse</code> is required to create the table.</li> <li>E.g. <code>iceberg+thrift://&lt;THRIFT URL&gt;</code></li> </ul> </li> <li> <p>REST Catalog</p> <ul> <li>Connection string must be of the form <code>iceberg+http(s)://&lt;catalog_host&gt;?&lt;params&gt;</code>.</li> <li>Parameter <code>type</code> will be ignored if specified.</li> <li>Parameter <code>warehouse</code> is required.</li> <li>Parameter <code>token</code> or <code>credential</code> is required for authentication and should be retrieved from the REST catalog provider.</li> <li>E.g.<code>iceberg+http(s)://&lt;rest-uri&gt;?warehouse=&lt;warehouse&gt;&amp;token=&lt;token&gt;</code></li> </ul> </li> <li> <p>S3 Tables</p> <ul> <li>Connection string must be of the form <code>iceberg+arn:aws:s3tables:&lt;region&gt;:&lt;account_number&gt;:bucket/&lt;bucket&gt;</code></li> <li><code>params</code> is unused</li> <li>E.g. <code>iceberg+arn:aws:s3tables:us-west-2:123456789012:bucket/mybucket</code></li> </ul> </li> </ul>"},{"location":"iceberg/read_write/#iceberg-pandas","title":"Pandas APIs","text":"<p>Example code for reading:</p> <pre><code>@bodo.jit\ndef example_read_iceberg() -&gt; pd.DataFrame:\n    return pd.read_sql_table(\n        table_name=\"&lt;... Name of the Iceberg Table ...&gt;\",\n        con=\"&lt;... Connection String. See previous section ...&gt;\",\n        schema=\"&lt;... Namespace Path to Iceberg Table ...&gt;\",\n        _snapshot_id=\"&lt;... Optional (int): Snapshot ID to read the table from ...&gt;\",\n        _snapshot_timestamp_ms=\"&lt;... Optional (int): Timestamp to read the table from ...&gt;\"\n\n    )\n</code></pre> <p>Note</p> <ul> <li> <p>The <code>schema</code> argument is required for reading Iceberg tables.</p> </li> <li> <p>The Iceberg table to read should be located at <code>&lt;warehouse-location&gt;/&lt;schema&gt;/&lt;table_name&gt;</code>,   where <code>schema</code> and <code>table_name</code> are the arguments to <code>pd.read_sql_table</code>, and <code>warehouse-location</code>   is inferred from the connection string based on the description provided above.</p> </li> </ul> <p>An example for writing to Iceberg via <code>pandas.DataFrame.to_sql</code>:</p> <pre><code>@bodo.jit(distributed=[\"df\"])\ndef write_iceberg_table(df: pandas.DataFrame):\n    df.to_sql(\n        name=\"&lt;... Name of the Iceberg Table ...&gt;\",\n        con=\"&lt;... Connection String. See previous section ...&gt;\",\n        schema=\"&lt;... Namespace Path to Iceberg Table ..&gt;\",\n        if_exists=\"replace\"\n    )\n</code></pre> <p>Note</p> <ul> <li><code>schema</code> argument is required for writing Iceberg tables.</li> <li>Writing a Pandas Dataframe index to an Iceberg table is not supported. If <code>index</code> and <code>index_label</code>   are provided, they will be ignored.</li> <li><code>chunksize</code>, <code>dtype</code> and <code>method</code> arguments are not supported and will be ignored if provided.</li> </ul>"},{"location":"iceberg/read_write/#iceberg-partitioning-sorting","title":"Table Partitioning and Sorting","text":"<p>Bodo supports reading and writing Iceberg tables with partitioning and sorting. Bodo doesn't support creating a new table with partitioning or sorting yet. We recommend using PyIceberg to create a new empty table with partitioning and sorting specified and using Bodo for read and write of actual data. PyIceberg is a dependency of Bodo and is automatically installed when you install Bodo.</p> <p>Example: <pre><code>import pandas as pd\nfrom pyiceberg import schema, types\nfrom pyiceberg.partitioning import PartitionField, PartitionSpec\nfrom pyiceberg.transforms import BucketTransform, DayTransform\n\nimport bodo\nfrom bodo.io.iceberg.catalog import conn_str_to_catalog\n\n# Establish a connection to the Iceberg catalog\ncatalog = conn_str_to_catalog(\"iceberg+file://iceberg_db\")\n\n# Define the schema for the Iceberg table\nschema = schema.Schema(\n        types.NestedField(0, \"id\", types.LongType()),  \n        types.NestedField(1, \"timestamp\", types.TimestamptzType()),  \n        types.NestedField(2, \"data\", types.StringType()),  # \n)\n\n\n# Create the Iceberg table if it doesn't already exist\nif not catalog.table_exists(\"default.test_table\"):\n    # Specify partitioning: bucket by ID and partition by day for timestamp\n    partition_spec = PartitionSpec(\n                PartitionField(\n                        transform=BucketTransform(3),  # Bucketing with 3 buckets\n                        source_id=0,\n                        field_id=1000,\n                        name=\"bucket\",\n                    ),\n                PartitionField(\n                        transform=DayTransform(),  # Partitioning by day\n                        source_id=1,\n                        field_id=1001,\n                        name=\"day\",\n                    ),\n            )\n        table = catalog.create_table(\n                \"default.test_table\",\n                schema=schema,\n                partition_spec=partition_spec,\n            )\n\n@bodo.jit()\ndef append(df):\n    # Append a DataFrame to the Iceberg table\n    df.to_sql(\"test_table\", schema=\"default\", con=\"iceberg+file://iceberg_db\", if_exists=\"append\")\n\n# Example data to append: two rows with IDs, timestamps, and string data\nappend(pd.DataFrame({\n        \"id\": [1, 2],\n        \"timestamp\": [pd.Timestamp.now(), pd.Timestamp.now() + pd.Timedelta(3, \"days\")],\n        \"data\": [\"a\",]\n}))\n</code></pre></p>"},{"location":"installation_and_setup/","title":"Bodo Installation and Setup","text":"<p>You can install Bodo engine locally, or use the Bodo Cloud Platform which is Bodo's managed cloud service.</p>"},{"location":"installation_and_setup/#local-and-on-prem-cluster-installation","title":"Local and On-Prem Cluster Installation","text":"<p>For local and on-premise installations, please refer to the Local Installation Guide.</p>"},{"location":"installation_and_setup/#bodo-cloud-platform","title":"Bodo Cloud Platform","text":"<p>Pick the cloud provider of your choice to get started with Bodo Platform:</p> <ul> <li>AWS</li> <li>Azure</li> </ul>"},{"location":"installation_and_setup/bodo_platform_aws/","title":"Bodo Managed Cloud Platform on AWS","text":""},{"location":"installation_and_setup/bodo_platform_aws/#registration","title":"Registration","text":"<p>a.  Subscribe through the AWS Marketplace.</p> <p>b.  After confirming your subscription, you should click Set up your Account in the top right corner of the page. Bodo Platform's registration page will open in a new tab.</p> <p></p> <p>c.  Fill out the fields with your information. If this is your     individual account, use a unique name such as     firstname_lastname for the Organization Name     field.</p> <p>d.  Check the box for accepting terms and conditions and click on     <code>SIGN UP</code>:     </p> <p>e.  A page confirming that an activation link was sent to your email     will appear. Please open the email and click on the activation link:          Clicking on the confirmation link will take you to the bodo platform     page where you can use your newly created credentials to sign in:     </p>"},{"location":"installation_and_setup/bodo_platform_aws/#setting_aws_credentials","title":"Setting AWS Credentials","text":"<p>To use Bodo on AWS, you need to link your AWS account to the Bodo platform.</p> <p>This can be done using the Cloud Configuration page in the left sidebar and followed by clicking on Create Cloud Configuration at the top right corner of the page as shown in the picture below:</p> <p></p> <p>To be able to use the Bodo Platform to launch clusters and notebooks, you must grant it permission to access your AWS account and provision the required resources in it. This can be done through three ways:</p>"},{"location":"installation_and_setup/bodo_platform_aws/#cloud_formation_stack","title":"1. AWS CloudFormation Quick Start","text":"<p>Tip</p> <p>You need the following set of permissions to successfully create the resources defined in the CloudFormation stack:</p> <ul> <li><code>AWSCloudFormationFullAccess</code></li> <li><code>AWSIAMFullAccess</code></li> </ul> <p>Once you have ensured that you have all permissions necessary to create the resources, follow the steps below to create a Cloud Configuration:</p> <ol> <li>Fill in the following values :<ul> <li>Cloud Configuration Name: A name for your Cloud Configuration.</li> <li>CloudFormation Region: Fill this with the region where you want to deploy the stack.</li> </ul> </li> </ol> <p></p> <ol> <li> <p>Click on Launch CloudFormation Template. This will open the AWS CloudFormation console in a new tab in the selected region.</p> <p>Important</p> <p>All values are pre-filled in the CloudFormation template. Please do not modify.</p> </li> <li> <p>Click on \"Create Stack\" to create the stack. This will create the necessary resources in your AWS account.</p> </li> </ol> <p></p> <p>Note</p> <p>The stack creation process may take a few minutes to complete.   Please wait until the stack is created successfully.</p> <ol> <li>You can check the status of the stack from Bodo Platform as shown below. Once the stack is created successfully, Cloud Configuration will be created.    </li> </ol>"},{"location":"installation_and_setup/bodo_platform_aws/#create_manually","title":"2. Manual Process","text":"<p>Open the Cloud Configuration Form and note down the <code>External ID</code>.</p> <p>We need to create an IAM Role the AWS Console and provide details about it in  the Cloud Configuration Form.</p>"},{"location":"installation_and_setup/bodo_platform_aws/#setup-iam-role","title":"Setup IAM role","text":"<ul> <li> <p>IAM role</p> <ol> <li> <p>Log in to the AWS Management Console and navigate to the     IAM Service.</p> </li> <li> <p>Select the Roles tab in the sidebar, and click <code>Create Role</code>.</p> </li> <li> <p>In Select type of trusted entity, select <code>Another AWS Account</code>.</p> </li> <li> <p>Enter the Bodo Platform Account ID <code>481633624848</code> in the     Account ID field.</p> </li> <li> <p>Check the <code>Require external ID</code> option.</p> <p></p> <p>In the External ID field, copy over the External ID from the Cloud Configuration form on the Bodo Platform.</p> <p></p> </li> <li> <p>Click the <code>Next: Permissions</code> button.</p> </li> <li> <p>Click the <code>Next: Tags</code> button.</p> </li> <li> <p>Click the <code>Next: Review</code> button.</p> </li> <li> <p>In the Role name field, enter a role name, e.g.     <code>BodoPlatformUser</code>.</p> <p></p> </li> <li> <p>Click <code>Create Role</code>. You will be taken back to the list of IAM Roles     in your account.</p> </li> <li> <p>In the list of IAM Roles, click on the role you just created.</p> </li> <li> <p>Under Permissions tab, Click on <code>Add Permissions</code> and select <code>Create inline policy</code> from the dropdown.</p> <p></p> </li> <li> <p>Click the <code>JSON</code> tab.</p> <p></p> </li> <li> <p>Bodo Cloud Platform requires a specific set of AWS permissions which     are documented in Bodo-Platform Policy.     Paste the contents of the linked JSON file into the policy editor.</p> </li> <li> <p>Click on <code>Review policy</code>.</p> </li> <li> <p>In the Name field, add a policy name, e.g.     <code>Bodo-Platform-User-Policy</code>. Click on <code>Create policy</code>.     You will be taken back to the Role Summary.</p> </li> <li> <p>From the role summary, copy the <code>Role ARN</code>. This is the value that     you will enter into the Role ARN field on the Setting Page on     the Bodo Platform.</p> <p></p> </li> </ol> </li> </ul> <p>Once you have generated an IAM Role using the steps described above,  you can fill the remaining fields in the Cloud Configuration form on the Bodo Platform.</p> <ol> <li>Enter the Name of the configuration. </li> <li>Enter the Role ARN in the Role ARN field.</li> <li>Click on <code>Create</code>.</li> </ol> <p>Important</p> <p>Validation is not run during Cloud Configuration creation. Some errors are detected only when you use the configuration to create a new workspace.  These errors can include an invalid IAM Role ARN or incorrect permissions for the role.</p> <p></p> <p>Important</p> <p>We highly recommend that you ensure sufficient limits on your AWS account to launch resources. See here for details on the resources required for Bodo Cloud Platform.</p>"},{"location":"installation_and_setup/bodo_platform_aws/#resources_created_in_aws_env","title":"Resources Created in Your AWS Environment","text":"<p>Bodo deploys cluster/notebook resources in your own AWS environment to ensure security of your data. Below is a list of AWS resources that the Bodo Platform creates in your account to enable clusters and notebooks.</p> AWS Service Purpose EC2 Instances Cluster/notebook workers EFS Shared file system for clusters VPC, Subnets, NAT Gateway, Elastic IP, ENI, Security Groups, ... Secure networking for clusters/notebooks S3 Storage AWS AutoScaling Group Managing EC2 instances SSM Parameter Store Cluster secrets (e.g. SSH keys) IAM Role for Clusters Allow cluster workers to access resources above <p>Note</p> <p>These resources incur additional AWS infrastructure charges and are not included in the Bodo Platform charges.</p>"},{"location":"installation_and_setup/bodo_platform_aws/#using-bodo-platform","title":"Using Bodo Platform","text":"<p>Please refer to the platform usage guides to explain the basics of using the Bodo Cloud Platform and associated concepts.</p>"},{"location":"installation_and_setup/bodo_platform_aws/#aws_billing","title":"Billing","text":"<p>Users subscribed to the Bodo Platform through the AWS Marketplace will be charged for their use of the platform as part of their regular AWS bill. The platform charges are based on the type of instances deployed and the duration of their usage (to the nearest minute). The hourly rate for the supported instance types can be found on our website. For any cluster deployed through the platform, users are charged starting from when the cluster has been successfully deployed, until the time the user requests the cluster to be removed.</p> <p>Note</p> <p>Users are not charged in case of failures in cluster creation.</p> <p>As mentioned previously, the AWS resources set up by the platform in your AWS environment incur additional AWS infrastructure charges, and are not included in the Bodo Platform charges.</p>"},{"location":"installation_and_setup/bodo_platform_aws/#billing-alarms","title":"Billing Alarms","text":"<p>You can set up AWS alarms to monitor usage using cloudwatch alarms on AWS.</p> <p></p>"},{"location":"installation_and_setup/bodo_platform_aws/#steps-to-create-an-alarm-on-your-aws-account-for-all-ec2-usage","title":"Steps to create an alarm on your AWS account for all EC2 usage:","text":"<p>Steps to create an alarm on your AWS account for all EC2 usage:</p> <p></p> <ol> <li> <p>Select the region from which you would like to create the alarm and click <code>Create Alarm</code>. </p> </li> <li> <p>Click on the <code>Select metric</code> which would bring you to a search bar that allows you  to search and select the metric of your choice. Make sure to click on the check box of the  metric of your choice. In this example, we choose vCPU for monitoring EC2 usage. </p> </li> <li> <p>Set a reasonable number for the threshold for an alarm to go off based on your usage expectations. If you do not have this you can use the history of the metric to get an estimate. The history can be viewed by clicking on the expand button on the graph. You can toggle the time range by clicking on the available options (e.g. <code>1w</code> for 1 week) on the top panel. You can use the graph to set the desired threshold for your alarm. The  default period for the alarm threshold is 5 minutes but can be altered based on your requirement.  In this example above we set the alarm to become active, if the vCPU count is greater than 1000  for 5 minutes as the highest value found from the last week was ~850. Click <code>Next</code> at the bottom  of the page after you have set the threshold and period. </p> </li> <li> <p>You will now be asked to select the Simple Notification Service (SNS) Topic for this alarm. </p> <p>a. If you already have an existing SNS Topic you can choose it from the dropdown list  by clicking on the <code>Select an Existing SNS Topic</code> radio button. </p> <p>b. If you do not have an SNS Topic, then create a new SNS Topic by clicking on the  <code>Create New Topic</code> radio button. Fill in the form with an appropriate <code>Topic Name</code> and  provide the emails (Those who should be alerted by the alarm) in the  <code>Email endpoints that will receive the notification</code> tab.</p> <p>Once you have provided these details you can click on <code>Next</code> at the bottom of the page.</p> </li> <li> <p>You will now be required to fill out the details of the alarm itself, fill the fields with the  appropriate details and click <code>Next</code> at the bottom of the page. </p> </li> <li> <p>Finally, preview your alarm before you click on <code>Create Alarm</code> at the bottom of the page to create the Alarm.</p> </li> </ol>"},{"location":"installation_and_setup/bodo_platform_azure/","title":"Bodo Managed Cloud Platform on Azure","text":""},{"location":"installation_and_setup/bodo_platform_azure/#setting_azure_credentials","title":"Setting Azure Credentials","text":"<p>To use Bodo on Azure, you need to link your Azure account to the Bodo platform. This can be done using the Cloud Configuration page in the left bar as shown in the picture below:</p> <p></p> <p>In order to use the Bodo Platform to launch clusters and notebooks, you must grant it permission to access your Azure account and provision the required resources in it. You can do this by creating a Service Principal for the Bodo Platform application and assigning a role to it.</p>"},{"location":"installation_and_setup/bodo_platform_azure/#create_service_principal","title":"Create a Service Principal","text":"<p>Login to your Azure Portal. Click on the icon next to the search bar to open a Cloud-Shell. Execute the following command to create a service principal:</p> <pre><code>az ad sp create --id APP_ID\n</code></pre> <p>where <code>APP_ID</code> is the Application ID for Bodo-Platform which is displayed on the Cloud Configuration Form.</p> <p></p> <p>Once you have created a service principal, you need to assign a role to it. As shown below, go to the IAM section of your resource group and add a <code>Contributor</code> Role and <code>Storage Blob Data Contributor</code> Role to the service principal you created for the Bodo Platform Application.</p> <p></p> <p>See Also</p> <p>Required Azure resource providers</p> <p>Once you have created the service principal and assigned a role to it, you are now ready to fill the Cloud Configuration Form on the Bodo Platform.</p> <p></p> <ol> <li> <p>Enter your Azure subscription ID in the Subscription ID field.     You can find this in the Subscription Overview.</p> <p></p> </li> <li> <p>Enter your Azure Tenant ID in the Tenant ID field. You can find     this in Azure AD.</p> <p></p> </li> <li> <p>Enter the name of the resource group where the infrastructure should be deployed.</p> </li> <li> <p>Select a region from the dropdown list. This region refers to the region of      the resource group mentioned in the previous step. We will also create a storage account and a blob container in this region to store metadata such as the state of the deployed infrastructure, logs, etc.</p> </li> <li> <p>Click on <code>CREATE</code>.</p> </li> </ol> <p>Note</p> <p>We highly recommend that you ensure sufficient limits on your Azure subscription to launch resources. See here for the resources required for Bodo Cloud Platform.</p>"},{"location":"installation_and_setup/bodo_platform_azure/#required_az_resource_providers","title":"Required Resource Providers on Azure subscription","text":"<p>Ensure that the following resource providers are registered on your Azure subscription:</p> <ul> <li>Microsoft.Authorization</li> <li>Microsoft.Compute</li> <li>Microsoft.KeyVault</li> <li>Microsoft.ManagedIdentity</li> <li>Microsoft.Network</li> <li>Microsoft.Resources</li> <li>Microsoft.Storage</li> </ul> <p></p>"},{"location":"installation_and_setup/bodo_platform_azure/#resources_created_in_azure_env","title":"Resources Created in Your Azure Environment","text":"<p>Bodo deploys cluster/notebook resources in your own Azure environment to ensure security of your data. Below is a list of Azure resources that the Bodo Platform creates in your account to enable clusters and notebooks.</p> Azure Service Purpose Virtual Machines Cluster/notebook workers Storage Accounts, File-Shares Shared file system for clusters Virtual Network with Subnets and NAT Gateway, Public IP, NIC, Proximity Placement Groups, Availability Sets, Security Groups, ... Secure networking for clusters/notebooks Blob Containers, Resource states KeyVault Cluster secrets (e.g. SSH keys) VM Identity for Clusters Allow cluster workers to access resources above <p>Note</p> <p>These resources incur additional Azure infrastructure charges and are not included in the Bodo Platform charges.</p>"},{"location":"installation_and_setup/bodo_platform_azure/#using-bodo-platform","title":"Using Bodo Platform","text":"<p>Please refer to the platform usage guides to explain the basics of using the Bodo Cloud Platform and associated concepts.</p>"},{"location":"installation_and_setup/install/","title":"Installing Bodo Engine","text":"<p>Bodo compute engine can be installed using either <code>pip</code> or <code>conda</code> (see how to install conda below). To install Bodo and its dependencies with <code>pip</code>, use the following command:</p> <pre><code>pip install -U bodo\n</code></pre> <p>For production environments, we recommend creating a <code>conda</code> environment and installing Bodo and its dependencies in it as shown below:</p> <pre><code>conda create -n Bodo python=3.13 -c conda-forge\nconda activate Bodo\nconda install bodo -c conda-forge\n</code></pre> <p>Bodo uses MPI for parallelization, which is automatically installed as part of the <code>pip</code> and <code>conda</code> install commands above.</p>","tags":["install"]},{"location":"installation_and_setup/install/#conda","title":"How to Install Conda","text":"<p>You can install Miniforge, Anaconda, or other distributions of Conda. Miniforge can be installed easily on Linux, MacOS and WSL using terminal commands:</p> <pre><code>curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre>","tags":["install"]},{"location":"installation_and_setup/install/#windows-wsl","title":"Windows WSL","text":"<p>The Windows Subsystem for Linux (WSL) lets developers install a Linux distribution on Windows, which provides a convenient environment for installing and using Bodo. Here are example commands to install Python and Bodo on WSL:</p> <pre><code>sudo apt update\nsudo apt install python3-pip\nsudo apt install python3-venv\npython3 -m venv bodo-test\nsource bodo-test/bin/activate\npip install -U bodo\n</code></pre>","tags":["install"]},{"location":"installation_and_setup/install/#optionaldep","title":"Optional Dependencies","text":"<p>Some Bodo functionality may require other dependencies, as summarized in the table below.</p> <p>All optional dependencies except Hadoop, HDF5, and OpenJDK can be installed through pip using the command:</p> <pre><code>pip install sqlalchemy snowflake-connector-python deltalake\n</code></pre> <p>All optional dependencies except Hadoop can be installed through conda using the command:</p> <pre><code>conda install sqlalchemy snowflake-connector-python hdf5='1.14.*=*mpich*' openjdk=17 deltalake -c conda-forge\n</code></pre> <p> Functionality Dependency <code>pd.read_sql / df.to_sql</code> <code>sqlalchemy</code> <code>Snowflake I/O</code> <code>snowflake-connector-python</code> <code>Delta Lake</code> <code>deltalake</code> <code>HDFS or ADLS Gen2</code> hadoop (only the Hadoop client is needed) <code>HDF5</code> <code>hdf5 (MPI version)</code> <p></p>","tags":["install"]},{"location":"installation_and_setup/install/#testinstall","title":"Testing your Installation","text":"<p>Once you have installed Bodo with pip or activated your <code>conda</code> environment and installed Bodo in it, you can test it using the example program below. This program has two functions:</p> <ul> <li>The function <code>gen_data</code> creates a sample dataset with 20,000 rows     and writes to a parquet file called <code>example1.pq</code>.</li> <li>The function <code>test</code> reads <code>example1.pq</code> and performs multiple     computations on it.</li> </ul> <pre><code>import bodo\nimport pandas as pd\nimport numpy as np\nimport time\n\n@bodo.jit\ndef gen_data():\n    NUM_GROUPS = 30\n    NUM_ROWS = 20_000_000\n    df = pd.DataFrame({\n        \"A\": np.arange(NUM_ROWS) % NUM_GROUPS,\n        \"B\": np.arange(NUM_ROWS)\n    })\n    df.to_parquet(\"example1.pq\")\n\n@bodo.jit\ndef test():\n    df = pd.read_parquet(\"example1.pq\")\n    t0 = time.time()\n    df2 = df.groupby(\"A\")[\"B\"].agg(\n        (lambda a: (a==1).sum(), lambda a: (a==2).sum(), lambda a: (a==3).sum())\n    )\n    m = df2.mean()\n    print(\"Result:\", m, \"\\nCompute time:\", time.time() - t0, \"secs\")\n\ngen_data()\ntest()\n</code></pre> <p>Save this code in a file called <code>example.py</code>, and run it on all cores core as follows:</p> <pre><code>python example.py\n</code></pre> <p>Alternatively, to run it on a single core:</p> <pre><code>BODO_NUM_WORKERS=1 python example.py\n</code></pre> <p>Note</p> <p>You may need to delete <code>example1.pq</code> between consecutive runs.</p>","tags":["install"]},{"location":"installation_and_setup/install/#cluster_setup","title":"Enabling parallelism in clusters","text":"<p>Bodo relies on MPI for parallel compute. MPI can be configured on clusters easily. The cluster nodes need to have passwordless SSH enabled between them, and there should be a host file listing their addresses (see an example tutorial here). MPI usually needs to be configured to launch one process per physical core for best performance. This avoids potential resource contention between processes due to the high efficiency of MPI. For example, a cluster of four nodes, each with 16 physical cores, can use up to 64 MPI processes:</p> <pre><code>BODO_NUM_WORKERS=64 python example.py\n</code></pre> <p>For cloud instances, one physical core typically corresponds to two vCPUs. For example, an instance with 32 vCPUs has 16 physical cores.</p>","tags":["install"]},{"location":"installation_and_setup/install/#passwordless_ssh","title":"Setting up passwordless SSH on your multi-node cluster","text":"<p>Using MPI on a multi-node cluster requires setting up passwordless SSH between the hosts. There are multiple ways to do this. Here is one way:</p> <ol> <li> <p>Generate an SSH key pair using a tool like <code>ssh-keygen</code>, for     instance:</p> <pre><code>ssh-keygen -b 2048 -f cluster_ssh_key -N \"\"\n</code></pre> </li> <li> <p>Copy over the generated private key (<code>cluster_ssh_key</code>) and public key (<code>cluster_ssh_key.pub</code>) to all the hosts and     store them in <code>~/.ssh/id_rsa</code> and <code>~/.ssh/id_rsa.pub</code> respectively.</p> </li> <li> <p>Add the public key to <code>~/.ssh/authorized_keys</code> on all hosts.</p> </li> <li> <p>To disable host key checking, add the following to <code>~/.ssh/config</code>     on each host:</p> <pre><code>Host *\n    StrictHostKeyChecking no\n</code></pre> </li> </ol>","tags":["install"]},{"location":"installation_and_setup/recommended_cluster_config/","title":"Recommended Cluster Configuration","text":"<p>This page describes best practices for configuring compute clusters for Bodo applications.</p>"},{"location":"installation_and_setup/recommended_cluster_config/#minimizing-communication-overheads","title":"Minimizing Communication Overheads","text":"<p>Communication across cores is usually the largest overhead in parallel applications including Bodo. To minimize it:</p> <ul> <li> <p>For a given number of physical cores, use fewer large nodes with high core count rather than many     small nodes with a low core count.</p> <p>This ensures that more cross core communication happens inside nodes. For example, a cluster with two <code>c5n.18xlarge</code> AWS instances will generally perform better than a cluster with four <code>c5n.9xlarge</code> instances, even though the two options have equivalent cost and compute power.</p> </li> <li> <p>Use node types that support high bandwidth networking.</p> <p>AWS instance types with <code>n</code> in their name, such as <code>c5n.18xlarge</code>, <code>m5n.24xlarge</code> and <code>r5n.24xlarge</code> provide high bandwidth. On Azure, use virtual machines that support  Accelerated Networking.</p> </li> <li> <p>Use instance types that support     RDMA     networking. </p> <p>Examples of such instance types are Elastic Fabric Adapter (EFA) (AWS) and Infiniband (Azure). In our empirical testing, we found that EFA can significantly accelerate inter-node communication during expensive operations such as shuffle (which is used in join, groupby, sorting and others).</p> <ul> <li> <p>List of AWS EC2 instance types that support EFA.     For more information about EFA refer to the section on     Recommended AWS Network Interface .</p> </li> <li> <p>RDMA capable Azure VM Sizes.</p> </li> </ul> </li> <li> <p>Ensure that the server nodes are located physically close to each     other.</p> <p>On AWS this can be done by adding all instances to a placement group with the <code>cluster</code> strategy. Similarly on Azure, you can use Proximity Placement Groups.</p> </li> </ul> <p>For most applications, we recommend using <code>c5n.18xlarge</code> instances on AWS for best performance. For memory intensive use cases <code>r5n.24xlarge</code> instances are a good alternative. Both instance types support 100 Gbps networking as well as EFA.</p>"},{"location":"installation_and_setup/recommended_cluster_config/#other-best-practices","title":"Other Best Practices","text":"<ul> <li> <p>Ensure that the file descriptor limit (<code>ulimit -n</code>) is set to a     large number like <code>65000</code>.</p> </li> <li> <p>Launch scripts with mpiexec on clusters e.g.</p> </li> </ul> <pre><code>mpiexec -f &lt;hostfile&gt; -usize SYSTEM python file.py\n</code></pre> <ul> <li> <p>Avoid unnecessary threading inside the application since it can     conflict with MPI parallelism. </p> <p>You can set the following environment variables in your shell (e.g. in <code>bashrc</code>) to avoid threading:</p> <pre><code>export OPENBLAS_NUM_THREADS=1\nexport OMP_NUM_THREADS=1\nexport MKL_NUM_THREADS=1\n</code></pre> </li> <li> <p>Use Intel MPI     for best performance.     See our recommended MPI settings for more     details.</p> </li> </ul>"},{"location":"installation_and_setup/recommended_mpi_settings/","title":"Recommended MPI Settings","text":"<p>These are our recommendations to tune your application environment and achieve the best possible performance with Bodo.</p> <p>Important</p> <p>These recommendations are only applicable when you are running your workload on a cluster. You do not need to do any of this on your laptop.</p> <p>Intel-MPI library is the preferred distribution for message passing interface (MPI) specification.</p> <p>Note that Bodo automatically installs <code>mpich</code>. Hence, after installing Intel-MPI, remove [mpich] using this command:</p> <pre><code>conda remove -y --force mpich mpi\n</code></pre> <p>Intel-MPI provides different tuning collective algorithms.</p> <p>Based on our internal benchmarking, we recommend setting these environment variables as follows:</p> <pre><code>export I_MPI_ADJUST_ALLREDUCE=4\nexport I_MPI_ADJUST_REDUCE=3\n</code></pre>"},{"location":"installation_and_setup/recommended_mpi_settings/#mpi-process-placement","title":"MPI Process Placement","text":"<p>Bodo assigns chunks of data and computation to MPI processes, also called ranks. For example, for a dataframe with a billion rows on a 1000-core cluster, the first one million rows are assigned to rank 0, the second one million rows to rank 1, and so on. MPI placement indicates how these ranks are assigned to physical cores across the cluster, and can significantly impact performance depending on hardware configuration and application behavior. We recommend trying block mapping and round-robin mapping options below for your application to achieve the best performance.</p> <p>Important</p> <p>These options are only supported in SPMD launch mode.</p>"},{"location":"installation_and_setup/recommended_mpi_settings/#block-mapping","title":"Block Mapping","text":"<p>In block mapping, cores of each node in the <code>hostfile</code> are filled with ranks before moving on to the next node. For example, for a cluster with 50-core nodes, the first 50 ranks will be on node 0, the second 50 ranks on node 1 and so on. This mapping has the advantage of fast communication between neighboring ranks on the same node.</p> <p>We provide instructions on setting block placement for MPICH and Intel MPI below. The following assumes the hostfile only contains a list of hosts (e.g. it does not specify number of processes per host) and the number of cores on each host is the same.</p> <p>Block Mapping with MPICH and Intel MPI:</p> <p><pre><code>mpiexec -n &lt;N&gt; -f &lt;hostfile&gt; -ppn &lt;P&gt; python bodo_file.py\n</code></pre> where <code>N</code> is the number of MPI processes, <code>hostfile</code> contains the list of hosts, and <code>P</code> the number of processes (cores) per node.</p>"},{"location":"installation_and_setup/recommended_mpi_settings/#round-robin-mapping","title":"Round-Robin Mapping","text":"<p>In round-robin mapping, MPI assigns one rank per node in hostfile and starts over when it reaches end of the host list. For example, for a cluster with 50-core nodes, rank 0 is assigned to node 0, rank 1 is assigned to node 1 and so on. Rank 50 is assigned to node 0, 51 to node 1, and so on. This mapping has the advantage of avoiding communication hotspots in the network and tends to make large shuffles faster.</p> <p>We provide instructions on setting round-robin placement for MPICH and Intel MPI below. The following assumes the hostfile only contains a list of hosts (e.g. it does not specify number of processes per host) and the number of cores on each host is the same.</p> <p>Round-Robin with MPICH:</p> <p><pre><code>mpiexec -n &lt;N&gt; -f &lt;hostfile&gt; python bodo_file.py\n</code></pre> Round-Robin with Intel MPI:</p> <pre><code>mpiexec -n &lt;N&gt; -f &lt;hostfile&gt; -rr python bodo_file.py\n</code></pre>"},{"location":"installation_and_setup/recommended_mpi_settings/#useful-references","title":"Useful References","text":"<ul> <li> <p>More information on controlling process placement with Intel MPI can be found here.</p> </li> <li> <p>See how to use the Hydra Process Manager for MPICH here.</p> </li> </ul>"},{"location":"installation_and_setup/recommended_mpi_settings/#recommended_aws_nic","title":"Recommended AWS Network Interface","text":"<p>Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that has shown better inter-node communications at scale on AWS.</p> <p>To enable EFA with Intel-MPI on your cluster, follow instructions here.</p> <p>Some points to note in addition to the referenced instructions:</p> <ol> <li> <p>All instances must be in the same subnet. For more information, see     the \"EFA Limitations\" section     here.</p> </li> <li> <p>All instances must be part of a security group that allows all     inbound and outbound traffic to and from the security group itself.     Follow these     instructions     to set up the security group correctly.</p> </li> <li> <p>For use with Intel-MPI, a minimal installation of the EFA drivers is     sufficient and recommended:</p> <pre><code>sudo ./efa_installer.sh -y --minimal\n</code></pre> <p>Depending on where the drivers were downloaded from, you might need to include a <code>--no-verify</code> flag:</p> <pre><code>sudo ./efa_installer.sh -y --minimal --no-verify\n</code></pre> </li> </ol> <p>We recommend the following versions for the EFA installer and Intel-MPI:</p> <pre><code>EFA_INSTALLER_VERSION: 1.13.0\nIntel-MPI: v3.1 (2021.3.1.315)\n</code></pre> <p>Other version combinations are not guaranteed to work as they have not been tested.</p> <p>For EFA installer versions &gt;= 1.12.0, enabling fork is required by setting environment variable <code>FI_EFA_FORK_SAFE=1</code>.</p> <p>To confirm correct settings are enabled, run following <code>mpiexec</code> with <code>I_MPI_DEBUG=5</code> :</p> <pre><code>I_MPI_DEBUG=5 mpiexec -f hostfile -rr -n &lt;CORES&gt; python -u -c \"from bodo.mpi4py import MPI\"\n</code></pre> <p>Check that <code>libfabric provider</code> is <code>efa</code> and environment variables are set as shown below:</p> <pre><code>[0] MPI startup(): Intel(R) MPI Library, Version 2021.3.1  Build 20210719 (id: 48425b416)\n[0] MPI startup(): Copyright (C) 2003-2021 Intel Corporation.  All rights reserved.\n[0] MPI startup(): library kind: release\n[0] MPI startup(): libfabric version: 1.13.0rc1-impi\n[0] MPI startup(): libfabric provider: efa\n...\n[0] MPI startup(): I_MPI_ADJUST_ALLREDUCE=4\n[0] MPI startup(): I_MPI_ADJUST_REDUCE=3\n[0] MPI startup(): I_MPI_DEBUG=5\n</code></pre>"},{"location":"installation_and_setup/recommended_mpi_settings/#automatic-worker-number-detection","title":"Automatic Worker Number Detection","text":"<p>Bodo can automatically detect the number of workers to spawn based on MPI_UNIVERSE_SIZE. The following sections demonstrate how to configure Intel MPI and MPICH to set MPI_UNIVERSE_SIZE.</p>"},{"location":"installation_and_setup/recommended_mpi_settings/#intel-mpi","title":"Intel MPI","text":"<p>For Intel MPI to correctly set MPI_UNIVERSE_SIZE, you need to create <code>~/.mpiexec.conf</code> if it doesn't exist and add <code>--usize SYSTEM</code>. A valid hostfile is also required.</p>"},{"location":"installation_and_setup/recommended_mpi_settings/#mpich","title":"MPICH","text":"<p>For MPICH to correctly set MPI_UNIVERSE_SIZE, you need to pass the <code>-f &lt;path_to_hostfile&gt;</code> and <code>-usize SYSTEM</code> flags to mpiexec. Mpich doesn't have a way to set either of these flags in a configuration file so the flags must be passed on the command line every time. We recommend creating an alias for <code>mpiexec</code> with these flags. The hostfile should specify how many processes to run on each host. For example:</p> <pre><code>host1:4\nhost2:4\n</code></pre> <p>This specifies that 4 processes should run on each of <code>host1</code> and <code>host2</code>. Generally, the number of processes should be equal to the number of physical cores on each host.</p>"},{"location":"integrating_bodo/","title":"Bodo Usage Guides","text":"<p>This section provides a collection of guides to help you go from beginner to expert with Bodo. Choose a guide from the list below to get started.</p>"},{"location":"integrating_bodo/#installation-and-setup-guides","title":"Installation and Setup Guides","text":"<p>This section provides guides to help you install and set up Bodo on your local machine or get onboarded on Bodo Platform on AWS or Azure.</p> <ul> <li>Local Installation</li> <li>Bodo Platform on AWS</li> <li>Bodo Platform on Azure</li> </ul>"},{"location":"integrating_bodo/#python-developers-guide","title":"Python Developer's Guide","text":"<p>If you are a developer and want to get started with Bodo, this guide will help you get started with Bodo on your local machine.</p>"},{"location":"integrating_bodo/#understanding-parallelism-with-bodo","title":"Understanding Parallelism with Bodo","text":"<p>This section provides a collection of guides to help you understand how Bodo parallelizes your code and how to write code that can benefit from Bodo's parallelism.</p> <ul> <li>Basics of Bodo Parallelism</li> <li>Advanced Parallelism Topics</li> <li>Typing Considerations</li> <li>Unsupported Programs</li> </ul>"},{"location":"integrating_bodo/#scalable-data-io-with-bodo","title":"Scalable Data I/O with Bodo","text":"<p>This guide demonstrates how to use Bodo's file I/O APIs to read and write data.</p>"},{"location":"integrating_bodo/#iceberg","title":"Iceberg","text":"<p>This guide demonstrates how to use Bodo for reading and writing Iceberg tables.</p>"},{"location":"integrating_bodo/#bodo-cloud-platform","title":"Bodo Cloud Platform","text":"<p>This set of guides explains the basics of using the Bodo cloud platform and associated concepts.</p> <ul> <li>Organization Basics</li> <li>Creating a Cluster</li> <li>Using Notebooks</li> <li>Running Jobs</li> <li>Native SQL with Catalogs</li> <li>Instance Role for a Cluster</li> <li>Managing Packages on the cluster using Jupyter magics - Conda and Pip</li> <li>Running shell commands on the cluster using Jupyter magics</li> <li>Connecting to a Cluster</li> <li>Troubleshooting</li> </ul>"},{"location":"integrating_bodo/#using-regular-python-inside-jit-with-bodowrap_python","title":"Using Regular Python inside JIT with @bodo.wrap_python","text":"<p>This guide teaches you how to can interleave regular Python code with Bodo functions using Bodo's object mode.</p>"},{"location":"integrating_bodo/#measuring-performance","title":"Measuring Performance","text":"<p>This guide provides an overview of how to correctly measure performance of your Bodo code. </p>"},{"location":"integrating_bodo/#caching","title":"Caching","text":"<p>This guide outlines how caching works in Bodo and best practices for using it.</p>"},{"location":"integrating_bodo/#inlining","title":"Inlining","text":"<p>This guide discusses advanced Bodo feature that allows you to inline functions to perform additional compiler optimizations.</p>"},{"location":"integrating_bodo/#bodo-errors","title":"Bodo Errors","text":"<p>This guide provides a list of common Bodo errors and tips on how to resolve them.</p>"},{"location":"integrating_bodo/#compilation-tips","title":"Compilation Tips","text":"<p>This guide provides a list of tips to help you optimize your Bodo code for compilation and get the most out of Bodo. </p>"},{"location":"integrating_bodo/#verbose-mode","title":"Verbose Mode","text":"<p>For advanced developers, this guide provides an overview of Bodo's verbose mode and how to use it to debug your code. </p>"},{"location":"integrating_bodo/#deploying-bodo-with-kubernetes","title":"Deploying Bodo with Kubernetes","text":"<p>This guide walks through an example showing how to deploy a Bodo application with Kubernetes.</p> <p>Don't see what you're looking for? Check out the Bodo API Reference for more information.</p>"},{"location":"integrating_bodo/data_visualization/","title":"Data Visualization","text":"<p>[!NOTE]</p> <p>This is an experimental feature and may not function as intended in all scenarios. Please share any feedback or issues encountered for further improvement.</p> <p>Bodo supports Matplotlib visualization natively inside JIT functions. This page specifies the supported Matplotlib APIs and classes. In general, these APIs support all arguments except for the restrictions specified in each section.</p>"},{"location":"integrating_bodo/data_visualization/#plotting-apis","title":"Plotting APIs","text":"<p>Currently, Bodo automatically supports the following plotting APIs.</p> <ul> <li><code>matplotlib.pyplot.plot</code></li> <li><code>matplotlib.pyplot.scatter</code></li> <li><code>matplotlib.pyplot.bar</code></li> <li><code>matplotlib.pyplot.contour</code></li> <li><code>matplotlib.pyplot.contourf</code></li> <li><code>matplotlib.pyplot.quiver</code></li> <li><code>matplotlib.pyplot.pie</code>      (<code>autopct</code> must be a constant boolean or omitted)</li> <li><code>matplotlib.pyplot.fill</code></li> <li><code>matplotlib.pyplot.fill_between</code></li> <li><code>matplotlib.pyplot.step</code></li> <li><code>matplotlib.pyplot.errorbar</code></li> <li><code>matplotlib.pyplot.barbs</code></li> <li><code>matplotlib.pyplot.eventplot</code></li> <li><code>matplotlib.pyplot.hexbin</code></li> <li><code>matplotlib.pyplot.xcorr</code>     (<code>autopct</code> must be a constant boolean or omitted)</li> <li><code>matplotlib.pyplot.imshow</code></li> <li><code>matplotlib.pyplot.plot</code></li> <li><code>matplotlib.pyplot.scatter</code></li> <li><code>matplotlib.pyplot.bar</code></li> <li><code>matplotlib.axes.Axes.contour</code></li> <li><code>matplotlib.axes.Axes.contourf</code></li> <li><code>matplotlib.axes.Axes.quiver</code></li> <li><code>matplotlib.axes.Axes.pie</code>     (<code>usevlines</code> must be a constant boolean or omitted)</li> <li><code>matplotlib.axes.Axes.fill</code></li> <li><code>matplotlib.axes.Axes.fill_between</code></li> <li><code>matplotlib.axes.Axes.step</code></li> <li><code>matplotlib.axes.Axes.errorbar</code></li> <li><code>matplotlib.axes.Axes.barbs</code></li> <li><code>matplotlib.axes.Axes.eventplot</code></li> <li><code>matplotlib.axes.Axes.hexbin</code></li> <li><code>matplotlib.axes.Axes.xcorr</code>     (<code>usevlines</code> must be a constant boolean or omitted)</li> <li><code>matplotlib.axes.Axes.imshow</code></li> </ul> <p>These APIs have the following restrictions:</p> <ul> <li>The data being plotted must be Numpy arrays and not Pandas data     structures.</li> <li>Use of lists is not currently supported. If you need to plot     multiple arrays use a tuple or a 2D Numpy array.</li> </ul> <p>These functions work by automatically gathering all of the data onto one machine and then plotting the data. If there is not enough memory on your machine, a sample of the data can be selected. The example code below demonstrates calling plot with a sample of the data:</p> <pre><code>import matplotlib.pyplot as plt\n\n%matplotlib inline\n\n@bodo.jit\ndef dist_plot(n):\n    X = np.arange(n)\n    Y = np.exp(-X/3.0)\n    plt.plot(X[::10], Y[::10]) # gather every 10th element\n    plt.show()\n\ndist_plot(100)\n</code></pre> <pre><code>[output:0]\n</code></pre> <p></p>"},{"location":"integrating_bodo/data_visualization/#formatting-apis","title":"Formatting APIs","text":"<p>In addition to plotting, we also support a variety of formatting APIs to modify your figures.</p> <ul> <li><code>matplotlib.pyplot.gca</code></li> <li><code>matplotlib.pyplot.gcf</code></li> <li><code>matplotlib.pyplot.text</code></li> <li><code>matplotlib.pyplot.subplots</code>     (<code>nrows</code> and <code>ncols</code> must be constant integers)</li> <li><code>matplotlib.pyplot.suptitle</code></li> <li><code>matplotlib.pyplot.tight_layout</code></li> <li><code>matplotlib.pyplot.savefig</code></li> <li><code>matplotlib.pyplot.draw</code></li> <li><code>matplotlib.pyplot.show</code>      (Output is only displayed on rank 0)</li> <li><code>matplotlib.figure.Figure.suptitle</code></li> <li><code>matplotlib.figure.Figure.tight_layout</code></li> <li><code>matplotlib.figure.Figure.subplots</code>     (<code>nrows</code> and <code>ncols</code> must be constant integers)</li> <li><code>matplotlib.figure.Figure.show</code>     (Output is only displayed on rank 0)</li> <li><code>matplotlib.axes.Axes.annotate</code></li> <li><code>matplotlib.axes.Axes.text</code></li> <li><code>matplotlib.axes.Axes.set_xlabel</code></li> <li><code>matplotlib.axes.Axes.set_ylabel</code></li> <li><code>matplotlib.axes.Axes.set_xscale</code></li> <li><code>matplotlib.axes.Axes.set_yscale</code></li> <li><code>matplotlib.axes.Axes.set_xticklabels</code></li> <li><code>matplotlib.axes.Axes.set_yticklabels</code></li> <li><code>matplotlib.axes.Axes.set_xlim</code></li> <li><code>matplotlib.axes.Axes.set_ylim</code></li> <li><code>matplotlib.axes.Axes.set_xticks</code></li> <li><code>matplotlib.axes.Axes.set_yticks</code></li> <li><code>matplotlib.axes.Axes.set_axis_on</code></li> <li><code>matplotlib.axes.Axes.set_axis_off</code></li> <li><code>matplotlib.axes.Axes.draw</code></li> <li><code>matplotlib.axes.Axes.set_title</code></li> <li><code>matplotlib.axes.Axes.legend</code></li> <li><code>matplotlib.axes.Axes.grid</code></li> </ul> <p>In general these APIs support all arguments except for the restrictions specified. In addition, APIs have the following restrictions:</p> <ul> <li>Use of lists is not currently supported. If you need to provide a     list, please use a tuple instead.</li> <li>Formatting functions execute on all ranks by default. If you need     to execute further Matplotlib code on all of your processes,     please close any figures you opened inside Bodo.</li> </ul>"},{"location":"integrating_bodo/data_visualization/#matplotlib_classes","title":"Matplotlib Classes","text":"<p>Bodo supports the following Matplotlib classes when used with the previously mentioned APIs:</p> <ul> <li><code>matplotlib.figure.Figure</code></li> <li><code>matplotlib.axes.Axes</code></li> <li><code>matplotlib.text.Text</code></li> <li><code>matplotlib.text.Annotation</code></li> <li><code>matplotlib.lines.Line2D</code></li> <li><code>matplotlib.collections.PathCollection</code></li> <li><code>matplotlib.container.BarContainer</code></li> <li><code>matplotlib.contour.QuadContourSet</code></li> <li><code>matplotlib.quiver.Quiver</code></li> <li><code>matplotlib.patches.Wedge</code></li> <li><code>matplotlib.patches.Polygon</code></li> <li><code>matplotlib.collections.PolyCollection</code></li> <li><code>matplotlib.image.AxesImage</code></li> <li><code>matplotlib.container.ErrorbarContainer</code></li> <li><code>matplotlib.quiver.Barbs</code></li> <li><code>matplotlib.collections.EventCollection</code></li> <li><code>matplotlib.collections.LineCollection</code></li> </ul>"},{"location":"integrating_bodo/data_visualization/#working-with-unsupported-apis","title":"Working with Unsupported APIs","text":"<p>For other visualization functions, you can call them from regular Python and manually gather the data. If the data does not fit in a single machine's memory, you may need to take a sample. The example code below demonstrates gathering a portion of data in Bodo and calling polar (which Bodo doesn't support yet) in regular Python:</p> <pre><code>import bodo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n@bodo.jit\ndef dist_gather_test(n):\n    X = np.arange(n)\n    Y = 3 - np.cos(X)\n    return bodo.gatherv(X[::10]), bodo.gatherv(Y[::10])  # gather every 10th element\n\nX_Sample, Y_Sample = dist_gather_test(1000)\nif bodo.get_rank() == 0:\n    plt.polar(X_Sample, Y_Sample)\n    plt.show()\n</code></pre>"},{"location":"integrating_bodo/database_catalog/","title":"Native SQL with Database Catalogs","text":"<p>Database Catalogs are configuration objects that grant BodoSQL access to load tables from a remote database.  Bodo platform now supports adding Database catalogs through the UI and provides users the option to write native SQL code to run on the tables in the connected remote database.  </p>"},{"location":"integrating_bodo/database_catalog/#adding-a-database-catalog","title":"Adding a Database Catalog","text":"<p> Supported On AWS \u00b7  Supported on Azure </p> <p>In your workspaces view, navigate to the Catalogs section in the sidebar. Click on CREATE CATALOG and fill up the form with the required values.  </p> <p> </p> <p>Currently, we support Snowflake Database Catalogs, and AWS Glue Catalogs on the Bodo Platform. See <code>SnowflakeCatalog</code> and <code>GlueCatalog</code> for details on the required parameters.</p> <p>Upon submitting the form, you will see that your Catalog has been created and is now available to use in your interactive notebook. </p> <p></p>"},{"location":"integrating_bodo/database_catalog/#running-a-job-with-a-database-catalog","title":"Running a Job With a Database Catalog","text":"<p> Supported On AWS \u00b7  Supported on Azure </p> <p>To run a SQL job with the database catalog you need to create a job template in the jobs tab.</p> <p></p> <p>Configure the job template normally and under Advanced Options, you can select the Catalog you want to use.</p> <p></p> <p>Catalogs can also be given to bodosdk jobs. When the job is run, the SQL code will be executed on the tables in the connected remote database.</p>"},{"location":"integrating_bodo/database_catalog/#using-database-catalogs-in-interactive-notebooks","title":"Using Database Catalogs in Interactive Notebooks","text":"<p> Supported On AWS \u00b7  Supported on Azure</p> <p>Important</p> <p>Using Database Catalogs in Interactive Notebooks is only supported for Snowflake Database Catalogs.</p> <p>When you create a code cell in your interactive notebook, you will notice a blue selector on the top right hand corner of the code cell. By default, this will be set to Parallel-Python. This means that any code written in this cell will execute on all cores of the attached cluster. </p> <p></p> <p>To enable running native SQL code, you can set the cell type in the blue selector to SQL, and you  will need to select your Catalog from the Catalog selector to the left of the cell type selector as shown in the  figure below. </p> <p></p> <p>The output of the SQL query is automatically saved in a distributed dataframe named LAST_SQL_OUTPUT. This dataframe will be overwritten every time a SQL query is run. </p>"},{"location":"integrating_bodo/database_catalog/#viewing-database-catalogs-data","title":"Viewing Database Catalogs Data","text":"<p>To view the connection data stored in a catalog first connect to a cluster and then run the following in a code cell:</p> <pre><code>import bodo_platform_utils\nbodo_platform_utils.catalog.get_data(\"catalog_name\")\n</code></pre> <p>See Also</p> <p>Database Catalogs, BodoSDK Catalog API</p>"},{"location":"integrating_bodo/kubernetes/","title":"Deploying Bodo with Kubernetes","text":"<p>This section demonstrates an example showing how to deploy a Bodo application with Kubernetes.  We deploy Bodo with the Kubeflow MPI-Operator. </p>"},{"location":"integrating_bodo/kubernetes/#setting-up","title":"Setting Up","text":"<p>You need the following to deploy your Bodo application using Kubernetes:</p> <ul> <li> <p>Access to a Kubernetes cluster.</p> <p>For this example, we'll use kops on AWS. See the section below on creating a Kubernetes cluster to see how we set it up.</p> </li> <li> <p>A Docker image containing the Bodo application scripts and their intended Bodo version made available on a Docker registry, so that Kubernetes can pull it.</p> <p>For this example, we created a Docker image using this Dockerfile and uploaded it to Docker Hub. It includes a Bodo application called <code>pi.py</code> that calculates the value of pi using the Monte Carlo method, and can be used to validate your setup.</p> <p>You can use this as a base image for your own Docker image. If you want to use a private registry, you can follow the instructions here.</p> </li> </ul> <p>Warning</p> <p>Make sure to provide the correct CPU and Memory requests in the YAML file for your Bodo jobs. If correct values are not provided or the cluster doesn't have sufficient CPU or Memory required for the job, the job will be terminated and worker pods may keep respawning. You can get a good estimate of the CPU and Memory requirements by extrapolation from running the job locally on a smaller dataset.</p>"},{"location":"integrating_bodo/kubernetes/#kops","title":"Creating a Kubernetes Cluster using KOPS","text":"<p>Here are the steps create an AWS EKS cluster using KOPS.</p> <ul> <li> <p>Install KOPS on your local machine:</p> <pre><code># Mac\nbrew install kops\n\n# Linux\ncurl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64\nchmod +x kops-linux-amd64\nsudo mv kops-linux-amd64 /usr/local/bin/kops\n</code></pre> </li> <li> <p>Create a location to store your cluster configuration:</p> <p>First you need to create an S3 bucket to use as your <code>KOPS_STATE_STORE</code>.</p> <pre><code>export KOPS_CLUSTER_NAME=imesh.k8s.local\nexport KOPS_STATE_STORE=s3://&lt;your S3 bucket name&gt;\n</code></pre> </li> <li> <p>Create your cluster:</p> <p>The following code block creates a cluster of 2 nodes each with 4 cores . You can modify the <code>node-count</code> argument to change the number of instances. To change the number of worker nodes, update <code>node-size</code>. You can deploy the cluster in a different AWS region and availability zone by modifying the <code>zones</code> argument.</p> <pre><code>kops create cluster \\\n--node-count=2 \\\n--node-size=c5.2xlarge \\\n--control-plane-size=c5.large \\\n--zones=us-east-2c \\\n--name=${KOPS_CLUSTER_NAME}\n</code></pre> <p>Tip</p> <p>The parameter <code>control-plane-size</code> refers to the leader that manages K8s but doesn\u2019t do any Bodo computation, so you should keep the instance size small.</p> </li> <li> <p>Finish creating the cluster with the following command.</p> <pre><code>kops update cluster --name $KOPS_CLUSTER_NAME --yes --admin\n</code></pre> <p>Note</p> <p>This might take several minutes to finish.</p> </li> <li> <p>Verify that the cluster setup is finished by running:</p> <pre><code>kops validate cluster\n</code></pre> </li> </ul>"},{"location":"integrating_bodo/kubernetes/#deploying-bodo-on-a-kubernetes-cluster-manually","title":"Deploying Bodo on a Kubernetes Cluster Manually","text":""},{"location":"integrating_bodo/kubernetes/#install-mpijob-custom-resource-definitionscrd","title":"Install MPIJob Custom Resource Definitions(CRD)","text":"<p>The most up-to-date installation guide is available at MPI-Operator Github. This example was tested using v0.4.0, as shown below:</p> <pre><code>git clone https://github.com/kubeflow/mpi-operator --branch v0.4.0\ncd mpi-operator\nkubectl apply -f deploy/v2beta1/mpi-operator.yaml\n</code></pre> <p>You can check whether the MPI Job custom resource is installed via:</p> <pre><code>kubectl get crd\n</code></pre> <p>The output should include <code>mpijobs.kubeflow.org</code> similar to:</p> <pre><code>NAME                   CREATED AT\nmpijobs.kubeflow.org    2024-04-02T19:43:04Z\n</code></pre>"},{"location":"integrating_bodo/kubernetes/#run-your-bodo-application","title":"Run your Bodo application","text":"<ol> <li>Define a kubernetes resource for your Bodo workload, such as the one defined in <code>mpijob.yaml</code> that runs the pi example. You can modify it based on your cluster configuration:</li> <li>Update <code>spec.slotsPerWorker</code> with the number of physical cores (not vCPUs) on each node</li> <li>Set <code>spec.mpiReplicaSpecs.Worker.replicas</code> to the number of worker nodes in your cluster.</li> <li>Build the image using the Dockerfile or use <code>bodoaidocker/bodo-kubernetes</code> and replace the image at    <code>spec.mpiReplicaSpecs.Launcher.template.spec.containers.image</code> and <code>spec.mpiReplicaSpecs.Worker.template.spec.containers.image</code>.</li> <li>Check the container arguments is referring to the python file you have intended to run         <pre><code> args:\n    - mpirun\n    - -n\n    - \"8\"\n    - python\n    - /home/mpiuser/pi.py\n</code></pre></li> <li>Lastly, make sure <code>-n</code> is equal to <code>spec.mpiReplicaSpecs.Worker.replicas</code> multiplied by <code>spec.slotsPerWorker</code>, i.e. the total number of physical cores on your worker nodes.</li> <li>Run the example by deploying it in your cluster with <code>kubectl create -f mpijob.yaml</code>. This should add 1 pod to each worker and a launcher pod to your master node.</li> <li>View the generated pods by this deployment with <code>kubectl get pods</code>. You may inspect any logs by looking at the individual pod's logs.</li> </ol>"},{"location":"integrating_bodo/kubernetes/#retrieve-the-results","title":"Retrieve the Results","text":"<pre><code>When the job finishes running, your launcher pod will change its status to completed and any stdout information can be found in the logs of the launcher pod:\n\n```shell\nPODNAME=$(kubectl get pods -o=name)\nkubectl logs -f ${PODNAME}\n```\n</code></pre>"},{"location":"integrating_bodo/kubernetes/#teardown","title":"Teardown","text":"<ul> <li>When a job has finished running, you can remove it by running <code>kubectl delete -f mpijob.yaml</code>.</li> <li>If you want to delete the MPI-Operator crd, please follow the steps on the MPI-Operator Github repository.</li> </ul>"},{"location":"integrating_bodo/spark/","title":"Spark Examples","text":"<p>Bodo offers simplicity and maintainability of Python codes while unlocking orders of magnitude performance improvement. Spark APIs are usually equivalent to simpler Python/Pandas APIs, which are automatically parallelized by Bodo. This page aims to assist spark users with their transition to Bodo. Here, we show the most common data wrangling methods in PySpark and Pandas through brief code examples. We used the COVID-19 World Vaccination Progress dataset that can be downloaded from Kaggle. If you want to execute the code as shown below, make sure that you have Bodo installed. Here is a list of examples. </p>"},{"location":"integrating_bodo/spark/#Environment","title":"Environment Setup","text":"<p>With Bodo: <pre><code>import bodo\nimport pandas as pd\nimport numpy as np \n</code></pre></p> <p>With PySpark: <pre><code>from pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Migration From Spark\") \\\n    .getOrCreate()\n</code></pre></p>"},{"location":"integrating_bodo/spark/#Load","title":"Load Data","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df'])\ndef load_data():\ndf = pd.read_csv('country_vaccinations_by_manufacturer.csv')\n    return df\n\ndf = load_data()\n</code></pre> <p>With PySpark:</p> <pre><code>data = spark.read.csv('country_vaccinations_by_manufacturer.csv', header = True)\n</code></pre>"},{"location":"integrating_bodo/spark/#Display","title":"Display the Schema of the DataFrame","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df'])\ndef schema(df):\n    print(df.dtypes)\n\nschema(df)\n</code></pre> <p>With PySpark:</p> <pre><code>print(data.printSchema())\n</code></pre>"},{"location":"integrating_bodo/spark/#Change","title":"Change Data Types of the DataFrame","text":"<p>With Bodo:</p> <pre><code>    @bodo.jit(distributed = ['df'])\n    def load_data():\n        df = pd.read_csv('country_vaccinations_by_manufacturer.csv', \n                         dtype = {'location' : 'str', 'vaccine' : 'str',\n                                  'total_vaccinations' : 'Int64'}, \n                         parse_dates=['date'])\n        print(df.info())\n        return df\n\n    df = load_data()\n</code></pre> <p>With PySpark:</p> <pre><code>from pyspark.sql.types import StructField,IntegerType, StringType, DateType, StructType\n\nnew_schema = [StructField('location', StringType(), True),\n              StructField('date', DateType(), True), \n              StructField('vaccine', StringType(), True),\n              StructField('total_vaccinations', IntegerType(), True)]\n\ndata = spark.read.csv('country_vaccinations_by_manufacturer.csv', header = True,\n                  schema = StructType(fields = new_schema))\ndata.printSchema()\n</code></pre>"},{"location":"integrating_bodo/spark/#Display","title":"Display the Head of the DataFrame","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df'])\ndef head_data(df):\n    print(df.head())\n\nhead_data(df)\n</code></pre> <p>With PySpark:</p> <pre><code>data.show(5)\ndata.take(5)\n</code></pre>"},{"location":"integrating_bodo/spark/#Select","title":"Select Columns from the DataFrame","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df', 'df_columns'])\ndef load_data(df):\ndf_columns = df[['location', 'vaccine']]\n    return df_columns\n\ndf_columns = load_data(df)\n</code></pre> <p>With PySpark:</p> <pre><code>data_columns = data.select('location', 'vaccine').show()\n</code></pre>"},{"location":"integrating_bodo/spark/#Show","title":"Show the Statistics of the DataFrame","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df'])\ndef get_describe(df):\n    print(df.describe())\n\nget_describe(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>data.describe().show()\n</code></pre>"},{"location":"integrating_bodo/spark/#Drop","title":"Drop Duplicate Values","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df', 'df_cleaned'])\ndef drop(df):\n    df_cleaned = df.drop_duplicates()\n    return df_cleaned\n\ndf_cleaned = drop(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>data.dropDuplicates().show()\n</code></pre>"},{"location":"integrating_bodo/spark/#Missing","title":"Missing Values","text":""},{"location":"integrating_bodo/spark/#count-na","title":"Count NA","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df'])\ndef count_na(df):\n    print(df.isnull().sum())\n\ncount_na(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>from pyspark.sql.functions import isnan, when, count, col\n\ndata.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_s.columns]).show()\n</code></pre>"},{"location":"integrating_bodo/spark/#drop-na","title":"Drop NA","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df', 'df_valid'])\ndef drop_na(df):\n    df_valid = df.dropna(how ='any')\n    return df_valid\n\ndf_valid = drop_na(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>data_valid = data.dropna(how='any')\n</code></pre>"},{"location":"integrating_bodo/spark/#replace-na","title":"Replace NA","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df', 'df_filled'])\ndef replace_na(df):\n    df_filled = df.fillna(0)\n    return df_filled\n\ndf_filled = replace_na(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>data_replaced = data.na.fill(value = 0)\n</code></pre>"},{"location":"integrating_bodo/spark/#DateTime","title":"DateTime Manipulation","text":"<p>Convert String to Datetime :</p> <p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df'])\ndef convert_date(df):\n    df['record_date'] = pd.to_datetime(df['date'])\n    return df\n\ndf = convert_date(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>from pyspark.sql.types import DateType\n\ndata = data.withColumn(\"record_date\", data[\"date\"].cast(DateType()))\n</code></pre> <p>Extract Day / Month / Year from Datetime :</p> <p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df'])\ndef extract_date(df):\n    print(df['record_date'].dt.year)\n\nextract_date(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>from pyspark.sql.functions import year\n\ndata.select(year(df_s.record_date)).show()\n</code></pre>"},{"location":"integrating_bodo/spark/#Filter","title":"Filter Data Based on Conditions","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df', 'df_filtered'])\ndef sort_data(df):\n    df_filtered = df[df.vaccine =='Pfizer/BioNTech']\n    return df_filtered\n\ndf_filtered = sort_data(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>data_filtered = data.where(data.vaccine =='Pfizer/BioNTech')\n</code></pre>"},{"location":"integrating_bodo/spark/#Aggregation","title":"Aggregation Functions: (sum, count, mean, max, min, etc)","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df'])\ndef group_by(df):\n    print(df.groupby('location').agg({'total_vaccinations' : 'sum'}))\n\ngroup_by(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>data.groupBy('location').agg({'total_vaccinations' : 'sum'}).show()\n</code></pre>"},{"location":"integrating_bodo/spark/#Sort","title":"Sort Data","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df', 'df_sorted'])\ndef sort_data(df):\n    df_sorted = df.sort_values(by = ['total_vaccinations'], ascending=False)\n    return df_sorted\n\ndf_sorted = sort_data(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>from pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import desc \n\ndata_sorted = data.withColumn(\"total_vaccinations\", col(\"total_vaccinations\") \n              .cast(IntegerType())).select(\"total_vaccinations\") \n                  .sort(desc('total_vaccinations')).show()\n</code></pre>"},{"location":"integrating_bodo/spark/#Rename","title":"Rename Columns","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df', 'df_renamed'])\ndef rename_column(df):\n    df_renamed = df.rename(columns = {'location' : 'country'}, inplace = True)\n\n    return data_renamed\n\ndf_renamed = rename_column(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>data_renamed = data.withColumnRenamed(\"location\",\"country\").show()\n</code></pre>"},{"location":"integrating_bodo/spark/#Create","title":"Create New Columns","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df'])\ndef create_column(df):\n    df['doubled'] = 2 * df['total_vaccinations']\n    return df\n\ndf = create_column(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>from pyspark.sql.functions import col\n\ndata = data.withColumn(\"doubled\", 2*col(\"total_vaccinations\")).show()\n</code></pre>"},{"location":"integrating_bodo/spark/#User-Defined","title":"User-Defined Functions","text":"<p>With Bodo:</p> <pre><code>@bodo.jit(distributed = ['df'])\ndef udf(df):\n    df['new_column'] = df['location'].apply(lambda x: x.upper())\n    return df\n\ndf = udf(df)\n</code></pre> <p>With Pyspark:</p> <pre><code>from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\npyspark_udf = udf(lambda x: x.upper(), StringType())\ndata = data.withColumn(\"new_column\", pyspark_udf(df_s.location)).show()\n</code></pre>"},{"location":"integrating_bodo/spark/#Create","title":"Create a DataFrame","text":"<p>With Bodo:</p> <pre><code>@bodo.jit\ndef create():\n    df = pd.DataFrame({'id': [1, 2], 'label': [\"one\", \"two\"]})\n    return df\n\ndf = create()\n</code></pre> <p>With Pyspark:</p> <pre><code>data = spark.createDataFrame([(1, \"one\"),(2, \"two\"),],[\"id\", \"label\"])\n</code></pre>"},{"location":"integrating_bodo/spark/#Export","title":"Export the Data","text":"<p>With Bodo:</p> <pre><code>@bodo.jit\ndef export_data():\n    df = pd.DataFrame({'id': [1, 2], 'label': [\"one\", \"two\"]})\n    df_pandas = df.to_csv('pandas_data.csv')\n    return df_pandas\n\nexport_data()\n</code></pre> <p>With Pyspark:</p> <pre><code>df = spark.createDataFrame([(1, \"one\"),(2, \"two\"),],[\"id\", \"label\"])\ndf_spark.write.csv(\"df_spark.csv\", header = True)\n</code></pre>"},{"location":"integrating_bodo/sparkcheatsheet/","title":"PySpark Bodo Cheatsheet","text":"<p>References of PySpark methods and their Python equivalents supported by Bodo.</p>"},{"location":"integrating_bodo/sparkcheatsheet/#pssession","title":"pyspark.sql.SparkSession","text":"<p>The table below is a reference of SparkSession methods and their equivalents in Python, which are supported by Bodo.</p> PySpark Method Python Equivalent <code>pyspark.sql.SparkSession.read.csv</code> - <code>pd.read_csv()</code> <code>pyspark.sql.SparkSession.read.text</code> <code>pd.read_csv(\"file.txt\", sep=\"\\n\", names=[\"value\"], dtype={\"value\": \"str\"})</code> <code>pyspark.sql.SparkSession.read.parquet</code> <code>pd.read_parquet()</code> <code>pyspark.sql.SparkSession.read.json</code> <code>pd.read_json()</code>"},{"location":"integrating_bodo/sparkcheatsheet/#psdataframe","title":"pyspark.sql.DataFrame","text":"<p>The table below is a reference of Spark DataFrame methods and their equivalents in Python, which are supported by Bodo.</p> PySpark Method Python Equivalent <code>pyspark.sql.DataFrame.alias</code> <code>alias = df</code> <code>pyspark.sql.DataFrame.approxQuantile</code> <code>df[['A', 'B', 'C']].quantile(q)</code> <code>pyspark.sql.DataFrame.columns</code> <code>df.columns</code> <code>pyspark.sql.DataFrame.corr</code> <code>df[['A', 'B']].corr()</code> <code>pyspark.sql.DataFrame.count</code> <code>df.count()</code> <code>pyspark.sql.DataFrame.cov</code> <code>df[['A', 'B']].cov()</code> <code>pyspark.sql.DataFrame.crossJoin</code> <code>df1.assign(key=1).merge(df2.assign(key=1), on=\"key\").drop(\"key\", axis=1)</code> <code>pyspark.sql.DataFrame.describe</code> <code>df.describe()</code> <code>pyspark.sql.DataFrame.distinct</code> <code>df.distinct()</code> <code>pyspark.sql.DataFrame.drop</code> <code>df.drop(col, axis=1)</code> <code>pyspark.sql.DataFrame.dropDuplicates</code> <code>df.drop_duplicates()</code> <code>pyspark.sql.DataFrame.drop_duplicates</code> <code>df.drop_duplicates()</code> <code>pyspark.sql.DataFrame.dropna</code> <code>df.dropna()</code> <code>pyspark.sql.DataFrame.fillna</code> <code>df.fillna(value)</code> <code>pyspark.sql.DataFrame.filter</code> <code>df[cond]</code> <code>pyspark.sql.DataFrame.first</code> <code>df.head(1)</code> <code>pyspark.sql.DataFrame.foreach</code> <code>df.apply(f, axis=1)</code> <code>pyspark.sql.DataFrame.groupBy</code> <code>df.groupby(\"col\")</code> <code>pyspark.sql.DataFrame.groupby</code> <code>df.groupby(\"col\")</code> <code>pyspark.sql.DataFrame.head</code> <code>df.head(n)</code> <code>pyspark.sql.DataFrame.intersect</code> <code>pd.merge(df1[['col1', 'col2']].drop_duplicates(), df2[['col1', 'col2']].drop_duplicates(), on =['col1', 'col2'])</code> <code>pyspark.sql.DataFrame.intersectAll</code> <code>pd.merge(df1[['col1', 'col2']], df2[['col1', 'col2']].drop_duplicates(), on =['col1', 'col2'])</code> <code>pyspark.sql.DataFrame.join</code> <code>df1.join(df2)</code> <code>pyspark.sql.DataFrame.orderBy</code> <code>df.sort_values('colname')</code> <code>pyspark.sql.DataFrame.show</code> <code>print(df.head(n))</code> <code>pyspark.sql.DataFrame.sort</code> <code>df.sort_values('colname')</code>"},{"location":"integrating_bodo/sparkcheatsheet/#psfunctions","title":"pyspark.sql.functions","text":"<p>The table below is a reference of Spark SQL functions and their equivalents in Python, which are supported by Bodo.</p> PySpark Function Python Equivalent <code>pyspark.sql.functions.abs</code> <code>df.col.abs()</code> <code>pyspark.sql.functions.acos</code> <code>np.arccos(df.col)</code> <code>pyspark.sql.functions.acosh</code> <code>np.arccosh(df.col)</code> <code>pyspark.sql.functions.add_months</code> <code>df.col + pd.DateOffset(months=num_months)</code> <code>pyspark.sql.functions.approx_count_distinct</code> <code>df.col.nunique()</code> <code>pyspark.sql.functions.array_contains</code> <code>df.col.apply(lambda a, value: value in a, value=value)</code> <code>pyspark.sql.functions.array_distinct</code> <code>df.col.map(lambda x: np.unique(x))</code> <code>pyspark.sql.functions.array_except</code> <code>df[['col1', 'col2']].apply(lambda x: np.setdiff1d(x[0], x[1]), axis=1)</code> <code>pyspark.sql.functions.array_join</code> <code>df.col.apply(lambda x, sep: sep.join(x), sep=sep)</code> <code>pyspark.sql.functions.array_max</code> <code>df.col.map(lambda x: np.nanmax(x))</code> <code>pyspark.sql.functions.array_min</code> <code>df.col.map(lambda x: np.nanmin(x))</code> <code>pyspark.sql.functions.array_position</code> <code>df.col.apply(lambda x, value: np.append(np.where(x == value)[0], -1)[0], value=value)</code> <code>pyspark.sql.functions.array_repeat</code> <code>df.col.apply(lambda x, count: np.repeat(x, count), count=count)</code> <code>pyspark.sql.functions.array_sort</code> <code>df.col.map(lambda x: np.sort(x))</code> <code>pyspark.sql.functions.array_union</code> <code>df[['col1', 'col2']].apply(lambda x: np.union1d(x[0], x[1]), axis=1)</code> <code>pyspark.sql.functions.array_overlap</code> <code>df[['A', 'B']].apply(lambda x: len(np.intersect1d(x[0], x[1])) &gt; 0, axis=1)</code> <code>pyspark.sql.functions.asc</code> <code>df.sort_values('col')</code> <code>pyspark.sql.functions.asc_nulls_first</code> <code>df.sort_values('col', na_position='first')</code> <code>pyspark.sql.functions.asc_nulls_last</code> <code>df.sort_values('col')</code> <code>pyspark.sql.functions.ascii</code> <code>df.col.map(lambda x: ord(x[0]))</code> <code>pyspark.sql.functions.asin</code> <code>np.arcsin(df.col)</code> <code>pyspark.sql.functions.asinh</code> <code>np.arcsinh(df.col)</code> <code>pyspark.sql.functions.atan</code> <code>np.arctan(df.col)</code> <code>pyspark.sql.functions.atanh</code> <code>np.arctanh(df.col)</code> <code>pyspark.sql.functions.atan2</code> <code>df[['col1', 'col2']].apply(lambda x: np.arctan2(x[0], x[1]), axis=1)</code> <code>pyspark.sql.functions.avg</code> <code>df.col.mean()</code> <code>pyspark.sql.functions.bin</code> <code>df.col.map(lambda x: \"{0:b}\".format(x))</code> <code>pyspark.sql.functions.bitwiseNOT</code> <code>np.invert(df.col)</code> <code>pyspark.sql.functions.bround</code> <code>df.col.apply(lambda x, scale: np.round(x, scale), scale=scale)</code> <code>pyspark.sql.functions.cbrt</code> <code>df.col.map(lambda x: np.cbrt(x))</code> <code>pyspark.sql.functions.ceil</code> <code>np.ceil(df.col)</code> <code>pyspark.sql.functions.col</code> <code>df.col</code> <code>pyspark.sql.functions.collect_list</code> <code>df.col.to_numpy()</code> <code>pyspark.sql.functions.collect_set</code> <code>np.unique(df.col.to_numpy())</code> <code>pyspark.sql.functions.column</code> <code>df.col</code> <code>pyspark.sql.functions.corr</code> <code>df[['col1', 'col2']].corr(method = 'pearson')</code> <code>pyspark.sql.functions.cos</code> <code>np.cos(df.col)</code> <code>pyspark.sql.functions.cosh</code> <code>np.cosh(df.col)</code> <code>pyspark.sql.functions.count</code> <code>df.col.count()</code> <code>pyspark.sql.functions.countDistinct</code> <code>df.col.drop_duplicates().count()</code> <code>pyspark.sql.functions.current_date</code> <code>datetime.datetime.now().date()</code> <code>pyspark.sql.functions.current_timestamp</code> <code>datetime.datetime.now()</code> <code>pyspark.sql.functions.date_add</code> <code>df.col + pd.tseries.offsets.DateOffset(num_days)</code> <code>pyspark.sql.functions.date_format</code> <code>df.col.dt.strftime(format_str)</code> <code>pyspark.sql.functions.date_sub</code> <code>df.col - pd.tseries.offsets.DateOffset(num_days)</code> <code>pyspark.sql.functions.datediff</code> <code>(df.col1 - df.col2).dt.days</code> <code>pyspark.sql.functions.dayofmonth</code> <code>df.col.dt.day</code> <code>pyspark.sql.functions.dayofweek</code> <code>df.col.dt.dayofweek</code> <code>pyspark.sql.functions.dayofyear</code> <code>df.col.dt.dayofyear</code> <code>pyspark.sql.functions.degrees</code> <code>np.degrees(df.col)</code> <code>pyspark.sql.functions.desc</code> <code>df.sort_values('col', ascending=False)</code> <code>pyspark.sql.functions.desc_nulls_first</code> <code>df.sort_values('col', ascending=False, na_position='first')</code> <code>pyspark.sql.functions.desc_nulls_last</code> <code>df.sort_values('col', ascending=False)</code> <code>pyspark.sql.functions.exp</code> <code>np.exp(df.col)</code> <code>pyspark.sql.functions.expm1</code> <code>np.exp(df.col) - 1</code> <code>pyspark.sql.functions.factorial</code> <code>df.col.map(lambda x: math.factorial(x))</code> <code>pyspark.sql.functions.filter</code> <code>df.filter()</code> <code>pyspark.sql.functions.floor</code> <code>np.floor(df.col)</code> <code>pyspark.sql.functions.format_number</code> <code>df.col.apply(lambda x,d : (\"{:,.\" + str(d) + \"f}\").format(np.round(x, d)), d=d)</code> <code>pyspark.sql.functions.format_string</code> <code>df.col.apply(lambda x, format_str : format_str.format(x), format_str=format_str)</code> <code>pyspark.sql.functions.from_unixtime</code> <code>df.col.map(lambda x: pd.Timestamp(x, 's')).dt.strftime(format_str)</code> <code>pyspark.sql.functions.greatest</code> <code>df[['col1', 'col2']].apply(lambda x: np.nanmax(x), axis=1)</code> <code>pyspark.sql.functions.hash</code> <code>df.col.map(lambda x: hash(x))</code> <code>pyspark.sql.functions.hour</code> <code>df.col.dt.hour</code> <code>pyspark.sql.functions.hypot</code> <code>df[['col1', 'col2']].apply(lambda x: np.hypot(x[0], x[1]), axis=1)</code> <code>pyspark.sql.functions.initcap</code> <code>df.col.str.title()</code> <code>pyspark.sql.functions.instr</code> <code>df.col.str.find(sub=substr)</code> <code>pyspark.sql.functions.isnan</code> <code>np.isnan(df.col)</code> <code>pyspark.sql.functions.isnull</code> <code>df.col.isna()</code> <code>pyspark.sql.functions.kurtosis</code> <code>df.col.kurtosis()</code> <code>pyspark.sql.functions.last_day</code> <code>df.col + pd.tseries.offsets.MonthEnd()</code> <code>pyspark.sql.functions.least</code> <code>df.min(axis=1)</code> <code>pyspark.sql.functions.locate</code> <code>df.col.str.find(sub=substr, start=start)</code> <code>pyspark.sql.functions.log</code> <code>np.log(df.col) / np.log(base)</code> <code>pyspark.sql.functions.log10</code> <code>np.log10(df.col)</code> <code>pyspark.sql.functions.log1p</code> <code>np.log(df.col) + 1</code> <code>pyspark.sql.functions.log2</code> <code>np.log2(df.col)</code> <code>pyspark.sql.functions.lower</code> <code>df.col.str.lower()</code> <code>pyspark.sql.functions.lpad</code> <code>df.col.str.pad(len, flllchar=char)</code> <code>pyspark.sql.functions.ltrim</code> <code>df.col.str.lstrip()</code> <code>pyspark.sql.functions.max</code> <code>df.col.max()</code> <code>pyspark.sql.functions.mean</code> <code>df.col.mean()</code> <code>pyspark.sql.functions.min</code> <code>df.col.min()</code> <code>pyspark.sql.functions.minute</code> <code>df.col.dt.minute</code> <code>pyspark.sql.functions.monotonically_increasing_id</code> <code>pd.Series(np.arange(len(df)))</code> <code>pyspark.sql.functions.month</code> <code>df.col.dt.month</code> <code>pyspark.sql.functions.nanvl</code> <code>df[['A', 'B']].apply(lambda x: x[0] if not pd.isna(x[0]) else x[1], axis=1)</code> <code>pyspark.sql.functions.overlay</code> <code>df.A.str.slice_replace(start=index, stop=index+len, repl=repl_str)</code> <code>pyspark.sql.functions.pandas_udf</code> <code>df.apply(f)</code> or <code>df.col.map(f)</code> <code>pyspark.sql.functions.pow</code> <code>np.power(df.col1, df.col2)</code> <code>pyspark.sql.functions.quarter</code> <code>df.col.dt.quarter</code> <code>pyspark.sql.functions.radians</code> <code>np.radians(df.col)</code> <code>pyspark.sql.functions.rand</code> <code>pd.Series(np.random.rand(1, num_cols))</code> <code>pyspark.sql.functions.randn</code> <code>pd.Series(np.random.randn(num_cols))</code> <code>pyspark.sql.functions.regexp_replace</code> <code>df.col.str.replace(pattern, repl_string)</code> <code>pyspark.sql.functions.repeat</code> <code>df.col.str.repeat(count)</code> <code>pyspark.sql.functions.reverse</code> <code>df.col.map(lambda x: x[::-1])</code> <code>pyspark.sql.functions.rint</code> <code>df.col.map(lambda x: int(np.round(x, 0)))</code> <code>pyspark.sql.functions.round</code> <code>df.col.apply(lambda x, decimal_places: np.round(x, decimal_places), decimal_places=decimal_places)</code> <code>pyspark.sql.functions.rpad</code> <code>df.col.str.pad(len, side='right', flllchar=char)</code> <code>pyspark.sql.functions.rtrim</code> <code>df.col.str.rstrip()</code> <code>pyspark.sql.functions.second</code> <code>df.col.dt.second</code> <code>pyspark.sql.functions.sequence</code> <code>df[['col1', 'col2', 'col3']].apply(lambda x: np.arange(x[0], x[1], x[2]), axis=1)</code> <code>pyspark.sql.functions.shuffle</code> <code>df.col.map(lambda x: np.random.permutation(x))</code> <code>pyspark.sql.functions.signum</code> <code>np.sign(df.col)</code> <code>pyspark.sql.functions.sin</code> <code>np.sin(df.col)</code> <code>pyspark.sql.functions.sinh</code> <code>np.sinh(df.col)</code> <code>pyspark.sql.functions.size</code> <code>df.col.map(lambda x: len(x))</code> <code>pyspark.sql.functions.skewness</code> <code>df.col.skew()</code> <code>pyspark.sql.functions.slice</code> <code>df.col.map(lambda x: x[start : end])</code> <code>pyspark.sql.functions.split</code> <code>df.col.str.split(pat, num_splits)</code> <code>pyspark.sql.functions.sqrt</code> <code>np.sqrt(df.col)</code> <code>pyspark.sql.functions.stddev</code> <code>df.col.std()</code> <code>pyspark.sql.functions.stddev_pop</code> <code>df.col.std(ddof=0)</code> <code>pyspark.sql.functions.stddev_samp</code> <code>df.col.std()</code> <code>pyspark.sql.functions.substring</code> <code>df.col.str.slice(start, start+len)</code> <code>pyspark.sql.functions.substring_index</code> <code>df.col.apply(lambda x, sep, count: sep.join(x.split(sep)[:count]), sep=sep, count=count)</code> <code>pyspark.sql.functions.sum</code> <code>df.col.sum()</code> <code>pyspark.sql.functions.sumDistinct</code> <code>df.col.drop_duplicates().sum()</code> <code>pyspark.sql.functions.tan</code> <code>np.tan(df.col)</code> <code>pyspark.sql.functions.tanh</code> <code>np.tanh(df.col)</code> <code>pyspark.sql.functions.timestamp_seconds</code> <code>pd.to_datetime(\"now\")</code> <code>pyspark.sql.functions.to_date</code> <code>df.col.apply(lambda x, format_str: pd.to_datetime(x, format=format_str).date(), format_str=format_str)</code> <code>pyspark.sql.functions.to_timestamp</code> <code>df.A.apply(lambda x, format_str: pd.to_datetime(x, format=format_str), format_str=format_str)</code> <code>pyspark.sql.functions.translate</code> <code>df.col.str.split(\"\").apply(lambda x: \"\".join(pd.Series(x).replace(to_replace, values).tolist()), to_replace=to_replace, values=values)</code> <code>pyspark.sql.functions.trim</code> <code>df.col.str.strip()</code> <code>pyspark.sql.functions.udf</code> <code>df.apply</code> or <code>df.col.map</code> <code>pyspark.sql.functions.unix_timestamp</code> <code>df.col.apply(lambda x, format_str: (pd.to_datetime(x, format=format_str) - pd.Timestamp(\"1970-01-01\")).total_seconds(), format_str=format_str)</code> <code>pyspark.sql.functions.upper</code> <code>df.col.str.upper()</code> <code>pyspark.sql.functions.var_pop</code> <code>df.col.var(ddof=0)</code> <code>pyspark.sql.functions.var_samp</code> <code>df.col.var()</code> <code>pyspark.sql.functions.variance</code> <code>df.col.var()</code> <code>pyspark.sql.functions.weekofyear</code> <code>df.col.dt.isocalendar().week</code> <code>pyspark.sql.functions.when</code> <code>df.A.apply(lambda a, cond, val, other: val if cond(a) else other, cond=cond, val=val, other=other)</code> <code>pyspark.sql.functions.year</code> <code>df.col.dt.year</code>"},{"location":"integrating_bodo/sparkcheatsheet/#special-cases","title":"Special Cases","text":""},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsconcat","title":"<code>pyspark.sql.functions.concat</code>","text":"<ul> <li><code>pyspark.sql.functions.concat</code><ul> <li>for Arrays : <code>df[['col1', 'col2', 'col3']].apply(lambda x: np.hstack(x), axis=1)</code></li> <li>for Strings : <code>df[['col1', 'col2', 'col3']].apply(lambda x: \"\".join(x), axis=1)</code></li> </ul> </li> </ul>"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsconv","title":"<code>pyspark.sql.functions.conv</code>","text":"<ul> <li> <p><code>pyspark.sql.functions.conv</code></p> <p>pandas equivalent: </p> <pre><code>base_map = {2: \"{0:b}\", 8: \"{0:o}\", 10: \"{0:d}\", 16: \"{0:x}\"}\nnew_format = base_map[new_base]\ndf.col.apply(lambda x, old_base, new_format: new_format.format(int(x, old_base)), old_base=old_base, new_format=new_format)\n</code></pre> </li> </ul>"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsdate_trunc","title":"<code>pyspark.sql.functions.date_trunc</code>","text":"<ul> <li> <p><code>pyspark.sql.functions.date_trunc</code> </p> <ul> <li>For frequencies day and below: <code>df.col.dt.floor(freq=trunc_val)</code></li> <li>For month: <code>df.col.map(lambda x: pd.Timestamp(year=x.year, month=x.month, day=1))</code></li> <li>For year: <code>df.col.map(lambda x: pd.Timestamp(year=x.year, month=1, day=1))</code></li> </ul> </li> </ul>"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsregexp_extract","title":"<code>pyspark.sql.functions.regexp_extract</code>","text":"<ul> <li> <p><code>pyspark.sql.functions.regexp_extract</code></p> <p>Here's a small pandas function equivalent:</p> <pre><code>def f(x, pat):\n    res = re.search(pat, x)\n    return \"\" if res is None else res[0]\ndf.col.apply(f, pat=pat)  \n</code></pre> </li> </ul>"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsshiftleft","title":"<code>pyspark.sql.functions.shiftLeft</code>","text":"<ul> <li> <p><code>pyspark.sql.functions.shiftLeft</code></p> <ul> <li>If the type is uint64 <code>np.left_shift(df.col.astype(np.int64), numbits).astype(np.uint64))</code></li> <li>Other integer types: <code>np.left_shift(df.col, numbits)</code></li> </ul> </li> </ul>"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsshiftright","title":"<code>pyspark.sql.functions.shiftRight</code>","text":"<ul> <li> <p><code>pyspark.sql.functions.shiftRight</code></p> <ul> <li>If the type is uint64 use <code>shiftRightUnsigned</code></li> <li>Other integer types: <code>np.right_shift(df.col, numbits)</code></li> </ul> </li> </ul>"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionsshiftrightunsigned","title":"<code>pyspark.sql.functions.shiftRightUnsigned</code>","text":"<ul> <li> <p><code>pyspark.sql.functions.shiftRightUnsigned</code></p> <p>Here's a small pandas function equivalent:</p> <pre><code>def shiftRightUnsigned(col, num_bits):\n    bits_minus_1 = max((num_bits - 1), 0)\n    mask_bits = (np.int64(1) &lt;&lt; bits_minus_1) - 1\n    mask = ~(mask_bits &lt;&lt; (63 - bits_minus_1))\n    return np.right_shift(col.astype(np.int64), num_bits) &amp; mask).astype(np.uint64)\nshiftRightUnsigned(df.col, numbits)  \n</code></pre> </li> </ul>"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionssort_array","title":"<code>pyspark.sql.functions.sort_array</code>","text":"<ul> <li> <p><code>pyspark.sql.functions.sort_array</code></p> <ul> <li>Ascending:  <code>df.col.map(lambda x: np.sort(x))</code></li> <li>Descending: <code>df.col.map(lambda x: np.sort(x)[::-1])</code></li> </ul> </li> </ul>"},{"location":"integrating_bodo/sparkcheatsheet/#pysparksqlfunctionstrunc","title":"<code>pyspark.sql.functions.trunc</code>","text":"<ul> <li> <p><code>pyspark.sql.functions.trunc</code></p> <pre><code>def f(date, trunc_str):\n    if trunc_str == 'year':\n        return pd.Timestamp(year=date.year, month=1, day=1)\n    if trunc_str == 'month':\n        return pd.Timestamp(year=date.year, month=date.month, day=1)\ndf.A.apply(f, trunc_str=trunc_str)\n</code></pre> </li> </ul>"},{"location":"performance/caching/","title":"Caching","text":"<p>In many situations, Bodo can save the binary resulting from the compilation of a function to disk, to be reused in future runs. This avoids the need to recompile functions the next time that you run your application.</p> <p>Recompiling a function is only necessary when it is called with new input types, and the same applies to caching. In other words, an application can be run multiple times and process different data without having to recompile any code if the data types remain the same (which is the most common situation).</p> <p>Warning</p> <p>Caching works in most (but not all) situations, and is disabled by default. See caching limitations below for more information.</p>"},{"location":"performance/caching/#caching-example","title":"Caching Example","text":"<p>To cache a function, we only need to add the option <code>cache=True</code> to the JIT decorator:</p> <pre><code>import time\nimport pandas as pd\nimport bodo\n\n\n@bodo.jit(cache=True)\ndef mean_power_speed():\n    df = pd.read_parquet(\"data/cycling_dataset.pq\")\n    return df[[\"power\", \"speed\"]].mean()\n\n\nt0 = time.time()\nresult = mean_power_speed()\nprint(result)\nprint(\"Total execution time:\", round(time.time() - t0, 3), \"secs\")\n</code></pre> <p>The first time that the above code runs, Bodo compiles the function and caches it to disk. The code times the whole function call, which includes compilation time the first time the function is run:</p> <p><pre><code>power    102.078421\nspeed      5.656851\ndtype: float64\nTotal execution time: 4.614 secs\n</code></pre> In subsequent runs, it will recover the function from cache and as a result, the execution time will be much faster:</p> <pre><code>power    102.078421\nspeed      5.656851\ndtype: float64\nTotal execution time: 0.518 secs\n</code></pre> <p>Note</p> <p><code>data/cycling_dataset.pq</code> is located in the Bodo tutorial repo.</p>"},{"location":"performance/caching/#cache-location-and-portability","title":"Cache Location and Portability","text":"<p>In most cases, the cache is saved in the <code>__pycache__</code> directory inside the directory where the source files are located. The variable <code>NUMBA_DEBUG_CACHE</code> can be set to <code>1</code> in order to see where exactly the cache is and whether it is being written to or read from.</p> <p>On Jupyter notebooks, the cache directory is called <code>numba_cache</code> and is located in <code>IPython.paths.get_ipython_cache_dir()</code>. See here for more information on these and other alternate cache locations. For example, when running in a notebook:</p> <pre><code>import os\nimport IPython\n\n\ncache_dir = IPython.paths.get_ipython_cache_dir() + \"/numba_cache\"\nprint(\"Cache files:\")\nos.listdir(cache_dir)\n</code></pre> <pre><code>Cache files:\n['ipython-input-bce41f829e09.mean_power_speed-4444615264.py38.nbi',\n'ipython-input-bce41f829e09.mean_power_speed-4444615264.py38.1.nbc']\n</code></pre> <p>Cached objects work across systems with the same CPU model and CPU features. Therefore, it is safe to share and reuse the contents in the cache directory on a different machine. See here for more information.</p>"},{"location":"performance/caching/#cache-invalidation","title":"Cache Invalidation","text":"<p>The cache is invalidated automatically when the corresponding source code is modified. One way to observe this behavior is to modify the above example after it has been cached a first time, by changing the name of the variable <code>df</code>. The next time that we run the code, Bodo will determine that the source code has been modified, invalidate the cache and recompile the function.</p> <p>Warning</p> <p>It is sometimes necessary to clear the cache manually (see caching limitations below). To clear the cache, the cache files can simply be removed.</p>"},{"location":"performance/caching/#tips-for-reusing-the-cache","title":"Tips for Reusing the Cache","text":"<p>As explained above, caching is invalidated for a function any time any of the source code in the file changes. If we define a function and call it in the same file, and modify the arguments passed to the function, the cache will be invalidated.</p>"},{"location":"performance/caching/#caching-file-io","title":"Caching File IO","text":"<p>For example: a typical use case is calling an IO function with a different file name.</p> <pre><code>@bodo.jit(cache=True)\ndef io_call(file_name):\n    ...\nio_call(\"mydata.parquet\")\n</code></pre> <p>The above function would need to be recompiled if the argument to <code>io_call</code> changes from <code>mydata.parquet</code>. By separating into separate files the function call from the function definition, the function definition does not need to be recompiled for each function call with new arguments. The cached IO function will work for a change in file name so long as the file schema is the same. For example, the below code snippet</p> <pre><code>import IO_function from IO_functions\nIO_function(file_name)\n</code></pre> <p>would not need to recompile <code>IO_function</code> each time <code>file_name</code> is modified since <code>IO_function</code> is isolated from that code change.</p>"},{"location":"performance/caching/#caching-notebook-cells","title":"Caching Notebook Cells","text":"<p>For IPython notebooks the function to be cached should be in a separate cell from the function call.</p> <pre><code>@bodo.jit(cache=True)\ndef io_call(file_name):\n    ...\n</code></pre> <pre><code>io_call(file_name)\nio_call(another_file_name)\n...\n</code></pre> <p>If a cell with a cached function is modified, then its cache is invalidated and the function must be compiled again.</p>"},{"location":"performance/caching/#current-caching-limitations","title":"Current Caching Limitations","text":"<ul> <li>Changes in compiled functions are not seen across files. For     example, if we have a cached Bodo function that calls a cached Bodo     function in a different file, and modify the latter, Bodo will not     update its cache (and therefore run with the old version of the     function).</li> <li>Global variables are treated as compile-time constants. When a     function is compiled, the value of any globals that the function     uses are embedded in the binary at compilation time and remain     constant. If the value of the global changes in the source code     after compilation, the compiled object (and cache) will not rebind     to the new value.</li> </ul>"},{"location":"performance/caching/#troubleshooting","title":"Troubleshooting","text":"<p>During execution, Bodo will print information on caching if the environment variable <code>NUMBA_DEBUG_CACHE</code> is set to <code>1</code>. For example, on first run it will show if the cache is being saved to and where, and on subsequent runs it will show if the compiler is successfully loading from cache.</p> <p>If the compiler reports that it is not able to cache a function, or load a function from cache, please report the issue on our respository.</p>"},{"location":"performance/inlining/","title":"Inlining","text":"<p>Inlining allows the compiler to perform optimizations across functions, at the cost of increased compilation time. Use inlining when you have your code split into multiple Bodo functions and there are important optimizations that need to performed on some functions, that are dependent on the code of other functions. We will explain this with examples below.</p> <p>Danger</p> <p>Inlining should be used sparingly as it can cause increased compilation time. We strongly recommend against inlining functions with 10 or more lines of code.</p> <p>Bodo's compiler translates high-level code inside <code>bodo.jit</code> decorated functions to highly optimized lower level code. It can perform many optimizations on the generated code based on the structure of the code inside the function being compiled.</p> <p>Let's consider the following example where <code>data.pq</code> is a dataset with 1000 columns:</p> <pre><code>@bodo.jit\ndef example():\n    df = pd.read_parquet(\"data.pq\")\n    return df.groupby(\"A\")[\"B\", \"C\"].sum()\n</code></pre> <p>To execute the query inside the <code>example</code> function, Bodo doesn't need to read all the columns from the file. It only needs three columns (<code>A</code>, <code>B</code> and <code>C</code>), and can save a lot of time and memory by just reading those. When compiling <code>example</code>, Bodo automatically optimizes the <code>read_parquet</code> call to only read the three required columns.</p> <p>Warning</p> <p>If you have separate Bodo functions and their code needs to be optimized jointly, you need to use inlining.</p> <p>Any code that needs to be optimized jointly needs to be compiled as part of the same JIT compilation. If we have the following:</p> <pre><code>@bodo.jit\ndef read_data(fname):\n    return pd.read_parquet(fname)\n\n@bodo.jit\ndef query():\n    df = read_data(\"data.pq\")\n    return df.groupby(\"A\")[\"B\", \"C\"].sum()\n</code></pre> <p>Bodo will compile the functions separately, and won't be able to optimize the <code>read_parquet</code> call because it doesn't know how the return value of <code>read_data</code> is used. To structure the code into different functions and still allow the compiler to do holistic optimizations, you can specify the <code>inline=\"always\"</code> option to the jit decorator to tell the compiler to include that function during compilation of another one.</p> <p>For example:</p> <pre><code>@bodo.jit(inline=\"always\")\ndef read_data(fname):\n    return pd.read_parquet(fname)\n\n@bodo.jit\ndef query():\n    df = read_data(\"data.pq\")\n    return df.groupby(\"A\")[\"B\", \"C\"].sum()\n</code></pre> <p>The option <code>inline=\"always\"</code> in this example tells the compiler to compile and include <code>read_data</code> when it is compiling <code>query</code>.</p>"},{"location":"performance/performance/","title":"Performance Measurement","text":"<p>This page provides tips on measuring performance of Bodo programs. It is important to keep the following in mind when measuring program run time:</p> <ol> <li>Every program has some overhead, so large datasets may be     necessary for useful measurements.</li> <li>Performance can vary from one run to another. Several measurements     are always needed.</li> <li>Longer computations typically provide more reliable run time     information.</li> <li>It is important to use a sequence of tests with increasing input     size, which helps understand the impact of problem size on program     performance.</li> <li>Testing with different data (in terms statistical distribution and     skew) can be useful to see the impact of data skew on performance     and scaling.</li> <li>Simple programs are useful to study performance factors. Complex     programs are impacted by multiple factors and their performance is     harder to understand.</li> </ol>"},{"location":"performance/performance/#measuring-execution-time-of-bodo-functions","title":"Measuring execution time of Bodo functions","text":"<p>Since Bodo-decorated functions are JIT-compiled, the compilation time is non-negligible but it only happens the first time a function is compiled. Compiled functions stay in memory and don't need to be re-compiled, and they can also be cached to disk (see caching) to be reused across different executions.</p> <p>To avoid measuring compilation time, place timers inside the functions. For example:</p> <pre><code>\"\"\"\ncalc_pi.py: computes the value of Pi using Monte-Carlo Integration\n\"\"\"\n\nimport numpy as np\nimport bodo\nimport time\n\nn = 2 * 10**8\n\ndef calc_pi(n):\n    t1 = time.time()\n    x = 2 * np.random.ranf(n) - 1\n    y = 2 * np.random.ranf(n) - 1\n    pi = 4 * np.sum(x**2 + y**2 &lt; 1) / n\n    print(\"Execution time:\", time.time()-t1, \"\\nresult:\", pi)\n    return pi\n\nbodo_calc_pi = bodo.jit(calc_pi)\nprint(\"python:\")\ncalc_pi(n)\nprint(\"\\nbodo:\")\nbodo_calc_pi(n)\n</code></pre> <p>The output of this code on a single core is as follows:</p> <pre><code>python:\nExecution time: 5.060443162918091\nresult: 3.14165914\n\nbodo:\nExecution time: 2.165610068012029\nresult: 3.14154512\n</code></pre> <p>Bodo's parallel speedup can be measured similarly:</p> <pre><code>\"\"\"\ncalc_pi.py: computes the value of Pi using Monte-Carlo Integration\n\"\"\"\n\nimport numpy as np\nimport bodo\nimport time\n\n@bodo.jit\ndef calc_pi(n):\n    t1 = time.time()\n    x = 2 * np.random.ranf(n) - 1\n    y = 2 * np.random.ranf(n) - 1\n    pi = 4 * np.sum(x**2 + y**2 &lt; 1) / n\n    print(\"Execution time:\", time.time()-t1, \"\\nresult:\", pi)\n    return pi\n\ncalc_pi(2 * 10**8)\n</code></pre> <p>Launched on eight parallel cores:</p> <pre><code>$ BODO_NUM_WORKERS=8 python calc_pi.py\nExecution time: 0.5736249439651147\nresult: 3.14161474\n</code></pre> <p>And the time it takes can be compared with Python performance. Here, we have a <code>5.06/0.57 ~= 9x</code> speedup (from parallelism and sequential optimizations).</p> <p>In addition, SPMD launch mode is recommended for performance measurements since it has lower overheads.</p>"},{"location":"performance/performance/#measuring-sections-inside-bodo-functions","title":"Measuring sections inside Bodo functions","text":"<p>We can add multiple timers inside a function to see how much time each section takes:</p> <pre><code>\"\"\"\ncalc_pi.py: computes the value of Pi using Monte-Carlo Integration\n\"\"\"\n\nimport numpy as np\nimport bodo\nimport time\n\nn = 2 * 10**8\n\ndef calc_pi(n):\n    t1 = time.time()\n    x = 2 * np.random.ranf(n) - 1\n    y = 2 * np.random.ranf(n) - 1\n    t2 = time.time()\n    print(\"Initializing x,y takes: \", t2-t1)\n\n    pi = 4 * np.sum(x**2 + y**2 &lt; 1) / n\n    print(\"calculation takes:\", time.time()-t2, \"\\nresult:\", pi)\n    return pi\n\nbodo_calc_pi = bodo.jit(calc_pi)\nprint(\"python: ------------------\")\ncalc_pi(n)\nprint(\"\\nbodo: ------------------\")\nbodo_calc_pi(n)\n</code></pre> <p>The output is as follows:</p> <pre><code>python: ------------------\nInitializing x,y takes:  3.9832258224487305\ncalculation takes: 1.1460411548614502\nresult: 3.14156454\n\nbodo: ------------------\nInitializing x,y takes:  3.0611653940286487\ncalculation takes: 0.35728363902308047\nresult: 3.14155538\n</code></pre> <p>Note</p> <p>Note that Bodo execution took longer in the last example than previous ones, since the presence of timers in the middle of computation can inhibit some code optimizations (e.g. code reordering and fusion). Therefore, one should be cautious about adding timers in the middle of computation.</p>"},{"location":"performance/performance/#disable-jit","title":"Disabling JIT Compilation","text":"<p>Sometimes it is convenient to disable JIT compilation without removing the <code>jit</code> decorators in the code, to enable easy performance comparison with regular Python or perform debugging. This can be done by setting the environment variable <code>NUMBA_DISABLE_JIT</code> to <code>1</code>, which makes the jit decorators act as if they perform no operation. In this case, the invocation of decorated functions calls the original Python functions instead of compiled versions.</p>"},{"location":"performance/performance/#load-imbalance","title":"Load Imbalance","text":"<p>Bodo distributes and processes equal amounts of data across cores as much as possible. There are certain cases, however, where depending on the statistical properties of the data and the operation being performed on it, some cores will need to process much more data than others at certain points in the application, which limits the scaling that can be achieved. How much this impacts performance depends on the degree of imbalance and the impact the affected operation has on overall execution time.</p> <p>For example, consider the following operation:</p> <pre><code>df.groupby(\"A\")[\"B\"].nunique()\n</code></pre> <p>Where <code>df</code> has one billion rows, <code>A</code> only has 3 unique values, and we are running this on a cluster with 1000 cores. Although the work can be distributed to a certain extent, the final result for each group of <code>A</code> has to be computed on a single core. Because there are only 3 groups, during computation of the final result there will only be at most three cores active.</p>"},{"location":"performance/performance/#expected-scaling","title":"Expected Scaling","text":"<p>Scaling can be measured as the speedup achieved with n cores compared to running on a single core, that is, the ratio of execution time with 1 core vs n cores.</p> <p>For a fixed input size, the speed up achieved by Bodo with increasing number of cores (also known as strong scaling) depends on a combination of various factors: size of the input data (problem size), properties of the data, compute operations used, and the hardware platform's attributes (such as effective network throughput).</p> <p>For example, the program above can scale almost linearly (e.g. 100x speed up on 100 cores) for large enough problem sizes, since the only communication overhead is parallel summation of the partial sums obtained by <code>np.sum</code> on each processor.</p> <p>On the other hand, some operations such as join and groupby may require communicating significant amounts of data across the network, depending on the characteristics of the data and the exact operation (e.g. <code>groupby.sum</code>, <code>groupby.nunique</code>, <code>groupy.apply</code>, inner vs outer <code>join</code>, etc.), requiring fast cluster interconnection networks to scale to large number of cores.</p> <p>Load imbalance, as described above, can also significantly impair scaling in certain situations.</p>"},{"location":"quick_start/","title":"Getting Started","text":"<p>This section provides quick start guides to help you get started with Bodo quickly.</p>"},{"location":"quick_start/#python-quick-start","title":"Python Quick Start","text":"<p>This guide will walk you through the process of running a simple Python code using Bodo engine on your local machine.</p>"},{"location":"quick_start/#sql-quick-start","title":"SQL Quick Start","text":"<p>This guide will walk you through the process of running a simple SQL query using Bodo engine on your local machine using a local SQL table.</p>"},{"location":"quick_start/#iceberg-quick-start","title":"Iceberg Quick Start","text":"<p>This guide will walk you through the process of running a simple Python code using Bodo engine on your local machine to write a local Iceberg table and read from it.</p>"},{"location":"quick_start/#bodo-platform-quick-start","title":"Bodo Platform Quick Start","text":"<p>This guide provides a walkthrough of the Bodo subscription process through AWS Marketplace as well as Bodo platform set up process for your AWS account.</p>"},{"location":"quick_start/#bodo-platform-sdk-quick-start","title":"Bodo Platform SDK Quick Start","text":"<p>This quickstart guide will walk you through running a job on the Bodo Platform using the Bodo Platform SDK installed on your local machine.</p>"},{"location":"quick_start/dev_guide/","title":"Python JIT Development","text":"","tags":["getting started"]},{"location":"quick_start/dev_guide/#devguide","title":"Python JIT Development Guide","text":"<p>This page provides an introduction to Python programming with Bodo JIT and explains its important concepts briefly.</p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#installation","title":"Installation","text":"<p>Install Bodo to get started with Python development (e.g., <code>pip install -U bodo</code> or <code>conda install bodo -c conda-forge</code>).</p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#data-transform-example-with-bodo","title":"Data Transform Example with Bodo","text":"<p>We use a simple data transformation example to discuss some of the key Bodo concepts.</p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#generate-data","title":"Generate data","text":"<p>Let's generate some example data and write to a Parquet file:</p> <pre><code>import pandas as pd\nimport numpy as np\n\n# 10m data points\ndf = pd.DataFrame(\n    {\n        \"A\": np.repeat(pd.date_range(\"2013-01-03\", periods=1000), 10_000),\n        \"B\": np.arange(10_000_000),\n    }\n)\n# set some values to NA\ndf.iloc[np.arange(1000) * 3, 0] = pd.NA\n# using row_group_size helps with efficient parallel read of data later\ndf.to_parquet(\"pd_example.pq\", row_group_size=100_000)\n</code></pre> <p>Save this code in <code>gen_data.py</code> and run in command line:</p> <pre><code>python gen_data.py\n</code></pre>","tags":["getting started"]},{"location":"quick_start/dev_guide/#example_code_in_pandas","title":"Example Pandas Code","text":"<p>Here is a simple data transformation code in Pandas that processes a column of datetime values and creates two new columns:</p> <pre><code>import pandas as pd\nimport time\n\n\ndef data_transform():\n    t0 = time.time()\n    df = pd.read_parquet(\"pd_example.pq\")\n    df[\"B\"] = df.apply(lambda r: \"NA\" if pd.isna(r.A) else \"P1\" if r.A.month &lt; 5 else \"P2\", axis=1)\n    df[\"C\"] = df.A.dt.month\n    df.to_parquet(\"pandas_output.pq\")\n    print(\"Total time: {:.2f}\".format(time.time() - t0))\n\n\nif __name__ == \"__main__\":\n    data_transform()\n</code></pre> <p>Save this code in <code>data_transform.py</code> and run in command line:</p> <pre><code>$ python data_transform.py\nTotal time: 166.18\n</code></pre> <p>Standard Python is quite slow for these data transforms since:</p> <ol> <li>The use of custom code inside <code>apply()</code> does not let Pandas run an     optimized prebuilt C library in its backend. Therefore, the Python     interpreter overheads dominate.</li> <li>Python uses a single CPU core and does not parallelize     computation.</li> </ol> <p>Bodo solves both of these problems as we demonstrate below.</p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#using-the-bodo-jit-decorator","title":"Using the Bodo JIT Decorator","text":"<p>Bodo optimizes and parallelizes data workloads by providing just-in-time (JIT) compilation. This code is identical to the original Pandas code, except that it annotates the <code>data_transform</code> function with the <code>bodo.jit</code> decorator:</p> <pre><code>import pandas as pd\nimport time\nimport bodo\n\n@bodo.jit\ndef data_transform():\n    t0 = time.time()\n    df = pd.read_parquet(\"pd_example.pq\")\n    df[\"B\"] = df.apply(lambda r: \"NA\" if pd.isna(r.A) else \"P1\" if r.A.month &lt; 5 else \"P2\", axis=1)\n    df[\"C\"] = df.A.dt.month\n    df.to_parquet(\"bodo_output.pq\")\n    print(\"Total time: {:.2f}\".format(time.time()-t0))\n\nif __name__ == \"__main__\":\n    data_transform()\n</code></pre> <p>Save this code in <code>bodo_data_transform.py</code> and run on a single core from command line:</p> <pre><code>$ BODO_NUM_WORKERS=1 python bodo_data_transform.py\nTotal time: 1.78\n</code></pre> <p>This code is 94x faster with Bodo than Pandas even on a single core, because Bodo compiles the function into a native binary, eliminating the interpreter overheads in <code>apply</code>.</p> <p>Now let's run the code on all CPU cores - the example below assumes an 8 core machine.</p> <pre><code>$ python bodo_data_transform.py\nTotal time: 0.38\n</code></pre> <p>Using 8 cores gets an additional ~5x speedup. The same program can be scaled to larger datasets and as many cores as necessary in compute clusters and cloud environments. An explicit limit on the number of cores used can be set with the environment variable <code>BODO_NUM_WORKERS</code>, but by default Bodo will use all available cores.</p> <p>See the documentation on bodo parallelism basics for more details about Bodo's JIT compilation workflow and parallel computation model.</p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#compilation-time-and-caching","title":"Compilation Time and Caching","text":"<p>Bodo's JIT workflow compiles the function the first time it is called, but reuses the compiled version for subsequent calls. In the previous code, we added timers inside the function to avoid measuring compilation time. Let's move the timers outside and call the function twice:</p> <pre><code>import pandas as pd\nimport time\nimport bodo\n\n@bodo.jit\ndef data_transform():\n    df = pd.read_parquet(\"pd_example.pq\")\n    df[\"B\"] = df.apply(lambda r: \"NA\" if pd.isna(r.A) else \"P1\" if r.A.month &lt; 5 else \"P2\", axis=1)\n    df[\"C\"] = df.A.dt.month\n    df.to_parquet(\"bodo_output.pq\")\n\nif __name__ == \"__main__\":\n    t0 = time.time()\n    data_transform()\n    print(\"Total time first call: {:.2f}\".format(time.time()-t0))\n    t0 = time.time()\n    data_transform()\n    print(\"Total time second call: {:.2f}\".format(time.time()-t0))\n</code></pre> <p>Save this code in <code>data_transform2.py</code> and run in command line:</p> <pre><code>$ python data_transform2.py\nTotal time first call: 4.72\nTotal time second call: 1.92\n</code></pre> <p>The first call is slower due to compilation of the function, but the second call reuses the compiled version and runs faster.</p> <p>Compilation time can be avoided across program runs by using the <code>cache=True</code> flag:</p> <pre><code>import pandas as pd\nimport time\nimport bodo\n\n\n@bodo.jit(cache=True)\ndef data_transform():\n    df = pd.read_parquet(\"pd_example.pq\")\n    df[\"B\"] = df.apply(lambda r: \"NA\" if pd.isna(r.A) else \"P1\" if r.A.month &lt; 5 else \"P2\", axis=1)\n    df[\"C\"] = df.A.dt.month\n    df.to_parquet(\"bodo_output.pq\")\n\n\nif __name__ == \"__main__\":\n    t0 = time.time()\n    data_transform()\n    print(\"Total time: {:.2f}\".format(time.time() - t0))\n</code></pre> <p>Save this code in <code>data_transform_cache.py</code> and run in command line twice:</p> <pre><code>$ python data_transform_cache.py\nTotal time: 4.70\n$ python data_transform_cache.py\nTotal time: 1.96\n</code></pre> <p>In this case, Bodo saves the compiled version of the function to a file and reuses it in the second run since the code has not changed. We plan to make caching default in the future. See caching for more information.</p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#parallel-python-processes","title":"Parallel Python Processes","text":"<p>Bodo will execute code decorated with <code>bodo.jit</code> in parallel. The function is run on all cores, but Bodo divides the data and computation in JIT functions to exploit parallelism.</p> <p>Let's try a simple example that demonstrates how chunks of data are loaded in parallel:</p> <pre><code>import pandas as pd\nimport bodo\n\n\ndef load_data_pandas():\n    df = pd.read_parquet(\"pd_example.pq\")\n    print(\"pandas dataframe: \", df)\n\n\n@bodo.jit\ndef load_data_bodo():\n    df = pd.read_parquet(\"pd_example.pq\")\n    print(\"Bodo dataframe: \", df)\n\n\nif __name__ == \"__main__\":\n    load_data_pandas()\n    load_data_bodo()\n</code></pre> <p>Save this code in <code>load_data.py</code> and run on two cores:</p>  Click to expand output <pre><code>$ BODO_NUM_WORKERS=2 python load_data.py\npandas dataframe:\n                 A        B\n0              NaT        0\n1       2013-01-03        1\n2       2013-01-03        2\n3              NaT        3\n4       2013-01-03        4\n...            ...      ...\n9999995 2015-09-29  9999995\n9999996 2015-09-29  9999996\n9999997 2015-09-29  9999997\n9999998 2015-09-29  9999998\n9999999 2015-09-29  9999999\n\n[10000000 rows x 2 columns]\n\nBodo dataframe:\n                 A        B\n0       1970-01-01        0\n1       2013-01-03        1\n2       2013-01-03        2\n3       2013-01-03        3\n4       2013-01-03        4\n...            ...      ...\n4999995 2014-05-17  4999995\n4999996 2014-05-17  4999996\n4999997 2014-05-17  4999997\n4999998 2014-05-17  4999998\n4999999 2014-05-17  4999999\n\n[5000000 rows x 2 columns]\n\n5000000 2014-05-18  5000000\n5000001 2014-05-18  5000001\n5000002 2014-05-18  5000002\n5000003 2014-05-18  5000003\n5000004 2014-05-18  5000004\n...            ...      ...\n9999995 2015-09-29  9999995\n9999996 2015-09-29  9999996\n9999997 2015-09-29  9999997\n9999998 2015-09-29  9999998\n9999999 2015-09-29  9999999\n\n[5000000 rows x 2 columns]\n</code></pre> <p>The first dataframe is a regular Pandas dataframe and has all 10 million rows. However, the second dataframe is a Bodo parallelized Pandas dataframe which is split into two chunks with 5 million rows each. In this case, Bodo parallelizes <code>read_parquet</code> automatically and loads different chunks of data in different cores.</p> <p>When parallelizable input is given to a function (e.g. Pandas DataFrames/Series, numpy Arrays), Bodo will automatically distribute input for parallel computation, freeing users from having to manually reason about transforming data for parallel computation (see more on that below). For values returned by JIT'ed functions, Bodo will avoid gathering all of the output back onto a single process unless the full data is actually used outside of a parallel context. This means that for large programs running across distributed clusters, one does not need to worry about crashes due to running out of memory when retrieving a large object from a JIT call.</p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#parallel-computation","title":"Parallel Computation","text":"<p>Bodo automatically divides computation and manages communication across cores as this example demonstrates:</p> <pre><code>import pandas as pd\nimport bodo\n\n\n@bodo.jit\ndef data_groupby():\n    df = pd.read_parquet(\"pd_example.pq\")\n    df2 = df.groupby(\"A\", as_index=False).sum()\n    df2.to_parquet(\"bodo_output.pq\")\n\n\nif __name__ == \"__main__\":\n    data_groupby()\n</code></pre> <p>Save this code as <code>data_groupby.py</code> and run from command line:</p> <pre><code>$ BODO_NUM_WORKERS=8 python data_groupby.py\n</code></pre> <p>This program uses <code>groupby</code> which requires rows with the same key to be aggregated together. Therefore, Bodo shuffles the data automatically, and the user doesn't need to worry about parallelism challenges like communication.</p> <p></p> <p></p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#bodo-jit-requirements","title":"Bodo JIT Requirements","text":"<p>To take advantage of the Bodo JIT compiler and avoid errors, make sure only compute and data-intensive code is in JIT functions. Other Python code for setup and configuration should run in regular Python. For example, consider this simple script:</p> <pre><code>import os\nimport pandas as pd\n\ndata_path = os.environ[\"JOB_DATA_PATH\"]\n\ndf = pd.read_parquet(data_path)\nprint(df.A.sum())\n</code></pre> <p>The Bodo version performs the computation in JIT functions, but keeps the setup code (finding <code>data_path</code>) in regular Python:</p> <pre><code>import os\nimport pandas as pd\nimport bodo\n\ndata_path = os.environ[\"JOB_DATA_PATH\"]\n\n@bodo.jit\ndef f(path):\n    df = pd.read_parquet(path)\n    print(df.A.sum())\n\nf(data_path)\n</code></pre> <p>In addition, the Bodo version passes the file path <code>data_path</code> as an argument to the JIT function <code>f</code>, allowing Bodo to find the input dataframe schema which is necessary for type inference (more in Scalable Data I/O).</p> <p>Bodo JIT supports specific APIs in Pandas currently, and other APIs cannot be used inside JIT functions. For example:</p> <pre><code>import pandas as pd\nimport bodo\n\n\n@bodo.jit\ndef df_unsupported():\n    df = pd.DataFrame({\"A\": [1, 2, 3]})\n    df2 = df.transpose()\n    return df2\n\n\nif __name__ == \"__main__\":\n    df_unsupported()\n</code></pre> <p>Save this code as <code>df_unsupported.py</code> and run from command line:</p> <pre><code>$ python df_unsupported.py\n# bodo.utils.typing.BodoError: Dataframe.transpose() not supported yet.\n</code></pre> <p>As the error indicates, Bodo doesn't currently support the <code>transpose</code> call in JIT functions. In these cases, an alternative API should be used or this portion of the code should be either be in regular Python or in Bodo's @bodo.wrap_python. See supported Pandas API for the complete list of supported Pandas operations.</p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#type-stability","title":"Type Stability","text":"<p>The main requirement of JIT compilation is being able to infer data types for all variables and values. In Bodo, column names are part of dataframe data types, so Bodo tries to infer column name related inputs in all operations. For example, key names in <code>groupby</code> are used to determine the output data type and need to be known to Bodo:</p> <pre><code>import pandas as pd\nimport bodo\n\n\n@bodo.jit\ndef groupby_keys(extra_keys):\n    df = pd.read_parquet(\"pd_example.pq\")\n    keys = [c for c in df.columns if c not in [\"B\", \"C\"]]\n    if extra_keys:\n        keys.append(\"B\")\n    df2 = df.groupby(keys).sum()\n    print(df2)\n\n\nif __name__ == \"__main__\":\n    groupby_keys(False)\n</code></pre> <p>Save this code as <code>groupby_keys.py</code> and run from command line:</p> <pre><code>$ python groupby_keys.py\n# bodo.utils.typing.BodoError: groupby(): argument 'by' requires a constant value but variable 'keys' is updated inplace using 'append'\n</code></pre> <p>In this case, the list of groupby keys is determined using the runtime value of <code>extra_keys</code> in a way that Bodo is not able to infer it from the program during compilation time. The alternative is to compute the keys in a separate JIT function to make it easier for Bodo to infer:</p> <pre><code>import pandas as pd\nimport bodo\n\n\n@bodo.jit\ndef get_keys(df_columns, extra_keys):\n    keys = [c for c in df_columns if c not in [\"B\", \"C\"]]\n    if extra_keys:\n        keys.append(\"B\")\n    return keys\n\n\n@bodo.jit\ndef groupby_keys(extra_keys):\n    df = pd.read_parquet(\"pd_example.pq\")\n    keys = get_keys(df.columns, extra_keys)\n    df2 = df.groupby(keys).sum()\n    print(df2)\n\n\nif __name__ == \"__main__\":\n    keys = get_keys([\"A\"], False)\n    groupby_keys(False)\n</code></pre> <p>This program works since <code>get_keys</code> can be evaluated in compile time. It only uses <code>df.columns</code> and <code>extra_keys</code> values that can be constant at compile time, and does not use non-deterministic features like I/O.</p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#python-features","title":"Python Features","text":"<p>Bodo uses Numba for compiling regular Python features and some of Numba's requirements apply to Bodo as well. For example, values in data structures like lists should have the same data type. This example fails since list values are either integers or strings:</p> <pre><code>import bodo\n\n\n@bodo.jit\ndef create_list():\n    out = []\n    out.append(0)\n    out.append(\"A\")\n    out.append(1)\n    out.append(\"B\")\n    return out\n\n\nif __name__ == \"__main__\":\n    create_list()\n</code></pre> <p>Using tuples can often solve these problems since tuples can hold values of different types:</p> <pre><code>import bodo\n\n\n@bodo.jit\ndef create_list():\n    out = []\n    out.append((0, \"A\"))\n    out.append((1, \"B\"))\n    return out\n\n\nif __name__ == \"__main__\":\n    create_list()\n</code></pre> <p>Please refer to the Unsupported Python Programs documentation for more details.</p>","tags":["getting started"]},{"location":"quick_start/dev_guide/#bodo-jit-integration-quickstart","title":"Bodo JIT Integration Quickstart","text":"<p>Here are high level steps for integrating Bodo into Python workloads:</p> <ol> <li> <p>Installation and Import</p> <p>a. Install Bodo (e.g. <code>pip install -U bodo</code>).</p> <p>b. In each file where parallelization is desired, add:     <pre><code>import bodo\n</code></pre></p> </li> <li> <p>JIT-Compile Your Main Processing Functions</p> <p>a. Decorate computationally intensive functions with <code>@bodo.jit(cache=True)</code>.</p> <p>b. Gather all configuration inputs (e.g., file lists) outside of the jitted function and pass those inputs into the jitted function as parameters.</p> <ul> <li> <p>Any file discovery logic (e.g., <code>glob.glob</code>, or referencing environment variables) should happen outside your jitted function.</p> </li> <li> <p>The jitted function should receive the final list of files/tables and other necessary parameters explicitly.</p> </li> </ul> <p>c. Keep I/O of large data inside JIT functions if the storage format is supported by Bodo (Parquet, CSV, JSON, Iceberg, Snowflake).</p> <p>Example code:</p> <pre><code>@bodo.jit(cache=True)\ndef process_data(file_list):\n   df = pd.read_parquet(file_list)\n   print(df.A.sum())\n\nfile_list = get_file_list()\nprocess_data(file_list)\n</code></pre> </li> <li> <p>Avoid Python Features Incompatible With Bodo</p> <p>Inside jitted functions, avoid:</p> <ul> <li>Using list/set/dict data structures for large data.</li> <li>Unusual dynamic Python features (e.g., closures capturing changing state).</li> <li>Unused imports or library calls that Bodo cannot compile.</li> </ul> <p>Generally, use Pandas DataFrames and Numpy arrays for large data and use idiomatic Pandas code to process data.</p> </li> <li> <p>Use <code>@bodo.wrap_python</code> for Calling Non-JIT Libraries inside JIT</p> <p>A common pattern is calling a Python function using a domain-specific library on every row of a dataframe. Use <code>@bodo.wrap_python</code> for this case. Provide output type of the Python call using a sample of representative output:</p> <pre><code>tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\nout_list_type = bodo.typeof([1, 2])\n\n@bodo.wrap_python(out_list_type)\ndef run_tokenizer(text):\n    tokenized = tokenizer(text)\n    return tokenized[\"input_ids\"]\n\n@bodo.jit\ndef preprocess_pile(file_list):\n    df = pd.read_parquet(file_list)\n    df[\"input_ids\"] = df[\"text\"].map(run_tokenizer)\n    ...\n</code></pre> </li> </ol> <p>See Compilation Tips and Troubleshooting for more tips on handling compilation issues.</p>","tags":["getting started"]},{"location":"quick_start/quickstart_local_iceberg/","title":"Iceberg Quick Start","text":""},{"location":"quick_start/quickstart_local_iceberg/#quickstart-local-iceberg","title":"Bodo Iceberg Quick Start","text":"<p>This quickstart guide will walk you through the process of creating and reading from an Iceberg table using Bodo on your local machine.</p>"},{"location":"quick_start/quickstart_local_iceberg/#installation","title":"Installation","text":"<p>Install Bodo to get started (e.g., <code>pip install -U bodo[iceberg]</code> or <code>conda install bodo pyiceberg -c conda-forge</code>).</p>"},{"location":"quick_start/quickstart_local_iceberg/#create-a-local-iceberg-table-with-bodo-dataframe-library","title":"Create a Local Iceberg Table with Bodo DataFrame Library","text":"<p>This example demonstrates simple write of a table on the filesystem without a catalog:</p> <pre><code>import bodo.pandas as pd\nimport numpy as np\n\nn = 20_000_000\ndf = pd.DataFrame({\"A\": np.arange(n) % 30, \"B\": np.arange(n)})\ndf.to_iceberg(\"test_table\", location=\"./iceberg_warehouse\")\n</code></pre> <p>Now let's read the Iceberg table:</p> <pre><code>print(pd.read_iceberg(\"test_table\", location=\"./iceberg_warehouse\"))\n</code></pre> <p>See DataFrame Library API reference for more information. Note that this quickstart uses a local Iceberg table, but you can also use Bodo with Iceberg tables on S3, ADLS, and GCS as well.</p>"},{"location":"quick_start/quickstart_local_iceberg/#amazon-s3-tables","title":"Amazon S3 Tables","text":"<p>Amazon S3 Tables simplify Iceberg use and table maintenance by providing builtin Apache Iceberg support. Bodo supports S3 Tables in both Python and SQL seamlessly. Here is a step by step example for using S3 Tables in Bodo.</p> <p>Make sure you have your environment ready:</p> <ol> <li>Create a Table bucket on S3 (not a regular bucket).    You can simply use the console with this link (replace region if desired).</li> <li>Make sure you have AWS credentials in your environment (e.g. <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>).</li> <li>Make sure the user associated with your credentials has <code>AmazonS3TablesFullAccess</code> policy attached. You can use IAM in the AWS console (e.g. this link).</li> <li>Set default region to the bucket region in the environment. For example:     <pre><code>export AWS_REGION=\"us-east-2\"\n</code></pre></li> <li>Make sure you have the latest AWS CLI (see here)    since this is a new feature and create a namespace in the table bucket. For example (replace region, account number and bucket name):     <pre><code>aws s3tables create-namespace --table-bucket-arn arn:aws:s3tables:us-east-2:111122223333:bucket/my-test-bucket --namespace my_namespace\n</code></pre></li> </ol> <p>Now you are ready to use Bodo to read and write S3 Tables. Run this example code (replace bucket name, account ID, region, namespace):</p> <pre><code>import numpy as np\n\nimport bodo.pandas as pd\n\nBUCKET_NAME=\"my-test-bucket\"\nACCOUNT_ID=\"111122223333\"\nREGION=\"us-east-2\"\nNAMESPACE=\"my_namespace\"\nARN=f\"arn:aws:s3tables:{REGION}:{ACCOUNT_ID}:bucket/{BUCKET_NAME}\"\n\nNUM_GROUPS = 30\nNUM_ROWS = 20_000_000\n\n\ndf = pd.DataFrame({\n    \"A\": np.arange(NUM_ROWS) % NUM_GROUPS,\n    \"B\": np.arange(NUM_ROWS)\n})\ndf.to_iceberg(f\"{NAMESPACE}.my_table_1\", location=ARN)\n\n\ndf_read = pd.read_iceberg(\n    f\"{NAMESPACE}.my_table_1\",\n    location=ARN,\n)\nprint(df_read)\n</code></pre> <p>You can use BodoSQL to work with S3 Tables as well. Here is a simple example:</p> <pre><code>import pandas as pd\nimport bodosql\n\nBUCKET_NAME=\"my-test-bucket\"\nACCOUNT_ID=\"111122223333\"\nREGION=\"us-east-2\"\nNAMESPACE=\"my_namespace\"\nARN_STR=f\"arn:aws:s3tables:{REGION}:{ACCOUNT_ID}:bucket/{BUCKET_NAME}\"\n\ncatalog = bodosql.S3TablesCatalog(ARN_STR)\nbc = bodosql.BodoSQLContext(catalog=catalog)\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [\"a\", \"b\", \"c\"]})\nbc = bc.add_or_replace_view(\"TABLE1\", df)\n\nquery = f\"\"\"\nCREATE OR REPLACE TABLE \"{NAMESPACE}\".\"my_table\" AS SELECT * FROM __bodolocal__.table1\n\"\"\"\nbc.sql(query)\n\ndf_read = bc.sql(f\"SELECT * FROM \\\"{NAMESPACE}\\\".\\\"my_table\\\"\")\nprint(df_read)\n</code></pre>"},{"location":"quick_start/quickstart_local_python/","title":"Python Quick Start","text":""},{"location":"quick_start/quickstart_local_python/#quickstart-local-python","title":"Bodo Python Quick Start","text":"<p>This quickstart guide will walk you through the process of running a simple Python computation using Bodo on your local machine.</p>"},{"location":"quick_start/quickstart_local_python/#installation","title":"Installation","text":"<p>Install Bodo to get started (e.g., <code>pip install -U bodo</code> or <code>conda install bodo -c conda-forge</code>).</p>"},{"location":"quick_start/quickstart_local_python/#drop-in-pandas-replacement-with-bodo-dataframes","title":"Drop-in Pandas Replacement with Bodo DataFrames","text":"<p>Bodo DataFrame library can be used as a drop-in replacement for Pandas by changing <code>import pandas as pd</code> with <code>import bodo.pandas as pd</code>. For example:</p> <pre><code>import bodo.pandas as pd\nimport numpy as np\n\nn = 20_000_000\ndf = pd.DataFrame({\"A\": np.arange(n) % 30, \"B\": np.arange(n)})\ndf2 = df.groupby(\"A\", as_index=False)[\"B\"].max()\ndf2.to_parquet(\"my_data.pq\")\n</code></pre> <p>Bodo DataFrame library will optimize and parallelize the code automatically when possible. It will fall back to Pandas seamlessly when some API isn't supported yet and throw a warning. See the DataFrame Library API reference for supported Pandas APIs.</p>"},{"location":"quick_start/quickstart_local_python/#bodo-jit-compilation-for-custom-code","title":"Bodo JIT Compilation for Custom Code","text":"<p>JIT compilation converts Python functions to optimized parallel binaries, which can provide orders of magnitude performance boost for custom code like user defined functions (UDFs). For example:</p> <pre><code>import bodo\nimport bodo.pandas as pd\nimport numpy as np\n\n@bodo.jit\ndef f(df):\n    return df.apply(lambda r: 0 if r.A == 0 else (r.B // r.A), axis=1)\n\nn = 20_000_000\ndf = pd.DataFrame({\"A\": np.arange(n) % 30, \"B\": np.arange(n)})\nS = f(df)\npd.DataFrame({\"C\": S}).to_parquet(\"my_data.pq\")\n</code></pre> <p>All the code in JIT functions has to be compilable by Bodo JIT (will throw appropriate errors otherwise). See JIT development guide and JIT API reference for supported Python features and APIs.</p>"},{"location":"quick_start/quickstart_local_sql/","title":"SQL Quick Start","text":""},{"location":"quick_start/quickstart_local_sql/#quickstart-local-sql","title":"Bodo SQL Quick Start","text":"<p>This quickstart guide will walk you through the process of running a simple SQL query using Bodo on your local machine.</p>"},{"location":"quick_start/quickstart_local_sql/#prerequisites","title":"Prerequisites","text":"<p>Install Bodo to get started (e.g., <code>pip install -U bodo</code> or <code>conda install bodo -c conda-forge</code>). Additionally, install bodosql with pip or conda:</p> <pre><code>pip install bodosql\n</code></pre> <pre><code>conda install bodosql -c bodo.ai -c conda-forge\n</code></pre>"},{"location":"quick_start/quickstart_local_sql/#generate-sample-data","title":"Generate Sample Data","text":"<p>Let's start by creating a Parquet file with some sample data. The following Python code creates a Parquet file with two columns <code>A</code> and <code>B</code> and 20 million rows. The column <code>A</code> contains values from 0 to 29, and the column <code>B</code> contains values from 0 to 19,999,999.</p> <pre><code>import pandas as pd\nimport numpy as np\nimport bodo\nimport bodosql\n\nNUM_GROUPS = 30\nNUM_ROWS = 20_000_000\ndf = pd.DataFrame({\n    \"A\": np.arange(NUM_ROWS) % NUM_GROUPS,\n    \"B\": np.arange(NUM_ROWS)\n})\ndf.to_parquet(\"my_data.pq\")\n</code></pre>"},{"location":"quick_start/quickstart_local_sql/#create-a-local-in-memory-sql-table","title":"Create a local in-memory SQL Table","text":"<p>Now let's create a local in-memory SQL table from the Parquet file. We can use the <code>TablePATH</code> API to register the table into our <code>BodoSQLContext</code>.</p> <pre><code>bc = bodosql.BodoSQLContext(\n    {\n        \"TABLE1\": bodosql.TablePath(\"my_data.pq\", \"parquet\")\n    }\n)\n</code></pre>"},{"location":"quick_start/quickstart_local_sql/#write-a-sql-query","title":"Write a SQL Query","text":"<p>Now we can write a SQL query to compute the sum of column <code>A</code> for all rows where <code>B</code> is greater than 4.</p> <pre><code>df1 = bc.sql(\"SELECT SUM(A) as SUM_OF_COLUMN_A FROM TABLE1 WHERE B &gt; 4\")\nprint(df1)\n</code></pre>"},{"location":"quick_start/quickstart_local_sql/#running-your-code","title":"Running your code","text":"<p>Bringing it all together, the complete code looks like this:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport bodosql\n\nNUM_GROUPS = 30\nNUM_ROWS = 20_000_000\n\ndf = pd.DataFrame({\n    \"A\": np.arange(NUM_ROWS) % NUM_GROUPS,\n    \"B\": np.arange(NUM_ROWS)\n})\n\ndf.to_parquet(\"my_data.pq\")\n\nbc = bodosql.BodoSQLContext(\n    {\n        \"TABLE1\": bodosql.TablePath(\"my_data.pq\", \"parquet\")\n    }\n)\n\ndf1 = bc.sql(\"SELECT SUM(A) as SUM_OF_COLUMN_A FROM TABLE1 WHERE B &gt; 4\")\nprint(df1)\n</code></pre> <p>To run the code, save it to a file, e.g. <code>test_bodo_sql.py</code>, and run the following command in your terminal:</p> <pre><code>python test_bodo_sql.py\n</code></pre> <p>By default Bodo will use all available cores. To set a limit on the number of processes spawned, set the environment variable <code>BODO_NUM_WORKERS</code>. Check the SQL API Reference for the full list of supported SQL operations.</p>"},{"location":"quick_start/quickstart_platform/","title":"Getting started with the Bodo Cloud Platform","text":"<p>This page provides a quick start guide to set up Bodo platform in your AWS account easily and quickly.</p>","tags":["getting started","platform"]},{"location":"quick_start/quickstart_platform/#aws-marketplace-registration","title":"AWS Marketplace Registration","text":"<p>a.  Subscribe through the AWS Marketplace.</p> <p>Important</p> <p>Make sure you have the necessary permissions to subscribe to the product in the AWS Marketplace. You need at least <code>AWSMarketplaceManageSubscriptions</code> to manage subscriptions on AWS Marketplace. If you are not able to subscribe, please contact your AWS account administrator to get the necessary permissions.</p> <p>b.  After confirming your subscription, click Set up your Account at the top right corner of the page. Bodo Platform's registration page will open in a new tab.</p> <p></p> <p>c.  Fill out the fields with your information. If this is your     individual account, use a unique name such as     firstname_lastname for the Organization Name     field.</p> <p>d.  Check the box for accepting terms and conditions and click on     <code>SIGN UP</code>:     </p> <p>e.  A page confirming that an activation link was sent to your email     will appear. Please open the email and click on the activation link:          Clicking on the confirmation link will take you to the Bodo Platform     page where you can use your newly created credentials to sign in:     </p>","tags":["getting started","platform"]},{"location":"quick_start/quickstart_platform/#aws-cloudformation-quick-start","title":"AWS CloudFormation Quick Start","text":"<p>To be able to use the Bodo Platform to launch clusters and notebooks, you must grant it permission to access your AWS account and provision the required resources in it. These permissions are stored in the Bodo Platform using a \"Cloud Configuration\". We provide an AWS CloudFormation template that you can execute in your AWS account, which will create the necessary resources to support the Cloud Configuration in the Bodo Platform.</p> <p>Tip</p> <p>You need the following set of permissions to successfully create the resources defined in the CloudFormation stack:</p> <ul> <li><code>AWSCloudFormationFullAccess</code></li> <li><code>AWSIAMFullAccess</code></li> </ul> <p>Once you have ensured that you have all permissions necessary to create the resources, follow the steps below to create a Cloud Configuration:</p> <ol> <li> <p>Access the Cloud Configuration page from the left sidebar. Then, click on Create Cloud Configuration in the top right corner of the page as shown in the picture below:</p> <p></p> </li> <li> <p>Select \"AWS\" as the cloud provider, and click on \"Cloud Formation Template\".</p> <p>Important</p> <p>Do not close the form until you have completed all the steps following this one.</p> </li> <li> <p>Fill in the following values :</p> <ul> <li>Cloud Configuration Name: A name for your Cloud Configuration.</li> <li>CloudFormation Stack Region: Fill this with the region where you want to deploy the stack.</li> </ul> </li> <li> <p>Click on Launch CloudFormation Template. This will open the AWS CloudFormation console in a new tab in the selected region.</p> <p>Important</p> <p>All the values are pre-filled in the CloudFormation template. Please do not change any values.</p> </li> <li> <p>Click on \"Create Stack\" to create the stack. This will create the necessary resources in your AWS account.     </p> <p>Important</p> <p>The stack creation process may take a few minutes to complete. Please wait until the stack is created successfully.</p> </li> <li> <p>You can check the status of the stack from Bodo Platform as shown below. Once the stack is created successfully, Cloud Configuration will be created.</p> <p></p> </li> </ol>","tags":["getting started","platform"]},{"location":"quick_start/quickstart_platform/#bodo-workspace-creation","title":"Bodo Workspace Creation","text":"<p>Once your Cloud Configuration has been created, navigate to the Workspaces tab and click the Create Workspace button in the  top right corner. Once your workspace has finished creating, you will be able to enter it.  </p> <p>Once inside the Workspace, navigate to the Notebooks tab.    </p> <p>That\u2019s it, you\u2019re all set to experience Bodo. Follow along one of our tutorials or go through the curated list of bodo-examples.  See <code>bodo-examples</code> for a set of notebooks ready to be run in your free trial environment.</p> <p>See Also</p> <p>If you want to get started on using Bodo on Azure, please refer to the Azure Platform Setup guide.</p>","tags":["getting started","platform"]},{"location":"quick_start/quickstart_platform/#basic-terminology","title":"Basic Terminology","text":"","tags":["getting started","platform"]},{"location":"quick_start/quickstart_platform/#jupyter-notebooks","title":"Notebooks","text":"<p>A Notebook is a simple instances of a JupyterLab Server. You can use   a notebook to navigate the files in your workspace, define and execute   your workflows on the clusters, etc.</p>","tags":["getting started","platform"]},{"location":"quick_start/quickstart_platform/#clusters","title":"Clusters","text":"<p>A Cluster provides your notebooks with necessary compute resources.   Each cluster is tied to a specific Bodo version. A cluster also has   additional configuration parameters such as the number of instances,   auto-pause duration, etc. You can read more about creating a cluster here.</p>","tags":["getting started","platform"]},{"location":"quick_start/quickstart_platform/#workspaces","title":"Workspaces","text":"<p>Workspaces are a basic unit of organization on Bodo platform. You can   think of a workspace as a directory on a UNIX file system. Each Workspace   contains zero or more Notebooks, Clusters, etc.</p>","tags":["getting started","platform"]},{"location":"quick_start/quickstart_platform_sdk/","title":"Bodo Platform SDK Quick Start","text":"<p>This quickstart guide will walk you through running a job on the Bodo Platform using the Bodo Platform SDK installed on your local machine.</p>"},{"location":"quick_start/quickstart_platform_sdk/#getting-started","title":"Getting Started","text":""},{"location":"quick_start/quickstart_platform_sdk/#installation","title":"Installation","text":"<pre><code>pip install bodosdk\n</code></pre>"},{"location":"quick_start/quickstart_platform_sdk/#create-a-workspace-client","title":"Create a workspace client","text":"<p>To authenticate with the Bodo Platform API, you need to create an API Token:</p> <ol> <li>Log in to your workspace at https://platform.bodo.ai/.</li> <li>Navigate to API Tokens in the Admin Console. </li> <li>Generate a token and copy the Client ID and Secret Key.</li> </ol> <p>Use these credentials to define a <code>BodoWorkspaceClient</code> for interacting with the platform:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient(\n    client_id=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    secret_key=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n)\n</code></pre>"},{"location":"quick_start/quickstart_platform_sdk/#create-a-cluster","title":"Create a cluster","text":"<p>Create a single-node cluster in your workspace with the latest available Bodo version:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient(\n    client_id=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    secret_key=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n)\n\nmy_cluster = my_workspace.ClusterClient.create(\n    name='My first cluster',\n    instance_type='c6i.large',\n    workers_quantity=1\n)\nmy_cluster.wait_for_status(['RUNNING'])\nprint(my_cluster.id)\n</code></pre>"},{"location":"quick_start/quickstart_platform_sdk/#run-python-job","title":"Run Python Job","text":"<p>Step 1: Write the Job Script</p> <p>Access https://platform.bodo.ai and open the Jupyter notebook in your workspace. Create the following test.py file in your main directory:</p> <pre><code>import bodo\nimport time\nimport numpy as np\n\n@bodo.jit\ndef calc_pi(n):\n    t1 = time.time()\n    x = 2 * np.random.ranf(n) - 1\n    y = 2 * np.random.ranf(n) - 1\n    pi = 4 * np.sum(x**2 + y**2 &lt; 1) / n\n    print(\"Execution time:\", time.time()-t1, \"\\nresult:\", pi)\n\ncalc_pi(2 * 10**6)\n</code></pre> <p>Step 2: Run the Job</p> <p>Use the SDK to run the job on your cluster, wait for it to SUCCEED, and check its logs:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient(\n    client_id=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    secret_key=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n)\n\nmy_cluster = my_workspace.ClusterClient.get(\"cluster_id\")\nmy_job = my_cluster.run_job(\n    code_type='PYTHON',\n    source={'type': 'WORKSPACE', 'path': '/'},\n    exec_file='test.py'\n)\n\n# Print stdout from job\nprint(my_job.wait_for_status(['SUCCEEDED']).get_stdout())\n</code></pre>"},{"location":"quick_start/quickstart_platform_sdk/#run-a-sql-job","title":"Run a SQL Job","text":"<p>To run a SQL job, create a test.sql file and a catalog in https://platform.bodo.ai. Then, run the job as follows: <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient(\n    client_id=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    secret_key=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n)\n\nmy_cluster = my_workspace.ClusterClient.get(\"cluster_id\")\nmy_job = my_cluster.run_job(\n    code_type='SQL',\n    source={'type': 'WORKSPACE', 'path': '/'},\n    exec_file='test.sql',\n    catalog=\"MyCatalog\"\n)\n\n# Print stdout from job\nprint(my_sql_job.wait_for_status(['SUCCEEDED']).get_stdout())\n</code></pre></p>"},{"location":"quick_start/quickstart_platform_sdk/#execute-sql-query","title":"Execute SQL query","text":"<p>Execute SQL queries by passing just query text like following:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient(\n    client_id=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    secret_key=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n)\n\nmy_cluster = my_workspace.ClusterClient.get(\"cluster_id\")\n\n# Execute query\nmy_sql_job = my_cluster.run_sql_query(sql_query=\"SELECT 1\", catalog=\"MyCatalog\")\n\n# Print stdout from job\nprint(my_sql_job.wait_for_status(['SUCCEEDED']).get_stdout())\n</code></pre>"},{"location":"quick_start/quickstart_platform_sdk/#connector","title":"Connector","text":"<p>Execute SQL queries using a cluster Connector via a Cursor:</p> <pre><code>from bodosdk import BodoWorkspaceClient\n\nmy_workspace = BodoWorkspaceClient(\n    client_id=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    secret_key=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n)\n\nmy_cluster = my_workspace.ClusterClient.get(\"cluster_id\")\n\n# Connect and execute query\nconnection = my_cluster.connect('MyCatalog')\nresult = connection.cursor().execute(\"SELECT 1\").fetchone()\nprint(result)\n</code></pre> <p>See Also</p> <ul> <li>Bodo SDK Guide</li> <li>Bodo SDK Reference</li> <li>Bodo SDK PyPi</li> </ul>"},{"location":"release_notes/","title":"Release Notes","text":"<ul> <li>Bodo.ai 2025.7 Release</li> <li>Bodo.ai 2025.6 Release</li> <li>Bodo.ai 2025.5 Release</li> <li>Bodo.ai 2025.4 Release</li> <li>Bodo.ai 2025.3 Release</li> <li>Bodo.ai 2025.2 Release</li> <li>Bodo.ai 2025.1 Release</li> <li>Bodo.ai 2024.12 Release</li> <li>Bodo.ai 2024.11 Release</li> <li>Bodo.ai 2024.10 Release</li> <li>Bodo.ai 2024.9 Release</li> <li>Bodo.ai 2024.8 Release</li> <li>Bodo.ai 2024.7 Release</li> <li>Bodo.ai 2024.6 Release</li> <li>Bodo.ai 2024.5 Release</li> <li>Bodo.ai 2024.4 Release</li> <li>Bodo.ai 2024.3 Release</li> <li>Bodo.ai 2024.2 Release</li> <li>Bodo.ai 2024.1 Release</li> <li>Bodo.ai 2023.12 Release</li> <li>Bodo.ai 2023.11 Release</li> <li>Bodo.ai 2023.10 Release</li> <li>Bodo.ai 2023.9 Release</li> <li>Bodo.ai 2023.7 Release</li> <li>Bodo.ai 2023.6 Release</li> <li>Bodo.ai 2023.1 Release</li> <li>Bodo.ai 2022.9 Release</li> <li>Bodo.ai 2022.8 Release</li> <li>Bodo.ai 2022.7 Release</li> <li>Bodo.ai 2022.6 Release</li> <li>Bodo.ai 2022.5 Release</li> <li>Bodo.ai 2022.4 Release</li> <li>Bodo.ai 2022.3 Release</li> <li>Bodo.ai 2022.2 Release</li> <li>Bodo.ai 2022.1 Release</li> <li>Bodo.ai 2021.12 Release</li> <li>Bodo.ai 2021.11 Release</li> <li>Bodo.ai 2021.10 Release</li> <li>Bodo.ai 2021.9 Release</li> <li>Bodo.ai 2021.8 Release</li> <li>Bodo.ai 2021.7 Release</li> <li>Bodo.ai 2021.5 Release</li> <li>Bodo.ai 2021.4 Release</li> <li>Bodo.ai 2021.3 Release</li> <li>Bodo.ai 2021.2 Release</li> <li>Bodo.ai 2021.1 Release</li> <li>Bodo.ai 2020.12 Release</li> <li>Bodo.ai 2020.11 Release</li> <li>Bodo.ai 2020.10 Release</li> <li>Bodo.ai 2020.9 Release</li> <li>Bodo.ai 2020.8 Release</li> <li>Bodo.ai 2020.7 Release</li> <li>Bodo.ai 2020.6 Release</li> <li>Bodo.ai 2020.5 Release</li> <li>Bodo.ai 2020.4 Release</li> <li>Bodo.ai 2020.2 Release</li> </ul>"},{"location":"release_notes/2020.02/","title":"Bodo 2020.02 Release (Date: 02/14/2020)","text":""},{"location":"release_notes/2020.02/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo now utilizes the following packages:</p> <pre><code>-   pandas &gt;= 1.0.0\n-   numba 0.48.0\n-   Apache Arrow 0.16.0\n</code></pre> </li> <li> <p>Custom S3 endpoint is supported as well as S3-like object storage     systems such as MinIO</p> </li> <li> <p>Reading and writing of parquet files with S3 is more robust</p> </li> <li> <p>Parquet read now supports reading columns where elements are list of     strings</p> </li> <li> <p>pandas.read_csv() now also accepts a list of column names for the     parse_date parameter</p> </li> <li> <p>pandas groupby.agg() supports list of functions for a column:</p> <pre><code>df = pd.DataFrame(\n    {\"A\": [2, 1, 1, 1, 2, 2, 1], \"B\": [\"a\", \"b\", \"c\", \"c\", \"b\", \"c\", \"a\"]}\n)\ngb = df.groupby(\"B\").agg({\"A\": [\"sum\", \"mean\"]})\n</code></pre> </li> <li> <p>pandas groupby.agg() now supports a tuple of built-in functions:</p> <pre><code>gb = df.groupby(\"B\")[\"A\"].agg((\"sum\", \"median\"))\n</code></pre> </li> <li> <p>User-defined functions can now be used with groupby.agg() and     constant dict:</p> <pre><code>gb = df.groupby(\"B\").agg({\"A\": my_function})\n</code></pre> </li> <li> <p>The compilation time and run time have been improved for pandas     groupby with <code>median</code>, <code>cumsum</code>, and     <code>cumprod</code>.</p> </li> <li> <p>pandas groupby now supports <code>cumsum</code>, <code>max</code>,     <code>min</code>, <code>prod</code>, <code>sum</code> functions     for string columns.</p> </li> <li> <p>pandas groupby.agg() now supports mixing median and nunique with     other functions, and use of multiple \"cumulative\" operations in     the same groupby (example: cumsum, cumprod, etc).</p> </li> <li> <p>Selecting groupby columns using attribute is now possible:</p> <pre><code>df = pd.DataFrame(\n    {\"A\": [2, 1, 1, 1, 2, 2, 1], \"B\": [3, 5, 6, 5, 4, 4, 3]}\n)\ndf.groupby('A').B.sum()\n</code></pre> </li> <li> <p>pandas <code>Series.str.extractall</code>,     <code>Series.all()</code> and <code>Series.any()</code> are     supported</p> </li> <li> <p>Support for returning MultiIndex in groupby operations</p> </li> <li> <p>Various forms of UDFs in df.apply and Series.map are supported</p> </li> <li> <p>Comparison of datetime fields with datetime constants is now     possible</p> </li> <li> <p>Converting <code>date</code> and <code>datetime</code> of Python     <code>datetime</code> module to pandas Timestamp is now supported</p> </li> <li> <p>Conversion to float using float class as dtype for pandas     Series.astype() is now supported:</p> <pre><code>S = pd.Series(['1', '2', '3'])\nS.astype(float)\n</code></pre> </li> </ul>"},{"location":"release_notes/2020.02/#bug-fix","title":"Bug Fix","text":"<ul> <li>Fixed a memory leak issue when returning a dataframe from a Bodo     function</li> <li>pandas DataFrame.sort_values() now returns correct output for input     cases that contain NA</li> <li>Groupby.agg: explicit column selection when using constant     dictionary is no longer required</li> <li>Fixed an issue that Bodo always dropped the index in reset_index()</li> </ul>"},{"location":"release_notes/2020.04/","title":"Bodo 2020.04 Release (Date: 04/08/2020)","text":""},{"location":"release_notes/2020.04/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li>Support for <code>scatterv</code> operation</li> <li>Improved memory management for DataFrame and Series data</li> <li>Initial support for <code>pandas.read_sql()</code></li> <li><code>pandas.read_csv()</code> reads a directory of csv files</li> <li><code>pandas.read_csv()</code> reads from S3, and Hadoop Distributed File     System (HDFS)</li> <li><code>pandas.read_parquet()</code> now reads all integer types (like int16) and     gets nullable information for integer columns from pandas metadata</li> <li><code>pandas.read_parquet()</code> now supports reading columns of list of     string elements</li> <li>avoid type error for unselected columns in Parquet files</li> <li>support <code>pandas.RangeIndex</code> when reading a non-partitioned parquet     dataset</li> <li><code>pandas.Dataframe.to_parquet()</code> to Hadoop Distributed File System     (HDFS)</li> <li><code>pandas.Dataframe.to_parquet()</code> always writes <code>pandas.RangeIndex</code> to     Parquet metadata</li> <li>support <code>pandas.Dataframe.to_parquet()</code> writing datetime64 (default     in Pandas) and <code>datatime.date</code> types to Parquet files</li> <li>support <code>decimal.Decimal</code> type in dataframes and Parquet I/O</li> <li>Support for <code>&amp;</code>, <code>|</code>, and <code>pandas.Series.dt</code> in     <code>pandas.Dataframe.query()</code></li> <li>Support added for groupby <code>last</code> operation</li> <li><code>min</code>, <code>max</code>, and <code>sum</code> support in <code>groupby()</code> for string columns</li> <li>non-constant list of column names as argument support for functions     like <code>groupby()</code></li> <li>MultiIndex support for <code>groupby(...).agg(as_index=False)</code></li> <li><code>pandas.Dataframe.merge()</code> one dataframe on index, and the other on     a column</li> <li>sorting compilation time improvement</li> <li>supports for integer, float, string, string list, <code>datetime.date</code>,     <code>datetime.datetime</code>, and <code>datetime.timedelta</code> types in     <code>pandas.Series.cummin()</code>, <code>pandas.DataFrame.cummin()</code>,     <code>pandas.Series.cummax()</code>, and <code>pandas.DataFrame.cummax()</code></li> <li><code>NA</code>s in <code>datetime.date</code> array</li> <li>better <code>datetime.timedelta</code> support</li> <li>Support for <code>min</code> and <code>max</code> in <code>pandas.Timestamp</code> and     <code>datetime.date</code></li> <li><code>pandas.DataFrame.all()</code> for boolean series</li> <li><code>pandas.Series.astype()</code> to float, int, str</li> <li>Convert string columns to float using <code>astype()</code></li> <li><code>NA</code> support for <code>Series.str.split()</code></li> <li>refactored and improved Dataframe indexing: <code>pandas.loc()</code>,     <code>pandas.Dataframe.iloc()</code>, and <code>pandas.Dataframe.iat()</code></li> <li>better support for <code>pandas.Series.shift()</code>,     <code>pandas.Series.pct_change()</code>, <code>pandas.Dataframe.drop()</code></li> <li>set dataframe column using a scalar</li> <li>support for <code>Index.values</code></li> <li>Addition support for String columns</li> </ul>"},{"location":"release_notes/2020.04/#bug-fix","title":"Bug Fix","text":"<ul> <li><code>pandas.join()</code> produce the correct index.</li> <li><code>pandas.groupby()</code> use the latest schema</li> <li><code>groupby(...).cumsum()</code> preserves index</li> <li><code>groupby(...).agg()</code> when passing a dictionary of functions: support     mix of multi-function lists and single functions</li> <li>Fixed Numpy slicing error in a corner case when the slice is     equivalent to array and array size is a constant</li> <li>proper construction of dataframe from slicing Numpy 2D array</li> <li><code>pandas.read_csv</code> reads a dataframe containing only datetime like     columns</li> <li>When using <code>pandas.merge()</code> and <code>pandas.join()</code> integer columns     which can have a missing value <code>NA</code> are returned as     nullable integer array (as opposed to <code>0</code> and     <code>-1</code> before)</li> <li>avoid errors in comparing Pandas and Numpy</li> </ul>"},{"location":"release_notes/2020.05/","title":"Bodo 2020.05 Release (Date: 05/06/2020)","text":""},{"location":"release_notes/2020.05/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> Bodo is updated to use the latest versions of Numba and Apache Arrow packages: <ul> <li>numba 0.49.0</li> <li>Apache Arrow 0.17.0</li> </ul> </li> <li> <p>Various improvements to clarity and conciseness of error messages</p> </li> <li> <p>Initial support for <code>pandas.DataFrame.to_sql()</code></p> </li> <li> <p><code>pandas.read_sql()</code> support <code>sql</code> and <code>con</code> passed to Bodo-decorated     functions</p> </li> <li> <p>Added support for <code>pandas.read_json()</code> and     <code>pandas.DataFrame.to_json()</code> from &amp; to POSIX, S3, and Hadoop File     Systems.</p> </li> <li> <p>Initial support for <code>pandas.read_excel()</code></p> </li> <li> <p><code>numpy.fromfile()</code> and <code>numpy.tofile()</code> from and to S3, and Hadoop     File Systems.</p> </li> <li> <p>Reduction in number of requests in I/O read calls</p> </li> <li> <p>Initial support for array of lists of fixed sized values</p> </li> <li> <p>List of strings data type support for <code>pandas.DataFrame.join()</code>,     <code>pandas.DataFrame.drop_duplicates()</code>, and     <code>pandas.DataFrame.groupby()</code></p> </li> <li> <p><code>pandas.Timestamp</code> subtraction, min and max</p> </li> <li> <p>Improved support for null values in datetime and timedelta     operations</p> </li> <li> <p>Support <code>copy()</code> function for Series of <code>decimal.Decimal</code> and     <code>datetime.date</code> data types and most Index types</p> </li> <li> <p>Improved support for Series <code>decimal.Decimal</code> dtype</p> </li> <li> <p>String Series and Dataframe Column are now mutable and support     inplace <code>fillna()</code></p> </li> <li> <p><code>pandas.Series.round()</code></p> </li> <li> <p><code>pandas.Dataframe.assign()</code></p> </li> <li> <p>Support <code>groupby(...).first()</code> operation</p> </li> <li> <p><code>pandas.Dataframe.iloc</code> support for extracting a subset of columns</p> </li> <li> <p><code>numpy.array.sum(axis=0)</code></p> </li> <li> <p><code>numpy.reshape()</code> multi-dimensional distributed arrays</p> </li> <li> <p>Initial implementation of experimental legacy mode</p> </li> <li> <p>Proper error when using unsupported <code>pandas.(...)</code> &amp;     <code>pandas.Series.(...)</code> functions</p> </li> <li> <p>Improved robustness of <code>pandas.DataFrame</code> inplace operations</p> </li> <li> <p>Memory usage improvements</p> </li> <li> <p>Type safety improvements</p> </li> <li> <p>Compilation time improvements</p> </li> </ul>"},{"location":"release_notes/2020.05/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed an issue in <code>pandas.read_csv()</code> reading a large CSV file in     specific distributed cases</li> <li><code>numpy.dot()</code> with empty vector/matrix input</li> </ul>"},{"location":"release_notes/2020.06/","title":"Bodo 2020.06 Release (Date: 06/12/2020)","text":""},{"location":"release_notes/2020.06/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use the latest minor releases of Numba and Apache     Arrow packages:</p> <ul> <li>numba 0.49.1</li> <li>Apache Arrow 0.17.1</li> </ul> </li> <li> <p>Significant optimizations in read CSV/JSON/Parquet to reduce number     of requests, files opened and overall load on the filesystem (for     local filesystems, S3 and HDFS).</p> </li> <li> <p>Improvements in <code>pandas.read_csv()</code> and <code>pandas.read_json()</code>:</p> <ul> <li>Support reading compressed JSON and CSV files (gzip and bz2)</li> <li>Can read directories containing files with any extension</li> <li>Correctly handle CSV files with headers when reading a     directory of CSV files</li> <li>Support automatic data type inference of JSON files when     <code>orient='records'</code> and <code>lines=True</code></li> </ul> </li> <li> <p>Bodo can now automatically infer the required constant values (e.g.     list of key names for groupby) from the program in many cases. In     addition, Bodo raises informative errors for cases that are not     possible to infer automatically.</p> </li> <li> <p>Various improvements to support caching of Bodo functions, including     adding support for caching inside Jupyter Notebook (see     here     for more information)</p> </li> <li> <p>Support NA value check with <code>pandas.isna(A[i])</code></p> </li> <li> <p>Support creating empty dataframes and setting columns on empty     dataframes</p> </li> <li> <p>More balanced workload distribution across processor cores</p> </li> <li> <p>Support for user-defined functions calling other JIT functions, and     improved error messages for invalid cases</p> </li> <li> <p><code>pandas.read_parquet()</code>: support reading columns of list of     integers/floats</p> </li> <li> <p>Support <code>bodo.scatterv()</code> for arrays of list of     strings/integers/floats.</p> </li> <li> <p>Improved support for <code>pd.to_datetime()</code> to handle optional arguments     and cases such as string and integer array/Series inputs</p> </li> <li> <p>Improved <code>pd.concat</code> support to handle arrays of list,     <code>Decimal</code> and <code>datetime.date</code> values</p> </li> <li> <p>Improved array indexing (getitem/setitem) support for various data     types such as date/time cases and Decimals</p> </li> <li> <p>Support sorting of Decimal series</p> </li> <li> <p>Support Dataframe <code>merge</code> and <code>groupby</code> with Decimal columns</p> </li> <li> <p>Groupby: ignore non-numeric columns for numeric-only operations like     sum (same behavior as pandas).</p> </li> <li> <p>Support for comparison of Timedelta data types (<code>datetime.timedelta</code>     and <code>timedelta64</code>)</p> </li> <li> <p>Parallelization of <code>numpy.full()</code></p> </li> <li> <p>Support <code>glob.glob(...)</code> inside Bodo functions</p> </li> <li> <p>Error messages and warnings:</p> <ul> <li>Improvements to clarity and conciseness of error messages</li> <li>Can use numba syntax highlighting for Bodo errors (enable with     NUMBA_COLOR_SCHEME     environment variable)</li> </ul> </li> <li> <p>Documentation:</p> <ul> <li>New theme and style</li> <li>Revamped introductory material and guide</li> <li>Improved documentation for <code>pd.read_csv()</code> and     <code>pd.read_json()</code></li> <li>Documented Bodo's coverage of data types</li> </ul> </li> </ul> <p>Overall, 82 code patches are merged since the last release.</p>"},{"location":"release_notes/2020.07/","title":"Bodo 2020.07 Release (Date: 07/16/2020)","text":"<p>This release includes many new features and bug fixes. Overall, 59 code patches were merged since the last release, including the major addition of support for columns of array and struct values with arbitrary nesting.</p>"},{"location":"release_notes/2020.07/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use the latest version of Numba (Numba 0.50.1)</p> </li> <li> <p>Series and dataframe columns can have values that are arrays. For     example:</p> <pre><code>A          B\n0  0  [1, 2, 3]\n1  1     [4, 5]\n2  2        [6]\n3  3  [7, 8, 9]\n</code></pre> </li> <li> <p>Series and dataframe columns can have values that are structs. For     example:</p> <pre><code>A                   B\n0  0  {'A': 1, 'B': 2.1}\n1  1  {'A': 3, 'B': 4.3}\n2  2  {'A': 5, 'B': 6.5}\n3  3  {'A': 7, 'B': 8.4}\n</code></pre> </li> <li> <p>Array and Struct values can contain other arrays/structs with     arbitrary nesting. For example:</p> <pre><code>A                                     B\n0  0               {'A': [1, 2], 'B': [3]}\n1  1            {'A': [4, 5, 6], 'B': [7]}\n2  2  {'A': [8, 9, 10, 11], 'B': [12, 13]}\n3  3                {'A': [14], 'B': [15]}\n</code></pre> </li> <li> <p><code>df.drop_duplicates()</code> and <code>df.merge()</code> is supported for nested     array/struct columns.</p> </li> <li> <p>Added support for categorical array data type without explicit     categories. Added support for <code>Series.astype(cat_dtype)</code> and     <code>Series.astype(\"category\")</code>.</p> </li> <li> <p>Generalized <code>df.dropna()</code> to all arrays, and added support for     'how' and 'subset' options.</p> </li> <li> <p>Support for <code>Series.explode()</code></p> </li> <li> <p><code>series.median()</code>: support 'skipna' option and Decimal type, and     bug fixes.</p> </li> <li> <p>Added Series.radd/rsub/rmul/rdiv/rpow/rmod</p> </li> <li> <p>Support for Series.dot/kurt/kurtosis/skew/sem</p> </li> <li> <p>Added Series.mad (mean absolute deviation)</p> </li> <li> <p><code>Series.var()</code> and <code>Series.std()</code>: Added support for 'skipna' and     'ddof' options</p> </li> <li> <p>Support Series.equals</p> </li> <li> <p>Series product/sum: added support for 'min_count' and 'skipna'     options</p> </li> <li> <p>Support Index.map() for all Index type</p> </li> <li> <p>Support all Bodo array types as output of Series.map/apply, df.apply</p> </li> <li> <p>Support df.values with nullable int columns</p> </li> <li> <p>Bodo release builds now enforce licensing (expiration date and     maximum core count) via license keys provided as a file or an     environment variable called \"BODO_LICENSE\".</p> </li> </ul>"},{"location":"release_notes/2020.08/","title":"Bodo 2020.08 Release (Date: 08/21/2020)","text":"<p>This release includes many new features, bug fixes and performance improvements. Overall, 112 code patches were merged since the last release.</p>"},{"location":"release_notes/2020.08/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use the latest versions of Numba, pandas and     Arrow:</p> <ul> <li>Numba 0.51.0</li> <li>pandas 1.1.0</li> <li>Arrow 1.0</li> </ul> </li> <li> <p>Support reading and writing Parquet files with columns where values     are arrays or structs, which can contain other arrays/structs with     arbitrary nesting.</p> </li> <li> <p>S3 I/O: automatically determine the region of the S3 bucket when     reading and writing.</p> </li> <li> <p>Initial support for scikit-learn RandomForestClassifier (fit,     predict and score methods)</p> </li> <li> <p>Support <code>sklearn.metrics.precision_score</code>,     <code>sklearn.metrics.recall_score</code> and <code>sklearn.metrics.f1_score</code>.</p> </li> <li> <p>Improved caching support (caching <code>@bodo.jit</code> functions with     cache=True)</p> </li> <li> <p>Initial support for arrays of map data structures</p> </li> <li> <p>Support <code>count</code> and <code>offset</code> arguments of <code>np.fromfile</code></p> </li> <li> <p>New <code>bodo.rebalance()</code> function for load balancing dataframes     manually if desired</p> </li> <li> <p>Support setting dataframe column as attribute, for example:     <code>df.B = \"AA\"</code></p> </li> <li> <p>Support DataFrame min/max/sum/prod/mean/median functions with     <code>axis=1</code></p> </li> <li> <p>Support <code>df.loc[:,columns]</code> indexing</p> </li> <li> <p><code>pd.concat</code> support for mix of Numpy and nullable integer/bool     arrays</p> </li> <li> <p>Support parallel append to dataframes (concatenation reduction)</p> </li> <li> <p>Support <code>GroupBy.idxmin</code> and <code>GroupBy.idxmax</code></p> </li> <li> <p>Improvements and optimizations in user-defined function (UDF)     handling</p> </li> <li> <p>Basic support for <code>Series.where()</code></p> </li> <li> <p>Support calling bodo.jit functions inside prange loops</p> </li> <li> <p>Support <code>DataFrame.select_dtypes</code> with constant strings</p> </li> <li> <p>Support <code>DataFrame.sample</code></p> </li> <li> <p>Support <code>Series.replace()</code> and <code>df.replace()</code> (scalars and lists)</p> </li> <li> <p>Support for Series.dt methods: <code>total_seconds()</code> and     <code>to_pytimedelta()</code></p> </li> <li> <p>Improved support for Categorical data types</p> </li> <li> <p>Support for <code>pandas.Timestamp.isocalendar()</code></p> </li> <li> <p>Support <code>np.digitize()</code></p> </li> <li> <p>Improved error handling during I/O when input CSV or Parquet file     does not exist</p> </li> <li> <p>Support pd.concat(axis=1) for dataframes</p> </li> <li> <p>Significant improvements in compilation time for dataframes with     large number of columns</p> </li> <li> <p><code>bodo.is_jit_execution()</code> can be used to know if a function is     running with Bodo.</p> </li> </ul>"},{"location":"release_notes/2020.09/","title":"Bodo 2020.09 Release (Date: 09/17/2020)","text":"<p>This release includes many new features, bug fixes and performance improvements. Overall, 88 code patches were merged since the last release.</p>"},{"location":"release_notes/2020.09/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use the latest versions of Numba, pandas and     Arrow:</p> <ul> <li>Numba 0.51.2</li> <li>pandas 1.1.2</li> <li>Arrow 1.0.1</li> </ul> </li> <li> <p>Major improvements in memory management. Bodo's memory consumption     is reduced significantly by releasing memory as soon as possible in     various operations such as Join, GroupBy, and Sort.</p> </li> <li> <p>Significant improvements in checking and handling various errors in     I/O, providing clear error messages and graceful exits.</p> </li> <li> <p>Improvements in speed and scalability of <code>read_parquet</code>     when reading from directories with large number of files.</p> </li> <li> <p>Distributed diagnostics is improved to provide clear messages on why     a variable was assigned REP distribution.</p> </li> <li> <p>Improvements in caching support for I/O calls and groupby     user-defined functions (UDFs).</p> </li> <li> <p>Support for more distributed getitem/setitem cases on arrays.</p> </li> <li> <p>Improvements on checking for unsupported functions and optional     arguments.</p> </li> <li> <p>Significant performance improvements in groupby transformations     (e.g. <code>GroupBy.cumsum</code>).</p> </li> <li> <p>Enhanced support for <code>DataFrame.select_dtypes</code>.</p> </li> <li> <p>Support for <code>axis=1</code> in <code>DataFrame.var/std</code>.</p> </li> <li> <p>Support for <code>Series.autocorr</code>.</p> </li> <li> <p>Support for <code>Series.is_monotonic_increasing/is_monotonic_decreasing</code>.</p> </li> <li> <p>Support <code>pd.Series()</code> constructor with a scalar data     value.</p> </li> <li> <p>Support for <code>dayofweek</code>, <code>is_leap_year</code> and     <code>days_in_month</code> in <code>Timestamp</code> and     `Series.dt]{.title-ref}.</p> </li> <li> <p>Support for <code>isocalendar</code> in <code>Series.dt</code> and     <code>DatetimeIndex</code>.</p> </li> <li> <p>Support for <code>Series.cumsum/cummin/cummax</code>.</p> </li> <li> <p>Support for <code>Decimal</code> values in nested data structures.</p> </li> <li> <p>Improvements in table join performance.</p> </li> <li> <p>Support for <code>Series.drop_duplicates</code>.</p> </li> <li> <p>Support for <code>np.dot</code> and <code>@</code> operator on     <code>Series</code>.</p> </li> <li> <p>Improvements in <code>pd.concat</code> support.</p> </li> <li> <p>Optimized <code>Series.astype(str)</code> for <code>int64</code>     values.</p> </li> <li> <p>Support for <code>pd.Index</code> constructor.</p> </li> </ul>"},{"location":"release_notes/2020.10/","title":"Bodo 2020.10 Release (Date: 10/20/2020)","text":"<p>This release includes many new features, bug fixes and performance improvements. Overall, 117 code patches were merged since the last release.</p>"},{"location":"release_notes/2020.10/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Initial support for Python classes using <code>bodo.jitclass</code> decorator.</p> </li> <li> Scikit-learn: <ul> <li></li> </ul> <pre><code>    Initial support for these scikit-learn classes:\n\n    :   -   `sklearn.linear_model.SGDClassifier`\n        -   `sklearn.linear_model.SGDRegressor`\n        -   `sklearn.cluster.KMeans`\n\n        For more information please refer to the documentation\n        [here](https://docs.bodo.ai/latest/source/sklearn.html)\n\n-   Improved scaling of `RandomForestClassifier` training\n</code></pre> </li> <li> <p>Memory management and memory consumption improvements</p> </li> <li> Improvements for User-defined functions (UDFs): <ul> <li>Compilation errors are now clearly shown for UDFs</li> <li>Support more complex UDFs (by running a full compiler     pipeline)</li> <li>Support passing keyword arguments to UDF in     <code>DataFrame.apply()</code> and <code>Series.apply()</code></li> <li>Support much wider range of UDF types in <code>groupby.agg</code></li> </ul> </li> <li> Connectors: <ul> <li>Improved connector error handling</li> <li>Improved performance of <code>pd.read_csv</code> (further improvements     in next release)</li> <li><code>pd.read_parquet</code> supports column containing all NA (null)     values</li> </ul> </li> <li> <p>Caching: for Bodo functions that receive parquet file names as     string arguments, the cache will now be reused when file name     arguments differ but have the same parquet dataset type (schema).</p> </li> <li> <p>Significantly improved the performance of merge/join operations in     some cases</p> </li> <li> <p>Support for loops over dataframe columns by automatic loop     unrolling</p> </li> <li> <p>Support using global dataframe/array values inside jit functions</p> </li> <li> <p>Performance optimization for the <code>series.str.split().explode()</code>     pattern</p> </li> <li> Pandas coverage: <ul> <li>Support setting <code>df.columns</code> and <code>df.index</code></li> <li>Support setting values in Categorical arrays</li> <li><code>series.str.split</code>: added support for regular expression and     <code>n</code> parameter</li> <li><code>Series.replace</code> support for more array types</li> <li>Support <code>pd.series.dt.quarter</code></li> <li>Support <code>series.str.slice_replace</code></li> <li>Support <code>series.str.repeat</code></li> <li>Improved support for <code>df.pivot_table</code> and <code>pd.crosstab</code></li> <li>Support for <code>Series.notnull</code></li> <li>Support integer label indexing for Dataframes and Series     with RangeIndex</li> <li>Support setting <code>None</code> and <code>Optional</code> values for most arrays</li> </ul> </li> <li> NumPy coverage: <ul> <li>Support for <code>np.union1d</code></li> <li><code>np.where</code>, <code>np.unique</code>, <code>np.sort</code>, <code>np.repeat</code>: support for     Series and most array types</li> <li>Support <code>np.argmax</code> with <code>axis=1</code></li> <li>Support for <code>np.min</code>, <code>np.max</code>, <code>min</code>, <code>max</code>, <code>np.sum</code>,     <code>sum</code>, <code>np.prod</code> on nullable arrays</li> </ul> </li> </ul>"},{"location":"release_notes/2020.11/","title":"Bodo 2020.11 Release (Date: 11/19/2020)","text":"<p>This release includes many new features, bug fixes and performance improvements. Overall, 126 code patches were merged since the last release.</p>"},{"location":"release_notes/2020.11/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use Apache Arrow 2.0 (latest)</p> </li> <li> <p>Performance and memory optimizations</p> <ul> <li>Significant memory usage optimizations for several operations     involving string arrays</li> <li>Up to 2x speedup for many string operations such as     <code>Series.str.replace/get/contains</code> and <code>groupby.sum()</code></li> </ul> </li> <li> <p>User-defined functions (UDFs)</p> <ul> <li>Support for returning datafarames from <code>DataFrame.apply()</code> and     <code>Series.apply()</code></li> <li>Support for returning nested arrays</li> </ul> </li> <li> <p>Caching: for Bodo functions that receive CSV and JSON file names as     string arguments, the cache will now be reused when file name     arguments differ but have the same dataset type (schema).</p> </li> <li> <p>Support for distributed deep learning with Tensorflow and PyTorch:     https://docs.bodo.ai/latest/source/dl.html</p> </li> <li> <p>Pandas coverage:</p> <ul> <li>Support for tuple values in Series and DataFrame columns</li> <li>Improvements to error checking and handling</li> <li>Automatic unrolling of loops over dataframe columns when     necessary for type stability</li> <li>Support integer column names for Dataframes</li> <li>Support for <code>pd.Timedelta</code> values</li> <li>Support for <code>pd.tseries.offsets.DateOffset</code> and     <code>pd.tseries.offsets.Monthend</code></li> <li>Support for Series.dt, Timestamp, and DateTimeIndex attributes     (<code>is_month_start</code>, <code>is_month_end</code>, <code>is_quarter_start</code>,     <code>is_quarter_end</code>, <code>is_year_start</code>, <code>is_year_end</code>, <code>week</code>,     <code>weekofyear</code>, <code>weekday</code>)</li> <li>Support for Series.dt and Timestamp <code>normalize</code> method</li> <li>Support for <code>Timestamp.components</code> and <code>Timestamp.strftime</code></li> <li>Support for <code>Series.dt.ceil</code> and <code>Series.dt.round</code></li> <li>Support for <code>pd.to_timedelta</code></li> <li>Support <code>Series.replace</code> for categorical arrays where     <code>value</code> and <code>to_replace</code> are scalars or lists</li> <li>Support for comparison operators on Decimal types</li> <li>Support for Series.add() with String, datetime, and timedelta</li> <li>Support for Series.mul() with string and int literal</li> <li>Support for setting values in categorical arrays</li> <li>Initial support for <code>pd.get_dummies()</code></li> <li>Support for <code>Series.groupby()</code></li> </ul> </li> <li> <p>Scikit-learn: the following classes and functions are supported     inside jit functions:</p> <ul> <li><code>sklearn.linear_model.LinearRegression</code></li> <li><code>sklearn.linear_model.LogisticRegression</code></li> <li><code>sklearn.linear_model.Ridge</code></li> <li><code>sklearn.linear_model.Lasso</code></li> <li><code>sklearn.svm.LinearSVC</code></li> <li><code>sklearn.naive_bayes.MultinomialNB</code></li> <li><code>sklearn.metrics.accuracy_score</code></li> <li><code>sklearn.metrics.mean_squared_error</code></li> <li><code>sklearn.metrics.mean_absolute_error</code></li> </ul> </li> <li> <p>XGBoost: Training XGBoost model (with Scitkit-learn like API) is now     supported inside jit functions:</p> <ul> <li><code>xgboost.XGBClassifier</code></li> <li><code>xgboost.XGBRegressor</code></li> </ul> <p>Visit &lt;https://docs.bodo.ai/latest/source/ml.htmlfor more information about supported ML functions.</p> </li> <li> <p>NumPy coverage:</p> <ul> <li>Support for <code>numpy.any</code> and <code>numpy.all</code> for all array types</li> <li>Support for <code>numpy.cbrt</code></li> <li>Support for <code>numpy.linspace</code> arguments <code>endpoint</code>, <code>retstep</code>,     and <code>dtype</code></li> <li><code>np.argmin</code> with axis=1</li> <li>Support for <code>np.float32(str)</code></li> </ul> </li> <li> <p>Support for <code>str.format</code>, <code>math.factorial</code>, <code>zlib.crc32</code></p> </li> </ul>"},{"location":"release_notes/2020.12/","title":"Bodo 2020.12 Release (Date: 12/30/2020)","text":"<p>This release includes many new features, bug fixes and performance improvements. Overall, 60 code patches were merged since the last release.</p>"},{"location":"release_notes/2020.12/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use Numba 0.52 (latest)</p> </li> <li> <p>Support for reading CSV and Parquet from Azure Data Lake Storage     (ADLS)</p> </li> <li> Improved support for UDFs <ul> <li>More robust user function handling</li> <li>Improved support for date/time data types in UDFs</li> </ul> </li> <li> Improved support for rolling window functions <ul> <li>Support <code>raw</code> argument of <code>apply()</code></li> <li>Support column selection from rolling objects</li> <li>Support for nullable int values</li> </ul> </li> <li> Pandas coverage: <ul> <li>Support for <code>groupby.apply</code></li> <li>Support for groupby rolling functions</li> <li>Improved support for dataframe indexing using df.loc/iloc</li> <li>Improve dtype handling in <code>read_csv</code></li> <li>Support for <code>Series.mask</code></li> <li>Improved robustness for highly skewed string data (e.g. most     of string data is on a few processes due to uneven data     distribution)</li> <li>Support for dataframes with repeated column names</li> <li>Support for <code>datetime.date</code> arrays as Index in <code>pivot_table</code>     and as argument to <code>pd.DatetimeIndex</code></li> <li>Improved error checking in Pandas implementations</li> <li>Unroll constant loops for type stability in more cases</li> </ul> </li> <li> Numpy coverage: <ul> <li>Support for <code>np.hstack</code></li> </ul> </li> <li> Scikit-learn: <ul> <li>Support for <code>sklearn.preprocessing.StandardScaler</code> inside     jit functions.</li> </ul> </li> </ul>"},{"location":"release_notes/2021.01/","title":"Bodo 2021.1 Release (Date: 1/26/2021)","text":"<p>This release includes many new features, bug fixes and performance improvements. Overall, 61 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.01/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Connectors:</p> <ul> <li>Support filter pushdown when reading partitioned parquet     datasets: at compile time, Bodo detects if filters are applied     to a dataframe after <code>read_parquet</code>, and generates code that     applies those filters at read time so that only the required     parquet files are read.</li> <li>Support for <code>Series.to_csv()</code></li> <li>Supports passing <code>file</code> and <code>dtype</code> arguments of <code>np.fromfile</code>     as kwargs.</li> </ul> </li> <li> <p>Support for f-strings in Bodo jitted functions</p> </li> <li> <p>Support passing Bodo distributed JIT functions to other Bodo JIT     functions</p> </li> <li> <p>Pandas coverage:</p> <ul> <li>Support groupby with <code>pd.NamedAgg()</code></li> <li>Support for <code>groupby.size</code></li> <li>Support for <code>groupby.shift</code></li> <li>Match input row order of pandas in <code>groupby.apply</code> when     applicable</li> <li>Support <code>min_periods</code> in rolling calls</li> <li>Support passing a dictionary of data types to <code>df.astype()</code></li> <li>Support dataframe setitem of multiple columns. For example:     <code>df[[\"A\", \"B\"]] = 1.3</code></li> <li>Support for <code>Index.get_loc()</code></li> <li>Support <code>ddof</code> argument (delta degrees of freedom) of     <code>Series.cov</code></li> <li>Support <code>Series.is_monotonic</code> property</li> <li>Initial support for dictionaries in <code>Series.replace</code></li> <li>Support <code>Series.reset_index(drop=True)</code></li> <li>Support level argument with all levels in <code>reset_index()</code></li> <li>Several documentation improvements</li> </ul> </li> <li> <p>Scikit-learn:</p> <ul> <li>Support for <code>sklearn.model_selection.train_test_split</code> inside     jit functions.</li> <li>Support for <code>sklearn.preprocessing.MinMaxScaler</code> inside jit     functions.</li> </ul> </li> </ul>"},{"location":"release_notes/2021.02/","title":"Bodo 2021.2 Release (Date: 2/16/2021)","text":"<p>This release includes many new features, bug fixes and usability improvements. Overall, 70 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.02/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use pandas 1.2 and Arrow 3.0 (latest)</p> </li> <li> <p>Many improvements to error checking and reporting</p> </li> <li> <p>Several documentation improvements</p> </li> <li> <p>Support tuple return from Bodo functions where elements of the tuple     have a mix of distributed and replicated distributions</p> </li> <li> <p>Improvements in automatic loop unrolling to support column names     generated in loops, e.g.     <code>pd.DataFrame(X, columns=[\"y\"] + [\"x{}\".format(i) for i in range(m)])</code></p> </li> <li> <p>Improvements in caching to cover missing cases</p> </li> <li> <p>Pandas coverage:</p> <ul> <li>Support column indices in <code>read_csv()</code> <code>dtype</code> argument. For     example: <code>df = pd.read_csv(fname, dtype={3: str})</code></li> <li>Support for <code>df.to_string()</code></li> <li>Initial support for <code>pd.Categorical()</code></li> <li>Support <code>Series.min</code> and <code>Series.max</code> for categorical data</li> <li>Support <code>pd.to_datetime()</code> with categorical string input</li> <li>Support <code>pd.Series()</code> constructor without <code>data</code> argument     specified</li> <li>Support <code>dtype=\"str\"</code> in Series constructor</li> <li>Support for <code>Series.to_dict()</code></li> <li>Support for <code>Series.between()</code></li> <li>Support <code>Series.loc[]</code> setitem with boolean array index, such     as <code>S.loc[idx] = val</code> where <code>idx</code> is a boolean array or Series</li> <li>Support dictionary input in <code>Series.map()</code>, such as     <code>S.map({1.0: \"A\", 4.0: \"DD\"})</code></li> <li>Support for <code>pd.TimedeltaIndex</code> min and max</li> <li>Support for <code>pd.tseries.offsets.Week</code></li> </ul> </li> <li> <p>Numpy coverage:</p> <ul> <li>Support <code>axis=1</code> in distributed <code>np.concatenate</code></li> <li>Initial support for <code>np.random.multivariate_normal</code></li> </ul> </li> <li> <p>Scikit-learn:</p> <ul> <li>Add <code>coef_</code> attribute to SGDClassifier model.</li> <li>Add <code>coef_</code> attribute to LinearRegression model.</li> <li>Support for <code>sklearn.preprocessing.LabelEncoder</code> inside jit     functions.</li> <li>Support for <code>sklearn.metrics.r2_score</code> inside jit functions.</li> </ul> </li> </ul>"},{"location":"release_notes/2021.03/","title":"Bodo 2021.3 Release (Date: 3/25/2021)","text":"<p>This release includes many new features, bug fixes and usability improvements. Overall, 148 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.03/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use Numba 0.53 (latest) and support Python 3.9</p> </li> <li> <p>Many improvements to error checking and reporting</p> </li> <li> <p>Compilation time is reduced, especially for user-defined functions     (UDFs)</p> </li> <li> <p>Reduced initialization time when importing Bodo</p> </li> <li> <p>Distributed diagnostics improvements:</p> <ul> <li>Show distributed diagnostics when raising errors for     distributed flag</li> <li>Only show user defined variables in diagnostics level one</li> </ul> </li> <li> <p>Performance optimizations:</p> <ul> <li>Faster groupby <code>nunique</code> with improved scaling</li> <li>Faster <code>setitem</code> for categorical arrays</li> </ul> </li> <li> <p>Connectors:</p> <ul> <li>Google Cloud Storage (GCS) support with Parquet</li> <li>Support reading Delta Lake tables</li> <li>Improved Snowflake support</li> <li>Removed s3fs dependency (Bodo now fully relies on Apache Arrow     for S3 connectivity)</li> </ul> </li> <li> <p>Change default parallelism semantics of <code>unique()</code> to replicated     output to match user expectations better</p> </li> <li> <p>Support <code>objmode</code> in groupby apply UDFs</p> </li> <li> <p>Pandas coverage:</p> <ul> <li>Support <code>pd.DataFrame.duplicated()</code> with categorical data </li> <li>Groupby support for min/max on categorical data </li> <li>Support for categorical in <code>pd.Series.dropna</code> </li> <li>Support nullable int array in <code>pd.Categorical</code> constructor </li> <li>Support for <code>pd.Series.where</code> and <code>pd.Series.mask</code> with     categorical data and a scalar value. </li> <li>Support for <code>pd.Series.diff()</code> </li> <li>Support for <code>pd.DataFrame.diff()</code> </li> <li>Support for <code>pd.Series.repeat()</code> </li> <li>Support list of functions in <code>groupby.agg()</code> </li> <li>Support tuple of UDFs inside <code>groupby.agg()</code> dictionary case </li> <li>Support single row and scalar UDF output in <code>groupby.apply()</code> </li> <li>Support Categorical values in <code>Groupby.shift</code> </li> <li>Support <code>case=False</code> in <code>Series.str.contains</code> </li> <li>Support <code>mapper</code> with <code>axis=1</code> for <code>pd.DataFrame.rename</code>. </li> <li>Support <code>Timedelta64</code> data in <code>pd.Groupby</code> </li> <li>Support for <code>datetime.date</code> arrays in <code>Series.max</code> and     <code>Series.min</code> </li> <li>Support for <code>pd.timedelta_range</code> </li> <li>Support equality between <code>datetime64</code>/<code>pd.Timestamp</code> and     <code>timedelta64</code>/<code>pd.Timedelta</code> </li> <li>Support for iterating across most index types </li> <li>Support getting the <code>name</code> attribute of data inside <code>df.apply</code> </li> <li>Support <code>Series.reset_index(drop=False)</code> for common cases </li> <li>Support <code>==</code> and <code>!=</code> on Dataframe and a scalar with a     different type </li> <li> <pre><code>Sequential support for `pd.Series.idxmax`, `pd.Series.idxmin`,\n\n:   `pd.DataFrame.idxmax`, and `pd.DataFrame.idxmin` with\n    Nullable and Categorical arrays.\n</code></pre> </li> </ul> </li> <li> <p>Python coverage:</p> <ul> <li>Support <code>datetime.date.replace()</code></li> <li>Improved support for <code>datetime.date.strftime()</code></li> <li>Support for <code>calendar.month_abbr</code></li> </ul> </li> <li> <p>SciPy:</p> <ul> <li>Initial support for <code>scipy.sparse.csr_matrix</code></li> </ul> </li> <li> <p>Scikit-learn:</p> <ul> <li>Support for     <code>sklearn.feature_extraction.text.HashingVectorizer</code></li> </ul> </li> </ul>"},{"location":"release_notes/2021.04/","title":"Bodo 2021.4 Release (Date: 4/19/2021)","text":"<p>This release includes many new features, bug fixes and usability improvements. Overall, 98 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.04/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is available for Windows as a Conda package (similar to Linux     and macOS)</p> </li> <li> <p>Removed boost library dependency</p> </li> <li> <p>Many improvements to error checking and reporting, including:</p> <ul> <li>Internal compiler errors and stack traces are now avoided more     effectively (clear errors are thrown)</li> <li>Ensure that an error is thrown if user specifies an argument     as distributed but it must be replicated</li> <li>Improvements in error checking for user-defined functions     (UDFs)</li> </ul> </li> <li> <p>Connectors:</p> <ul> <li>Support for writing partitioned Parquet datasets     (<code>df.to_parquet</code> with <code>partition_cols</code> parameter)</li> <li>Support for S3 anonymous access with     <code>storage_options={\"anon\": True}</code> in <code>pd.read_parquet()</code></li> <li>Parquet read: optimized metadata collection for nested parquet     directories (includes hive-partitioned dataset)</li> <li>To reduce Parquet read time, schema validation of multi-file     parquet datasets can be disabled with     <code>bodo.parquet_validate_schema=False</code></li> </ul> </li> <li> <p>Reduced compilation time for Pandas APIs</p> </li> <li> <p>Improved compilation time for <code>df.head/tail</code></p> </li> <li> <p>Support for format spec in f-strings, for example: <code>f\"{a:0.0%}\"</code></p> </li> <li> <p>Support for arrays in <code>bodo.rebalance()</code></p> </li> <li> <p>Pandas coverage:</p> <ul> <li>Support for <code>df.filter</code> for filtering columns</li> <li>Support for <code>indicator=True</code> in <code>pd.merge()</code></li> <li>Support for <code>DataFrame/Series/GroupBy.pipe()</code></li> <li>Support for setting dataframe columns using a 2D array</li> <li>Support for string and nullable arrays (e.g. pd.Int64Dtype) in     <code>DataFrame/Series.shift()</code></li> <li>Support for <code>pandas.tseries.offsets.MonthBegin</code></li> <li><code>Series.where</code> and <code>Series.mask</code>: support for nullable arrays     (e.g. pd.Int64Dtype)</li> </ul> </li> <li> <p>Scikit-learn:</p> <ul> <li>Support for <code>sklearn.ensemble.RandomForestRegressor</code></li> </ul> </li> </ul>"},{"location":"release_notes/2021.05/","title":"Bodo 2021.5 Release (Date: 5/19/2021)","text":"<p>This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 70 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.05/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use Arrow 4.0 (latest)</p> </li> <li> <p>Connectors:</p> <ul> <li>Improved performance of <code>pd.read_parquet</code> significantly for     large multi-file datasets by optimizing Parquet metadata     collection</li> <li>Bodo nows reads only the first few rows from a Parquet dataset     if the program only requires <code>df.head(n)</code> and/or <code>df.shape</code>.     This helps with exploring large datasets without the need for     a large cluster to load the full data in memory.</li> </ul> </li> <li> <p>Visualization: Bodo now supports calling many Matplotlib plotting     functions directly from JIT code. See the \"Data Visualization\"     section of our documentation for more details. The current support     gathers the data into one process but this will be avoided in future     releases.</p> </li> <li> <p>Improved compilation time for dataframe functions</p> </li> <li> <p>Improved the performance and scalability of <code>groupby.nunique</code></p> </li> <li> <p>Many improvements to error checking and reporting</p> </li> <li> <p>Bodo now avoids printing empty slices of distributed data to make     print output easier to read.</p> </li> <li> <p>Pandas coverage:</p> <ul> <li>Support for <code>DataFrame.info()</code></li> <li>Support for <code>memory_usage()</code> for DataFrame and Series</li> <li>Support for <code>nbytes</code> for array and Index types</li> <li>Support for <code>df.describe()</code> with datetime data (assumes     <code>datetime_is_numeric=True</code>)</li> <li>Support for <code>groupby.value_counts()</code></li> <li>Support for <code>pd.NamedAgg</code> with <code>nunique</code> in groupby</li> <li>Initial support for CategoricalIndex type and categorical keys     in groupby</li> <li>Support for groupby <code>idxmin</code> and <code>idxmax</code> with nullable     Integer and Boolean arrays</li> <li>Support for timedelta64 in <code>Groupby.agg</code></li> <li>Support for <code>bins</code> and other optional arguments in     <code>Series.value_counts()</code></li> <li>Support for <code>df.dtypes</code></li> <li>Support passing <code>df.dtypes</code> to <code>df.astype()</code>, for example:     <code>df1.astype(df2.dtypes)</code></li> <li>Support for boolean <code>pd.Index</code></li> <li>Support for <code>Series.sort_index()</code></li> <li>Support for <code>Timestamp.day_name()</code> and <code>Series.dt.day_name()</code></li> <li>Support for <code>Series.quantile()</code> with datetime</li> <li>Support for passing list of quantile values to     <code>Series.quantile()</code></li> <li>Support for <code>Series.to_frame()</code></li> <li>Support for <code>sum()</code> method of Boolean Arrays</li> <li>Initial support for <code>MultiIndex.from_product</code></li> <li>String array comparison returns a Pandas nullable boolean     array instead of a Numpy boolean array</li> </ul> </li> </ul>"},{"location":"release_notes/2021.07/","title":"Bodo 2021.7 Release (Date: 7/23/2021)","text":"<p>This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 109 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.07/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Documentation has been reorganized and updated, with improved     navigation and a detailed walkthrough of Pandas equivalents of     PySpark functions.</p> </li> <li> <p>Improvements to enable BodoSQL features</p> </li> <li> Connectors: <ul> <li>Improved performance of <code>pd.read_parquet</code> when reading from     remote storage systems like S3</li> <li>Support reading categorical columns of Parquet</li> </ul> </li> <li> Performance improvements: <ul> <li>Improved performance and scalability of <code>sort_values</code></li> <li>Optimized <code>pd.Series.isin(values)</code> performance for long list     of <code>values</code>.</li> </ul> </li> <li> <p>UDFs in Series.apply and Dataframe.apply: the Bodo compiler     transforms the code to pass main function values referenced in the     UDF (\"free variables\") as arguments to <code>apply()</code> automatically if     possible (to simplify UDF usage).</p> </li> <li> <p>Support passing Bodo data types to objmode directly (in addition to     string representation of the data types). For example, the following     code sets the return type an int64 type:</p> <pre><code>@bodo.jit\ndef f(a, b):\n    with bodo.objmode(res=bodo.int64):\n        res = random.randint(a, b)\n    return res\n</code></pre> </li> <li> <p>Compilation time improvements for some dataframe operations</p> </li> <li> <p>Distributed support for <code>pd.RangeIndex</code> calls</p> </li> <li> Pandas coverage: <ul> <li>Initial support for binary arrays, including within     series/dataframes</li> </ul> <pre><code>-   Support for `groupby.transform`\n\n-   Groupby: support repeated input columns. For example:\n\n        df.groupby(\"A\").agg(\n                D=pd.NamedAgg(column=\"B\", aggfunc=lambda A: A.sum()),\n                F=pd.NamedAgg(column=\"C\", aggfunc=\"max\"),\n                E=pd.NamedAgg(column=\"B\", aggfunc=\"min\"),\n        )\n\n-   Support Groupby with `dropna=False`\n\n-   Support for `dropna` in `Series.nunique`,\n    `DataFrame.nunique`, and `groupby.nunique`\n\n-   Support for `DataFrame.insert()`\n\n-   Support `tolist()` for string and numpy arrays\n\n-\n\n    Expanded `astype` support:\n\n    :   -   str to timedelta64/datetime64\n        -   timedelta64/datetime64 to int64\n        -   date arrays\n        -   Numeric-like inputs to datetime/timedelta\n        -   Support for `pd.StringDtype()` in `astype`\n        -   numeric-like to nullable integer types\n\n-   Support for `pd.Timestamp.now()`\n\n-   Support Timestamp in `pd.to_datetime`\n\n-   Support for Timestamp/Timedelta as the scalar value for a\n    Series\n\n-   Support for `Series.dt.month_name`, `Timestamp.month_name`\n\n-   Support for min/max on timedelta64 series/arrays\n</code></pre> </li> <li> Python coverage: <ul> <li>Support for <code>bytes.fromhex()</code></li> <li>Support for <code>bytes.__hash__</code></li> <li>Support for <code>min</code> and <code>max</code> for string values</li> </ul> </li> </ul>"},{"location":"release_notes/2021.08/","title":"Bodo 2021.8 Release (Date: 8/30/2021)","text":"<p>This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 74 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.08/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use pandas 1.3 and Arrow 5.0 (latest)</p> </li> <li> <p>Updated <code>bodo.jit</code> flag handling to remove the need for the     <code>distributed</code> flag in most cases. Arguments and return types are now     automatically inferred as distributed in many cases. Automatic     inference can be disabled using the new <code>returns_maybe_distributed</code>     and <code>args_maybe_distributed</code> flags if necessary.</p> </li> <li> <p>Connectors:</p> <ul> <li>Improved <code>pd.read_sql</code> performance on Snowflake by using the     Snowflake connector APIs directly.</li> <li>Improved performance of <code>pd.read_parquet</code> when reading large     partitioned datasets</li> </ul> </li> <li> <p>Performance improvements:</p> <ul> <li>Reduced compilation time for some DataFrame operations</li> <li>General performance improvements in Bodo's execution engine     resulting in better speed and memory efficiency for a wide     range of operations</li> <li>Improved performance of <code>merge</code> and <code>join</code> operations</li> <li>Improved performance and scalability of <code>groupby</code> operations</li> <li>Improved performance of <code>groupby.apply</code></li> <li>Improved performance of <code>groupby.transform</code></li> <li>Significantly optimized <code>Series.str.contains(..., regex=True)</code></li> <li>Improved performance of filtering operations involving string     arrays</li> </ul> </li> <li> <p>Pandas:</p> <ul> <li>Support for passing string function names to <code>Series.apply</code>.     The string can refer to a Series method or a Numpy ufunc.</li> <li>Support for passing string function names to     <code>DataFrame.apply</code>. The string can refer to a DataFrame method.     <code>axis</code> can be provided if the method takes an <code>axis</code> argument.</li> <li>Enhanced support for binary arrays, including within     series/dataframes</li> <li><code>astype()</code> support for casting strings to nullable integers</li> <li>Support for <code>operator.mul</code> between a <code>Timedelta</code> scalar and     integers Series</li> <li>Support for <code>std</code> in <code>groupby.transform</code>.</li> </ul> </li> <li> <p>Scikit-learn:</p> <ul> <li>Support for <code>sklearn.feature_extraction.text.CountVectorizer</code></li> <li>Support for <code>coef_</code> attribute for <code>Ridge</code></li> </ul> </li> </ul> <p>BodoSQL 2021.8beta Release (Date: 8/30/2021)</p> <p>This release adds more SQL coverage, introduces new BodoSQL specific features, and fixes bugs. Overall, 53 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.08/#new-features-and-improvements_1","title":"New Features and Improvements","text":"<ul> <li> <p>Parameterized Queries:</p> <p>Parameterized queries allow replacing scalars with Python variables during runtime execution. This enables caching more complex BodoSQL queries When paired with Bodo jit: a query parameter can change without the need to recompile the query. More information and example usage can be found in our documentation.</p> </li> <li> <p>SQL Coverage:</p> <p>This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage.</p> <ul> <li>Support for != and &lt;=operators </li> <li>Support for CAST </li> <li>Support for LEAST </li> <li>Support for [NOT] IN with lists of literals </li> <li>Support for the offset optional argument in queries with LIMIT     (i.e. SELECT A from table LIMIT 1, 4) </li> <li>Initial support for YEAR/MONTH interval literals/scalars.     Currently these are only supported with addition and     subtraction operators and cannot be used as a column type. </li> <li>Support the following string functions: <ul> <li>CHAR (to convert a value to a string)</li> <li>LENGTH </li> </ul> </li> <li>Support for the following Timestamp functions: <ul> <li>ADDDATE</li> <li>SUBDATE</li> <li>TIMESTAMPDIFF</li> <li>WEEKDAY</li> <li>YEARWEEK</li> <li>LAST_DAY</li> </ul> </li> </ul> </li> </ul>"},{"location":"release_notes/2021.09/","title":"Bodo 2021.9 Release (Date: 9/29/2021)","text":"<p>This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 98 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.09/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use Numba 0.54 (latest)</p> </li> <li> <p>Performance improvements:</p> <ul> <li>Significantly improved the performance and scalability of     parallel <code>merge</code> and <code>join</code> operations</li> <li>Improved the performance and scalability of <code>groupby.nunique</code></li> <li>General performance improvements for operations involving data     shuffling</li> <li>Optimized many compilation paths, especially those involving     DataFrames. This will lead to shorter compilation times for     many use cases.</li> <li>Optimizations in <code>pd.read_sql</code> to limit the data read when     <code>LIMIT</code> is provided.</li> </ul> </li> <li> <p>Pandas:</p> <ul> <li>Support for <code>Series.shift</code> on timedelta64 data</li> <li>Support for <code>pd.cut()</code> and <code>pd.qcut()</code></li> <li>Support for <code>first</code>, <code>last</code>, <code>median</code>, <code>nunique</code>, <code>prod</code>, and     <code>var</code> in <code>groupby.transform</code></li> <li>Support for multiplication with DateOffset</li> <li>Support for <code>Series.round()</code> on nullable integers</li> <li>Support for <code>to_strip</code> argument in     <code>series.str.strip/lstrip/rstrip</code></li> <li>Increased Binary Array/Series/DataFrame support. In particular:<ul> <li>Support for <code>first</code>, <code>last</code>, <code>shift</code>, <code>count</code>, <code>nunique</code>,     <code>size</code>, <code>value_counts</code> for Binary Series and DataFrames.</li> <li>Groupby support with binary keys/values.</li> <li>Support for <code>sort_values</code> with binary columns.</li> <li>Join with binary keys</li> <li>Most generic Series/DataFrame operations.</li> </ul> </li> <li>Support for equi-join with additional non-equi-join conditions     through our general merge condition syntax. Please refer to the     documentation for more information.</li> </ul> </li> </ul> <p>BodoSQL 2021.9beta Release (Date: 9/29/2021)</p> <p>This release adds SQL bug fixes and various usability improvements, including a reduced package size. BodoSQL users should also benefit from compilation time improvements due to improvements in the engine. Overall, 25 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.09/#new-features-and-improvements_1","title":"New Features and Improvements","text":"<ul> <li> <p>Decreased package size and removed external dependencies.</p> </li> <li> <p>Improved error messages with shortened stack traces.</p> </li> <li> <p>SQL Coverage</p> <p>This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage.</p> <ul> <li>Support for <code>UTC_TIMESTAMP</code> function </li> <li>Support for <code>UTC_DATE</code> function </li> <li>Support for <code>PIVOT</code> </li> <li>Support for the following Window Functions: <ul> <li><code>MAX</code></li> <li><code>MIN</code></li> <li><code>COUNT/COUNT(*)</code></li> <li><code>SUM</code></li> <li><code>AVG</code></li> <li><code>STDDEV</code></li> <li><code>STDDEV_POP</code></li> <li><code>VARIANCE</code></li> <li><code>VAR_POP</code></li> <li><code>LEAD</code></li> <li><code>LAG</code></li> <li><code>FIRST_VALUE</code></li> <li><code>LAST_VALUE</code></li> <li><code>NTH_VALUE</code></li> <li><code>NTILE</code></li> <li><code>ROW_NUMBER</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"release_notes/2021.10/","title":"Bodo 2021.10 Release (Date: 10/28/2021)","text":"<p>This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 71 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.10/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>The Bodo Community Edition can now run on up to 8 cores.</p> </li> <li> <p>Bodo is updated to use Numba 0.54.1 (latest).</p> </li> <li> <p>Improved error messages and documentation.</p> </li> <li> <p>Connectors:</p> <ul> <li><code>pandas.read_csv</code>: support for <code>chunksize</code> and <code>nrows</code>     parameters </li> <li>Snowflake: <ul> <li>Improved performance and scalability using the new     parallel fetch functionality of Snowflake's Python     connector, which retrieves data as batches of Arrow     tables</li> <li>Support for removing unused columns from the SQL query.</li> <li>Support for filter pushdown of Pandas comparison     operations into the SQL query.</li> </ul> </li> </ul> </li> <li> <p>Reduced compilation time for <code>DataFrame.describe</code></p> </li> <li> <p>Pandas:</p> <ul> <li><code>DataFrame.sort_values</code>: supports passing <code>na_position</code> as a     list with one value per column.</li> </ul> </li> </ul> <p>BodoSQL 2021.10beta Release (Date: 10/28/2021)</p> <p>This release includes SQL bug fixes, increased SQL coverage and various usability improvements. Overall, 27 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.10/#new-features-and-improvements_1","title":"New Features and Improvements","text":"<ul> <li> <p>Improved error messages with expanded documentation.</p> </li> <li> <p>Support for passing <code>CategoricalArray</code> and     <code>DateArray</code> to BodoSQL. BodoSQL will automatically     convert these arrays to supported types.</p> </li> <li> <p>SQL Coverage</p> <p>This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage.</p> <ul> <li>Support for <code>TO_DATE</code> function</li> <li>Support for string column casting inside     <code>DATE_ADD</code> and <code>DATE_SUB</code></li> <li>Support for <code>nulls first</code> and <code>nulls last</code> inside <code>order by</code>.</li> <li>Support for String columns in Window Aggregation Functions.</li> <li>Provided more efficient implementations for <code>NVL</code> and <code>IFNULL</code>     when there is a column and a scalar.</li> </ul> </li> </ul>"},{"location":"release_notes/2021.11/","title":"Bodo 2021.11 Release (Date: 11/30/2021)","text":"<p>This release includes many new features, optimizations, bug fixes and usability improvements. Overall, 107 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.11/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Support for \"wide\" DataFrames with large number of columns:</p> <ul> <li> <p>Bodo compiler is transitioning to a new internal dataframe     compilation format that substantially decreases compliation time     for dataframes with thousands of columns.</p> <p>All DataFrame APIs will transition to this new format over time.</p> </li> <li> <p><code>read_csv</code>, <code>read_parquet</code>, <code>bodo.gatherv</code> and dataframe     filtering are upgraded to support this new format in this     release.</p> </li> <li>Connectors:</li> <li>Significantly improved performance when reading Parquet from S3     (up to 10x faster read depending on the dataset).</li> <li>General support for predicate pushdown when reading from Parquet     (filtering rows at the storage level).</li> <li>Improvements to BodoSQL's filter pushdown, such as higher     compiler accuracy in detecting possible filters.</li> <li>Faster <code>read_parquet</code> compilation time by validating the schema     only at runtime.</li> <li>Faster <code>pd.read_csv()</code> execution time with large numbers of     columns.</li> <li>Bodo automatically maintains type information when passing DataFrames and Series between Bodo and regular Python. This avoids potential typing issues when parallel data chunks do not have enough non-null data for automatic type inference.</li> <li>Improved error messages and documentation.</li> <li>Pandas:</li> <li>Support for Array of dictionary outputs of <code>DataFrame.apply()</code>     and <code>Series.apply()</code></li> <li>Support for Array of dictionary inputs to <code>pd.concat()</code></li> <li>Support for <code>Series.astype(str)</code> with Categorical type for     non-string categories.</li> <li>Support for callable arguments to <code>DataFrame.assign()</code></li> <li>Support for passing a list as <code>skiprows</code> of <code>pd.read_csv()</code></li> <li>Support for <code>low_memory</code> argument in <code>pd.read_csv()</code></li> <li>Support for using a string label for indexing Series with string     Index (for non-parallel Series)</li> <li>Support for initializing a Series with a constant dictionary</li> <li>Support for <code>subset</code> argument to <code>DataFrame.drop_duplicates</code></li> <li>Support for <code>DataFrame.plot</code> with arguments <code>x</code>, <code>y</code>, <code>kind</code>,     <code>figsize</code>, <code>xlabel</code>, <code>ylabel</code>, <code>title</code>, <code>legend</code>, <code>fontsize</code>,     <code>xticks</code>, <code>yticks</code>, and <code>ax</code>. <code>DataFrame.plot</code> behaves the same     as Bodo's Matplotlib support.</li> <li>Support for <code>DataFrame.groupby.head</code></li> <li>Numpy:</li> <li>Support for <code>np.select</code></li> <li>ML:</li> <li>Support <code>predict_proba</code> and <code>predict_log_proba</code> for     <code>RandomForestClassifier</code>, <code>SGD Classifier</code> and     <code>LogisticRegression</code></li> <li>Support <code>predict_proba</code> for XGBoostClassifier</li> <li>Support for <code>sklearn.metrics.confusion_matrix</code></li> </ul> </li> </ul> <p>BodoSQL 2021.11beta Release (Date: 11/30/2021)</p> <p>This release includes SQL bug fixes and support for Bodo's filter pushdown from BodoSQL. Most of the improvements to BodoSQL are integrating enhancements made to Bodo. Overall, 10 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.11/#new-features-and-improvements_1","title":"New Features and Improvements","text":"<ul> <li> <p>Support for a new filepath API <code>bodosql.TablePath</code>. This API takes     the path and file type and uses this to load/remove the data within     the query.</p> <p>For example:</p> <pre><code>bc = bodosql.BodoSQLContext(\"table1\": bodosql.TablePath(\"myfile.pq\", \"parquet\"))\nreturn bc.query(\"Select A from table1\")\n</code></pre> <p>This is functionally equivalent to using the Pandas <code>read_</code> functions inside a Bodo function, but it may have some additional performance optimizations.</p> <p>Currently only Parquet files are supported.</p> </li> <li> <p>Support for Bodo's filter pushdown when using the     <code>bodosql.TablePath</code> API.</p> </li> <li> <p>Reduced compliation and execution time when using the <code>FIRST_VALUE</code>     function repeatedly on the same exact window.</p> </li> <li> <p>SQL Coverage</p> <p>This release added the following additional SQL coverage to BodoSQL. Please refer to our documentation for more details regarding usage.</p> <ul> <li>Support for omitting the second argument from the <code>ROUND</code>     function (defaults to 0).</li> <li>Support for providing an integer as the second argument     <code>DATE_ADD</code> and <code>DATE_SUB</code>. If you pass     an integer, it is assigned <code>days</code> as its unit.</li> </ul> </li> </ul>"},{"location":"release_notes/2021.12/","title":"Bodo 2021.12 Release (Date: 12/29/2021)","text":"<p>This release includes many new features and usability improvements. Overall, 67 code patches were merged since the last release.</p>"},{"location":"release_notes/2021.12/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Significantly upgrades to the Bodo documentation to improve the     developer experience</p> </li> <li> <p>Improvements to documentation and unsupported attribute handling for     Pandas APIs</p> </li> <li> <p>Significant enhancements to objmode user experience and robustness,     such as automatic output data type checking and automatic conversion     if possible</p> </li> <li> <p>Improved support for <code>re</code> package, such as support for <code>re</code> flags,     better support for returning <code>None</code> when necessary, and better     catching of unsupported corner cases</p> </li> <li> <p>Support caching functions that take a string as input and create a     file path using concatenation. For example:</p> <pre><code>@bodo.jit(cache=True)\ndef f(folder):\n  return pd.read_parquet(folder + \"/example.pq\")\n</code></pre> </li> <li> <p>Connectors:</p> <ul> <li>Improved <code>read_parquet</code> runtime performance when reading from S3</li> <li>Decreased compilation time for <code>read_csv</code> on DataFrames with     large number of columns (100)</li> </ul> </li> <li> <p>Improved compilation time for dataframes with large number of     columns (&gt;10,000)</p> </li> <li> <p>Improved NA handling in User Defined Functions with df.apply when     functions are not inlined</p> </li> <li> <p>Support for using <code>logging.RootLogger.info</code> when passing the logger     as an argument to a JIT function</p> </li> <li> <p>Support for <code>datetime.datetime.today</code></p> </li> <li> <p>Simpler <code>bodo.scatterv</code> usage from regular Python. Other ranks are     ignored but not required to have <code>None</code> as their data</p> </li> <li> <p>Improved support for map arrays in various operations</p> </li> <li> <p>Support <code>feature_importances_</code> of XGBoost</p> </li> <li> <p>Support <code>predict_proba</code> and <code>predict_log_proba</code> in Scikit-learn     classifier algorithms</p> </li> <li> <p>Pandas:</p> <ul> <li>Support for Bodo specific argument <code>_bodo_upcast_to_float64</code> in     pd.read_csv. This can be used when all data is numeric but     schema inference cannot accurate predict data types.</li> <li>Support for using <code>DataFrame.to_parquet</code> with \"wide\"     DataFrames with large number of columns</li> <li>Support for storing a <code>DateTimeIndex</code> with     <code>DataFrame.to_parquet</code></li> <li>Support for the 'method' argument in <code>DataFrame.fillna</code> and     <code>Series.fillna</code></li> <li>Support for <code>Series.bfill</code>, <code>Series.ffill</code>, <code>Series.pad</code>, and     <code>Series.backfill</code></li> <li>Support for <code>Series.keys</code></li> <li>Support for <code>Series.infer_objects</code> and <code>DataFrame.infer_objects</code></li> <li>Decreased runtime when calling <code>.astype(\"categorical\")</code> on     Series with large numbers of categories</li> </ul> </li> </ul>"},{"location":"release_notes/2022.01/","title":"Bodo 2022.1 Release (Date: 1/31/2022)","text":"<p>This release includes many new features and usability improvements. Overall, 71 code patches were merged since the last release.</p>"},{"location":"release_notes/2022.01/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is now available with <code>pip</code> on both Linux and Windows</p> </li> <li> <p>Bodo is upgraded to use Numba 0.55.0 (the latest release)</p> </li> <li> <p>Bodo can now evaluate JIT functions at compilation time if possible     to extract constant values. This improves user experience by     simplifying type stability requirements. For example, the function     below can be refactored to be type stable easily:</p> <pre><code>@bodo.jit\ndef f(df):\n    df.columns = [f\"m_{c}\" if c not in [\"B\", \"C\"] else c for c in df.columns]\n    return df\n</code></pre> <pre><code>@bodo.jit\ndef g(df_cols):\n    return [f\"m_{c}\" if c not in [\"B\", \"C\"] else c for c in df_cols]\n&gt;\n@bodo.jit\ndef f(df):\n    df.columns = g(df.columns)\n    return df\n</code></pre> </li> <li> <p>Connectors:</p> <ul> <li><code>read_csv</code> now skips hidden files when reading from a directory.</li> <li><code>read_parquet</code> now supports reading a list of files.</li> <li>Improved error handling for both <code>read_csv</code> and <code>read_sql</code></li> </ul> </li> <li> <p>Improved null value handling in user-defined-functions that aren't     inlined.</p> </li> <li> <p>Truncated error messages with DataFrames with large numbers of     columns to improve readability.</p> </li> <li> <p>Improved support for the <code>logging</code> standard library:</p> <ul> <li>Support regular <code>logging.Logger</code> in addition to the     <code>logging.RootLogger</code>.</li> <li>Supports passing a logger as a constant global.</li> <li>Supports the attributes: <code>level</code>, <code>name</code>, <code>propagate</code>,     <code>disabled</code>, and <code>parent</code>.</li> <li>Supports the methods: <code>debug</code>, <code>warning</code>, <code>warn</code>, <code>error</code>,     <code>exception</code>, <code>critical</code>, <code>log</code>, and <code>setLevel</code>.</li> </ul> </li> <li> <p>Improvments to global value handling of the compiler to avoid memory     leaks in corner cases.</p> </li> <li> <p>Pandas:</p> <ul> <li>Support for <code>DataFrame.pivot()</code> and <code>DataFrame.pivot_table()</code>     without requiring a constant list of output columns. Bodo     currently only supports limited operations on output DataFrames     of pivot, so users are recommended to immediately return these     DataFrames to Python before doing any further processing in     Bodo.</li> <li>Support for <code>Index.rename</code></li> <li>Support for <code>Index.is_monotonic</code>,     <code>Index.is_montonic_increasing</code>, and     <code>Index.is_monotonic_decreasing</code></li> <li>Support for <code>Index.notna</code> and <code>Index.notnull</code></li> <li>Support for <code>Index.drop_duplicates</code></li> <li>Support for <code>groupby.min</code>, <code>groupby.max</code>, <code>groupby.first</code>, and     <code>groupby.last</code> on DataFrames with Categorical columns</li> <li>Support for column slice assignment with <code>df.iloc</code> (e.g.     <code>df.iloc[0,:] = 0</code>)</li> <li>Support for <code>Series.first</code>, <code>Series.last</code>, <code>DataFrame.first</code>,     and <code>DataFrame.last</code></li> </ul> </li> </ul>"},{"location":"release_notes/2022.02/","title":"Bodo 2022.2 Release (Date: 2/28/2022)","text":"<p>This release includes many new features and usability improvements. Overall, 82 code patches were merged since the last release.</p>"},{"location":"release_notes/2022.02/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Reduced the import time of the Bodo package substantially</p> </li> <li> <p>Bodo is now available with <code>pip</code> on x86 Mac</p> </li> <li> <p>Bodo is upgraded to use Numba 0.55.1 (the latest release)</p> </li> <li> <p>Bodo is upgraded to use scikit-learn v1</p> </li> <li> <p>Bodo now supports MPICH version 3.4</p> </li> <li> <p>Connectors:</p> <ul> <li><code>pd.read_sql</code>: Support and getting start documentation for for Oracle DB     and PostgreSQL.</li> <li><code>pd.read_parquet</code> now supports glob patterns</li> <li>Support for <code>escapechar</code> argument in <code>pd.read_csv</code></li> <li>Decreased compilation time when reading wide schemas with 1000s     of columns usings <code>pd.read_parquet</code>.</li> <li>Optimized runtime of <code>pd.read_parquet</code> with <code>head(0)</code> to skip     any unnecessary schema collection for each parquet file and just     look at the metadata. This optimization is helpful when loading     a DataFrame schema.</li> <li>Support using filter pushdown with a single filter consisting of     <code>Series.isna</code>, <code>Series.isnull</code>, <code>Series.notna</code>, or <code>Series.notnull</code>.</li> <li>Full filter pushdown support with <code>hdfs</code> and <code>gcs</code> using <code>pd.read_parquet</code></li> <li>Improved performance and error handling when using <code>DataFrame.to_sql</code>     with Snowflake.</li> <li>Bodo now prints a warning if the number of Parquet row groups is too small for effective parallel I/O.</li> </ul> </li> <li> <p>Support for using lists and sets as constant global values.</p> </li> <li> <p>Support for distributed global dataframe values</p> </li> <li> <p>Added a compiler optimizations for forcing the columns in a DataFrame     to match a DataFrame with an existing schema via <code>DataFrame.dtypes</code>.     In particular when Bodo encounters code like:</p> <pre><code>@bodo.jit\ndef f(df1, df2):\n    return df1.astype(df2.dtypes)\n</code></pre> <p>Bodo will automatically use the internal Bodo types for all columns in <code>df2</code>. This enables using astypes for conversions that are typically not possible in Pandas because the column has an <code>object</code> dtype. For example, this can be used to convert a column from <code>datetime64[ns]</code> to <code>datetime.date</code> with <code>astype</code>.</p> </li> <li> <p>Improved runtime performance when copying a string data from one array to     another or when computing an array of string lengths.</p> </li> <li> <p>Pandas:</p> <ul> <li>Support for passing multiple columns to <code>values</code> and <code>index</code> with     <code>DataFrame.pivot()</code> and <code>DataFrame.pivot_table()</code></li> <li>Support for using <code>pd.pivot()</code> and <code>pd.pivot_table()</code>. Functionality     is equivalent to <code>DataFrame.pivot()</code> and <code>DataFrame.pivot_table()</code></li> <li>Support for <code>DataFrame.explode()</code></li> <li>Support for <code>DataFrame.where()</code> and <code>DataFrame.mask()</code></li> <li>Support for <code>Series.duplicated()</code> and <code>Index.duplicated()</code>.</li> <li>Support for <code>Series.rename_axis()</code></li> <li>Support for using <code>object</code> in <code>DataFrame.astype</code>. Bodo doesn't     have a generic \"object\" type, so the type of the column remains     the same.</li> </ul> </li> </ul>"},{"location":"release_notes/2022.03/","title":"Bodo 2022.3 Release (Date: 3/31/2022)","text":"<p>This release includes many new features, usability and performance improvements, and bug fixes. Overall, 74 code patches were merged since the last release.</p>"},{"location":"release_notes/2022.03/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Bodo is updated to use Arrow 7.0 (latest)</p> </li> <li> <p>Initial support for dictionary-encoded string arrays.     Dictionary encoding can improve performance and reduce memory usage significantly     when data has many repeated values which is common in practice (see here).     Bodo now uses dictionary encoding automatically in <code>pd.read_parquet</code> when a string column can benefit from it.     Join, sort and parquet write operations support dictionary-encoded string arrays as well, and     the support will expand to others in the future.     Bodo will fall back to regular string arrays automatically if an     operation does not support dictionary encoding.</p> </li> <li> <p>Connectors:</p> <ul> <li><code>pd.read_parquet</code> performance improvements when multiple processes read     from the same file.</li> <li>Support for filter pushdown in Parquet and Snowflake when using <code>Series.isin</code></li> <li>Support for SparkSQL's <code>input_file_name</code> functionality for <code>read_parquet</code> using a new <code>_bodo_input_file_name_col</code> argument.</li> <li>Support for <code>chunksize</code> in <code>pd.to_sql</code></li> <li>Optimized <code>df.to_parquet</code> memory usage when writing string columns</li> <li>Support for passing list of columns as <code>columns</code> parameter of <code>df.to_csv</code></li> <li>Support in <code>pd.read_sql</code> for returning an empty DataFrame from Snowflake, either     due to an empty query or the result of filter pushdown.</li> <li>Changed default value of <code>orient</code> and <code>lines</code> in <code>DataFrame.to_json</code> to <code>records</code> and <code>True</code> respectively to enable parallel write (Pandas uses <code>columns</code> and <code>False</code> as default).</li> </ul> </li> <li> <p>Bodo now provides compiler optimization logging through <code>bodo.set_verbose_level()</code>.     This can be used to display certain optimizations performed at compile time,     such as filter pushdown, column pruning, and which columns are read with dictionary     encoding when reading from Parquet. See Verbose Mode for more details. </p> </li> <li> <p>Improvements in error checking and quality of error messages.</p> </li> <li> <p>Avoid hang when encountering unhandled exceptions on a single process.</p> </li> <li> <p>Introduced <code>replicated</code> JIT decorator flag (opposite of <code>distributed</code>).</p> </li> <li> <p>If the user provided <code>distributed</code> JIT flag for some input and return values but not all, bodo can now infer distribution of the rest.</p> </li> <li> <p>Performance optimizations:</p> <ul> <li>Improved memory usage during parallel <code>groupby.apply</code></li> <li>Improved <code>df.sample</code> performance when <code>frac=1</code> and <code>replace=False</code></li> </ul> </li> <li> <p>Pandas:</p> <ul> <li>Initial support for Timezone-Aware arrays and timestamps<ul> <li>Added support for <code>array.tz_convert</code>, <code>Series.dt.tz_convert</code>, <code>Timestamp.tz_convert</code>, <code>DatetimeIndex.tz_convert</code>, <code>Timestamp.tz_localize</code></li> </ul> </li> <li>Support for <code>Series.str.cat</code></li> <li>Support for <code>pd.unique</code> on Series and 1-D arrays</li> <li>Support for comparison operators between <code>DatetimeIndex</code> and <code>pd.Timestamp</code>     and <code>TimedeltaIndex</code> and <code>pd.Timedelta</code></li> <li>Support for <code>DataFrame.set_index</code> on single-column DataFrames</li> <li>Support for <code>Series.first_valid_index</code> and <code>Series.last_valid_index</code></li> <li>Support for conversion between <code>pd.timestamp</code> and <code>np.datetime64</code></li> </ul> </li> </ul>"},{"location":"release_notes/2022.04/","title":"Bodo 2022.4 Release (Date: 4/29/2022)","text":"<p>This release includes many new features, usability and performance improvements, and bug fixes. Overall, 60 code patches were merged since the last release.</p>"},{"location":"release_notes/2022.04/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Support for Python 3.10 (Conda/pip packages will be available soon)</p> </li> <li> <p>Support for Pandas 1.4 (along with continued support for v1.3)</p> </li> <li> <p>Connectors:</p> <ul> <li>When passing a list of paths to <code>pd.read_parquet</code>, the paths can be a combination     of paths to files and glob strings.</li> <li>Improved performance of <code>pd.read_parquet</code> on remote filesystems when passing     long lists of files.</li> <li><code>DataFrame.to_parquet</code> now supports <code>row_group_size</code> parameter, which can be used to specify the maximum number     of rows in generated row groups. Bodo now has a default row group size of 1M rows, to improve     performance when reading the generated parquet datasets in parallel.</li> <li><code>pd.read_parquet</code>: string columns can be forced to be read with dictionary encoding     by passing a list of column names with <code>_bodo_read_as_dict</code> parameter.</li> <li>Support for S3 anonymous access with <code>storage_options={\"anon\": True}</code>     in <code>pd.read_csv</code> and <code>pd.read_json</code></li> <li>Improved performance and memory utilization of <code>pd.read_csv</code> at compilation and     run time (especially when reading first n rows from remote filesystems such as S3)</li> </ul> </li> <li> <p>Parallel support for <code>pd.date_range</code>: Bodo automatically creates a date range     that is distributed across processes</p> </li> <li> <p>Improved performance of <code>Series.str.startswith/endswith/contains</code> for dictionary-encoded string arrays</p> </li> <li> <p>Reduced compilation time for <code>DataFrame.memory_usage()</code></p> </li> <li> <p>Reduced compilation time when using <code>pandas.read_sql()</code> with wide tables.</p> </li> <li> <p>Pandas:</p> <ul> <li>Support for <code>DataFrame.melt()</code> and <code>pd.melt()</code></li> </ul> </li> </ul>"},{"location":"release_notes/2022.05/","title":"Bodo 2022.5 Release (Date: 5/31/2022)","text":"<p>This release includes many new features, usability and performance improvements, and bug fixes.</p>"},{"location":"release_notes/2022.05/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>Iceberg connector (alpha):</p> <ul> <li>Initial support for reading Iceberg tables using the <code>read_sql_table</code> API.   It supports automatic filter pushdown, and tables can be stored on local file system   or Hive Metastore.</li> </ul> </li> <li> <p>Improved write performance of dataframe <code>to_sql</code> to OracleDB.</p> </li> <li> <p>Better error messages linked to user documentation.</p> </li> <li> <p>Pandas:</p> <ul> <li> <p>Bodo now matches Pandas 1.4 date/time offset behavior for nansecond fragments.</p> </li> <li> <p>Support for <code>var_name</code> and <code>value_name</code> arguments in <code>pd.melt</code>.</p> </li> <li> <p>Super wide dataframe support for <code>copy</code> and <code>rename</code>.</p> </li> <li> <p>Support for Index <code>unique</code>, <code>isin</code>, and <code>contains</code>.</p> </li> <li> <p>Improved memory-efficiency and performance of <code>Series.str</code> <code>center</code>, <code>capitalize</code>, <code>lower</code>, <code>swapcase</code>, <code>title</code>, and <code>upper</code> with dictionary-encoded string arrays.</p> </li> </ul> </li> <li> <p>BodoSQL:</p> <ul> <li> <p>Support for Python 3.10</p> </li> <li> <p>Upgraded to Calcite 1.30 (latest release)</p> </li> </ul> </li> </ul>"},{"location":"release_notes/2022.06/","title":"Bodo 2022.6 Release (Date: 06/30/2022)","text":""},{"location":"release_notes/2022.06/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li>Bodo is upgraded to use Numba 0.55.2 (the latest release)</li> </ul> <p>Dataframe compilation improvements:</p> <ul> <li> <p><code>pandas.merge</code> is now much faster to compile and supports super wide dataframes (e.g. 100,000 columns).</p> </li> <li> <p><code>DataFrame.sort_values</code> is now much faster to compile and supports super wide dataframes.</p> </li> <li> <p><code>DataFrame.astype</code> is now much faster to compile and supports super wide dataframes.</p> </li> <li> <p><code>DataFrame.loc</code>, <code>DataFrame.iloc</code> and <code>DataFrame[col_list]</code> are now faster to compile and support super wide dataframes when returning a DataFrame.</p> </li> <li> <p>Bodo can now automatically optimize out unused output keys of join and sort operations (e.g. pd.merge, df.sort_values). This should result in significant runtime and memory usage improvements.</p> </li> </ul> <p>Iceberg connector (alpha):</p> <ul> <li> <p>Now supports reading from Nessie, Arctic, and Glue catalogs.</p> </li> <li> <p>Iceberg connector now uses py4j. This should remove any conflicts with other packages that use jpype.</p> </li> </ul> <p>Parquet I/O:</p> <ul> <li> <p>Improved performance and robustness when reading Parquet files.</p> </li> <li> <p>Several improvements to Dead Column Elimination and Filter Pushdown that enable faster Parquet read in many scenarios.</p> </li> </ul> <p>Pandas coverage:</p> <ul> <li> <p>Several Series operation are optimized to support dictionary-encoded string arrays, which reduces memory usage and execution time:</p> <ul> <li><code>pd.Series.str.get</code></li> <li><code>pd.Series.str.repeat</code></li> <li><code>pd.Series.str.slice</code></li> <li><code>pd.Series.str.pad</code></li> <li><code>pd.Series.str.rjust</code></li> <li><code>pd.Series.str.ljust</code></li> <li><code>pd.Series.str.zfill</code></li> <li><code>pd.Series.str.center</code></li> <li><code>pd.Series.str.count</code></li> <li><code>pd.Series.str.len</code></li> <li><code>pd.Series.str.find</code></li> <li><code>pd.Series.str.rfind</code></li> <li><code>pd.Series.str.strip</code></li> <li><code>pd.Series.str.lstrip</code></li> <li><code>pd.Series.str.rstrip</code></li> <li><code>pd.Series.str.extract</code></li> <li><code>pd.Series.str.extractall</code></li> <li><code>pd.Series.str.isalnum</code></li> <li><code>pd.Series.str.isalpha</code></li> <li><code>pd.Series.str.isdigit</code></li> <li><code>pd.Series.str.isspace</code></li> <li><code>pd.Series.str.islower</code></li> <li><code>pd.Series.str.isupper</code></li> <li><code>pd.Series.str.istitle</code></li> <li><code>pd.Series.str.isnumeric</code></li> <li><code>pd.Series.str.isdecimal</code></li> </ul> </li> <li> <p>Support for dictionary-encoded string arrays as the key values to <code>DataFrame.groupby</code>, which reduces memory usage and execution time.</p> </li> <li> <p>Bodo now supports <code>Index.is_integer()</code>, <code>Index.is_floating()</code>, <code>Index.is_boolean()</code>, <code>Index.is_numeric()</code>, <code>Index.is_interval()</code>, <code>Index.is_categorical()</code>, <code>Index.is_object()</code>, <code>Index.T, Index.size</code>, <code>Index.ndim</code>, <code>Index.nlevels</code>, <code>Index.is_all_dates</code>, <code>Index.inferred_type</code>, <code>Index.empty</code>, <code>Index.names</code>, <code>Index.shape</code> for all Index types.</p> </li> <li> <p>Bodo now supports <code>Index.argmax()</code>, <code>Index.argmin()</code>, <code>Index.argsort()</code>, and <code>Index.nunique()</code> for the follwing Index types:</p> <ul> <li>NumericIndex</li> <li>RangeIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> <li>CategoricalIndex</li> <li>PeriodIndex</li> </ul> </li> <li> <p>Bodo now supports <code>Index.all()</code> and <code>Index.any()</code> for the following index types:</p> <ul> <li>NumericIndex</li> <li>RangeIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> </ul> </li> <li> <p>Bodo now supports <code>Index.isin()</code>, <code>Index.union()</code>, <code>Index.intersection()</code>, <code>Index.difference()</code>, <code>Index.symmetric_difference()</code>, <code>Index.to_list()</code>, and <code>Index.tolist()</code> for the following index types:</p> <ul> <li>NumericIndex</li> <li>RangeIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> </ul> </li> <li> <p>Bodo now supports <code>Index.dtype</code> and <code>Index.to_frame()</code> for the following index types</p> <ul> <li>NumericIndex</li> <li>RangeIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> <li>CategoricalIndex</li> <li>MultiIndex</li> </ul> </li> <li> <p>Bodo now supports <code>Index.to_series()</code>, <code>Index.where()</code>, <code>Index.putmask()</code>, and <code>Index.sort_values()</code> for the following index types:</p> <ul> <li>NumericIndex</li> <li>RangeIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> <li>CategoricalIndex</li> </ul> </li> <li> <p>Bodo now supports <code>Index.unique()</code>, and <code>Index.to_numpy()</code> for the following index types:</p> <ul> <li>NumericIndex</li> <li>RangeIndex</li> <li>StringIndex</li> <li>BinaryIndex</li> <li>DatetimeIndex</li> <li>TimedeltaIndex</li> <li>CategoricalIndex</li> <li>IntervalIndex</li> </ul> </li> <li> <p>Added support for <code>Categorical Index iterator</code></p> </li> <li> <p>Added support for <code>Series.rank()</code> with replicated data</p> </li> </ul> <p>Scikit-Learn Coverage:</p> <ul> <li>Added support for the following functions:<ul> <li><code>sklearn.metrics.log_loss</code></li> <li><code>sklearn.metrics.pairwise.cosine_similarity</code></li> <li><code>sklearn.model_selection.KFold</code></li> <li><code>sklearn.model_selection.LeavePOut</code></li> <li><code>sklearn.preprocessing.OneHotEncoder</code></li> <li><code>sklearn.preprocessing.MaxAbsScaler</code></li> <li><code>sklearn.utils.shuffle</code></li> </ul> </li> </ul> <p>BodoSQL:</p> <ul> <li> <p>BodoSQL is available on pypi</p> </li> <li> <p>BodoSQL now uses py4j. This should remove any conflicts with other packages that use jpype.</p> </li> <li> <p>Significantly reduced compilation time when compiling queries with large numbers of columns for common operations (join, where, order by, limit)</p> </li> <li> <p>Optimized <code>first_value</code> and <code>last_value</code> window functions when a single value is repeated for the entire column.</p> </li> <li> <p>Reduced compilation with <code>LPAD</code> and <code>RPAD</code></p> </li> <li> <p>Increased filter pushdown coverage when loading data from Parquet.</p> </li> </ul>"},{"location":"release_notes/2022.07/","title":"Bodo 2022.7 Release (Date: 07/31/2022)","text":""},{"location":"release_notes/2022.07/#new-features-and-improvements","title":"New Features and Improvements","text":"<p>Compilation / Performance improvements:</p> <ul> <li><code>Groupby</code> operations are now faster to compile and support super-wide DataFrames</li> <li><code>Groupby.apply()</code> operations have improved compilation time, runtime memory usage and performance.</li> <li>Most <code>BodoSQL</code> select statements are now faster to compile.</li> <li>Cache is now automatically invalidated when upgrading Bodo.</li> </ul> <p>Iceberg:</p> <ul> <li>Added support for writing Iceberg tables via <code>to_sql</code></li> </ul> <p>I/O:</p> <ul> <li><code>to_csv</code>, <code>to_json</code>, and <code>to_parquet</code> now support a custom argument <code>_bodo_file_prefix</code> to specify the prefix of files written in distributed cases.</li> <li>Snowflake data load now supports filter pushdown with <code>Series.str.startswith</code> and <code>Series.str.endswith</code>.</li> </ul> <p>Pandas coverage:</p> <ul> <li><code>read_csv</code> and <code>read_json</code> now support argument <code>sample_nrows</code> to set the number of rows that are sampled to infer column dtypes (by default <code>sample_nrows=100</code>).</li> <li>Support for <code>DataFrame.rank</code></li> <li>Support for <code>Groupby.ngroup</code></li> <li>Added support for dictionary-encoded string arrays (that have reduced memory usage and execution time) in the following functions:<ul> <li><code>Groupby.min</code></li> <li><code>Groupby.max</code></li> <li><code>Groupby.first</code></li> <li><code>Groupby.last</code></li> <li><code>Groupby.shift</code></li> <li><code>Groupby.head</code></li> <li><code>Groupby.nunique</code></li> <li><code>Groupby.sum</code></li> <li><code>Groupby.cumsum</code></li> <li><code>Groupby.transform</code></li> </ul> </li> </ul> <p>BodoSQL:</p> <ul> <li> <p>Added support for the following query syntax</p> <ul> <li><code>QUALIFY</code></li> <li><code>GROUP BY GROUPING SETS</code></li> <li><code>GROUP BY CUBE</code></li> <li><code>GROUP BY ROLLING</code></li> </ul> </li> <li> <p>Added support for the following functions:</p> <ul> <li><code>IFF</code></li> <li><code>NULLIFZERO</code></li> <li><code>NVL2</code></li> <li><code>ZEROIFNULL</code></li> </ul> </li> <li> <p>Added support for the following windowed aggregation functions:</p> <ul> <li><code>RANK</code></li> <li><code>DENSE_RANK</code></li> <li><code>PERCENT_RANK</code></li> <li><code>CUME_DIST</code></li> </ul> </li> <li> <p>The following functions are much faster to compile:</p> <ul> <li><code>ADDDATE/DATE_ADD/SUBDATE/DATE_SUB</code> if the second argument is an integer column</li> <li><code>ASCII</code></li> <li><code>CHAR</code></li> <li><code>COALESCE</code></li> <li><code>CONV</code></li> <li><code>DAYNAME</code></li> <li><code>FORMAT</code></li> <li><code>FROM_DAYS</code></li> <li><code>FROM_UNIXTIME</code></li> <li><code>IF</code></li> <li><code>IFNULL</code></li> <li><code>INSTR</code></li> <li><code>LAST_DAY</code></li> <li><code>LEFT</code></li> <li><code>LOG</code></li> <li><code>LPAD</code></li> <li><code>MAKEDATE</code></li> <li><code>MONTHNAME</code></li> <li><code>NULLIF</code></li> <li><code>NVL</code></li> <li><code>ORD</code></li> <li><code>REPEAT</code></li> <li><code>REPLACE</code></li> <li><code>REVERSE</code></li> <li><code>RIGHT</code></li> <li><code>RPAD</code></li> <li><code>SPACE</code></li> <li><code>STRCMP</code></li> <li><code>SUBSTRING</code></li> <li><code>SUBSTRING_INDEX</code></li> <li><code>TIMESTAMPDIFF</code> (if the unit is Month, Quarter, or Year)</li> <li><code>Unary -</code></li> <li><code>WEEKDAY</code></li> <li><code>YEAROFWEEKISO</code></li> </ul> </li> <li> <p>Support for binary data in complex join operations</p> </li> <li>Support for UTF-8 string literals in queries (previously just ASCII).</li> </ul>"},{"location":"release_notes/2022.08/","title":"Bodo 2022.8 Release (Date: 08/31/2022)","text":""},{"location":"release_notes/2022.08/#new-features-and-improvements","title":"New Features and Improvements","text":"<p>Compilation / Performance improvements:</p> <ul> <li>BodoSQL generated plans are now more optimized to reduce runtime, compile time, and memory usage.</li> <li>Performance improvements to pivot_table by reducing the amount of data being shuffled.</li> <li>BodoSQL <code>CASE</code> statements are now faster to compile.</li> </ul> <p>I/O:</p> <ul> <li>Bodo now uses a new optimized connector to write to Snowflake efficiently in parallel (with standard <code>DataFrame.to_sql()</code> syntax).</li> <li>Support for reading strings columns with dictionary encoding when fetching data from Snowflake.</li> <li>Bodo is now upgraded to use Arrow 8.</li> <li>Bodo can avoid loading any columns with parquet if only the length needs to be computed.</li> </ul> <p>Iceberg:</p> <ul> <li>Support for limit pushdown with data read from Iceberg.</li> </ul> <p>Pandas coverage:</p> <ul> <li>Added support for dictionary-encoded string arrays (that have reduced memory usage and execution time) with <code>pandas.concat</code></li> <li>Support for <code>groupby.sum()</code> with boolean columns.</li> <li>Support for <code>MultiIndex.nbytes</code></li> <li>Support for <code>Series.str.index</code></li> <li>Support for <code>Series.str.rindex</code></li> </ul> <p>BodoSQL:</p> <ul> <li> <p>Update the default null ordering with <code>ORDER BY</code> (nulls first with ASC, nulls last with DESC).</p> </li> <li> <p>Updates aggregation without a <code>GROUP BY</code> to return a replicated result.</p> </li> <li> <p>Improved runtime performance when computing a <code>SUM</code> inside a window function.</p> </li> <li> <p>Added support for the following column functions</p> <ul> <li><code>ACOSH</code></li> <li><code>ASINH</code></li> <li><code>ATANH</code></li> <li><code>BITAND</code></li> <li><code>BITOR</code></li> <li><code>BITXOR</code></li> <li><code>BITNOT</code></li> <li><code>BITSHIFTLEFT</code></li> <li><code>BITSHIFTRIGHT</code></li> <li><code>BOOLAND</code></li> <li><code>BOOLNOT</code></li> <li><code>BOOLOR</code></li> <li><code>BOOLXOR</code></li> <li><code>CBRT</code></li> <li><code>COSH</code></li> <li><code>DATEADD</code></li> <li><code>DECODE</code></li> <li><code>DIV0</code></li> <li><code>EDITDISTANCE</code></li> <li><code>EQUAL_NULL</code></li> <li><code>FACTORIAL</code></li> <li><code>GETBIT</code></li> <li><code>HAVERSINE</code></li> <li><code>INITCAP</code></li> <li><code>REGEXP</code></li> <li><code>REGEXP_COUNT</code></li> <li><code>REGEXP_INSTR</code></li> <li><code>REGEXP_LIKE</code></li> <li><code>REGEXP_REPLACE</code></li> <li><code>REGEXP_SUBSTR</code></li> <li><code>REGR_VALX</code></li> <li><code>REGR_VALY</code></li> <li><code>RLIKE</code></li> <li><code>SINH</code></li> <li><code>SPLIT_PART</code></li> <li><code>SQUARE</code></li> <li><code>STRTOK</code></li> <li><code>TANH</code></li> <li><code>TRANSLATE</code></li> <li><code>WIDTH_BUCKET</code></li> </ul> </li> <li> <p>Added support for binary data with the following functions:</p> <ul> <li><code>LEFT</code></li> <li><code>LEN</code></li> <li><code>LENGTH</code></li> <li><code>LPAD</code></li> <li><code>REVERSE</code></li> <li><code>RIGHT</code></li> <li><code>RPAD</code></li> <li><code>SUBSTR</code></li> <li><code>SUBSTRING</code></li> </ul> </li> <li> <p>Added support for the following window/aggregation functions</p> <ul> <li><code>ANY_VALUE</code></li> <li><code>COUNT_IF</code></li> <li><code>CONDITIONAL_CHANGE_EVENT</code></li> <li><code>CONDITIONAL_TRUE_EVEN</code></li> </ul> </li> </ul>"},{"location":"release_notes/2022.09/","title":"Bodo 2022.9 Release (Date: 09/31/2022)","text":""},{"location":"release_notes/2022.09/#new-features-and-improvements","title":"New Features and Improvements","text":"<p>Compilation / Performance improvements:</p> <ul> <li>Passing string data from Bodo JIT to Python and back (boxing/unboxing) is now much faster using the new Arrow support in Pandas. Dictionary-encoded (compressed) string arrays stay dictionary-encoded between calls.</li> <li>Optimized <code>pd.to_numeric()</code> for compressed string data.</li> <li>Support for compressed strings in <code>read_csv()</code> using user-specified argument (<code>\u201c_bodo_read_as_dict\u201c</code>).</li> </ul> <p>I/O:</p> <ul> <li>Support for loading no data columns from Iceberg and Snowflake when just returning the length of a table.</li> <li>Support for limit pushdown with Snowflake.</li> <li>Update the verbose logging API to track limit pushdown with verbose level 1.</li> </ul> <p>Iceberg:</p> <ul> <li>Support for appending to Iceberg tables with pre-defined partition spec and/or sort-order.</li> <li>Support for compressed string read from Iceberg tables.</li> </ul> <p>BodoSQL:</p> <ul> <li>Introduced the <code>SnowflakeCatalog</code> object so users can connect their Snowflake account to BodoSQL easily. When added to a <code>BodoSQLContext</code>, BodoSQL will directly search and load tables from inside Snowflake. For more information please refer to the documentation.</li> <li>Added <code>BodoSQLContext</code> methods <code>add_or_replace_view</code>, <code>remove_view</code>, <code>add_catalog</code>, and <code>remove_catalog</code> for creating an updated <code>BodoSQLContext</code>.</li> <li>BodoSQL now pushes limits in front of projections/element-wise functions to enable limit pushdown in most queries.</li> <li>If passing unsupported types to BodoSQL, BodoSQL will now attempt to process the query without using those columns. This enables compilation when using only the columns in the table with supported types.</li> <li><code>LEAD</code> and <code>LAG</code> now support an optional fill value argument, and the explicit <code>RESPECT_NULLS</code> syntax.</li> <li>Support for explicitly passing <code>NULL</code> for the fill value.</li> <li>Support for issuing a delete query in Snowflake using <code>SnowflakeCatalog</code>. This works by pushing the entire query directly into Snowflake.</li> <li>Support for <code>ILIKE</code> operator</li> <li>Support for <code>CONTAINS</code> operator</li> <li>Support for <code>MEDIAN</code> aggregate function</li> <li>Support for <code>SQUARE</code>, <code>CBRT</code>, <code>FACTORIAL</code> functions</li> <li>Support for aliases <code>VARIANCE_POP</code> and <code>VARIANCE_SAMP</code>.</li> <li>Support for <code>NEXT_DAY</code> and <code>PREVIOUS_DAY</code>.</li> </ul>"},{"location":"release_notes/2023.01/","title":"Bodo 2023.1 Release (Date: 01/06/2023)","text":""},{"location":"release_notes/2023.01/#new-features-and-improvements","title":"New Features and Improvements","text":"<p>Bodo:</p> <ul> <li>Added support for returning timezone aware timestamp scalars with DataFrame attributes iat and iloc.</li> <li>Support for comparison operators between timezone aware Timestamp values in Series, array, and scalars.</li> <li>Improved the performance of coalesce for string columns.</li> <li>Improved performance on dictionary encoded columns for coalesce and support for dictionary encoded output.</li> <li>Support for tz-aware data in pd.concat.</li> <li>pd.merge now supports cross join type (how=\u201dcross\u201d). Cross joins in BodoSQL are now significantly faster and more scalable.</li> <li>Supported passing timezone to pd.Timestamp.now().</li> <li>Support for comparison operators between Timestamps (both timezone aware and timezone naive) and datetime.date values in Series, array, and scalars.</li> </ul> <p>BodoSQL:</p> <p>Added support for the following functions:</p> <ul> <li>CURRENT_TIMESTAMP</li> <li>DATE_PART</li> <li>GETDATE</li> <li>DATEADD (in the form DATEADD(unit_string_literal, integer_amount, starting_datetime))</li> <li>TIMEADD</li> <li>TO_BOOLEAN / TRY_TO_BOOLEAN</li> <li>TO_CHAR / TO_VARCHAR</li> <li>CHARINDEX (not supported for binary data)</li> <li>POSITION (only in the form POSITION(X IN Y), not supported for binary data)</li> <li>INSERT (behavior when numerical arguments are negative is currently not well defined)</li> <li>STARTSWITH / ENDSWITH</li> <li>RTRIMMED_LENGTH</li> <li>MODE (only as a window function, not as a generic aggregation)</li> <li>RATIO_TO_REPORT</li> <li>BOOLOR_AGG</li> </ul> <p>Parity Improvements:</p> <ul> <li>Added support for timezone aware data in all BodoSQL datetime functions.</li> <li>Adjusted TIMESTAMPDIFF to obey Snowflake SQL rounding rules (i.e. ignoring all units smaller than the selected unit).</li> <li>Support for loading views and other non-standard tables with the SnowflakeCatalog.</li> <li>Support for all join types offered by Snowflake.</li> <li>Support for tz-aware data outputs in Case statements.</li> </ul> <p>Other Improvements:</p> <ul> <li>Multiple top-level calls to window functions will now compile faster in BodoSQL if they use the same partition and order.</li> <li>Snowflake writes with df.to_sql can now use the more performant direct upload strategy for Azure based Snowflake accounts.</li> <li>Snowflake I/O (read and write) no longer requires the snowflake-sqlalchemy package.</li> <li>Improved performance for reading string data in compressed format from Snowflake.</li> <li>Performance Warning if running Bodo and Snowflake in different cloud regions.</li> <li>Added support for returning timezone aware timestamp scalars with Series attributes iat, iloc, loc and regular getitem.</li> </ul>"},{"location":"release_notes/2023.06/","title":"Bodo 2023.6 Release (Date: 06/23/2023)","text":""},{"location":"release_notes/2023.06/#new-features-and-improvements","title":"New Features and Improvements","text":"<p>Bodo:</p> <ul> <li>Added original date type support for all datetime functions.</li> <li>Support filter pushdown for various functions.</li> <li>Upgrade to Arrow 11.</li> <li>Improved performance on join operations by using streaming loop.</li> <li>Support nullable timestamp, float and boolean array.</li> <li>Zero-Copy support for most Arrow Arrays.</li> <li>Supported passing timezone to pd.Timestamp.now().</li> <li>Support for comparison operators between date and tz-aware/naive timestamps.</li> </ul> <p>BodoSQL:</p> <p>Added support for the following functions:</p> <ul> <li>CURRENT_DATE</li> <li>DATEDIFF/TIMEDIFF/TIMESTAMPDIFF</li> <li>TRY_CAST</li> <li>LEAD/LAG</li> <li>[TRY_]TO_BINARY</li> <li>[TRY_]TO_DECIMAL, [TRY_]TO_NUMBER, [TRY_]TO_NUMERIC, [TRY_]TO_DOUBLE</li> <li>[TRY_]TO_DOUBLE</li> <li>[TRY_]TO_TIME</li> <li>SAMPLE</li> <li>ILIKE/ANY/ALL</li> <li>LEAST/GREATEST</li> <li>ADD_MONTH/MONTH_BETWEEN</li> <li>HASH</li> <li>RANDOM</li> <li>UNIFORM</li> <li>TO_ARRAY</li> <li>ARRAY_TO_STRING</li> <li>SPLIT</li> </ul> <p>Parity Improvements:</p> <ul> <li>Added support for reading nested arrays.</li> <li>Support <code>repeats</code> for tz-aware data in join optimization.</li> <li>Support logical and comparison operators between Boolean and Numeric Types.</li> <li>Support for date outputs in Case statements.</li> </ul> <p>Other Improvements:</p> <ul> <li>More efficient fill templating for Join.</li> <li>Improved Snowflake sampling for dict-encoding detection.</li> <li>Revamped C++ array representation to be more robust.</li> <li>Support batching / streaming Snowflake Read Implementation.</li> </ul>"},{"location":"release_notes/2023.07/","title":"Bodo 2023.7 Release (Date: 07/31/2023)","text":""},{"location":"release_notes/2023.07/#new-features-and-improvements","title":"New Features and Improvements","text":"<p>BodoSQL:</p> <ul> <li>Fixed a rare bug where final column names lost assigned aliases.</li> <li>Reduced compilation time for some queries.</li> <li>INTERVAL_QUARTER support</li> <li>Optional argument support in REPLACE</li> <li>Remove interval related words from restricted keywords.</li> <li>Support optional arguments in CEIL and FLOOR</li> <li>Support unquoted units for DATE_PART</li> <li>Support TO_CHAR with format string for time and timestamp</li> </ul>"},{"location":"release_notes/2023.08/","title":"Bodo 2023.8 Release (Date: 08/31/2023)","text":""},{"location":"release_notes/2023.08/#new-features-and-improvements","title":"New Features and Improvements","text":"<p>Bodo: - Vectorized query engine:</p> <p>Several operations (join and reads) now operate in 'vectorized'   mode. Only results that are in use are kept in memory or read   in. This leads to drastically improved memory performance and avoids   OOM errors in many cases. There is a slight performance degradation   in this mode, but this will improve with coming releases.</p> <p>Some operations, such as GROUP BY and window functions are not yet   vectorized.</p> <p>Vectorized execution mode can be disabled by setting the environment variable <code>BODO_STREAMING_ENABLED</code> to <code>\"0\"</code>.</p> <ul> <li>Common subcolumn elimination in planner</li> <li>Better selectivity in metadata pushdown query</li> <li>Better type size estimates in planner</li> <li>Support <code>ignore_index=True</code> in <code>df.drop_duplicates</code></li> </ul> <p>BodoSQL:</p> <p>Added support for the following functions:</p> <ul> <li>PERCENTILE_CONT</li> <li>SHA2</li> <li>MD5</li> </ul> <p>Parity improvements: - Make treatment of type keywords compatible with Snowflake SQL</p> <p>Planner improvements:</p> <ul> <li>Improved planner based common sub-expression elimination</li> <li>Improved planner estimations for column selectivity</li> <li>Improved planner estimations for column sizes</li> <li>Planner based Snowflake pushdown support for <code>getdate()</code>, <code>current_date()</code>, and intervals</li> </ul>"},{"location":"release_notes/2023.09/","title":"Bodo 2023.9 Release (Date: 09/01/2023)","text":""},{"location":"release_notes/2023.09/#new-features-and-improvements","title":"New Features and Improvements","text":"<p>Compilation / Performance improvements:</p> <ul> <li>BodoSQL generated plans have been further optimized to reduce runtime, compile time, and memory usage.</li> <li>Performance and compilation time improvements to several window functions:<ul> <li><code>LEAD</code></li> <li><code>LAG</code></li> <li><code>AVG</code></li> <li><code>VARIANCE_POP</code> and equivalent functions</li> <li><code>VARIANCE_SAMP</code> and equivalent functions</li> <li><code>STDDEV_POP</code> and equivalent functions</li> <li><code>STDDEV_SAMP</code> and equivalent functions</li> <li><code>FIRST_VALUE</code></li> <li><code>LAST_VALUE</code></li> <li><code>RATIO_TO_REPORT</code></li> </ul> </li> </ul> <p>Python coverage:</p> <ul> <li> <p>Partial support for the <code>np.matrix</code> type with the following functionality (non-distributed):</p> <ul> <li><code>np.asmatrix</code> to convert a scalar, 1D array, 2D array, list of scalars, or list of 1D arrays to a matrix</li> <li><code>np.asarray</code> to convert a matrix to a 2D array</li> <li>Addition and subtraction with <code>+</code> and <code>-</code></li> <li>Matrix multiplication with <code>*</code>, <code>@</code> or <code>np.dot</code></li> <li>Calling <code>len()</code> on a Matrix</li> <li>Using <code>.ndim</code>, <code>.shape</code> and <code>.T</code> (non-distributed) on a Matrix</li> </ul> </li> <li> <p>Support for the following Numpy functions:</p> <ul> <li><code>np.interp</code> non-distributed (added support for keyword arguments <code>left</code> and <code>right</code>)</li> <li><code>np.tile</code> (added support for specific patterns, see Numpy docs)</li> <li><code>np.linalg.norm</code> (added support for keyword argument <code>axis=1</code> when the input is a 2D array)</li> <li><code>np.nan_to_num</code></li> <li><code>np.dot</code> (added support for heterogeneous typing between integer &amp; float array inputs)</li> <li><code>scipy.fftpack.fftshift</code> (non-distributed)</li> <li><code>scipy.fftpack.fft2</code> (non-distributed)</li> </ul> </li> </ul> <p>BodoSQL:</p> <ul> <li>Added support for <code>HASH(*)</code></li> <li>Added support for <code>PERCENTILE_CONT</code> and <code>PERCENTILE_DISC</code> (non-window support)</li> </ul>"},{"location":"release_notes/2023.09/#202395-new-features-and-improvements","title":"2023.9.5 New Features and Improvements","text":"<p>Compilation / Performance improvements:</p> <ul> <li>BodoSQL generated plans have been further optimized to reduce runtime and memory usage.</li> <li>Support for executing <code>UNION</code> in vectorized mode</li> <li>Support for executing <code>ARRAY_AGG</code> on numeric types in a <code>GROUP BY</code></li> </ul>"},{"location":"release_notes/2023.09/#202396-new-features-and-improvements","title":"2023.9.6 New Features and Improvements","text":"<p>Fix critical bugs in vectorized execution mode.</p> <ul> <li>BodoSQL generated plans have been further optimized to reduce runtime and memory usage.</li> <li><code>GET_PATH</code> and JSON field accesses via <code>:</code> are supported in some usages.</li> </ul>"},{"location":"release_notes/2023.10/","title":"Bodo 2023.10 Release (Date: 10/02/2023)","text":""},{"location":"release_notes/2023.10/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li>BodoSQL generated plans have been further optimized to reduce runtime and memory usage, as well as   resolve several bugs that would prevent a plan from being produced. These changes include but are   not limited to the following:</li> <li>Improved ability to push down filters</li> <li>Improved quality of join orderings based on meta data</li> <li>Improved simplification of expressions and constant folding to reduce computations</li> <li>Unreserve several keywords in the BodoSQL parser, allowing them to be column names, aliases,   or table names: <code>ROW_NUMBER</code>, <code>INTERVAL</code>, <code>PERCENT</code>, <code>COUNT</code>, <code>TRANSLATE</code>, <code>ROLLUP</code>, <code>MATCHES</code>,    <code>ABS</code>, <code>LAG</code>, and <code>MATCH_NUMBER</code>.</li> <li>Allow a BodoSQL Snowflake Catalog to be created from a connection string using <code>bodosql.SnowflakeCatalog.from_conn_str</code>.</li> <li>Support <code>ANY_VALUE</code> on array data</li> </ul>"},{"location":"release_notes/2023.10/#2023101-new-features-and-improvements","title":"2023.10.1 New Features and Improvements","text":"<ul> <li>Fix critical runtime bugs in vectorized execution mode.</li> <li>BodoSQL generated plans have been further optimized to reduce runtime and memory usage.</li> <li>Better compile time evaluation for datetime operations inside the planner.</li> <li>Major version upgrades</li> <li>Upgrade Python to 3.11</li> <li>Upgrade Numba to 0.57</li> <li>Upgrade Calcite to 1.31</li> <li>Upgrade Iceberg to 1.3.1</li> <li>Upgrade Pandas to 1.5</li> </ul>"},{"location":"release_notes/2023.10/#2023102-new-features-and-improvements","title":"2023.10.2 New Features and Improvements","text":"<ul> <li>Fix critical bugs</li> <li>Reduce memory usage in GROUPBY</li> </ul>"},{"location":"release_notes/2023.10/#2023103-new-features-and-improvements","title":"2023.10.3 New Features and Improvements","text":"<ul> <li>Fix critical bugs.</li> <li>BodoSQL generated plans have been further optimized to reduce runtime.</li> <li>Support more functionality in BodoSQL:</li> <li>Support <code>ARRAY_AGG</code> on strings and <code>ARRAY_AGG(DISTINCT)</code></li> <li>Support all call signatures for <code>TRUNC</code> and <code>CONCAT</code></li> <li>Support <code>current_database</code></li> <li>Support writing timezone aware data in output tables</li> </ul>"},{"location":"release_notes/2023.10/#2023104-new-features-and-improvements","title":"2023.10.4 New Features and Improvements","text":"<ul> <li>Fix critical runtime bugs.</li> </ul>"},{"location":"release_notes/2023.10/#2023105-new-features-and-improvements","title":"2023.10.5 New Features and Improvements","text":"<ul> <li>Fix critical runtime bugs.</li> </ul>"},{"location":"release_notes/2023.10/#2023106-new-features-and-improvements","title":"2023.10.6 New Features and Improvements","text":"<ul> <li>Fix critical runtime bugs.</li> <li>BodoSQL generated plans have been further optimized to reduce runtime and memory usage.</li> <li>Improved our ability to gather and use distinctness metadata </li> <li>Support <code>DATEDIFF</code> between a timezone aware and a timezone naive column</li> <li>Support <code>DATE_PART</code></li> </ul>"},{"location":"release_notes/2023.10/#2023107-new-features-and-improvements","title":"2023.10.7 New Features and Improvements","text":"<ul> <li>Fix critical runtime bugs.</li> <li>BodoSQL generated plans have been further optimized to reduce runtime and memory usage.</li> <li>Support more functionality in BodoSQL:</li> <li>Support <code>JAROWINKLER_SIMILARITY</code></li> <li>Support <code>BASE64_ENCODE</code></li> <li>Support <code>BASE64_DECODE_STRING</code></li> <li>Support <code>BASE64_DECODE_BINARY</code></li> <li>Support <code>TRY_BASE64_DECODE_STRING</code></li> <li>Support <code>TRY_BASE64_DECODE_BINARY</code></li> <li>Support <code>HEX_ENCODE</code></li> <li>Support <code>HEX_DECODE_STRING</code></li> <li>Support <code>HEX_DECODE_BINARY</code></li> <li>Support <code>TRY_HEX_DECODE_STRING</code></li> <li>Support <code>TRY_HEX_DECODE_BINARY</code></li> <li>Support <code>ARRAY_SIZE</code></li> <li>Support <code>OBJECT_KEYS</code></li> <li>Support <code>getitem/isna</code> on NULL columns</li> <li>Support <code>QUARTER</code> interval literals (and all aliases)</li> <li>Support all call signatures for <code>TIMESTAMP_FROM_PARTS</code></li> <li>Support all call signatures for <code>TRY_TO_BOOLEAN/TRY_TO_DOUBLE</code></li> <li>Support all call signatures for <code>DATEADD/TIMEADD/TIMESTAMPADD</code></li> </ul>"},{"location":"release_notes/2023.11/","title":"Bodo 2023.11 Release (Date: 11/07/2023)","text":""},{"location":"release_notes/2023.11/#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li>BodoSQL generated plans have been further optimized to reduce compile time, runtime and memory usage, as well as   resolve several bugs pertaining to stability.</li> <li>Support more functionality in BodoSQL:</li> <li><code>EQUAL_NULL</code> and <code>&lt;=&gt;</code> now supported on semi-structured data.</li> <li>Added support for the semi-structured functions <code>ARRAY_CONSTRUCT</code>, <code>ARRAYS_OVERLAP</code>, <code>ARRAY_POSITION</code>.</li> <li><code>CONCAT</code> (and <code>||</code>) now support concating binary data.</li> <li><code>LAST_DAY</code> now supports unquoted interval literals as arguments.</li> <li>Support for writing semi-structured array columns and some cases of semi-structured object columns to Snowflake.</li> <li>Some casting functions that previously had incorrect behavior with nullable   data are now fixed.</li> </ul>"},{"location":"release_notes/2023.11/#2023111-new-features-and-improvements","title":"2023.11.1 New Features and Improvements","text":""},{"location":"release_notes/2023.11/#feature-updates","title":"Feature Updates:","text":"<ul> <li>Added the table function <code>FLATTEN</code> which can be used to explode a column of arrays alongside the <code>LATERAL</code> keyword. Currently allows ARRAY columns being passed in to the INPUT argument, as well as JSON columns under limited circumstances. Currently only allows the defaults for named arguments <code>PATH</code>, <code>OUTER</code>, <code>RECURSIVE</code> and <code>MODE</code>.</li> <li>Added the function <code>OBJECT_CONSTRUCT_KEEP_NULL</code>, including the special syntax <code>OBJECT_CONSTRUCT_KEEP_NULL(*)</code>, as long as all of the key arguments are string literals.</li> <li>Added the aggregation function <code>OBJECT_AGG</code> which takes in a column of strings and a column of data of any type and combines them into a JSON value where the first column is the keys and the second column is the values. Currently only supported within a GROUP BY clause.</li> <li>Added the function <code>OBJECT_DELETE</code>. Non-literals key strings are partially supported.</li> <li>Support <code>ARRAY_CONTAINS</code>, <code>ARRAY_UNIQUE_AGG</code>, <code>ARRAY_EXCEPT</code>, <code>ARRAY_INTERSECTION</code>, <code>ARRAY_CAT</code>, and <code>STRTOK_TO_ARRAY</code>.</li> <li>Support for the table function <code>SPLIT_TO_TABLE</code>.</li> <li>Support all sub-second interval units and their aliases.</li> <li>Support all interval literal units without quotes.</li> <li>Support unquoted date/time units in all functions that accept date/time units.</li> <li>Support reading Object and deeply nested semi-structured columns from Snowflake.</li> </ul>"},{"location":"release_notes/2023.11/#improvements-and-bug-fixes","title":"Improvements and Bug Fixes:","text":"<ul> <li>Enabled streaming in pipelines with semi-structured data.</li> <li>Reduced out of memory risk during shuffle.</li> <li>Optimized handling of nullable columns during SQL plan optimization.</li> </ul>"},{"location":"release_notes/2023.11/#dependency-updates","title":"Dependency Updates:","text":"<ul> <li>Upgrade to Calcite 1.34</li> <li>Upgrade to Pandas 2</li> <li>Upgrade to Cython 3</li> <li>Upgrade to HDF5 1.14</li> </ul>"},{"location":"release_notes/2023.11/#2023112-new-features-and-improvements","title":"2023.11.2 New Features and Improvements","text":""},{"location":"release_notes/2023.11/#feature-updates_1","title":"Feature Updates:","text":"<ul> <li>General improvements to the spill-to-disk functionality.</li> <li>Support <code>ARRAY_COMPACT</code>.</li> <li>Support for indexing into array values via <code>GET()</code> and <code>arr[idx]</code>.</li> <li>Added logging for failed metadata collection during planning. This is available with verbose_level &gt;= 2.</li> <li>Support for precision and scale arguments with to_number try_to_number, and aliases.</li> <li><code>DATE_FROM_PARTS</code>/<code>TIME_FROM_PARTS</code>/<code>TIMESTAMP_FROM_PARTS</code> &amp; their aliases can take any numeric arguments, as opposed to only integers.</li> </ul>"},{"location":"release_notes/2023.11/#improvements-and-bug-fixes_1","title":"Improvements and Bug Fixes:","text":"<ul> <li>Fixed bug where output of certain aggregate calls would have the wrong nullability.</li> <li>Fixed various bugs that were limiting ability to call functions on columns of arrays of JSON data.</li> <li>Fixed a bug in the Union (Distinct) operator which could lead to a hang in some cases.</li> <li>Fixed <code>TO_ARRAY</code> and <code>ARRAY_TO_STRING</code> on column inputs and case statement.</li> <li>Fixed shuffle corner case bug for semi-structured data array nulls.</li> <li>Fixes to <code>COALESCE</code> typing behavior.</li> </ul>"},{"location":"release_notes/2023.11/#dependency-updates_1","title":"Dependency Updates:","text":"<ul> <li>Bodo now uses PyArrow arrays to box/unbox nested arrays with zero-copy (made possible by Pandas 2 upgrade).</li> </ul>"},{"location":"release_notes/2023.12/","title":"Bodo 2023.12 Release (Date: 12/01/2023)","text":""},{"location":"release_notes/2023.12/#new-features-and-improvements","title":"New Features and Improvements","text":""},{"location":"release_notes/2023.12/#new-features","title":"New Features:","text":"<ul> <li>Support for reading tables outside the default database.</li> <li>Initial support for expanding Snowflake views. When Bodo encounters a Snowflake view definition it will attempt to expand the view directly into the query, which will provide increased optimization opportunities and performance. Bodo will not be able to expand views if the user under which Bodo operates does not have permissions to access the underlying table or fetch the view definition. Bodo will not attempt to expand the view if it is a materialized view or a secure view, to comply with Snowflake users' expectations. If for any reason Bodo is unable to expand a view then the query will still execute by evaluating the view in Snowflake and reading it as a table. This is the first release offering this support, so the number of views that can be expanded will increase with future releases.</li> <li>Added support for the function <code>OBJECT_CONSTRUCT</code> including the syntactic sugar <code>OBJECT_CONSTRUCT(*)</code> so long as all of the values are of the same type (or trivially castable types).</li> <li>Add support for the functions <code>ARRAY_CONSTRUCT_COMPACT</code>, <code>ARRAY_REMOVE</code>, <code>ARRAY_REMOVE_AT</code>, <code>ARRAY_SLICE</code> and <code>TO_VARIANT</code>.</li> <li>Allow <code>FLATTEN</code> to be called on variant columns, so long as the variants contain array/json data.</li> <li>Bodo can now spill to S3.</li> <li>Support for GET function on array values</li> </ul>"},{"location":"release_notes/2023.12/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>Reduced peak memory usage when using the <code>FLATTEN</code> or <code>SPLIT_TO_TABLE</code> functions in streaming.</li> </ul>"},{"location":"release_notes/2023.12/#dependency-updates","title":"Dependency Updates:","text":"<ul> <li>Upgrade to Calcite 1.35</li> </ul>"},{"location":"release_notes/2023.12/#2023121-new-features-and-improvements","title":"2023.12.1 New Features and Improvements","text":""},{"location":"release_notes/2023.12/#new-features_1","title":"New Features:","text":"<ul> <li>Added support for the functions <code>IS_ARRAY</code>, <code>IS_OBJECT</code> and <code>OBJECT_PICK</code>.</li> <li>Added support for <code>TO_ARRAY</code> for struct and map elements.</li> <li>Added support for nulls in map array values.</li> <li>Support for parsing array literals. Note: just like <code>ARRAY_CONSTRUCT</code>, only homogeneous arrays are supported at this time.</li> <li>Support for nested array scalars.</li> <li>View inlining now supports all valid Snowflake CREATE VIEW syntax for the view definition.</li> </ul>"},{"location":"release_notes/2023.12/#performance-improvements_1","title":"Performance Improvements:","text":"<ul> <li>Process join data communicated across ranks in batches to reduce peak memory consumption and improve cache locality for better performance.</li> </ul>"},{"location":"release_notes/2023.12/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Fix truncation when writing TIMESTAMP_NTZ columns to Snowflake.</li> </ul>"},{"location":"release_notes/2023.12/#dependency-updates_1","title":"Dependency Updates:","text":"<ul> <li>Upgrade to Calcite 1.36</li> </ul>"},{"location":"release_notes/2023.12/#2023122-new-features-and-improvements","title":"2023.12.2 New Features and Improvements","text":""},{"location":"release_notes/2023.12/#new-features_2","title":"New Features:","text":"<ul> <li>Support <code>OBJECT_INSERT</code></li> <li>Support for writing columns of unnested JSON data with homogenous key and value types to Snowflake as Objects</li> <li>Improved error messages for Unsupported Snowflake UDFs. Future releases will contain increased Snowflake UDF support</li> <li>Support <code>COALESCE</code> with Datetime and Timestamp + timezone data</li> <li>Support <code>VALUES</code> syntax with multiple rows</li> <li>Support for struct columns with no fields</li> </ul>"},{"location":"release_notes/2023.12/#performance-improvements_2","title":"Performance Improvements:","text":"<ul> <li>Improved ability to push filters down to Snowflake and order joins based on the distinctness of columns from inlined views.</li> </ul>"},{"location":"release_notes/2023.12/#bug-fixes_1","title":"Bug Fixes:","text":"<ul> <li>Fixed a bug where FROM_DATE incorrectly returned TIMESTAMPs when the inputs were DATEs</li> </ul>"},{"location":"release_notes/2024.01/","title":"Bodo 2024.1 Release (Date: 01/05/2024)","text":""},{"location":"release_notes/2024.01/#new-features-and-improvements","title":"New Features and Improvements","text":""},{"location":"release_notes/2024.01/#new-features","title":"New Features:","text":"<ul> <li>Distributed complex, two dimensional fft and <code>fftshift</code>.</li> <li>Added support for the function <code>TO_OBJECT</code>.</li> <li>Added support for <code>OUTER=&gt;true</code> when calling <code>FLATTEN</code>.</li> <li>Support for creating a table with the <code>GENERATOR</code> function using only the <code>ROWCOUNT</code> argument.</li> <li>Increased support for variant arguments to numeric/datetime/string/array/object functions.</li> <li>Added support for <code>GET_PATH</code> and the alternate syntax using <code>:</code>.</li> <li>Support ARRAY_AGG on all types.</li> <li>Setting the environment variable BODO_DISABLE_SF_RESULT_CACHE to 1 will ensure that Snowflake doesn\u2019t return results from its result cache when Bodo reads tables from Snowflake. This can be useful for performance testing but is not recommended for production use.</li> </ul>"},{"location":"release_notes/2024.01/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>General improvements to disk spilling performance.</li> <li>General improvements to the spilling functionality for better memory utilization in Join and Groupby.</li> <li>Enabled several plan optimizations such as simplifying null predicates and push predicates deeper into the plan.</li> </ul>"},{"location":"release_notes/2024.01/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Coalesce type coercion now matches snowflake behavior</li> <li><code>OBJECT_AGG</code> now omits any rows where the key or value is null in the final object.</li> <li>Fixed a bug where default Snowflake connection settings could cause a timeout for long running writes. We now set ABORT_DETACHED_QUERY=False automatically to avoid this issue.</li> </ul>"},{"location":"release_notes/2024.01/#usability-improvements","title":"Usability Improvements","text":"<ul> <li>Improved the specificity of error messages regarding Snowflake UDFs to help better explain limitations in Bodo functionality for UDFs. Future releases will expand this functionality.</li> <li>Using an aggregation function other than first on a semi-structured column now throws an error.</li> </ul>"},{"location":"release_notes/2024.01/#202412-new-features-and-improvements","title":"2024.1.2 New Features and Improvements","text":""},{"location":"release_notes/2024.01/#new-features_1","title":"New Features:","text":"<ul> <li>Support for using semi-structured columns as keys to <code>JOIN</code> and <code>GROUP BY</code>.</li> <li>Support for <code>APPROX_PERCENTILE</code> as a window function.</li> <li>Support for object literals as syntactic sugar for <code>OBJECT_CONSTRUCT</code>.</li> <li>Added limited support for <code>PARSE_JSON</code> when it can be re-written as a call to JSON_EXTRACT_PATH_TEXT followed by a cast to another type. E.g.: <code>PARSE_JSON(S):field::integer</code> can be rewritten as <code>JSON_EXTRACT_PATH_TEXT(S, \u2018field\u2019)::integer</code>.</li> <li>Support for sub-millisecond resolution in <code>TIME</code> and <code>TIMESTAMP</code> literals.</li> <li>Support <code>SELECT * EXCLUDING (columns)</code> syntax.</li> <li> <p>We have updated the BodoSQL identifier handling to be more consistent with Snowflake and the SQL standard. This has the following new behavior which matches Snowflake:</p> <ul> <li>Unquoted identifiers are converted to uppercase letters.</li> <li>Quoted identifiers retain their current casing.</li> <li>Identifier matching is now case sensitive after this conversion. Please refer to our documentation for more detailed examples.</li> </ul> </li> <li> <p>Support boolean inputs in <code>TO_NUMBER.</code></p> </li> <li>Improved support for nested datatypes in BodoSQL originating from Python.</li> <li>Added support for Object literals.</li> <li>Support for distributed transpose of Numpy arrays.</li> <li>Distributed support for real/imaginary components of complex Numpy arrays.</li> <li>Support for distributed multi-dimensional getitem for Numpy arrays (only scalar output currently).</li> <li>Remote spilling to S3 is now enabled by default (on AWS workspaces) for increased reliability.</li> </ul>"},{"location":"release_notes/2024.01/#bug-fixes_1","title":"Bug Fixes:","text":"<ul> <li>Fixed <code>OBJECT_DELETE</code>, <code>OBJECT_PICK</code> and <code>OBJECT_CONSTRUCT</code> behavior when used inside of a CASE statement.</li> </ul>"},{"location":"release_notes/2024.01/#202413-new-features-and-improvements","title":"2024.1.3 New Features and Improvements","text":""},{"location":"release_notes/2024.01/#new-features_2","title":"New Features:","text":"<ul> <li>Support casting arrays to null array.</li> </ul>"},{"location":"release_notes/2024.01/#performance-improvements_1","title":"Performance Improvements:","text":"<ul> <li>Improved literal cast constant folding, which is especially impactful for datetime literals.</li> <li>General planner improvements on filter inference/simplification.</li> <li>Improved performance of <code>np.interp</code>.</li> </ul>"},{"location":"release_notes/2024.01/#bug-fixes_2","title":"Bug Fixes:","text":"<ul> <li>Ensure datetime literal default precision is always 9.</li> </ul>"},{"location":"release_notes/2024.01/#202414-new-features-and-improvements","title":"2024.1.4 New Features and Improvements","text":""},{"location":"release_notes/2024.01/#new-features_3","title":"New Features:","text":"<ul> <li>Expanded support for conditions in non-inner joins to always support conditions that only apply to one side of the join.</li> <li>Expanded support of conditional functions with variant inputs, and for accessing array indices &amp; object fields when the array or object is a variant.</li> <li>Support for the <code>UUID_STRING</code> function.</li> <li>Support string literals enclosed by dollar signs, e.g. <code>$$Hello World$$</code>.</li> <li>Support <code>OBJECT_AGG</code> as a window function.</li> <li>Support for comparing decimal columns to integer and float columns.</li> <li>Support for semi-structured arrays in <code>groupby.apply</code>.</li> <li>Double quoted unit arguments to <code>DATE_TRUNC</code> are now supported.</li> <li>Support for some Snowflake UDFs when the function body is an expression or a query with no arguments. Future releases will continue to expand Snowflake UDF support. </li> </ul>"},{"location":"release_notes/2024.01/#bug-fixes_3","title":"Bug Fixes:","text":"<ul> <li>Fixed behavior of <code>LISTAGG</code> when all data is null to match Snowflake.</li> </ul>"},{"location":"release_notes/2024.01/#202415-new-features-and-improvements","title":"2024.1.5 New Features and Improvements","text":""},{"location":"release_notes/2024.01/#new-features_4","title":"New Features:","text":"<ul> <li>Support for inlining some Snowflake UDFs with query bodies that do not reference columns.</li> <li>Throws a more detailed error message if a Snowflake UDF contains query functionality that we do not support yet.</li> <li>Support comments for the entire table or individual columns in CREATE TABLE AS SELECT statements.</li> <li>Support non-constant regexp pattern arguments for <code>REGEXP_LIKE</code>, <code>REGEX_COUNT</code> and <code>REGEXP_REPLACE</code>, as well as their function aliases.</li> <li>Increased support for dictionary encoded strings inside of semi-structured columns.</li> <li>Support for GET_IGNORE_CASE</li> </ul>"},{"location":"release_notes/2024.01/#dependency-upgrades","title":"Dependency Upgrades:","text":"<ul> <li>Upgrade PyArrow to v14.0.2</li> </ul>"},{"location":"release_notes/2024.01/#bug-fixes_4","title":"Bug Fixes:","text":"<ul> <li>Handle constant tuples in Numpy array reshape operations properly</li> </ul>"},{"location":"release_notes/2024.01/#202416-new-features-and-improvements","title":"2024.1.6 New Features and Improvements","text":""},{"location":"release_notes/2024.01/#new-features_5","title":"New Features:","text":"<ul> <li>Support for inlining UDFs with a unique definition where the body is a query and the arguments are columns.</li> <li>Support for inlining UDTFs with a unique definition.</li> <li>Support HASH on remaining datatypes (decimal, array, object).</li> <li>Unreserved more keywords (including all aggregation function names), allowing them as column names.</li> </ul>"},{"location":"release_notes/2024.01/#performance-improvements_2","title":"Performance Improvements:","text":"<ul> <li>Minor speedup for GET with integer index value.</li> </ul>"},{"location":"release_notes/2024.01/#bug-fixes_5","title":"Bug Fixes:","text":"<ul> <li>Prevent compilation failures while trying to infer the Bodo type of a Snowflake variant column that is all-null.</li> <li>Fixes a bug in Join that could lead to a hang and/or incorrect output.</li> <li>Fixed a bug where columns in the USING clause of a join would incorrectly be marked as ambiguous</li> <li>Fixed a bug where an interaction between coalesce and outer join could result in a failure to compile a SQL plan</li> </ul>"},{"location":"release_notes/2024.02/","title":"Bodo 2024.2 Release (Date: 02/07/2024)","text":""},{"location":"release_notes/2024.02/#new-features-and-improvements","title":"New Features and Improvements","text":""},{"location":"release_notes/2024.02/#new-features","title":"New Features:","text":"<ul> <li>Added support for overloaded UDF/UDTF definitions.</li> <li>Added support for the Snowflake table function <code>EXTERNAL_TABLE_FILES</code>, which requires connecting to a Snowflake Catalog.</li> <li>Increased support for variant arguments in functions relating to semi-structured arrays.</li> <li>Added recognition of <code>TIMESTAMPNTZ</code> as an alias for <code>TIMESTAMP_NTZ</code> and <code>TIMESTAMPLTZ</code> as an alias for <code>TIMESTAMP_LTZ</code>.</li> <li>Added support for queries where the source clause is in the form <code>FROM (tablename)</code>.</li> <li>Added support for the <code>COUNT</code> aggregation function on all semi-structured column types.</li> <li>Added support for casting decimal values to integers and vice versa.</li> <li>Added support for <code>DATE - DOUBLE</code>.</li> <li>Increased support for type coercion between semi-structured types.</li> <li>Enabled implicit lateral joins when using table functions like <code>FLATTEN</code>.</li> <li>Added support for writing decimal scalars into integer/float arrays.</li> </ul>"},{"location":"release_notes/2024.02/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>Reduced peak memory usage from queries containing filters on the condition <code>ROW_NUMBER() OVER (...) = 1</code>.</li> <li>Removed extra runtime casts when the source type and target type are represented the same in Bodo.</li> </ul>"},{"location":"release_notes/2024.02/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Fixed bug sometimes preventing codegen from completing when a join has a condition comparing a column to a constant string.</li> <li>Fixed bug causing incorrect behavior of a filter on the condition <code>ROW_NUMBER() OVER (...) = 1</code> when the columns to order by are of certain types.</li> <li>Fixed a bug that could lead to an error when some columns in an Aggregate operation are semi-structured.</li> <li>Added a logging message when a view can\u2019t be inlined due to access issues.</li> <li>Fixed a bug preventing correct recognition of table names when using <code>FLATTEN</code> in some situations.</li> </ul>"},{"location":"release_notes/2024.02/#dependency-upgrades","title":"Dependency Upgrades:","text":"<ul> <li>Upgrade Iceberg to 1.43.</li> </ul>"},{"location":"release_notes/2024.02/#202421-new-features-and-improvements","title":"2024.2.1 New Features and Improvements","text":""},{"location":"release_notes/2024.02/#new-features_1","title":"New Features:","text":"<ul> <li>Support calling <code>TO_DOUBLE</code> on variants.</li> <li>Support for <code>$</code> inside of compound identifiers.</li> </ul>"},{"location":"release_notes/2024.02/#performance-improvements_1","title":"Performance Improvements:","text":"<ul> <li>Improved BodoSQL query plans by increasing ability to infer predicates.</li> </ul>"},{"location":"release_notes/2024.02/#bug-fixes_1","title":"Bug Fixes:","text":"<ul> <li>Fixed bug sometimes preventing compiling queries on BodoSQL when they contain a join where the condition contains an <code>IN</code> subquery clause.</li> <li>Fixed bug sometimes preventing compiling queries on BodoSQL when they contain a <code>FLATTEN</code> or <code>SPLIT_TO_TABLE</code> call inside an <code>IN</code> subquery clause.</li> <li>Fixed bug preventing compiling some queries with no-groupby aggregations followed by an <code>ORDER BY</code> that referenced columns that wouldn\u2019t be present after the aggregation.</li> <li>Fixed an issue where certain strings would not be properly escaped in case statements.</li> <li>Fixed a false validation error issue that sometimes occurred with <code>IS DISTINCT FROM X</code>.</li> <li>Fixed an issue with variant and null type handling in UDFs.</li> </ul>"},{"location":"release_notes/2024.03/","title":"Bodo 2024.3 Release (Date: 03/12/2024)","text":""},{"location":"release_notes/2024.03/#new-features","title":"New Features:","text":"<ul> <li>Added parquet row count collection with TablePath API in BodoSQL to improve generated plans</li> <li>Introduced a FileSystemCatalog to enable treating a local file system or s3 bucket as a database.</li> <li>BodoSQL now supports streaming Parquet write with the FileSystemCatalog.</li> <li>Initial support for reading Iceberg tables that have gone through schema evolution. Full support will be added in the next release.</li> <li>Enable decorrelating subqueries that reference columns from multiple tables.</li> </ul>"},{"location":"release_notes/2024.03/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>Added an optimization to reduce memory usage and runtime of min row number filter aggregations.</li> <li>Improved simplification of certain expressions in the planner.</li> </ul>"},{"location":"release_notes/2024.03/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Reduced likelihood of Bodo exceeding timeouts while attempting to probe Snowflake for the true type of a semi-structured column on extremely large tables.</li> <li>Bodo now correctly writes Iceberg Field IDs as Parquet Field IDs in the generated Parquet files for Iceberg tables.</li> <li>Resolved a stack overflow issue with extremely complex SQL plans</li> </ul>"},{"location":"release_notes/2024.03/#dependency-upgrades","title":"Dependency Upgrades:","text":"<ul> <li>Upgraded to Python 3.12</li> <li>Upgraded to Numba 0.59</li> <li>Upgraded to Arrow 15</li> <li>Upgraded to Pandas 2.1</li> </ul>"},{"location":"release_notes/2024.03/#bodo-202431-release","title":"Bodo 2024.3.1 Release","text":""},{"location":"release_notes/2024.03/#new-features_1","title":"New Features:","text":"<ul> <li>Support the newer <code>scipy.fft</code> API over <code>scipy.fftpack</code></li> <li>Full support for reading Iceberg tables that have gone through schema evolution.</li> </ul>"},{"location":"release_notes/2024.03/#performance-improvements_1","title":"Performance Improvements:","text":"<ul> <li>Enabled better filter optimizations for Left and Right joins.</li> <li>Improved planner optimization on most <code>to_&lt;type&gt;</code> conversion functions to become equivalent to casts.</li> </ul>"},{"location":"release_notes/2024.04/","title":"Bodo 2024.4 Release (Date: 4/11/2024)","text":""},{"location":"release_notes/2024.04/#new-features","title":"New Features:","text":"<ul> <li>Added support for function <code>CURRENT_ACCOUNT</code>, which requires a Snowflake Catalog.</li> <li>Executing Snowflake Javascript UDFs in Bodo is now supported for all types except TIMESTAMP and Semi-Structured types.</li> <li>Added support for automatically generating lower_bound, upper_bound, value_counts, and null_value_counts for Iceberg data files written by Bodo.</li> <li>Improved support for handling skewed joins to avoid out of memory errors.</li> <li>Experimental support for TIMESTAMP_TZ</li> </ul>"},{"location":"release_notes/2024.04/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>Adds support for generating runtime filters on the probe side of joins based upon the values encountered on the build side. In many cases this will significantly improve performance.</li> </ul>"},{"location":"release_notes/2024.04/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Added support for additional types in Iceberg Filter pushdown</li> <li>Fixed a corner case when reading non-nullable VARIANT data from Snowflake</li> </ul>"},{"location":"release_notes/2024.04/#dependency-upgrades","title":"Dependency Upgrades:","text":"<ul> <li>Upgraded Numba to 0.59.1.</li> </ul>"},{"location":"release_notes/2024.04/#202441-new-features-and-improvements","title":"2024.4.1 New Features and Improvements","text":""},{"location":"release_notes/2024.04/#new-features_1","title":"New Features:","text":"<ul> <li>Support reading and writing Iceberg tables on Azure.</li> <li>Support for DROP TABLE on Snowflake and Iceberg.</li> <li>Minor update to add more filter pushdown information for logging level &gt;= 2</li> </ul>"},{"location":"release_notes/2024.04/#performance-improvements_1","title":"Performance Improvements:","text":"<ul> <li>Support pushing join runtime filters past the partition by columns for window functions.</li> </ul>"},{"location":"release_notes/2024.04/#bug-fixes_1","title":"Bug Fixes:","text":"<ul> <li>Fixed an issue using EC2 instance profile authentication in BodoSQL FileSystemCatalogs</li> <li>Fixed the expected column names for INSERT INTO queries</li> <li>Handling for extra whitespace in string inputs for TO_BOOLEAN, TO_NUMBER, and TO_DOUBLE</li> <li>Fixed a compilation error when join runtime filters occur after window functions with verbose mode enabled</li> <li>Fixed an issue in filter pushdown that could result in an runtime error</li> </ul>"},{"location":"release_notes/2024.05/","title":"Bodo 2024.5 Release (Date: 5/3/2024)","text":""},{"location":"release_notes/2024.05/#new-features","title":"New Features:","text":"<ul> <li>Added an environment variable <code>BODO_SQL_STYLE</code> to control some of the defaults for BodoSQL\u2019s SQL dialect. The default is <code>SNOWFLAKE</code> which uses the Snowflake protocols for identifier case sensitivity and null ordering defaults. Another option for the environment variable is <code>SPARK</code> which uses spark\u2019s defaults for identifier case sensitivity and null ordering defaults. The environment variable\u2019s value is not case-sensitive.</li> <li>Added support for groupby sum of decimal values</li> <li>Added support for writing puffin files with Iceberg writes by setting <code>BODO_ENABLE_THETA_SKETCHES=1</code>. A future release will enable this by default.</li> <li>Added support for casting between decimal values with different precision and scale</li> <li>Added support for multiplication of two decimal scalars/arrays.</li> <li>Added support for <code>CREATE SCHEMA</code> and <code>DROP SCHEMA</code> commands in Iceberg and Snowflake</li> </ul>"},{"location":"release_notes/2024.05/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>Slightly reduced the total number of metadata queries made to Snowflake during compilation time to determine when a string column should be dictionary encoded by removing unnecessary/redundant requests.</li> </ul>"},{"location":"release_notes/2024.05/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Fixed a bug that caused some Iceberg nullability filters to not be pushed down.</li> <li>Fixed a rarely occurring segfault when gathering a map array.</li> <li>Fixed an issue that caused an error while loading string columns from Snowflake managed Iceberg tables.</li> <li>Fixed some bugs in handling nested-types in joins.</li> </ul>"},{"location":"release_notes/2024.05/#dependency-upgrades","title":"Dependency Upgrades:","text":"<ul> <li>Upgraded to Iceberg 1.5.1</li> </ul>"},{"location":"release_notes/2024.05/#202451-new-features-and-improvements","title":"2024.5.1 New Features and Improvements","text":""},{"location":"release_notes/2024.05/#new-features_1","title":"New Features:","text":"<ul> <li>Support <code>MIN</code> and <code>MAX</code> on string columns when GROUP BY is not provided.</li> </ul>"},{"location":"release_notes/2024.05/#bug-fixes_1","title":"Bug Fixes:","text":"<ul> <li>Adjusted BodoSQL compile-time Snowflake metadata requests to avoid an error due to changing formats of Snowflake result sets for certain description queries.</li> </ul>"},{"location":"release_notes/2024.05/#202452-new-features-and-improvements","title":"2024.5.2 New Features and Improvements","text":""},{"location":"release_notes/2024.05/#new-features_2","title":"New Features:","text":"<ul> <li>Update BodoSQL plans to contain an explicit Cache Node to indicate when the planner is reusing part of a plan from a cached result.</li> <li>Simplified plans by concatenating join filters into a single node in BodoSQL plans.</li> <li>Updated BodoSQL plans to clarify when computation is being done by BodoSQL via updates to each plan operator\u2019s name.</li> <li>Enabled automatic creation or updating of Theta Sketches for columns with certain data types in BodoSQL when an Iceberg table is created with <code>CREATE TABLE AS SELECT</code> or updated with <code>INSERT INTO</code>. The Theta Sketches are written to a Puffin file during an Iceberg write. See the Bodo documentation on Puffin files for more details, including how to disable this feature.</li> <li>Support casting floats to decimals.</li> <li>Support multiplying integers and decimals.</li> <li>Users can now supply statistics for Parquet datasets when using the TablePath API. This can significantly improve the quality of the SQL plans in many cases.</li> <li>Support for <code>CREATE VIEW</code> with the Snowflake Catalog</li> </ul>"},{"location":"release_notes/2024.05/#performance-improvements_1","title":"Performance Improvements:","text":"<ul> <li>Support for merging aggregates in the planner to avoid unnecessary aggregations.</li> </ul>"},{"location":"release_notes/2024.05/#bug-fixes_2","title":"Bug Fixes:","text":"<ul> <li>Output of a <code>SUM</code> aggregation with a <code>GROUP BY</code> clause on integer columns is now up-casted to int64 to prevent overflows.</li> <li>Made loading from UDF/UDTF information from Snowflake more robust to handle future Snowflake changes to metadata query outputs.</li> <li>Fixed a gap where duplicate streaming joins wouldn\u2019t be cached.</li> </ul>"},{"location":"release_notes/2024.05/#202453202454-new-features-and-improvements","title":"2024.5.3/2024.5.4 New Features and Improvements","text":""},{"location":"release_notes/2024.05/#new-features_3","title":"New Features:","text":"<ul> <li>Added support for <code>CREATE [OR REPLACE] VIEW</code> for Snowflake catalogs and Iceberg catalogs that support views.</li> <li>Added support for reading Iceberg views from catalogs that support views.</li> <li>Changed the way that JavaScript UDFs are displayed in the emitted SQL plans so that it is possible to tell which JavaScript UDF is being called. Previously, all such UDFs would just display as a function call <code>SNOWFLAKE_NATIVE_UDF</code>, but now they display as <code>SNOWFLAKE_NATIVE_JAVASCRIPT_UDF::&lt;function_name&gt;</code>.</li> <li>Added support for join filters with TablePath and Local Tables</li> <li>Added Iceberg REST catalog support in pandas APIs</li> <li>Added BodoSQL TabularCatalog to connect to Tabular's Iceberg REST catalog</li> </ul>"},{"location":"release_notes/2024.05/#performance-improvements_2","title":"Performance Improvements:","text":"<ul> <li>Improved performance of decimal multiplication.</li> <li>Improved the propagation NDV estimates of the planner across filters with an <code>IS NOT NULL</code> condition.</li> <li>Bodo is now compiled with link-time optimizations by default, providing a ~5% performance boost.</li> </ul>"},{"location":"release_notes/2024.05/#bug-fixes_3","title":"Bug Fixes:","text":"<ul> <li>Fixed a bug where join filters pushed into a subsequent join wouldn\u2019t always use the bloom filter.</li> <li>Fixes a bug in ROUND which could lead to an incorrect result in case of overflows.</li> </ul>"},{"location":"release_notes/2024.06/","title":"Bodo 2024.6 Release (Date: 6/3/2024)","text":""},{"location":"release_notes/2024.06/#new-features","title":"New Features:","text":"<ul> <li>Added read support for Glue Catalogs in BodoSQL</li> <li> <p>Added support for more DDL commands for both Snowflake and Iceberg:</p> <ul> <li><code>DESCRIBE VIEW</code> (requires view support in catalog)</li> <li><code>DROP VIEW</code>  (requires view support in catalog)</li> <li><code>ALTER TABLE ... RENAME TO</code></li> <li><code>ALTER VIEW ... RENAME TO</code>  (requires view support in catalog)</li> <li><code>SHOW TERSE TABLES</code></li> <li><code>SHOW TERSE VIEWS</code>  (requires view support in catalog)</li> </ul> </li> <li> <p>Added support for pushing down join filters into cache nodes and uses join filters to determine whether or not to keep cache nodes.</p> </li> <li>Added support for casting strings to Decimal types</li> </ul>"},{"location":"release_notes/2024.06/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>Allowed limited support for streaming execution of window functions, particularly for <code>DENSE_RANK</code>.</li> <li>Allowed Join Filters to be pushed into Snowflake I/O calls, particularly when the build side of a join allows us to infer the minimum/maximum value of a join key.</li> </ul>"},{"location":"release_notes/2024.06/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Allowed Tabular Catalog to use testing environment</li> <li>Handled trailing slashes in Tabular Catalog's REST URI when created through the Bodo Platform</li> <li>Enabled further decorrelation with builtin table functions</li> </ul>"},{"location":"release_notes/2024.06/#dependency-upgrades","title":"Dependency Upgrades:","text":"<ul> <li>Upgraded to Arrow 16</li> <li>Upgraded to Iceberg 1.5.2</li> </ul>"},{"location":"release_notes/2024.06/#202461","title":"2024.6.1","text":""},{"location":"release_notes/2024.06/#new-features_1","title":"New Features:","text":"<ul> <li>Support min/max runtime join filters into Iceberg IO</li> <li><code>DATE_TRUNC</code> now accepts <code>TIMESTAMPTZ</code> input</li> <li>The <code>DIFF_*</code> family of functions now support <code>TIMESTAMPTZ</code> input</li> </ul>"},{"location":"release_notes/2024.06/#improvements","title":"Improvements:","text":"<ul> <li>Improves the memory manager to reduce the likelihood of out-of-memory errors in certain situations.</li> <li>Improved NDV estimates involving CASE expressions and IS NOT NULL filters.</li> <li>Runtime join filters are now applied in the order that's most likely to reduce intermediate costs.</li> </ul>"},{"location":"release_notes/2024.06/#bug-fixes_1","title":"Bug Fixes:","text":"<ul> <li>Fixed <code>ALTER TABLE RENAME TO</code> not being able to specify schemas or databases.</li> <li><code>TIMESTAMPTZ</code> can be compared with other <code>TIMESTAMP</code> types in all contexts</li> <li>Fixed calling GROUP BY with keys that mixed LIST columns with other types</li> <li>Automatically casts the decimal input to a floating point type when computing DECIMAL * FLOAT</li> </ul>"},{"location":"release_notes/2024.07/","title":"Bodo 2024.7 Release (Date: 7/9/2024)","text":""},{"location":"release_notes/2024.07/#new-features","title":"New Features:","text":"<ul> <li>Added support for adding Decimal values.</li> <li>Added support for binary arithmetic operations between floats and Decimals.</li> <li>Added support for enabling and disabling theta sketches on specific columns by setting the table property <code>bodo.write.theta_sketch_enabled.&lt;column_name&gt;</code>.</li> <li> <p>Added support for new DDL commands:</p> <ul> <li><code>ALTER TABLE RENAME COLUMN</code></li> <li><code>ALTER TABLE ALTER COLUMN COMMENT</code></li> <li><code>ALTER TABLE ALTER COLUMN DROP NOT NULL</code></li> <li><code>DESCRIBE SCHEMA</code></li> <li><code>SHOW TBLPROPERTIES</code></li> <li><code>SHOW TABLES/VIEWS/SCHEMAS/OBJECTS</code> without <code>TERSE</code></li> </ul> </li> <li> <p>(EXPERIMENTAL FEATURE) Added support for hints for indicating whether a join should be a broadcast join or for which side of a join should be the build side versus the probe side.</p> </li> <li>(EXPERIMENTAL FEATURE) Added support for increased caching based on covering expressions.</li> </ul>"},{"location":"release_notes/2024.07/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>Expanded support for min/max I/O join filters to strings dates, floats, and timezone-naive timestamps.</li> <li>Added limited datatype support for pushing low-cardinality join filters into I/O when the build side of a join has a small number of unique keys but where min/max filters are not very useful.</li> <li>Added support for passing bitmasks in/out of runtime join filters to reduce the amount of redundant copying and improve performance when multiple join filters are applied in a row.</li> <li>Optimized decimal scalar/array multiplication.</li> <li>Added a sort based implementation for dense_rank.</li> <li>Improved cost based decisions for filters and projections.</li> </ul>"},{"location":"release_notes/2024.07/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Fixed a bug occurring with join filters being pushed into Iceberg I/O on certain datatypes.</li> <li>Fixed a bug so <code>drop view / table if exists</code> no longer throws exceptions when the table/schema does not exist.</li> </ul>"},{"location":"release_notes/2024.08/","title":"Bodo 2024.8 Release (Date: 8/6/2024)","text":""},{"location":"release_notes/2024.08/#new-features","title":"New Features:","text":"<ul> <li>Expanded decimal support:<ul> <li>Added native decimal support for addition/subtraction.</li> <li>Added native decimal support for <code>MEDIAN</code> with <code>GROUP BY</code>.</li> <li>Added native decimal support for the following numeric functions: <code>ATAN</code>, <code>ATAN2</code>, <code>ATANH</code>, <code>COS</code>, <code>COSH</code>, <code>COT</code>, <code>DEGREES</code>, <code>RADIANS</code>, <code>SIN</code>, <code>SINH</code>, <code>TAN</code>, <code>TANH</code>, <code>EXP</code>, <code>POWER</code>, <code>LOG</code>, <code>LN</code>, <code>SQRT</code>, <code>SQUARE</code>, <code>ROUND</code>, <code>ABS</code>, <code>DIV0</code>, <code>SIGN</code>.</li> <li>Support for binary operations between decimal and integer.</li> </ul> </li> <li>Extended use of low-ndv IN join filters to Iceberg.</li> <li>Generate Join Filters with empty build tables and Snowflake IO.</li> <li>Support for Join Filters with Interval Joins.</li> <li>Streaming support for several more aggregate patterns, especially with <code>PIVOT</code>.</li> <li>Streaming support for grouping sets without the empty set.</li> <li>Support Create Table As Select (CTAS) with <code>TABLE COMMENTS</code>, <code>COLUMN COMMENTS</code>, and <code>TBLPROPERTIES</code> for Iceberg.</li> </ul>"},{"location":"release_notes/2024.08/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>Revamp shuffle in SQL join and groupby to use non-blocking MPI which can improve performance significantly.</li> <li>Improved streaming performance for more rank window functions (<code>RANK</code>, <code>PERCENT_RANK</code>, <code>CUME_DIST</code>, <code>ROW_NUMBER</code>).</li> <li>Added streaming support for <code>MAX(X) OVER ()</code>, <code>MIN(X) OVER ()</code>, <code>SUM(X) OVER ()</code>, and <code>COUNT(X) OVER ()</code>.</li> <li>Expanded sort for the streaming sort based implementation of window functions to all array types.</li> <li>Started decomposing <code>AVG</code>, var/std functions, covar functions, and <code>CORR</code> into combinations of sum/count to both allow for decimal support, and also allow for increased re-use of computations when multiple such functions are present.</li> <li>Significant improvements to Iceberg Scan Planning performance such as fetching the parquet metadata in parallel.</li> <li>Improved shuffle performance for variable length types (string/array).</li> <li>Improved BodoSQL\u2019s ability to prune empty plan sections.</li> <li>Improved performance of decimal to string and decimal to double casting.</li> </ul>"},{"location":"release_notes/2024.08/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Fixed bug related to string columns in streaming join sometimes causing segmentation faults.</li> <li>Fixed bug when calling <code>DATEADD</code> (and similar functions) with month/quarter/year units when the input date was a leapday.</li> <li>Fixed bug in verbose mode that would cause incorrect timer information to be displayed for non-streaming rel nodes.</li> <li>Fixed bug writing where writing to an iceberg table with a non-iceberg type (<code>int8</code>/<code>uint8</code>) caused an error instead of an upcast.</li> <li>Fixed a bug that would cause <code>SHOW TABLES</code> commands to error when the tables had a large number of rows/bytes.</li> <li>Fixed a bug in comparisons for elements of nested arrays with dictionary-encoded data.</li> </ul>"},{"location":"release_notes/2024.08/#dependency-upgrades","title":"Dependency Upgrades:","text":"<ul> <li>Upgraded to Numba 0.60.</li> </ul>"},{"location":"release_notes/2024.08/#202481-new-features-and-improvements","title":"2024.8.1 New Features and Improvements","text":""},{"location":"release_notes/2024.08/#new-features_1","title":"New Features:","text":"<ul> <li>Added support for storage account name and storage account key and VM-identities when using Azure.</li> <li>Added support for <code>SUM</code> on decimals when invoked as a window function or a regular aggregation without groupby.</li> <li>Added support for <code>VAR_POP</code>, <code>VAR_SAMP</code>, <code>STDDEV_POP</code>, <code>STDDEV_SAMP</code>, <code>COVAR_POP</code>, <code>COVAR_SAMP</code>, <code>CORR</code>, <code>PERCENTILE_DISC</code> and <code>PERCENTILE_CONT</code> on decimals.</li> <li>Added support for <code>FACTORIAL</code>, <code>CEIL</code>, <code>FLOOR</code>, <code>ROUND</code>, and <code>TRUNC</code> on decimals.</li> <li>Added support for <code>DIV0NULL</code>.</li> </ul>"},{"location":"release_notes/2024.08/#performance-improvements_1","title":"Performance Improvements:","text":"<ul> <li>SQL Planner improvements to better handle use of NDV values and nullability of functions.</li> <li>Ensured BodoSQL now decomposes <code>VAR_POP</code>, <code>VAR_SAMP</code>, <code>STDDEV_POP</code>, <code>STDDEV_SAMP</code>, <code>COVAR_POP</code>, <code>COVAR_SAMP</code> and <code>CORR</code> into combinations of <code>SUM</code> and <code>COUNT</code> to improve re-use of computations across multiple functions.</li> <li>Added streaming support for <code>AVG(X) OVER ()</code> and <code>COUNT(*) OVER ()</code>.</li> <li>Added streaming support for <code>SUM</code> as a window function when there are partitions but no frames.</li> <li>Improved performance of reading string columns for Iceberg tables. This is only available on the Bodo Cloud Platform at this stage.</li> <li>Improved robustness when pushing down very large IN queries with Iceberg.</li> <li>Enabled pruning an entire Iceberg table via a JoinFilter with an empty build table.</li> <li>Enabled rewriting <code>COUNT(DISTINCT)</code> to use an internal optimization via grouping sets that allows more equal work distribution across cores.</li> <li>Improve performance of reading small Iceberg tables.</li> </ul>"},{"location":"release_notes/2024.08/#bug-fixes_1","title":"Bug Fixes:","text":"<ul> <li>Fixed bug enabling streaming for <code>CUME_DIST</code> and <code>PERCENT_RANK</code> when there are no partition columns.</li> <li>Arrays of tuple values are now returned to regular Python properly as arrays of tuples instead of arrays of structs.</li> <li>Fixed bugs that prevented compiling complex nested UDF inlining cases.</li> </ul>"},{"location":"release_notes/2024.08/#dependency-upgrades_1","title":"Dependency Upgrades:","text":"<ul> <li>Upgraded to Calcite 1.37.</li> <li>Upgraded to Arrow 17.</li> </ul>"},{"location":"release_notes/2024.09/","title":"Bodo 2024.9 Release (Date: 9/25/2024)","text":""},{"location":"release_notes/2024.09/#new-features","title":"New Features:","text":"<ul> <li>Added support for <code>pd.Series.argmin</code>, <code>pd.Series.argmax</code>,  <code>pd.Series.str.removeprefix</code>, <code>pd.Series.str.removesuffix</code>, <code>pd.Series.str.casefold</code> and <code>Series.str.fullmatch</code>.</li> <li>Added support for <code>pd.Series.str.partition</code> with expand=True.</li> <li>Added support for support <code>HAVERSINE</code> with Decimal input data type.</li> <li>Changed Bodo logger defaults to stdout instead of stderr.</li> </ul>"},{"location":"release_notes/2024.09/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>Changed Iceberg write to use Arrow azurefs instead of hadoop.</li> <li>Changed to use Iceberg metadata instead of Parquet metadata for file scan planning to speed up Iceberg reads overall.</li> <li>Added ability to fetch metadata for Snowflake-managed Iceberg tables at the beginning of query execution and in-parallel for faster Iceberg file scan planning.</li> <li>Added streaming support for the window functions <code>COUNT(X)</code>, <code>COUNT_IF</code>, <code>BOOLAND_AGG</code>, <code>BOOLOR_AGG</code>, <code>BITAND_AGG</code>, <code>BITOR_AGG</code> and <code>BITXOR_AGG</code>.</li> <li>Added streaming support for the window functions <code>LEAD</code>, <code>LAG</code> and <code>NTILE</code> when a <code>PARTITION BY</code> clause is provided.</li> <li>Added streaming support for the window functions <code>FIRST_VALUE</code>, <code>LAST_VALUE</code>, <code>ANY_VALUE</code>, <code>MIN</code>, and <code>MAX</code> on numeric data.</li> <li>Ensured BodoSQL decomposes the window functions <code>PERCENT_RANK</code>, <code>CUME_DIST</code> and <code>RATIO_TO_REPORT</code> into other window functions that can be computed together with streaming.</li> <li>Enabled computation of multiple window functions at once while streaming.</li> <li>Enabled window functions computed with an <code>OVER ()</code> window in streaming to spill data to disk, reducing peak memory utilization.</li> <li>Improved the quality of BodoSQL planner to reduce redundant computation.</li> <li>Added various optimizations for the streaming sort operator.</li> <li>Made the BodoSQL planner more aggressive with eliminating common subexpressions that are not top-level expressions.</li> </ul>"},{"location":"release_notes/2024.09/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Improved the amount of possible query decorrelation in BodoSQL.</li> <li>Fixed a bug in Snowflake-managed Iceberg table writer where the catalog integration creation could fail in the presence of another concurrent writer.</li> <li>Fixed various bugs in the streaming sort operator.</li> <li>Fixed behavior of <code>pd.Series.str.split</code> when <code>n&gt;=1</code> but the delimiter is not provided.</li> <li>Improved stability when reading from CSV files.</li> </ul>"},{"location":"release_notes/2024.09/#dependency-upgrades","title":"Dependency Upgrades:","text":"<ul> <li>Upgraded to Pandas 2.2.</li> </ul>"},{"location":"release_notes/2024.10/","title":"Bodo 2024.10 Release (Date: 10/11/2024)","text":""},{"location":"release_notes/2024.10/#new-features","title":"New Features:","text":"<ul> <li>Bodo is now available on PyPi</li> </ul>"},{"location":"release_notes/2024.10/#performance-improvements","title":"Performance Improvements:","text":"<ul> <li>Improved performance of external sort in certain cases. </li> <li>Improved compilation time of function using BodoSQL by 10-20% in some situations. </li> </ul>"},{"location":"release_notes/2024.10/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Removed reference to opcodes only present in python 3.11+ when running in python 3.10.</li> </ul>"},{"location":"release_notes/2024.10/#dependency-upgrades","title":"Dependency Upgrades:","text":"<ul> <li>Included mpi4py as bodo.mpi4py instead of a dependency.</li> </ul>"},{"location":"release_notes/2024.11/","title":"Bodo 2024.11 Release (Date: 11/27/2024)","text":""},{"location":"release_notes/2024.11/#new-features","title":"New Features:","text":"<ul> <li><code>bodo.jit</code> will now use \u201cspawn\u201d mode by default, which allows Bodo JIT functions to be used in regular Python scripts without <code>mpiexec</code>. Bodo spawns worker MPI processes the first time a JIT function is called. For each JIT function, Bodo sends arguments, runs the function and returns the output to regular Python. Pandas DataFrames and Series have a lazy wrapper that collects the data only if necessary.</li> <li>Add support for <code>Series.str.encode</code></li> <li>Several array types are boxed in Arrow format now</li> </ul>"},{"location":"release_notes/2024.11/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Various bug fixes in the compiler</li> </ul>"},{"location":"release_notes/2024.12/","title":"Bodo 2024.12 Release (Date: 12/06/2024)","text":"<p>=====================================</p> <p>We're excited to announce that Bodo is now open source with no restrictions!</p> <p>Explore, contribute, and collaborate by accessing the codebase on Github.</p>"},{"location":"release_notes/2024.12/#2024121","title":"2024.12.1","text":""},{"location":"release_notes/2024.12/#new-features","title":"New Features","text":"<ul> <li>Revamped and simplified examples</li> <li>Added NYC Taxi benchmark code for Bodo, Spark, Dask and Ray</li> </ul>"},{"location":"release_notes/2024.12/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix caching for IPython and Jupyter cells</li> <li>Fix BodoSQL use outside JIT in Jupyter</li> <li>Dependency Changes</li> <li>Upgrade to Arrow 18.1</li> </ul>"},{"location":"release_notes/2024.12/#docs","title":"Docs","text":"<ul> <li>Added README to bodo-azurefs-sas-token-provider</li> </ul>"},{"location":"release_notes/2024.12/#2024122","title":"2024.12.2","text":""},{"location":"release_notes/2024.12/#new-features_1","title":"New Features","text":"<ul> <li>Handle slicing with negative start/step in lazy DataFrame/Series wrappers</li> </ul>"},{"location":"release_notes/2024.12/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Fix DataFrame.to_json handling of \u201clines\u201d and \u201corient\u201d arguments</li> <li>Fix errors during exit related to logging of lazy data structures</li> </ul>"},{"location":"release_notes/2024.12/#2024123","title":"2024.12.3","text":""},{"location":"release_notes/2024.12/#new-features_2","title":"New Features","text":"<ul> <li>Adds a <code>@bodo.wrap_python</code> decorator for calling regular Python functions from JIT code. It is both easier to use and also much faster than objmode (doesn't require compilation during runtime). This interface fits the common UDF case for calling regular Python especially well.</li> </ul>"},{"location":"release_notes/2024.12/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>Fixed an issue with objmode functionality used in nested UDF functions</li> <li>Fixed an issue with referencing global variables in objmode while using spawn mode <code>scatterv</code>/<code>gatherv</code> now supports scattering/gathering larger data sizes</li> </ul>"},{"location":"release_notes/2025.01/","title":"Bodo 2025.1 Release (Date: 01/13/2025)","text":"<p>=====================================</p>"},{"location":"release_notes/2025.01/#new-features","title":"New Features","text":"<ul> <li>Bodo now supports S3 Tables (Iceberg) in both Python and SQL.</li> <li>Bodo wheels now support manylinux 2_28.</li> <li>Conda Package and Pip Wheels Support Linux on ARM.</li> <li>Added several examples including parallel LLM inference and bioinformatics.</li> </ul>"},{"location":"release_notes/2025.01/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Updated typing for streaming state types to use Numba's type refinement and avoid repeated IR transformation. This makes BodoSQL compilation ~15% faster.</li> </ul>"},{"location":"release_notes/2025.01/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Buffer Pool will now catch potential out-of-memory calls before triggering OS terminations.</li> </ul>"},{"location":"release_notes/2025.02/","title":"Bodo 2025.2 Release (Date: 02/13/2025)","text":"<p>=====================================</p>"},{"location":"release_notes/2025.02/#highlights","title":"\ud83c\udf89 Highlights","text":"<p>We've started to revamp our CSV and JSON reader and writer to work more inline with our Parquet I/O. As part of this release, we standardized multiple smaller features and filesystem support. We\u2019ve just begun, so look forward to more changes!</p> <p>In addition, we also started working on improving our compilation time by first creating a global cache for internal functions. This can provide dramatic speedups on subsequent uses of Bodo.</p>"},{"location":"release_notes/2025.02/#new-features","title":"\u2728 New Features","text":"<ul> <li>Support reading CSV/JSON/Parquet data from HuggingFace</li> <li>Support Google Cloud Storage (GCS) for CSV/JSON/Parquet I/O</li> <li>Support glob format in CSV read</li> <li>Support zstd compression in CSV/JSON I/O</li> <li>Improved reliability of spawn destructors for lazy data structures</li> </ul>"},{"location":"release_notes/2025.02/#performance-improvements","title":"\ud83c\udfce\ufe0f Performance Improvements","text":"<ul> <li>Improved compilation time of dataframe unboxing</li> <li>Improved compilation time by caching functions internal to Bodo across program runs</li> </ul>"},{"location":"release_notes/2025.02/#bug-fixes","title":"\ud83d\udc1e Bug Fixes","text":"<ul> <li>Improved the error message when Bodo detects an OOM to provide potential solutions and paths forward</li> <li>Fixed a bug in parallel read of JSON lines files</li> <li>Fix the Pandas warning that appears when using to_csv or to_json</li> </ul>"},{"location":"release_notes/2025.02/#dependency-upgrades","title":"\u2699\ufe0f Dependency Upgrades","text":"<ul> <li>Upgraded Calcite to 1.38</li> <li>Upgraded Numba to 0.61</li> </ul>"},{"location":"release_notes/2025.02/#202521","title":"2025.2.1","text":""},{"location":"release_notes/2025.02/#highlights_1","title":"\ud83c\udf89 Highlights","text":"<p>This release focused on the introduction of our PyIceberg backend for Iceberg IO. Rather than using the Bodo Iceberg Connector, Bodo now uses the PyIceberg library to work with Iceberg tables. See our blog post for more details.</p>"},{"location":"release_notes/2025.02/#new-features_1","title":"\u2728 New Features","text":"<ul> <li>Iceberg IO now uses PyIceberg instead of our custom Iceberg connector behind-the-scenes for working with Iceberg tables.</li> </ul>"},{"location":"release_notes/2025.02/#behavior-changes","title":"\ud83d\udd04 Behavior Changes","text":"<ul> <li>Snowflake write will use the PUT method instead of direct upload which is less performant. This is in preparation for upgrading to Arrow 19 which will enable writing to Azure backed storage without Hadoop and should be ready soon.</li> </ul>"},{"location":"release_notes/2025.03/","title":"Bodo 2025.3 Release (Date: 03/06/2025)","text":"<p>=====================================</p>"},{"location":"release_notes/2025.03/#highlights","title":"\ud83c\udf89 Highlights","text":"<p>In this release, we're excited to add Windows support to Bodo. We also continue our focus on Iceberg and enhance our support for Iceberg Rest Catalogs.</p>"},{"location":"release_notes/2025.03/#new-features","title":"\u2728 New Features","text":"<ul> <li>Bodo is now available on Windows on pip and Conda.</li> <li>BodoSQL's TabularCatalog has become IcebergRestCatalog.</li> <li>Test suites for Iceberg Rest Catalog using Polaris.</li> </ul>"},{"location":"release_notes/2025.03/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fixed a hang when using spawn mode from an interactive Python session.</li> <li>Fixed an incorrect output array type when rewriting free variables to arguments in UDFs</li> </ul>"},{"location":"release_notes/2025.03/#202531","title":"2025.3.1","text":""},{"location":"release_notes/2025.03/#highlights_1","title":"\ud83c\udf89 Highlights","text":"<p>In this release, we introduce the <code>map_parititions</code> API, extend our Parquet I/O to support Multi-Index, and upgrade to the latest Arrow and Pyiceberg!</p>"},{"location":"release_notes/2025.03/#new-features_1","title":"\u2728 New Features","text":"<ul> <li>Added <code>BodoDataFrame.map_partitions</code> API, which applies a function in parallel over partitions of the dataframe.</li> <li>Extend our Parquet read and write to support Multi-Index for Pandas DataFrames.</li> </ul>"},{"location":"release_notes/2025.03/#dependency-upgrades","title":"\u2699\ufe0f Dependency Upgrades","text":"<ul> <li>Upgraded to Pyiceberg 0.9.</li> <li>Upgraded to Arrow 19.</li> <li>Removed bodo-azure-sas-token-provider, Bodo uses Arrow for Azure IO.</li> </ul>"},{"location":"release_notes/2025.03/#202532","title":"2025.3.2","text":"<p>Minor bugfix release. - Fixed bug in to_parquet when running from cache. - Fixed outputs for jupyter notebooks with spawn mode when the kernel is run separately from the notebook server.</p>"},{"location":"release_notes/2025.03/#202532_1","title":"2025.3.2","text":""},{"location":"release_notes/2025.03/#new-features_2","title":"\u2728 New Features","text":"<ul> <li>Experimental support for iceberg filesystem catalog support on GCS</li> </ul>"},{"location":"release_notes/2025.03/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Replace deprecated get_field_by_name with field</li> <li>Fix Snowflake write to ADLS</li> <li>Fix parquet write to ADLS</li> </ul>"},{"location":"release_notes/2025.04/","title":"Bodo 2025.4 Release (Date: 04/07/2025)","text":""},{"location":"release_notes/2025.04/#highlights","title":"\ud83c\udf89 Highlights","text":"<p>In this release, we're excited to support reading and writing GCS with FileSystemCatalog in Bodo and upgrading MPI4Py to version 4.</p>"},{"location":"release_notes/2025.04/#new-features","title":"\u2728 New Features","text":"<ul> <li>Support reading/writing to GCS with FileSystemCatalog.</li> <li>Add API <code>get_gpu_ranks</code> to return list of ranks pinned to GPU.</li> </ul>"},{"location":"release_notes/2025.04/#dependency-upgrades","title":"\u2699\ufe0f Dependency Upgrades","text":"<ul> <li>Upgrade to MPI4Py 4.</li> </ul>"},{"location":"release_notes/2025.04/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix caching in BodoSQLContext.sql</li> <li>Fix <code>BodoDataFrame.to_csv()/to_json()</code> string output in spawn mode.</li> <li>Fix error in writing parquet files with Snowflake.</li> </ul>"},{"location":"release_notes/2025.04/#202541","title":"2025.4.1","text":"<p>Mini release adding experimental support for Iceberg time travel in Python reader</p>"},{"location":"release_notes/2025.04/#202542","title":"2025.4.2","text":""},{"location":"release_notes/2025.04/#highlights_1","title":"\ud83c\udf89 Highlights","text":"<p>In this release, we are exited to add hooks for Bodo to accelerate User Defined Functions (UDFs) in Pandas. We also fixed several bugs related to our Pandas support in the compiler.</p>"},{"location":"release_notes/2025.04/#new-features_1","title":"\u2728 New Features","text":"<ul> <li>Added hooks for accelerating Pandas UDFs with Bodo. Starting in Pandas 3.0, you will be able to use Bodo to automatically jit compile your UDFs and execute them over a DataFrame in parallel by passing <code>engine=bodo.jit</code> to <code>DataFrame.apply()</code>.</li> </ul>"},{"location":"release_notes/2025.04/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Use Arrow for boxing/unboxing Datetime arrays.</li> <li>Support passing an empty tuple as an argument to a function in Spawn-mode.</li> <li>Support passing non-constant UDF <code>args</code> in <code>DataFrame.apply()</code>.</li> <li>Support handling Pandas classes imported directly in the compiler.</li> </ul>"},{"location":"release_notes/2025.05/","title":"Bodo 2025.5 Release (Date: 05/16/2025)","text":""},{"location":"release_notes/2025.05/#highlights","title":"\ud83c\udf89 Highlights","text":"<p>In this release, we are excited to introduce the first experimental version of the Bodo DataFrame library which is a drop-in replacement for Pandas. The current release has an initial feature set including parquet read, filtering, projection, UDFs, lazy evaluation, query plan optimization, and streaming parallel execution (both local and clusters). See our blog \u201cRethinking DataFrames\u201d for more details and refer to our docs page for supported features and examples.</p>"},{"location":"release_notes/2025.05/#new-features","title":"\u2728 New Features","text":"<ul> <li>Added support for <code>read_parquet</code> for reading parquet files into Bodo DataFrames and <code>from_pandas</code> for converting Pandas  DataFrames into Bodo DataFrames.</li> <li>Added support for <code>Series.map</code> and <code>DataFrame.apply</code> as well as Series string methods <code>Series.str.lower</code> and <code>Series.str.strip</code></li> <li>Added support for DataFrame and Series <code>head</code> and limit pushdown.</li> <li>Added support for column projections e.g. <code>df[\u201cA\u201d]</code> and projection pushdown</li> <li>Added support filtering e.g. <code>df[df.A &lt; 10]</code> and filter pushdown.</li> <li>Added support for setting columns in a DataFrame.</li> <li>Added graceful fallbacks to Pandas for cases the DataFrame library does not support yet.</li> <li>Added support for optimizing query plans using DuckDB\u2019s optimizer.</li> <li>Added support for streaming parallel execution to allow seamless scaling and prevent out of memory issues.</li> </ul>"},{"location":"release_notes/2025.05/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Improve error messages for Iceberg write schema validation.</li> <li>Fix bugs related to distribution of arguments in Spawn mode.</li> </ul>"},{"location":"release_notes/2025.05/#dependency-upgrades","title":"\u2699\ufe0f Dependency Upgrades","text":"<ul> <li>Added DuckDB as a vendored dependency for query optimization. We have removed some of the code we don\u2019t need and plan to remove more going forward.</li> <li>Upgraded Numba version to 0.61.2</li> </ul>"},{"location":"release_notes/2025.06/","title":"Bodo 2025.6 Release (Date: 06/06/2025)","text":""},{"location":"release_notes/2025.06/#highlights","title":"\ud83c\udf89 Highlights","text":"<p>In this release, we are excited to announce support for reading Iceberg tables, merge, sort, as well as 83 methods across Series, Series.str, and Series.dt in the DataFrame Library. Refer to our documentation for a complete list of features now supported.</p>"},{"location":"release_notes/2025.06/#new-features","title":"\u2728 New Features","text":"<ul> <li>Added support for <code>pd.read_iceberg</code> (from Pandas 3.0 APIs). The implementation includes optimizations such as limit, projection, and filter pushdown as well as features such as time travel.</li> <li>Added initial support for <code>merge()</code> for the <code>how=\u201dinner\u201d</code> case.</li> <li>Added initial support for <code>sort_values()</code>.</li> <li>Added 46 Series.str methods such as <code>str.find()</code> and <code>str.partition()</code>.</li> <li>Added 30 Series.dt accessors and methods such as <code>dt.dayofweek</code> and <code>dt.is_month_start</code>.</li> <li>Added 7 Series methods including <code>Series.isin</code> and <code>Series.clip</code>.</li> <li>Improved support for arbitrarily complex DataFrame filter expressions and additional date/time types.</li> <li>Added same arbitrarily complex Series expression support.</li> <li>Added basic infrastructure for groupby, full support will be included in the next release.</li> <li>Improved execution time of DataFrame and Series <code>head()</code> calls.</li> <li>Improved performance of many computations substantially.</li> </ul>"},{"location":"release_notes/2025.06/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Improved handling of null columns in DataFrame Library <code>read_parquet()</code>.</li> <li>Various bug fixes for <code>DataFrame.apply()</code> and <code>Series.map()</code>.</li> <li>Fixed bug related to copying Series with Indexes.</li> </ul>"},{"location":"release_notes/2025.07/","title":"Bodo 2025.7 Release (Date: 06/27/2025)","text":""},{"location":"release_notes/2025.07/#highlights","title":"\ud83c\udf89 Highlights","text":"<p>In this release, we are excited to announce support for writing Iceberg tables and Parquet files, DataFrame GroupBy operations, and numerous other features. Refer to our documentation for a complete list of features now supported.</p>"},{"location":"release_notes/2025.07/#new-features","title":"\u2728 New Features","text":"<ul> <li>Added Iceberg write support using <code>DataFrame.to_iceberg()</code>. Features include simple filesystem writes, partition spec, and sort order.</li> <li>Support writing Parquet files using <code>DataFrame.to_parquet()</code>.</li> <li>Added support for simple filesystem reads in <code>read_iceberg()</code>.</li> <li>Support for <code>DataFrame.groupby()</code> with aggregate functions including sum, count, and max.</li> <li>Support for DataFrameGroupBy and SeriesGroupBy <code>aggregate()</code>/<code>agg()</code>.</li> <li>Added 8 Series.str methods including <code>str.extract()</code> and <code>str.split()</code>, achieving Series.str method coverage of 96% (54 out of 56).</li> <li>Added 5 Series reduction methods including <code>Series.max()</code> and <code>Series.sum()</code>.</li> <li>Support for <code>pd.to_datetime()</code> and timedelta types/methods.</li> <li>Added top-level null check methods such as <code>pd.isnull()</code>.</li> <li>Support for Series <code>sort_values()</code>.</li> <li>Optimized support for <code>sort_values()</code> followed by <code>head()</code>.</li> <li>Support for DataFrame column renaming.</li> <li>Support for arithmetic expression on DataFrames, e.g., <code>df[\u201cnew_col\u201d] = df[\u201cA\u201d] + df[\u201cB\u201d]</code>.</li> <li>Support for bodo.pandas.DataFrame/Series constructors.</li> <li>Support for filtering expressions on Series, e.g., <code>s[s &gt; 10]</code>.</li> </ul>"},{"location":"release_notes/2025.07/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Added fallback warnings for unsupported Series methods.</li> <li>Improved DataFrame and Series expression support in filters and column assignments.</li> </ul>"},{"location":"release_notes/2025.07/#dependency-upgrades","title":"\u2699\ufe0f Dependency Upgrades","text":"<ul> <li>Bodo now supports Python 3.13.</li> <li>Removed many dependency version constraints.</li> </ul>"},{"location":"release_notes/2025.07/#202571","title":"2025.7.1","text":"<p>Minor release to fix a bug when reading Parquet files from S3.</p>"},{"location":"release_notes/2025.07/#202572","title":"2025.7.2","text":""},{"location":"release_notes/2025.07/#highlights_1","title":"\ud83c\udf89 Highlights","text":"<p>Bodo is now available on conda-forge for all supported platforms.</p>"},{"location":"release_notes/2025.07/#new-features_1","title":"\u2728 New Features","text":"<ul> <li>Bodo.pandas.read_iceberg_table for reading PyIceberg tables.</li> <li>Added 8 Series.dt methods/accessors including dt.isocalendar().</li> <li>Improve the performance of DataFrame.apply calls significantly in some cases.</li> <li>Support na_action and kws in Series.map() JIT support.</li> </ul>"},{"location":"release_notes/2025.07/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fixed a parallelism bug for distributing in-memory dataframes.</li> <li>Fixed issues with repeated sub-plans in join sides.</li> <li>Fixed memory leaks when distributing Pandas dataframes for plan execution.</li> </ul>"},{"location":"release_notes/2025.07/#dependency-upgrades_1","title":"\u2699\ufe0f Dependency Upgrades","text":"<ul> <li>Upgraded zstd, boost, aws-sdk-cpp and pyarrow dependencies.</li> </ul>"},{"location":"release_notes/2025.07/#202573","title":"2025.7.3","text":"<p>Minor release to re-enable Python 3.9 support.</p>"},{"location":"release_notes/2025.07/#202574","title":"2025.7.4","text":""},{"location":"release_notes/2025.07/#new-features_2","title":"\u2728 New Features","text":"<ul> <li>Updated quickstart Iceberg docs to use the dataframe library in examples.</li> <li>Support series.mean in dataframe library.</li> <li>Support s3a paths in Iceberg IO.</li> </ul>"},{"location":"release_notes/2025.07/#dependency-upgrades_2","title":"\u2699\ufe0f Dependency Upgrades","text":"<ul> <li>Removed maximum Python version restriction.</li> </ul>"},{"location":"release_notes/2025.07/#202575","title":"2025.7.5","text":""},{"location":"release_notes/2025.07/#new-features_3","title":"\u2728 New Features","text":"<ul> <li>Added support for Series methods including describe, agg, etc.</li> <li>Support setting output of Series.str.cat back into the dataframe as a new column</li> <li>Support filters on top of joins in plans that turn into non-equi joins</li> <li>Suppressed excessive fallback warnings</li> <li>Enabled column selection with Groupby getattr</li> </ul>"},{"location":"release_notes/2025.07/#performance-improvements","title":"\ud83c\udfce\ufe0f Performance Improvements","text":"<ul> <li>Added cache flag to read_csv inside DataFrame Library.</li> <li>Improved performance of argument initialization inside of lazy plans.</li> <li>Adjusted Iceberg and Parquet reader parameters to use less memory.</li> </ul>"},{"location":"release_notes/2025.07/#bug-fixes_2","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fixed merge output columns not created properly in some cases</li> <li>Fixed parquet read issue with filter columns not used anywhere else in the code</li> <li>Fixed Parquet read when Index metadata is missing</li> </ul>"},{"location":"release_notes/2025.08/","title":"Bodo 2025.8.1 Release (Date: 08/07/2025)","text":""},{"location":"release_notes/2025.08/#highlights","title":"\ud83c\udf89 Highlights","text":"<p>We're excited to add several APIs to enhance the ease of integrating Bodo DataFrames with your AI workflows. This includes support for embedding text using LLMs, using LLMs for text generation, and storing and retrieving embeddings in/from S3 Vectors.</p>"},{"location":"release_notes/2025.08/#new-features","title":"\u2728 New Features","text":"<ul> <li>BodoSeries.ai.llm_generate has been added to pass each element of a series to an OpenAI compatible generation endpoint.</li> <li>BodoSeries.ai.embed has been added to pass each element of a series to an OpenAI compatible embedding endpoint.</li> <li>BodoSeries.ai.tokenize has been added to tokenize a series of strings to a series of a list of tokens.</li> <li>spawn_process_on_workers has been added to create a process on each worker node to allow managing external processes that your program needs to interact with, such as a local LLM inference server.</li> <li>Support for BodoSeries.quantile via streaming approximate quantile</li> <li>Full support for BodoSeries.describe</li> <li>Added Series.map_with_state that takes an initialization routine run once per worker whose output state is passed to the mapping function along with each individual row.</li> <li>Support for BodoDataFrame/BodoSeries.reset_index</li> <li>Support for BodoSeries binary operations with DateOffsets, strings, etc.</li> <li>Series.map and DataFrame.apply by default will attempt to JIT compile user provided functions to improve performance. If JIT is not possible then the mapping function will be run as a normal Python function.</li> <li>Improved setting dataframe columns to handle more cases</li> <li>Improved filter expression handling</li> <li>Support left/right/outer/cross joins</li> <li>Support Series.isin(Series) use case (both Series parallel) and filtering dataframe with its output</li> <li>Support writing to S3 Vectors</li> <li>Support querying S3 Vectors</li> <li>Support dataframe dropduplicates.</li> </ul>"},{"location":"release_notes/2025.08/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Suppressed excessive subsequent fallback warnings</li> <li>Handle pd.NA in unboxing object arrays</li> </ul>"},{"location":"release_notes/2025.08/#performance-improvements","title":"\ud83c\udfce\ufe0f Performance Improvements","text":"<ul> <li>Improved performance of read_parquet on string columns</li> <li>Increased default streaming batch size for better performance</li> <li>Improved performance of operations that use Python functions under the hood</li> <li>Reduce memory pressure in execution pipeline and improve performance</li> <li>Process a subset of datetime properties in BodoSeries via Arrow Compute in C++ to improve performance</li> </ul>"}]}