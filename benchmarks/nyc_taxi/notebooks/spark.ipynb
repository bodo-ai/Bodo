{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import pyspark.pandas as ps\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    dayofweek,\n",
    "    hour,\n",
    "    month,\n",
    "    to_date,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup \n",
    "\n",
    "Download the datasets from the public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_weather_path_s3 = \"nyc-taxi/central_park_weather.csv\"\n",
    "bucket_name = \"bodo-example-data\"\n",
    "hvfhv_5M_path_s3 = \"nyc-taxi/fhvhv_5M_rows.pq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_s3(path_to_s3: str, local_data_dir: str = \"data\") -> str:\n",
    "    \"\"\"Download the dataset from S3 if already exists, skip download.\"\"\"\n",
    "    file_name = path_to_s3.split(\"/\", -1)[1]\n",
    "    local_path = os.path.join(local_data_dir, file_name)\n",
    "\n",
    "    if os.path.exists(local_path):\n",
    "        return local_path\n",
    "\n",
    "    print(\"Downloading dataset from S3...\")\n",
    "\n",
    "    s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "    if not os.path.exists(local_data_dir):\n",
    "        os.mkdir(local_data_dir)\n",
    "\n",
    "    s3.download_file(bucket_name, path_to_s3, local_path)\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_path = download_data_s3(central_park_weather_path_s3)\n",
    "hvfhv_5M_path = download_data_s3(hvfhv_5M_path_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_travels_weather(weather_dataset, hvfhv_dataset):\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"MonthlyTravelsWeather\")\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.2\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .config(\n",
    "            \"fs.s3a.aws.credentials.provider\",\n",
    "            \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n",
    "        )\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Read in weather data using pandas-on-Spark\n",
    "    central_park_weather_observations = ps.read_csv(\n",
    "        weather_dataset,\n",
    "    ).rename(columns={\"DATE\": \"date\", \"PRCP\": \"precipitation\"})\n",
    "\n",
    "    central_park_weather_observations[\"date\"] = ps.to_datetime(\n",
    "        central_park_weather_observations[\"date\"]\n",
    "    )\n",
    "\n",
    "    # Read in trip data using spark, this reads a re-written dataset because spark doesn't support reading the original dataset\n",
    "    # due to schema unification issues\n",
    "    fhvhv_tripdata = spark.read.parquet(hvfhv_dataset).drop(\"__index_level_0__\")\n",
    "\n",
    "    # Convert datetime columns and create necessary features\n",
    "    fhvhv_tripdata = (\n",
    "        (\n",
    "            fhvhv_tripdata.withColumn(\"date\", to_date(col(\"pickup_datetime\")))\n",
    "            .withColumn(\"month\", month(col(\"pickup_datetime\")))\n",
    "            .withColumn(\"hour\", hour(col(\"pickup_datetime\")))\n",
    "            .withColumn(\n",
    "                \"weekday\", dayofweek(col(\"pickup_datetime\")).isin([2, 3, 4, 5, 6])\n",
    "            )\n",
    "            # pandas-on-Spark doesn't like these datetime columns which is why we use Spark APIs for the read and this conversion\n",
    "        )\n",
    "        .drop(\"pickup_datetime\")\n",
    "        .drop(\"dropoff_datetime\")\n",
    "        .drop(\"on_scene_datetime\")\n",
    "        .drop(\"request_datetime\")\n",
    "    )\n",
    "    # Convert trip data to pandas-on-Spark\n",
    "    fhvhv_tripdata = ps.DataFrame(fhvhv_tripdata)\n",
    "\n",
    "    # Join trip data with weather observations on 'date'\n",
    "    monthly_trips_weather = fhvhv_tripdata.merge(\n",
    "        central_park_weather_observations, on=\"date\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    ## Create a new column for precipitation indicator\n",
    "    monthly_trips_weather[\"date_with_precipitation\"] = (\n",
    "        monthly_trips_weather[\"precipitation\"] > 0.1\n",
    "    )\n",
    "\n",
    "    ## Define time bucket based on hour of the day\n",
    "    def get_time_bucket(t):\n",
    "        bucket = \"other\"\n",
    "        if t in (8, 9, 10):\n",
    "            bucket = \"morning\"\n",
    "        elif t in (11, 12, 13, 14, 15):\n",
    "            bucket = \"midday\"\n",
    "        elif t in (16, 17, 18):\n",
    "            bucket = \"afternoon\"\n",
    "        elif t in (19, 20, 21):\n",
    "            bucket = \"evening\"\n",
    "        return bucket\n",
    "\n",
    "    monthly_trips_weather[\"time_bucket\"] = monthly_trips_weather.hour.map(\n",
    "        get_time_bucket\n",
    "    )\n",
    "    monthly_trips_weather.groupby(\n",
    "        [\n",
    "            \"PULocationID\",\n",
    "            \"DOLocationID\",\n",
    "            \"month\",\n",
    "            \"weekday\",\n",
    "            \"date_with_precipitation\",\n",
    "            \"time_bucket\",\n",
    "        ],\n",
    "        as_index=False,\n",
    "    ).agg({\"hvfhs_license_num\": \"count\", \"trip_miles\": \"mean\"})\n",
    "\n",
    "    sorted_data = monthly_trips_weather.sort_values(\n",
    "        by=[\n",
    "            \"PULocationID\",\n",
    "            \"DOLocationID\",\n",
    "            \"month\",\n",
    "            \"weekday\",\n",
    "            \"date_with_precipitation\",\n",
    "            \"time_bucket\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ## Write the results to a parquet file\n",
    "    sorted_data.to_parquet(\"spark_monthly_trips_weather.pq\", mode=\"overwrite\")\n",
    "    print(\"Execution time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_monthly_travels_weather(weather_path, hvfhv_5M_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on a Larger Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvfhv_20M_path_s3 = \"nyc-taxi/fhvhv_tripdata/fhvhv_tripdata_2019-02.parquet\"\n",
    "hvfhv_20M_path = download_data_s3(hvfhv_20M_path_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_monthly_travels_weather(weather_path, hvfhv_20M_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
