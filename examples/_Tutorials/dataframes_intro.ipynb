{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bodo DataFrames Developer Guide\n",
    "\n",
    "This guide provides an introduction to using Bodo DataFrames, explains some of the important concepts and gives tips on how to integrate Bodo DataFrames into existing Pandas workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating BodoDataFrames\n",
    "\n",
    "You can create a BodoDataFrame by reading data from a file or table using an I/O function. Currently supported IO functions include: \n",
    "\n",
    "* `pd.read_parquet`\n",
    "* `pd.read_iceberg`\n",
    "* `pd.read_csv`\n",
    "\n",
    "`pd.read_parquet` and `pd.read_iceberg` are lazy APIs, meaning that no actual data is read until needed in a subsequent operation.\n",
    "\n",
    "You can also create BodoDataFrames from a Pandas DataFrame using the `from_pandas`, which is useful when working with third party libraries that return Pandas DataFrames. \n",
    "\n",
    "Finally, you can create BodoDataFrames using the `bodo.pandas.DataFrame` constructor, which is similar to `pandas.DataFrame`.\n",
    "\n",
    "### Unsupported DataFrames\n",
    "\n",
    "Unlike Pandas, BodoDataFrames cannot support arbitrary Python types in columns. Each column in a BodoDataFrame should have a single supported type. Supported types include ints, floats, bool, decimal128, timestamps/datetime, dates, durations/timedelta, string, binary, list, map, and struct. \n",
    "\n",
    "Some examples of unsupported DataFrames:\n",
    "\n",
    "``` py\n",
    "# Mixed-type tuples are not supported (use structs instead)\n",
    "DataFrame({\"A\": [(\"a\", 1), (\"b\", 2)]})\n",
    "\n",
    "# Each column must hold a single type\n",
    "DataFrame({\"A\": [1, \"a\"]})\n",
    "\n",
    "# DataFrames cannot have arbitrary Python objects\n",
    "DataFrame({\"A\": [MyObject()] * 4})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation and Plans\n",
    "\n",
    "In Bodo, operations on DataFrames and Series are lazy, meaning that they return a lazy result representing a DataFrame, Series or Scalar which contains some metadata i.e. a schema, but not the actual data itself. Instead, lazy results contain a \"plan\" attribute. A plan is an expression tree describing how to go from data sources to the current object using relational operators like join, aggregate, or project.\n",
    "\n",
    "Lazy evaluation allows Bodo to optimize the plan before execution, which can have a huge impact (e.g. 100x) on workload performance. Common optimizations include reordering joins, pushing filters to I/O, or eliminating dead columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see an example of lazy evaluation, we will create a DataFrame, representing a Parquet read over a billion row dataset (NYC taxi). Normally, this dataset would be too large to fit into memory on most laptops, however since the `read_parquet` API is lazy, no actual data is materialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bodo.pandas as pd\n",
    "from bodo.ext import plan_optimizer\n",
    "\n",
    "df = pd.read_parquet(\"s3://bodo-example-data/nyc-taxi/fhvhv_tripdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately inspect the metadata of our lazy DataFrame such as the column names and data types. When `read_parquet` is called, Bodo opens the first couple parquet files in the dataset to infer the schema, which is typically pretty fast. \n",
    "\n",
    "We can also look at the plan for this DataFrame, which is just a single Parquet scan node.\n",
    "\n",
    "Finally, we can get the length of the dataset, which executes a small plan to scan the entire dataset and get the row count in each file without pulling any of the rows into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num',\n",
      "       'request_datetime', 'on_scene_datetime', 'pickup_datetime',\n",
      "       'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles',\n",
      "       'trip_time', 'base_passenger_fare', 'tolls', 'bcf', 'sales_tax',\n",
      "       'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay',\n",
      "       'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag',\n",
      "       'wav_request_flag', 'wav_match_flag'],\n",
      "      dtype='object')\n",
      "hvfhs_license_num              string[pyarrow]\n",
      "dispatching_base_num           string[pyarrow]\n",
      "originating_base_num           string[pyarrow]\n",
      "request_datetime        timestamp[us][pyarrow]\n",
      "on_scene_datetime       timestamp[us][pyarrow]\n",
      "pickup_datetime         timestamp[us][pyarrow]\n",
      "dropoff_datetime        timestamp[us][pyarrow]\n",
      "PULocationID                    int64[pyarrow]\n",
      "DOLocationID                    int64[pyarrow]\n",
      "trip_miles                     double[pyarrow]\n",
      "trip_time                       int64[pyarrow]\n",
      "base_passenger_fare            double[pyarrow]\n",
      "tolls                          double[pyarrow]\n",
      "bcf                            double[pyarrow]\n",
      "sales_tax                      double[pyarrow]\n",
      "congestion_surcharge           double[pyarrow]\n",
      "airport_fee                    double[pyarrow]\n",
      "tips                           double[pyarrow]\n",
      "driver_pay                     double[pyarrow]\n",
      "shared_request_flag            string[pyarrow]\n",
      "shared_match_flag              string[pyarrow]\n",
      "access_a_ride_flag             string[pyarrow]\n",
      "wav_request_flag               string[pyarrow]\n",
      "wav_match_flag                 string[pyarrow]\n",
      "dtype: object\n",
      "┌───────────────────────────┐\n",
      "│BODO_READ_PARQUET(HVFH...  │\n",
      "│    ────────────────────   │\n",
      "│      ~1036465968 Rows     │\n",
      "└───────────────────────────┘\n",
      "\n",
      "1036465968\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(df._plan.generate_duckdb().toString())\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bodo can automatically push filters down to IO, which is useful as it avoids materializing extra rows. When we run the optimizer in the example below, two plan nodes (a read into a filter) becomes a single node (a read with a filter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimizing:\n",
      "┌───────────────────────────┐\n",
      "│           FILTER          │\n",
      "│    ────────────────────   │\n",
      "│        Expressions:       │\n",
      "│        (#[2.7] = 1)       │\n",
      "│       (#[2.8] = 148)      │\n",
      "└─────────────┬─────────────┘\n",
      "┌─────────────┴─────────────┐\n",
      "│BODO_READ_PARQUET(HVFH...  │\n",
      "│    ────────────────────   │\n",
      "│      ~1036465968 Rows     │\n",
      "└───────────────────────────┘\n",
      "\n",
      "After optimizing:\n",
      "┌───────────────────────────┐\n",
      "│BODO_READ_PARQUET(HVFH...  │\n",
      "│    ────────────────────   │\n",
      "│          Filters:         │\n",
      "│       PULocationID=1      │\n",
      "│      DOLocationID=148     │\n",
      "│                           │\n",
      "│      ~207293193 Rows      │\n",
      "└───────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filt = df[(df['PULocationID'] == 1) & (df['DOLocationID'] == 148)]\n",
    "\n",
    "print(\"Before optimizing:\")\n",
    "print(filt._plan.generate_duckdb().toString())\n",
    "\n",
    "print(\"After optimizing:\")\n",
    "print(plan_optimizer.py_optimize_plan(filt._plan.generate_duckdb()).toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another optimization that Bodo can do is join reordering. In this example, we want to inner join two dataframes on a key column where one dataframe is much larger than the other. The optimizer swaps the sides of the join to avoid materializing the larger result in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimizing:\n",
      "┌───────────────────────────┐\n",
      "│         PROJECTION        │\n",
      "│    ────────────────────   │\n",
      "│        Expressions:       │\n",
      "│           #[5.0]          │\n",
      "│           #[4.0]          │\n",
      "└─────────────┬─────────────┘\n",
      "┌─────────────┴─────────────┐\n",
      "│      COMPARISON_JOIN      │\n",
      "│    ────────────────────   │\n",
      "│      Join Type: INNER     │\n",
      "│                           ├──────────────┐\n",
      "│        Conditions:        │              │\n",
      "│     (#[5.0] = #[4.0])     │              │\n",
      "└─────────────┬─────────────┘              │\n",
      "┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
      "│      BODO_READ_DF(A)      ││      BODO_READ_DF(B)      │\n",
      "│    ────────────────────   ││    ────────────────────   │\n",
      "│          ~5 Rows          ││         ~5000 Rows        │\n",
      "└───────────────────────────┘└───────────────────────────┘\n",
      "\n",
      "After optimizing:\n",
      "┌───────────────────────────┐\n",
      "│         PROJECTION        │\n",
      "│    ────────────────────   │\n",
      "│        Expressions:       │\n",
      "│           #[8.0]          │\n",
      "│           #[8.0]          │\n",
      "│                           │\n",
      "│         ~5000 Rows        │\n",
      "└─────────────┬─────────────┘\n",
      "┌─────────────┴─────────────┐\n",
      "│      COMPARISON_JOIN      │\n",
      "│    ────────────────────   │\n",
      "│      Join Type: INNER     │\n",
      "│                           │\n",
      "│        Conditions:        ├──────────────┐\n",
      "│     (#[7.0] = #[8.0])     │              │\n",
      "│                           │              │\n",
      "│         ~5000 Rows        │              │\n",
      "└─────────────┬─────────────┘              │\n",
      "┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
      "│      BODO_READ_DF(B)      ││      BODO_READ_DF(A)      │\n",
      "│    ────────────────────   ││    ────────────────────   │\n",
      "│         ~5000 Rows        ││          ~5 Rows          │\n",
      "└───────────────────────────┘└───────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame(\n",
    "    {\"A\": [1,2,3,4,5]}\n",
    ")\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    {\"B\": [1,2,3,4,5] * 1000}\n",
    ")\n",
    "\n",
    "jn1 = df1.merge(df2, left_on=\"A\", right_on=\"B\")\n",
    "\n",
    "print(\"Before optimizing:\")\n",
    "print(jn1._plan.generate_duckdb().toString())\n",
    "\n",
    "print(\"After optimizing:\")\n",
    "print(plan_optimizer.py_optimize_plan(jn1._plan.generate_duckdb()).toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan execution\n",
    "\n",
    "Plan optimization typically happens right before execution. The optimized plan is then converted into a sequence of pipelines that are executed in parallel on Bodo workers. Data is streamed through the operators in these pipelines in batches where it is ultimately collected either in a file or an in-memory result. \n",
    "\n",
    "Plan execution is triggered by operations like writing to a Parquet file or Iceberg table, printing data, or when an unsupported operation is encountered. The cell below gives examples of operations that return lazy results as well as operations that require collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean trip time:  1128.3018568\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"s3://bodo-example-data/nyc-taxi/fhvhv_5M_rows.pq\")\n",
    "\n",
    "# Selecting lists of columns returns a lazy DataFrame result\n",
    "df = df[[\"PULocationID\", \"DOLocationID\", \"base_passenger_fare\", \"trip_time\", \"tips\", \"pickup_datetime\"]]\n",
    "\n",
    "# Selecting a single column returns a lazy BodoSeries\n",
    "trip_time = df.trip_time\n",
    "\n",
    "# Reducing a single column to a value produces a lazy BodoScalar\n",
    "mean_trip_time = trip_time.mean()\n",
    "\n",
    "# Printing the lazy result triggers execution\n",
    "print(\"mean trip time: \", mean_trip_time)\n",
    "\n",
    "# Writing a lazy dataframe to a file or table triggers execution\n",
    "df.head(10000).to_parquet(\"taxi_data.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallback for unsupported methods\n",
    "\n",
    "While Bodo DataFrames supports most common compute intensive operations in Pandas, there are some operations, parameters, or combinations of operations that are not supported yet. [Refer to the DataFrames documentation page](https://docs.bodo.ai/latest/api_docs/dataframe_lib/) for the most up to date list of supported features. Note that while a function may say \"supported\", there could be a subset of parameters that are not supported yet. \n",
    "\n",
    "By default, Bodo automatically raises a warning when an unsupported operation is encountered and falls back to the Pandas implementation, which will typically collect the entire dataset in memory. After the Pandas operation finishes, if the result is a DataFrame or Series, it will be automatically cast back to Bodo so that subsequent operations continue lazily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bodo.pandas.frame.BodoDataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottroutledge/Documents/Bodo/bodo/pandas/utils.py:1215: BodoLibFallbackWarning: transform is not implemented in Bodo DataFrames yet. Falling back to Pandas (may be slow or run out of memory).\n",
      "  warnings.warn(BodoLibFallbackWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"A\": range(4), \"B\": range(1,5)})\n",
    "\n",
    "# Unsupported function: transform\n",
    "df = df.transform(lambda x: x + 1)\n",
    "print(type(df))\n",
    "\n",
    "# Subsequent operations will be lazily evaluated\n",
    "sum_A = df.A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using execute_plan to reuse expensive results\n",
    "\n",
    "Lazy Bodo DataFrames and Series can be explicitly collected using the `execute_plan` function. This function is useful when you have separate plans that depend on the same, expensive computation that produces a small result. In the example below, a larger Taxi dataset is joined with weather observations and aggregated, producing a smaller DataFrame. The result is then referenced in two separate locations.\n",
    "\n",
    "Without the `execute_plan` call, the plan for `expensive` (read parquet, read csv, join, groupby-aggregate etc) would be executed multiple times for each dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14348\n",
      "80.0\n"
     ]
    }
   ],
   "source": [
    "def expensive_computation() -> pd.DataFrame:\n",
    "    taxi_df = pd.read_parquet(\"s3://bodo-example-data/nyc-taxi/fhvhv_5M_rows.pq\")\n",
    "    taxi_df[\"date\"] = taxi_df.pickup_datetime.dt.date\n",
    "\n",
    "    central_park_weather_observations = pd.read_csv(\n",
    "            \"s3://bodo-example-data/nyc-taxi/central_park_weather.csv\",\n",
    "            parse_dates=[\"DATE\"],\n",
    "        )[[\"DATE\", \"PRCP\"]]\n",
    "    central_park_weather_observations[\"DATE\"] = central_park_weather_observations[\n",
    "        \"DATE\"\n",
    "    ].dt.date\n",
    "\n",
    "    central_park_weather_observations[\"date_with_precipitation\"] = (\n",
    "            central_park_weather_observations[\"PRCP\"] > 0.1\n",
    "        )\n",
    "\n",
    "    taxi_weather = taxi_df.merge(\n",
    "            central_park_weather_observations,\n",
    "            left_on=\"date\",\n",
    "            right_on=\"DATE\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "    weather_tips = taxi_weather.groupby(\n",
    "            [\n",
    "                \"PULocationID\",\n",
    "                \"DOLocationID\",\n",
    "                \"date_with_precipitation\",\n",
    "            ],\n",
    "            as_index=False,\n",
    "        ).agg(average_tip=(\"tips\", \"mean\"))\n",
    "\n",
    "    return weather_tips\n",
    "\n",
    "expensive = expensive_computation()\n",
    "\n",
    "expensive.execute_plan()\n",
    "\n",
    "result1 = print(expensive[expensive[\"average_tip\"] > 1].average_tip.count())\n",
    "\n",
    "result2 = print(expensive.average_tip.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User defined functions\n",
    "\n",
    "Bodo supports custom transformations on DataFrames and Series using APIs like `Series.map` or `DataFrame.apply`. By default, user defined functions (UDFs) are automatically JIT compiled to avoid Python overheads. In the cell below `get_time_bucket` is compiled eagerly when `map` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    other\n",
      "1    other\n",
      "2    other\n",
      "3    other\n",
      "4    other\n",
      "Name: hour, dtype: large_string[pyarrow]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"s3://bodo-example-data/nyc-taxi/fhvhv_5M_rows.pq\")\n",
    "\n",
    "df[\"hour\"] = df.pickup_datetime.dt.hour\n",
    "\n",
    "def get_time_bucket(t):\n",
    "    bucket = \"other\"\n",
    "    if t in (8, 9, 10):\n",
    "        bucket = \"morning\"\n",
    "    elif t in (11, 12, 13, 14, 15):\n",
    "        bucket = \"midday\"\n",
    "    elif t in (16, 17, 18):\n",
    "        bucket = \"afternoon\"\n",
    "    elif t in (19, 20, 21):\n",
    "        bucket = \"evening\"\n",
    "    return bucket\n",
    "\n",
    "print(df.hour.map(get_time_bucket).head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If compilation fails, a warning will be printed and the function will execute in Python mode, which will first run your custom function on a small sample of data to determine output types before continuing lazy evaluation. In the example below, `get_time_bucket` is used as a helper function in a UDF, but the definition is not exposed to JIT, leading to typing errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottroutledge/Documents/Bodo/bodo/pandas/frame.py:1417: BodoCompilationFailedWarning: DataFrame.apply(): Compiling user defined function failed or encountered an unsupported result type. Falling back to Python engine. Add engine='python' to ignore this warning. Original error: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mDataFrame.apply(): user-defined function not supported: \u001b[1mCannot call non-JIT function 'get_time_bucket' from JIT function (convert to JIT or use objmode).\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../var/folders/w_/z_0_fn150v36jdgzrrlcj8q00000gn/T/ipykernel_9201/4182062319.py\", line 2:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\u001b[0m\n",
      "\u001b[1m\n",
      "File \"bodo/pandas/frame.py\", line 1338:\u001b[0m\n",
      "\u001b[1m        def apply_wrapper_inner(df):\n",
      "\u001b[1m            return df.apply(func, axis=1, args=args)\n",
      "\u001b[0m            \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[1mDuring: Pass bodo_type_inference\u001b[0m\u001b[0m\u001b[0m.\n",
      "  warnings.warn(BodoCompilationFailedWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    other\n",
      "1    other\n",
      "2    other\n",
      "3    other\n",
      "4    other\n",
      "dtype: string[pyarrow]\n"
     ]
    }
   ],
   "source": [
    "def apply_with_python_fallback(row):\n",
    "    return get_time_bucket(row.hour)\n",
    "\n",
    "print(df.apply(apply_with_python_fallback, axis=1).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to avoid JIT compilation and run directly in Python mode, you can pass the `engine=\"python\"` argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    other\n",
      "1    other\n",
      "2    other\n",
      "3    other\n",
      "4    other\n",
      "dtype: string[pyarrow]\n"
     ]
    }
   ],
   "source": [
    "print(df.apply(apply_with_python_fallback, axis=1, engine='python').head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid compilation issues, your function should be type stable, and any helper functions should be decorated with `bodo.jit(spawn=False, distributed=False)`. You can also use most Pandas and Numpy functions inside UDFs.\n",
    "For additional tips on JIT compilation and troubleshooting, refer to our [Python JIT development guide](https://docs.bodo.ai/latest/quick_start/dev_guide/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    other\n",
      "1    other\n",
      "2    other\n",
      "3    other\n",
      "4    other\n",
      "dtype: large_string[pyarrow]\n"
     ]
    }
   ],
   "source": [
    "import bodo\n",
    "\n",
    "get_time_bucket_jit = bodo.jit(get_time_bucket, distributed=False, spawn=False)\n",
    "\n",
    "def apply_get_time_bucket(row):\n",
    "    return get_time_bucket_jit(row.hour)\n",
    "\n",
    "print(df.apply(apply_get_time_bucket, axis=1).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also apply custom transformations on groups of data via `groupby.agg` or `groupby.apply`, although currently this features does not support the `engine='python'` argument. If compilation fails, execution will fall back to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottroutledge/Documents/Bodo/bodo/pandas/utils.py:1215: BodoLibFallbackWarning: items is not implemented in Bodo DataFrames yet. Falling back to Pandas (may be slow or run out of memory).\n",
      "  warnings.warn(BodoLibFallbackWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>small_tip_fraction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <th>87</th>\n",
       "      <td>0.943548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <th>198</th>\n",
       "      <td>0.978206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>223</th>\n",
       "      <td>0.968514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <th>112</th>\n",
       "      <td>0.977278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <th>262</th>\n",
       "      <td>0.955056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           small_tip_fraction\n",
       "PULocationID DOLocationID                    \n",
       "87           87                      0.943548\n",
       "198          198                     0.978206\n",
       "7            223                     0.968514\n",
       "255          112                     0.977278\n",
       "161          262                     0.955056"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"s3://bodo-example-data/nyc-taxi/fhvhv_5M_rows.pq\")\n",
    "\n",
    "def get_small_tip_fraction(tips):\n",
    "    total_count = len(tips)\n",
    "    small_tip_count = len(tips[tips < 3])\n",
    "    return small_tip_count / total_count\n",
    "\n",
    "agg = df.groupby(['PULocationID', 'DOLocationID']).agg(small_tip_fraction=('tips', get_small_tip_fraction))\n",
    "\n",
    "agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrating Pandas Scripts to Bodo DataFrames\n",
    "\n",
    "Some general tips for migrating Pandas workflows to Bodo DataFrames:\n",
    "\n",
    "* **Replace the import one file at a time.**\n",
    "\n",
    "    Examine individual workflows on a case by case basis to determine if Bodo DataFrames is the right fit.\n",
    "    If a script uses a lot of unsupported functions or only ever runs on small data sizes, it might be better to keep it in pure Pandas.\n",
    "\n",
    "* **Measure performance on sufficiently large data sizes.**\n",
    "\n",
    "    Spawning parallel workers or loading heavy JIT modules can add extra overheads to your program. To get a better feel for the kinds of speedups you can expect when comparing performance vs. Pandas, try running on larger data sizes (e.g. >10,000,000 rows).\n",
    "\n",
    "* **Run on a sufficiently large machine.**\n",
    "\n",
    "    To see the benefits of Bodo's parallelism, make sure you are running on a sufficiently large machine or instance with more than one core. \n",
    "    Try increasing the number of cores if you need better performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
