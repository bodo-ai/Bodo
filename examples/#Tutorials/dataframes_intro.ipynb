{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bodo DataFrames Developer Guide\n",
    "\n",
    "This guide provides an introduction to using Bodo DataFrames, explains some of the important concepts and gives tips on how to integrate Bodo DataFrames into existing Pandas workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating BodoDataFrames\n",
    "\n",
    "You can create a BodoDataFrame by reading data from a file or table using an I/O function. Currently supported IO functions include: \n",
    "\n",
    "* `pd.read_parquet`\n",
    "* `pd.read_iceberg`\n",
    "* `pd.read_csv`\n",
    "\n",
    "`pd.read_parquet` and `pd.read_iceberg` are lazy APIs, meaning that no actual data is read until it is needed in a subsequent operation.\n",
    "\n",
    "You can also create BodoDataFrames from a Pandas DataFrame using the `from_pandas` function, which is useful when working with third party libraries that return Pandas DataFrames.\n",
    "\n",
    "### Unsupported DataFrames\n",
    "\n",
    "Unlike Pandas, BodoDataFrames cannot support arbitrary Python types in columns. Each column in a BodoDataFrame should have a single, well defined type. Supported types include ints, floats, bool, decimal128, timestamps/datetime, dates, durations/timedelta, string, binary, list, map, and struct. \n",
    "\n",
    "Some examples of unsupported DataFrames:\n",
    "\n",
    "``` py\n",
    "# Mixed-type tuples are not supported (use structs instead)\n",
    "DataFrame({\"A\": [(\"a\", 1), (\"b\", 2)]})\n",
    "\n",
    "# Each column must hold a single type\n",
    "DataFrame({\"A\": [1, \"a\"]})\n",
    "\n",
    "# DataFrames cannot have arbitrary Python objects\n",
    "DataFrame({\"A\": [MyObject()] * 4})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation and Plans\n",
    "\n",
    "In Bodo, operations on DataFrames and Series are lazy, meaning that they return a lazy result representing a DataFrame, Series or Scalar which contains some metadata i.e. a schema, but not the actual data itself. Instead, lazy results contain a \"plan\" attribute, which is an expression tree describing how to go from the data sources to the final object using relational operators like join, aggregate, or project.\n",
    "\n",
    "Lazy evaluation allows Bodo to optimize the expression tree before execution, which can have a huge impact (e.g. 100x) on workload performance. Common optimizations include reordering joins, pushing filters to I/O, or eliminating dead columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see an example of lazy evaluation, let's create a DataFrame, representing a Parquet read over a billion row dataset (NYC taxi). Normally, this dataset would be too large to fit into memory on most laptops, however since the `read_parquet` API is lazy, no actual data is materialized at this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import bodo.pandas as pd\n",
    "from bodo.ext import plan_optimizer\n",
    "\n",
    "df = pd.read_parquet(\"s3://bodo-example-data/nyc-taxi/fhvhv_tripdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately inspect the metadata of our lazy DataFrame, such as the column names and data types. When `read_parquet` is called, Bodo opens the first couple parquet files in the dataset to infer the schema, which is typically pretty fast. \n",
    "\n",
    "We can also look at the plan for this DataFrame. Bodo uses DuckDB plans as an intermediary representation to perform optimizations using the DuckDB optimizer. \n",
    "\n",
    "Finally, we can get the length of the dataset, which executes a small plan which scans the entire dataset, getting the row count in each file without pulling any of the rows into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num',\n",
      "       'request_datetime', 'on_scene_datetime', 'pickup_datetime',\n",
      "       'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles',\n",
      "       'trip_time', 'base_passenger_fare', 'tolls', 'bcf', 'sales_tax',\n",
      "       'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay',\n",
      "       'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag',\n",
      "       'wav_request_flag', 'wav_match_flag'],\n",
      "      dtype='object')\n",
      "hvfhs_license_num              string[pyarrow]\n",
      "dispatching_base_num           string[pyarrow]\n",
      "originating_base_num           string[pyarrow]\n",
      "request_datetime        timestamp[us][pyarrow]\n",
      "on_scene_datetime       timestamp[us][pyarrow]\n",
      "pickup_datetime         timestamp[us][pyarrow]\n",
      "dropoff_datetime        timestamp[us][pyarrow]\n",
      "PULocationID                    int64[pyarrow]\n",
      "DOLocationID                    int64[pyarrow]\n",
      "trip_miles                     double[pyarrow]\n",
      "trip_time                       int64[pyarrow]\n",
      "base_passenger_fare            double[pyarrow]\n",
      "tolls                          double[pyarrow]\n",
      "bcf                            double[pyarrow]\n",
      "sales_tax                      double[pyarrow]\n",
      "congestion_surcharge           double[pyarrow]\n",
      "airport_fee                    double[pyarrow]\n",
      "tips                           double[pyarrow]\n",
      "driver_pay                     double[pyarrow]\n",
      "shared_request_flag            string[pyarrow]\n",
      "shared_match_flag              string[pyarrow]\n",
      "access_a_ride_flag             string[pyarrow]\n",
      "wav_request_flag               string[pyarrow]\n",
      "wav_match_flag                 string[pyarrow]\n",
      "dtype: object\n",
      "┌───────────────────────────┐\n",
      "│BODO_READ_PARQUET(HVFH...  │\n",
      "│    ────────────────────   │\n",
      "│      ~1036465968 Rows     │\n",
      "└───────────────────────────┘\n",
      "\n",
      "1036465968\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(df._plan.generate_duckdb().toString())\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the DuckDB optimizer, Bodo can automatically push filters down to IO, which is useful as it avoids materializing extra rows. Notice how, when we run the optimizer, two plan nodes (a read into a filter) becomes a single node (a read with a filter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimizing:\n",
      "┌───────────────────────────┐\n",
      "│           FILTER          │\n",
      "│    ────────────────────   │\n",
      "│        Expressions:       │\n",
      "│        (#[2.7] = 1)       │\n",
      "│       (#[2.8] = 148)      │\n",
      "└─────────────┬─────────────┘\n",
      "┌─────────────┴─────────────┐\n",
      "│BODO_READ_PARQUET(HVFH...  │\n",
      "│    ────────────────────   │\n",
      "│      ~1036465968 Rows     │\n",
      "└───────────────────────────┘\n",
      "\n",
      "After optimizing:\n",
      "┌───────────────────────────┐\n",
      "│BODO_READ_PARQUET(HVFH...  │\n",
      "│    ────────────────────   │\n",
      "│          Filters:         │\n",
      "│       PULocationID=1      │\n",
      "│      DOLocationID=148     │\n",
      "│                           │\n",
      "│      ~207293193 Rows      │\n",
      "└───────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filt = df[(df['PULocationID'] == 1) & (df['DOLocationID'] == 148)]\n",
    "\n",
    "print(\"Before optimizing:\")\n",
    "print(filt._plan.generate_duckdb().toString())\n",
    "\n",
    "print(\"After optimizing:\")\n",
    "print(plan_optimizer.py_optimize_plan(filt._plan.generate_duckdb()).toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another optimization that Bodo can do is join reordering. In this example, we want to inner join two dataframes on a key column where one dataframe is much larger than the other. The optimizer recognizes that it is better to swap the sides of the join to avoid materializing the larger result in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimizing:\n",
      "┌───────────────────────────┐\n",
      "│         PROJECTION        │\n",
      "│    ────────────────────   │\n",
      "│        Expressions:       │\n",
      "│           #[5.0]          │\n",
      "│           #[4.0]          │\n",
      "└─────────────┬─────────────┘\n",
      "┌─────────────┴─────────────┐\n",
      "│      COMPARISON_JOIN      │\n",
      "│    ────────────────────   │\n",
      "│      Join Type: INNER     │\n",
      "│                           ├──────────────┐\n",
      "│        Conditions:        │              │\n",
      "│     (#[5.0] = #[4.0])     │              │\n",
      "└─────────────┬─────────────┘              │\n",
      "┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
      "│      BODO_READ_DF(A)      ││      BODO_READ_DF(B)      │\n",
      "│    ────────────────────   ││    ────────────────────   │\n",
      "│          ~5 Rows          ││         ~5000 Rows        │\n",
      "└───────────────────────────┘└───────────────────────────┘\n",
      "\n",
      "After optimizing:\n",
      "┌───────────────────────────┐\n",
      "│         PROJECTION        │\n",
      "│    ────────────────────   │\n",
      "│        Expressions:       │\n",
      "│           #[8.0]          │\n",
      "│           #[8.0]          │\n",
      "│                           │\n",
      "│         ~5000 Rows        │\n",
      "└─────────────┬─────────────┘\n",
      "┌─────────────┴─────────────┐\n",
      "│      COMPARISON_JOIN      │\n",
      "│    ────────────────────   │\n",
      "│      Join Type: INNER     │\n",
      "│                           │\n",
      "│        Conditions:        ├──────────────┐\n",
      "│     (#[7.0] = #[8.0])     │              │\n",
      "│                           │              │\n",
      "│         ~5000 Rows        │              │\n",
      "└─────────────┬─────────────┘              │\n",
      "┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
      "│      BODO_READ_DF(B)      ││      BODO_READ_DF(A)      │\n",
      "│    ────────────────────   ││    ────────────────────   │\n",
      "│         ~5000 Rows        ││          ~5 Rows          │\n",
      "└───────────────────────────┘└───────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.from_pandas(\n",
    "    pandas.DataFrame({\"A\": [1,2,3,4,5]})\n",
    ")\n",
    "\n",
    "df2 = pd.from_pandas(pandas.DataFrame(\n",
    "    {\"B\": [1,2,3,4,5] * 1000})\n",
    ")\n",
    "\n",
    "jn1 = df1.merge(df2, left_on=\"A\", right_on=\"B\")\n",
    "\n",
    "print(\"Before optimizing:\")\n",
    "print(jn1._plan.generate_duckdb().toString())\n",
    "\n",
    "print(\"After optimizing:\")\n",
    "print(plan_optimizer.py_optimize_plan(jn1._plan.generate_duckdb()).toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan execution\n",
    "\n",
    "Plan optimization happens right before execution. After the plan is optimized, it gets converted into a sequence of pipelines that are executed using parallel workers. Data is streamed through operators in these pipelines in batches. \n",
    "\n",
    "Plan execution is triggered by operations like writing to a Parquet file or Iceberg table, printing data, or when an unsupported operation is encountered. The cell below gives examples of operations that return lazy results as well as operations that require collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean trip time:  1128.3018568\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"s3://bodo-example-data/nyc-taxi/fhvhv_5M_rows.pq\")\n",
    "\n",
    "# Selecting lists of columns returns a lazy DataFrame result\n",
    "df = df[[\"PULocationID\", \"DOLocationID\", \"base_passenger_fare\", \"trip_time\", \"tips\", \"pickup_datetime\"]]\n",
    "\n",
    "# Selecting a single column returns a lazy BodoSeries\n",
    "trip_time = df.trip_time\n",
    "\n",
    "# Reducing a single column to a value produces a lazy BodoScalar\n",
    "mean_trip_time = trip_time.mean()\n",
    "\n",
    "# Printing the lazy result triggers execution\n",
    "print(\"mean trip time: \", mean_trip_time)\n",
    "\n",
    "# Writing a lazy dataframe to a file or table triggers execution\n",
    "df.head(10000).to_parquet(\"taxi_data.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallback for unsupported methods\n",
    "\n",
    "While Bodo DataFrames supports most common compute intensive operations in Pandas, there are some operations, parameters, or combinations of operations that are not supported yet. [Refer to the DataFrames documentation page]() for the most up to date list of supported features. Note that while a function might say it is supported, there may be a subset of parameters that are not supported yet. \n",
    "\n",
    "By default, Bodo automatically raises a warning when an unsupported operations are encountered and falls back to the Pandas implementation, which will typically collect the entire dataset in memory. After the Pandas operation finishes, if the result is a DataFrame or Series, it will be automatically cast back to Bodo so that subsequent operations continue to be lazily evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bodo.pandas.frame.BodoDataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottroutledge/Documents/Bodo/bodo/pandas/utils.py:1215: BodoLibFallbackWarning: transform is not implemented in Bodo DataFrames yet. Falling back to Pandas (may be slow or run out of memory).\n",
      "  warnings.warn(BodoLibFallbackWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "df = pd.from_pandas(pandas.DataFrame({\"A\": range(4), \"B\": range(1,5)}))\n",
    "\n",
    "# Unsupported function: transform\n",
    "df = df.transform(lambda x: x + 1)\n",
    "print(type(df))\n",
    "\n",
    "# Subsequent operations will be lazily evaluated\n",
    "sum_A = df.A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User defined functions\n",
    "\n",
    "Bodo supports custom transformations on DataFrames and Series using APIs like `Series.map` or `DataFrame.apply`. By default, user defined functions (UDFs) are automatically JIT compiled to avoid Python overheads. In the cell below `get_time_bucket` is compiled eagerly when `map` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    other\n",
      "1    other\n",
      "2    other\n",
      "3    other\n",
      "4    other\n",
      "Name: hour, dtype: large_string[pyarrow]\n"
     ]
    }
   ],
   "source": [
    "import bodo\n",
    "\n",
    "df = pd.read_parquet(\"s3://bodo-example-data/nyc-taxi/fhvhv_5M_rows.pq\")\n",
    "\n",
    "df[\"hour\"] = df.pickup_datetime.dt.hour\n",
    "\n",
    "def get_time_bucket(t):\n",
    "    bucket = \"other\"\n",
    "    if t in (8, 9, 10):\n",
    "        bucket = \"morning\"\n",
    "    elif t in (11, 12, 13, 14, 15):\n",
    "        bucket = \"midday\"\n",
    "    elif t in (16, 17, 18):\n",
    "        bucket = \"afternoon\"\n",
    "    elif t in (19, 20, 21):\n",
    "        bucket = \"evening\"\n",
    "    return bucket\n",
    "\n",
    "print(df.hour.map(get_time_bucket).head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If compilation fails, a warning will be printed and the function will execute in Python mode, which will first run your custom function on a small sample of data to determine output types. In the example below, `get_time_bucket` is used as a helper function in a UDF, but the definition is not exposed to JIT, leading to typing errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottroutledge/Documents/Bodo/bodo/pandas/frame.py:1417: BodoCompilationFailedWarning: DataFrame.apply(): Compiling user defined function failed or encountered an unsupported result type. Falling back to Python engine. Add engine='python' to ignore this warning. Original error: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mDataFrame.apply(): user-defined function not supported: \u001b[1mCannot call non-JIT function 'get_time_bucket' from JIT function (convert to JIT or use objmode).\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../var/folders/w_/z_0_fn150v36jdgzrrlcj8q00000gn/T/ipykernel_57032/4182062319.py\", line 2:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\u001b[0m\n",
      "\u001b[1m\n",
      "File \"bodo/pandas/frame.py\", line 1338:\u001b[0m\n",
      "\u001b[1m        def apply_wrapper_inner(df):\n",
      "\u001b[1m            return df.apply(func, axis=1, args=args)\n",
      "\u001b[0m            \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[1mDuring: Pass bodo_type_inference\u001b[0m\u001b[0m\u001b[0m.\n",
      "  warnings.warn(BodoCompilationFailedWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    other\n",
      "1    other\n",
      "2    other\n",
      "3    other\n",
      "4    other\n",
      "dtype: string[pyarrow]\n"
     ]
    }
   ],
   "source": [
    "def apply_with_python_fallback(row):\n",
    "    return get_time_bucket(row.hour)\n",
    "\n",
    "print(df.apply(apply_with_python_fallback, axis=1).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to avoid JIT compilation and run directly in Python mode, you can pass the `engine=\"python\"` argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    other\n",
      "1    other\n",
      "2    other\n",
      "3    other\n",
      "4    other\n",
      "dtype: string[pyarrow]\n"
     ]
    }
   ],
   "source": [
    "print(df.apply(apply_with_python_fallback, axis=1, engine='python').head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid compilation issues, your function should be type stable, and any helper functions should be decorated with `bodo.jit(spawn=False, distributed=False)`. You can also use most Pandas and Numpy functions inside UDFs.\n",
    "For additional tips on JIT compilation and troubleshooting, refer to our [Python JIT development guide](https://docs.bodo.ai/latest/quick_start/dev_guide/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    other\n",
      "1    other\n",
      "2    other\n",
      "3    other\n",
      "4    other\n",
      "dtype: large_string[pyarrow]\n"
     ]
    }
   ],
   "source": [
    "get_time_bucket_jit = bodo.jit(get_time_bucket, distributed=False, spawn=False)\n",
    "\n",
    "def apply_get_time_bucket(row):\n",
    "    return get_time_bucket_jit(row.hour)\n",
    "\n",
    "print(df.apply(apply_get_time_bucket, axis=1).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also apply custom transformations on groups of data via `groupby.agg` or `groupby.apply`, although currently this features does not support the `engine='python'` argument. If compilation fails, execution will fall back to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottroutledge/Documents/Bodo/bodo/pandas/utils.py:1215: BodoLibFallbackWarning: items is not implemented in Bodo DataFrames yet. Falling back to Pandas (may be slow or run out of memory).\n",
      "  warnings.warn(BodoLibFallbackWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>small_tip_fraction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <th>251</th>\n",
       "      <td>0.956938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <th>197</th>\n",
       "      <td>0.986562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <th>234</th>\n",
       "      <td>0.876161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">87</th>\n",
       "      <th>87</th>\n",
       "      <td>0.943548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.763636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           small_tip_fraction\n",
       "PULocationID DOLocationID                    \n",
       "245          251                     0.956938\n",
       "216          197                     0.986562\n",
       "261          234                     0.876161\n",
       "87           87                      0.943548\n",
       "             198                     0.763636"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"s3://bodo-example-data/nyc-taxi/fhvhv_5M_rows.pq\")\n",
    "\n",
    "def get_small_tip_fraction(tips):\n",
    "    total_count = len(tips)\n",
    "    small_tip_count = len(tips[tips < 3])\n",
    "    return small_tip_count / total_count\n",
    "\n",
    "agg = df.groupby(['PULocationID', 'DOLocationID']).agg(small_tip_fraction=('tips', get_small_tip_fraction))\n",
    "\n",
    "agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrating Pandas Scripts to Bodo DataFrames\n",
    "\n",
    "Some general tips for migrating Pandas scripts to Bodo DataFrames:\n",
    "\n",
    "* **Replace the import one file at a time.**\n",
    "\n",
    "    Examine individual workflows on a case by case basis to determine if Bodo DataFrames is the right fit.\n",
    "    If a script uses a lot of unsupported functions or only ever runs on small data sizes, it might be better to keep it in pure Pandas.\n",
    "\n",
    "* **Measure performance on sufficiently large data sizes.**\n",
    "\n",
    "    Spawning parallel workers or loading heavy JIT modules can add extra overheads to your program. To get a better feel for the kinds of speedups you can expect when comparing performance vs. Pandas, try running on larger data sizes (e.g. >10,000,000 rows).\n",
    "\n",
    "* **Run on a sufficiently large machine.**\n",
    "\n",
    "    To see the benefits of Bodo's parallelism, make sure you are running on a sufficiently large instance with more than one core. \n",
    "    Try increasing the number of cores if you need better performance.\n",
    "\n",
    "* **Avoid loading JIT if possible.**\n",
    "\n",
    "    APIs like `map` and `apply` load JIT modules, which can add extra overheads the first time they are called. Consider if your custom function application can be rewritten using builtin functions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
