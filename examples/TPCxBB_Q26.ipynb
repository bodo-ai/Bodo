{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Connecting to backend execution cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "c = ipp.Client(profile='mpi')\n",
    "view = c[:]\n",
    "view.activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPCxBB_Q26 Example Code\n",
    "Retail analytics example from [TPCxBB](http://www.tpc.org/tpcx-bb/default.asp): cluster customers into book buddies/club groups based on their in store book purchasing histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --block\n",
    "import bodo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "@bodo.jit\n",
    "def q26(ss_file, i_file, category, item_count):\n",
    "    t1 = time.time()\n",
    "    ss_dtype = {'ss_item_sk': np.int64, 'ss_customer_sk': np.int64}\n",
    "    store_sales = pd.read_csv(ss_file, sep='|', usecols=[2,3],\n",
    "        names=ss_dtype.keys(), dtype=ss_dtype)\n",
    "\n",
    "    i_dtype = {\n",
    "        'i_item_sk': np.int64, 'i_class_id': np.int32, 'i_category': str}\n",
    "    item = pd.read_csv(i_file, sep='|', usecols=[0, 9, 12],\n",
    "        names=i_dtype.keys(), dtype=i_dtype)\n",
    "\n",
    "    item2 = item[item['i_category']==category]\n",
    "    sale_items = pd.merge(\n",
    "        store_sales, item2, left_on='ss_item_sk', right_on='i_item_sk')\n",
    "\n",
    "    count1 = sale_items.groupby('ss_customer_sk')['ss_item_sk'].count()\n",
    "    gp1 = sale_items.groupby('ss_customer_sk')['i_class_id']\n",
    "\n",
    "    def id1(x): return (x==1).sum()\n",
    "    def id2(x): return (x==2).sum()\n",
    "    def id3(x): return (x==3).sum()\n",
    "    def id4(x): return (x==4).sum()\n",
    "    def id5(x): return (x==5).sum()\n",
    "    def id6(x): return (x==6).sum()\n",
    "    def id7(x): return (x==7).sum()\n",
    "    def id8(x): return (x==8).sum()\n",
    "    def id9(x): return (x==9).sum()\n",
    "    def id10(x): return (x==10).sum()\n",
    "    def id11(x): return (x==11).sum()\n",
    "    def id12(x): return (x==12).sum()\n",
    "    def id13(x): return (x==13).sum()\n",
    "    def id14(x): return (x==14).sum()\n",
    "    def id15(x): return (x==15).sum()\n",
    "\n",
    "    customer_i_class = gp1.agg((id1, id2, id3, id4, id5, id6, id7, id8, id9,\n",
    "        id10, id11, id12, id13, id14, id15))\n",
    "\n",
    "    customer_i_class['ss_item_count'] = count1\n",
    "\n",
    "    customer_i_class = customer_i_class[\n",
    "        customer_i_class.ss_item_count > item_count]\n",
    "    res = customer_i_class.values.astype(np.float64).sum()\n",
    "    print(\"checksum\", res)\n",
    "    print(\"exec time\", time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "checksum 3380326.0\n",
      "exec time 3.272716269000739\n"
     ]
    }
   ],
   "source": [
    "%%px --block\n",
    "ss_file = \"/Users/ehsan/dev/sw/data/store_sales_10.dat\"\n",
    "i_file =  \"/Users/ehsan/dev/sw/data/item_10.dat\"\n",
    "q26_i_category_IN = 'Books'\n",
    "q26_count_ss_item_sk = 5\n",
    "q26(ss_file, i_file, q26_i_category_IN, q26_count_ss_item_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Version\n",
    "The same computation in regular Python takes a long time, especially since user-defined aggregate functions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bodo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def q26(ss_file, i_file, category, item_count):\n",
    "    t1 = time.time()\n",
    "    ss_dtype = {'ss_item_sk': np.int64, 'ss_customer_sk': np.int64}\n",
    "    store_sales = pd.read_csv(ss_file, sep='|', usecols=[2,3],\n",
    "        names=ss_dtype.keys(), dtype=ss_dtype)\n",
    "\n",
    "    i_dtype = {\n",
    "        'i_item_sk': np.int64, 'i_class_id': np.int32, 'i_category': str}\n",
    "    item = pd.read_csv(i_file, sep='|', usecols=[0, 9, 12],\n",
    "        names=i_dtype.keys(), dtype=i_dtype)\n",
    "\n",
    "    item2 = item[item['i_category']==category]\n",
    "    sale_items = pd.merge(\n",
    "        store_sales, item2, left_on='ss_item_sk', right_on='i_item_sk')\n",
    "\n",
    "    count1 = sale_items.groupby('ss_customer_sk')['ss_item_sk'].count()\n",
    "    gp1 = sale_items.groupby('ss_customer_sk')['i_class_id']\n",
    "\n",
    "    def id1(x): return (x==1).sum()\n",
    "    def id2(x): return (x==2).sum()\n",
    "    def id3(x): return (x==3).sum()\n",
    "    def id4(x): return (x==4).sum()\n",
    "    def id5(x): return (x==5).sum()\n",
    "    def id6(x): return (x==6).sum()\n",
    "    def id7(x): return (x==7).sum()\n",
    "    def id8(x): return (x==8).sum()\n",
    "    def id9(x): return (x==9).sum()\n",
    "    def id10(x): return (x==10).sum()\n",
    "    def id11(x): return (x==11).sum()\n",
    "    def id12(x): return (x==12).sum()\n",
    "    def id13(x): return (x==13).sum()\n",
    "    def id14(x): return (x==14).sum()\n",
    "    def id15(x): return (x==15).sum()\n",
    "\n",
    "    customer_i_class = gp1.agg((id1, id2, id3, id4, id5, id6, id7, id8, id9,\n",
    "        id10, id11, id12, id13, id14, id15))\n",
    "\n",
    "    customer_i_class['ss_item_count'] = count1\n",
    "\n",
    "    customer_i_class = customer_i_class[\n",
    "        customer_i_class.ss_item_count > item_count]\n",
    "    res = customer_i_class.values.astype(np.float64).sum()\n",
    "    print(\"checksum\", res)\n",
    "    print(\"exec time\", time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checksum 3380326.0\n",
      "exec time 715.5162229537964\n"
     ]
    }
   ],
   "source": [
    "ss_file = \"/Users/ehsan/dev/sw/data/store_sales_10.dat\"\n",
    "i_file =  \"/Users/ehsan/dev/sw/data/item_10.dat\"\n",
    "q26_i_category_IN = 'Books'\n",
    "q26_count_ss_item_sk = 5\n",
    "q26(ss_file, i_file, q26_i_category_IN, q26_count_ss_item_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computation takes 715.5 seconds on Python but only 3.27 seconds on Bodo (220x speedup on 8 cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark/Scala/SQL Version\n",
    "The Spark/Scala/SQL version of the same code is much more complicated, and is also much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.language.existentials\n",
    "\n",
    "import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation\n",
    "import org.apache.spark.sql.catalyst.TableIdentifier\n",
    "import org.apache.spark.sql.execution.joins._\n",
    "\n",
    "import org.apache.spark.ml.clustering.{KMeansModel, KMeans}\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n",
    "import scala.language.reflectiveCalls\n",
    "import java.lang.management.ManagementFactory\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "object Query26 {\n",
    "  def main(args: Array[String]) {\n",
    "    // Starting time\n",
    "    val t0 = System.currentTimeMillis\n",
    "    val spark = SparkSession\n",
    "      .builder()\n",
    "      .appName(\"Q26\")\n",
    "      .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "      .getOrCreate()\n",
    "\n",
    "    import spark.implicits._\n",
    "    val table_store_sales_path = args(0)\n",
    "    val table_item_path = args(1)\n",
    "\n",
    "    val schema_store_sales = StructType(Array(\n",
    "      StructField(\"ss_sold_date_sk\", LongType, true),\n",
    "      StructField(\"ss_sold_time_sk\", LongType, true),\n",
    "      StructField(\"ss_item_sk\", LongType, true),\n",
    "      StructField(\"ss_customer_sk\", LongType, true),\n",
    "      StructField(\"ss_cdemo_sk\", LongType, true),\n",
    "      StructField(\"ss_hdemo_sk\", LongType, true),\n",
    "      StructField(\"ss_addr_sk\", LongType, true),\n",
    "      StructField(\"ss_store_sk\", LongType, true),\n",
    "      StructField(\"ss_promo_sk\", LongType, true),\n",
    "      StructField(\"ss_ticket_number\", LongType, true),\n",
    "      StructField(\"ss_quantity\", IntegerType, true),\n",
    "      StructField(\"ss_wholesale_cost\", FloatType, true),\n",
    "      StructField(\"ss_list_price\", FloatType, true),\n",
    "      StructField(\"ss_sales_price\", FloatType, true),\n",
    "      StructField(\"ss_ext_discount_amt\", FloatType, true),\n",
    "      StructField(\"ss_ext_sales_price\", FloatType, true),\n",
    "      StructField(\"ss_ext_wholesale_cost\", FloatType, true),\n",
    "      StructField(\"ss_ext_list_price\", FloatType, true),\n",
    "      StructField(\"ss_ext_tax\", FloatType, true),\n",
    "      StructField(\"ss_coupon_amt\", FloatType, true),\n",
    "      StructField(\"ss_net_paid\", FloatType, true),\n",
    "      StructField(\"ss_net_paid_inc_tax\", FloatType, true),\n",
    "      StructField(\"ss_net_profit\", FloatType, true)\n",
    "      ))\n",
    "    val df_store_sales = spark.read.schema(schema_store_sales).format(\"csv\").option(\"sep\", \"|\").load(table_store_sales_path)\n",
    "\n",
    "    val schema_item = StructType(Array(\n",
    "      StructField(\"i_item_sk\", LongType, true),\n",
    "      StructField(\"i_item_id\", StringType, true),\n",
    "      StructField(\"i_rec_start_date\", StringType, true),\n",
    "      StructField(\"i_rec_end_date\", StringType, true),\n",
    "      StructField(\"i_item_desc\", StringType, true),\n",
    "      StructField(\"i_current_price\", FloatType, true),\n",
    "      StructField(\"i_wholesale_cost\", FloatType, true),\n",
    "      StructField(\"i_brand_id\", IntegerType, true),\n",
    "      StructField(\"i_brand\", StringType, true),\n",
    "      StructField(\"i_class_id\", IntegerType, true),\n",
    "      StructField(\"i_class\", StringType, true),\n",
    "      StructField(\"i_category_id\", IntegerType, true),\n",
    "      StructField(\"i_category\", StringType, true),\n",
    "      StructField(\"i_manufact_id\", IntegerType, true),\n",
    "      StructField(\"i_manufact\", StringType, true),\n",
    "      StructField(\"i_size\", StringType, true),\n",
    "      StructField(\"i_formulation\", StringType, true),\n",
    "      StructField(\"i_color\", StringType, true),\n",
    "      StructField(\"i_units\", StringType, true),\n",
    "      StructField(\"i_container\", StringType, true),\n",
    "      StructField(\"i_manager_id\", IntegerType, true),\n",
    "      StructField(\"i_product_name\", StringType, true)\n",
    "      ))\n",
    "\n",
    "    val df_item = spark.read.schema(schema_item).format(\"csv\").option(\"sep\", \"|\").load(table_item_path)\n",
    "    // val df_item = spark.read.parquet(table_item_path)\n",
    "\n",
    "    df_store_sales.registerTempTable(\"store_sales_table\")\n",
    "    df_item.registerTempTable(\"item_table\")\n",
    "    // collect() fails so using first()\n",
    "    // df_store_sales.cache().first()\n",
    "    // df_item.cache().first()\n",
    "\n",
    "    val fin  = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "  ss.ss_customer_sk AS cid,\n",
    "  count(CASE WHEN i.i_class_id=1  THEN 1 ELSE NULL END) AS id1,\n",
    "  count(CASE WHEN i.i_class_id=2  THEN 1 ELSE NULL END) AS id2,\n",
    "  count(CASE WHEN i.i_class_id=3  THEN 1 ELSE NULL END) AS id3,\n",
    "  count(CASE WHEN i.i_class_id=4  THEN 1 ELSE NULL END) AS id4,\n",
    "  count(CASE WHEN i.i_class_id=5  THEN 1 ELSE NULL END) AS id5,\n",
    "  count(CASE WHEN i.i_class_id=6  THEN 1 ELSE NULL END) AS id6,\n",
    "  count(CASE WHEN i.i_class_id=7  THEN 1 ELSE NULL END) AS id7,\n",
    "  count(CASE WHEN i.i_class_id=8  THEN 1 ELSE NULL END) AS id8,\n",
    "  count(CASE WHEN i.i_class_id=9  THEN 1 ELSE NULL END) AS id9,\n",
    "  count(CASE WHEN i.i_class_id=10 THEN 1 ELSE NULL END) AS id10,\n",
    "  count(CASE WHEN i.i_class_id=11 THEN 1 ELSE NULL END) AS id11,\n",
    "  count(CASE WHEN i.i_class_id=12 THEN 1 ELSE NULL END) AS id12,\n",
    "  count(CASE WHEN i.i_class_id=13 THEN 1 ELSE NULL END) AS id13,\n",
    "  count(CASE WHEN i.i_class_id=14 THEN 1 ELSE NULL END) AS id14,\n",
    "  count(CASE WHEN i.i_class_id=15 THEN 1 ELSE NULL END) AS id15\n",
    "FROM store_sales_table ss\n",
    "INNER JOIN item_table i\n",
    "  ON (ss.ss_item_sk = i.i_item_sk AND i.i_category = \"Books\"\n",
    "  AND ss.ss_customer_sk IS NOT NULL\n",
    ")\n",
    "GROUP BY ss.ss_customer_sk\n",
    "HAVING count(ss.ss_item_sk) > 5\n",
    "    \"\"\")\n",
    "    val assembler = new VectorAssembler().setInputCols(\n",
    "      Array(\"id1\", \"id2\", \"id3\",\"id4\",\"id5\",\"id6\",\"id7\",\n",
    "      \"id8\",\"id9\",\"id10\",\"id11\",\"id12\",\"id13\",\"id14\",\"id15\")).setOutputCol(\"features\")\n",
    "    val ds = assembler.transform(fin)\n",
    "    ds.cache.first\n",
    "    val t1 = System.currentTimeMillis\n",
    "\n",
    "    // Measure time\n",
    "    println(\"Query 26 time(s) took: \" + (t1 - t0).toFloat / 1000)\n",
    "  }\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
